def fetch_orders(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}) -> List[Order]:
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' fetchOrders() requires a symbol argument')
        self.load_markets()
        market = self.market(symbol)
        states = self.safe_value(params, 'states', ['wait', 'done', 'cancel'])
        query = self.omit(params, 'states')
        request = {
            'market': market['id'],
            'states': states,
            'need_price': 'True',
        }
        if limit is not None:
            request['limit'] = limit
        response = self.privateGetOrdersFilter(self.extend(request, query))
        data = self.safe_value(response, 'data', [])
        result = []
        for i in range(0, len(data)):
            orders = self.safe_value(data[i], 'orders', [])
            status = self.parse_order_status(self.safe_value(data[i], 'state'))
            parsedOrders = self.parse_orders(orders, market, since, limit, {'status': status})
            result = self.array_concat(result, parsedOrders)
        return result
def test_accelerator_set_when_using_tpu(devices):
    
    assert isinstance(Trainer(accelerator="tpu", devices=devices).accelerator, TPUAccelerator)
def __init__(self, hass: HomeAssistant, legacy_data_file: str) -> None:
        
        self._legacy_data_file = legacy_data_file
        self._data: dict[str, struct_time] = {}
        self._hass = hass
        self._store: Store[dict[str, str]] = Store(hass, STORAGE_VERSION, DOMAIN)
def fetch_funding_rate_history(self, symbol=None, since=None, limit=None, params={}):
        
        
        
        self.load_markets()
        request = {}
        market = None
        if symbol in self.currencies:
            code = self.currency(symbol)
            request['symbol'] = code['id']
        elif symbol is not None:
            splitSymbol = symbol.split(':')
            splitSymbolLength = len(splitSymbol)
            timeframes = ['nearest', 'daily', 'weekly', 'monthly', 'quarterly', 'biquarterly', 'perpetual']
            if (splitSymbolLength > 1) and self.in_array(splitSymbol[1], timeframes):
                code = self.currency(splitSymbol[0])
                symbol = code['id'] + ':' + splitSymbol[1]
                request['symbol'] = symbol
            else:
                market = self.market(symbol)
                request['symbol'] = market['id']
        if since is not None:
            request['startTime'] = self.iso8601(since)
        if limit is not None:
            request['count'] = limit
        until = self.safe_integer_2(params, 'until', 'till')
        params = self.omit(params, ['until', 'till'])
        if until is not None:
            request['endTime'] = self.iso8601(until)
        response = self.publicGetFunding(self.extend(request, params))
        #
        #    [
        #        {
        #            "timestamp": "2016-05-07T12:00:00.000Z",
        #            "symbol": "ETHXBT",
        #            "fundingInterval": "2000-01-02T00:00:00.000Z",
        #            "fundingRate": 0.0010890000000000001,
        #            "fundingRateDaily": 0.0010890000000000001
        #        }
        #    ]
        #
        return self.parse_funding_rate_histories(response, market, since, limit)
def fetch_my_trades(self, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):
        
        
        
        self.load_markets()
        paginate = False
        paginate, params = self.handle_option_and_params(params, 'fetchMyTrades', 'paginate')
        if paginate:
            return self.fetch_paginated_call_dynamic('fetchMyTrades', symbol, since, limit, params, 100)
        market = None
        request = {}
        if symbol is not None:
            market = self.market(symbol)
            request['symbol'] = market['id']
        if since is not None:
            request['startTime'] = self.iso8601(since)
        if limit is not None:
            request['count'] = limit
        until = self.safe_integer_2(params, 'until', 'endTime')
        if until is not None:
            params = self.omit(params, ['until'])
            request['endTime'] = self.iso8601(until)
        request = self.deep_extend(request, params)
        # why the hassle? urlencode in python is kinda broken for nested dicts.
        # E.g. self.urlencode({"filter": {"open": True}}) will return "filter={'open':+True}"
        # Bitmex doesn't like that. Hence resorting to self hack.
        if 'filter' in request:
            request['filter'] = self.json(request['filter'])
        response = self.privateGetExecutionTradeHistory(request)
        #
        #     [
        #         {
        #             "execID": "string",
        #             "orderID": "string",
        #             "clOrdID": "string",
        #             "clOrdLinkID": "string",
        #             "account": 0,
        #             "symbol": "string",
        #             "side": "string",
        #             "lastQty": 0,
        #             "lastPx": 0,
        #             "underlyingLastPx": 0,
        #             "lastMkt": "string",
        #             "lastLiquidityInd": "string",
        #             "simpleOrderQty": 0,
        #             "orderQty": 0,
        #             "price": 0,
        #             "displayQty": 0,
        #             "stopPx": 0,
        #             "pegOffsetValue": 0,
        #             "pegPriceType": "string",
        #             "currency": "string",
        #             "settlCurrency": "string",
        #             "execType": "string",
        #             "ordType": "string",
        #             "timeInForce": "string",
        #             "execInst": "string",
        #             "contingencyType": "string",
        #             "exDestination": "string",
        #             "ordStatus": "string",
        #             "triggered": "string",
        #             "workingIndicator": True,
        #             "ordRejReason": "string",
        #             "simpleLeavesQty": 0,
        #             "leavesQty": 0,
        #             "simpleCumQty": 0,
        #             "cumQty": 0,
        #             "avgPx": 0,
        #             "commission": 0,
        #             "tradePublishIndicator": "string",
        #             "multiLegReportingType": "string",
        #             "text": "string",
        #             "trdMatchID": "string",
        #             "execCost": 0,
        #             "execComm": 0,
        #             "homeNotional": 0,
        #             "foreignNotional": 0,
        #             "transactTime": "2019-03-05T12:47:02.762Z",
        #             "timestamp": "2019-03-05T12:47:02.762Z"
        #         }
        #     ]
        #
        return self.parse_trades(response, market, since, limit)
def create_order(self, symbol: str, type: OrderType, side: OrderSide, amount, price=None, params={}):
        
        
        
        self.load_markets()
        market = self.market(symbol)
        orderType = self.capitalize(type)
        reduceOnly = self.safe_value(params, 'reduceOnly')
        if reduceOnly is not None:
            if (not market['swap']) and (not market['future']):
                raise InvalidOrder(self.id + ' createOrder() does not support reduceOnly for ' + market['type'] + ' orders, reduceOnly orders are supported for swap and future markets only')
        brokerId = self.safe_string(self.options, 'brokerId', 'CCXT')
        qty = self.parse_to_int(self.amount_to_precision(symbol, amount))
        request = {
            'symbol': market['id'],
            'side': self.capitalize(side),
            'orderQty': qty,  # lot size multiplied by the number of contracts
            'ordType': orderType,
            'text': brokerId,
        }
        # support for unified trigger format
        triggerPrice = self.safe_number_n(params, ['triggerPrice', 'stopPx', 'stopPrice'])
        trailingAmount = self.safe_string_2(params, 'trailingAmount', 'pegOffsetValue')
        isTriggerOrder = triggerPrice is not None
        isTrailingAmountOrder = trailingAmount is not None
        if isTriggerOrder or isTrailingAmountOrder:
            triggerDirection = self.safe_string(params, 'triggerDirection')
            triggerAbove = (triggerDirection == 'above')
            if (type == 'limit') or (type == 'market'):
                self.check_required_argument('createOrder', triggerDirection, 'triggerDirection', ['above', 'below'])
            if type == 'limit':
                if side == 'buy':
                    orderType = 'StopLimit' if triggerAbove else 'LimitIfTouched'
                else:
                    orderType = 'LimitIfTouched' if triggerAbove else 'StopLimit'
            elif type == 'market':
                if side == 'buy':
                    orderType = 'Stop' if triggerAbove else 'MarketIfTouched'
                else:
                    orderType = 'MarketIfTouched' if triggerAbove else 'Stop'
            if isTrailingAmountOrder:
                isStopSellOrder = (side == 'sell') and ((orderType == 'Stop') or (orderType == 'StopLimit'))
                isBuyIfTouchedOrder = (side == 'buy') and ((orderType == 'MarketIfTouched') or (orderType == 'LimitIfTouched'))
                if isStopSellOrder or isBuyIfTouchedOrder:
                    trailingAmount = '-' + trailingAmount
                request['pegOffsetValue'] = self.parse_to_numeric(trailingAmount)
                request['pegPriceType'] = 'TrailingStopPeg'
            else:
                if triggerPrice is None:
                    # if exchange specific trigger types were provided
                    raise ArgumentsRequired(self.id + ' createOrder() requires a triggerPrice(stopPx|stopPrice) parameter for the ' + orderType + ' order type')
                request['stopPx'] = self.parse_to_numeric(self.price_to_precision(symbol, triggerPrice))
            request['ordType'] = orderType
            params = self.omit(params, ['triggerPrice', 'stopPrice', 'stopPx', 'triggerDirection', 'trailingAmount'])
        if (orderType == 'Limit') or (orderType == 'StopLimit') or (orderType == 'LimitIfTouched'):
            request['price'] = self.parse_to_numeric(self.price_to_precision(symbol, price))
        clientOrderId = self.safe_string_2(params, 'clOrdID', 'clientOrderId')
        if clientOrderId is not None:
            request['clOrdID'] = clientOrderId
            params = self.omit(params, ['clOrdID', 'clientOrderId'])
        response = self.privatePostOrder(self.extend(request, params))
        return self.parse_order(response, market)
def _connect() -> ServeControllerClient:
    
    

    # Initialize Ray if needed.
    ray.worker.global_worker.filter_logs_by_job = False
    if not ray.is_initialized():
        ray.init(namespace=SERVE_NAMESPACE)

    # When running inside of a replica, _INTERNAL_REPLICA_CONTEXT is set to
    # ensure that the correct instance is connected to.
    if _INTERNAL_REPLICA_CONTEXT is None:
        controller_name = SERVE_CONTROLLER_NAME
    else:
        controller_name = _INTERNAL_REPLICA_CONTEXT._internal_controller_name

    # Try to get serve controller if it exists
    try:
        controller = ray.get_actor(controller_name, namespace=SERVE_NAMESPACE)
    except ValueError:
        raise RayServeException(
            "There is no "
            "instance running on this Ray cluster. Please "
            "call `serve.start(detached=True) to start "
            "one."
        )

    client = ServeControllerClient(
        controller,
        controller_name,
        detached=True,
    )
    set_global_client(client)
    return client
def is_on(self) -> bool:
        
        return cast(bool, self.api.data[self.entity_description.key] == 1)
def cache_frame(self, filename, image) -> None:
         
        
        frame_no = int(re.search(self.re_search, filename).group())
        self.cache[frame_no] = image
        logger.trace("Added to cache. Frame no: %s", frame_no)
        logger.trace("Current cache: %s", sorted(self.cache.keys()))
def output_filename(self, filename: str, separate_mask: bool = False) -> List[str]:
         
        
        filename = os.path.splitext(os.path.basename(filename))[0]
        out_filename = f"{filename}.{self.config['format']}"
        retval = [os.path.join(self.output_folder, out_filename)]
        if separate_mask:
            retval.append(os.path.join(self.output_folder, "masks", out_filename))

        if separate_mask and not self._subfolders_created:
            locations = [os.path.dirname(loc) for loc in retval]
            logger.debug("Creating sub-folders: %s", locations)
            for location in locations:
                os.makedirs(location, exist_ok=True)

        logger.trace("in filename: '%s', out filename: '%s'", filename, retval)  # type:ignore
        return retval
def list_models(filter='', module='', pretrained=False, exclude_filters=''):
     
    
    if module:
        models = list(_module_to_models[module])
    else:
        models = _model_entrypoints.keys()
    if filter:
        models = fnmatch.filter(models, filter)  # include these models
    if exclude_filters:
        if not isinstance(exclude_filters, list):
            exclude_filters = [exclude_filters]
        for xf in exclude_filters:
            exclude_models = fnmatch.filter(models, xf)  # exclude these models
            if len(exclude_models):
                models = set(models).difference(exclude_models)
    if pretrained:
        models = _model_has_pretrained.intersection(models)
    return list(sorted(models, key=_natural_key))
def get_pretrained_cfg_value(model_name, cfg_key):
     
    
    if model_name in _model_pretrained_cfgs:
        return getattr(_model_pretrained_cfgs[model_name], cfg_key, None)
    raise RuntimeError(f'No pretrained config exist for model {model_name}.')
def translate(text, lang):
    
    
    

    if "," in text:
        terms = text.split(",")
        translated_terms = [translate(term.strip(), lang) for term in terms]
        return ", ".join(translated_terms)

    translated = TRANSLATIONS.get(lang, {}).get(text.lower(), text)
    return translated
def get(self, field_):
        
        
        
        field = field_.encode('utf-8')

        if field_ in self.gettable_vectors:
            # get dims
            dims = np.ascontiguousarray(np.zeros((2,)), dtype=np.intc)
            dims_data = cast(dims.ctypes.data, POINTER(c_int))

            self.shared_lib.sim_dims_get_from_attr.argtypes = [c_void_p, c_void_p, c_char_p, POINTER(c_int)]
            self.shared_lib.sim_dims_get_from_attr(self.sim_config, self.sim_dims, field, dims_data)

            # allocate array
            out = np.ascontiguousarray(np.zeros((dims[0],)), dtype=np.float64)
            out_data = cast(out.ctypes.data, POINTER(c_double))

            self.shared_lib.sim_out_get.argtypes = [c_void_p, c_void_p, c_void_p, c_char_p, c_void_p]
            self.shared_lib.sim_out_get(self.sim_config, self.sim_dims, self.sim_out, field, out_data)

        elif field_ in self.gettable_matrices:
            # get dims
            dims = np.ascontiguousarray(np.zeros((2,)), dtype=np.intc)
            dims_data = cast(dims.ctypes.data, POINTER(c_int))

            self.shared_lib.sim_dims_get_from_attr.argtypes = [c_void_p, c_void_p, c_char_p, POINTER(c_int)]
            self.shared_lib.sim_dims_get_from_attr(self.sim_config, self.sim_dims, field, dims_data)

            out = np.zeros((dims[0], dims[1]), order='F')
            out_data = cast(out.ctypes.data, POINTER(c_double))

            self.shared_lib.sim_out_get.argtypes = [c_void_p, c_void_p, c_void_p, c_char_p, c_void_p]
            self.shared_lib.sim_out_get(self.sim_config, self.sim_dims, self.sim_out, field, out_data)

        elif field_ in self.gettable_scalars:
            scalar = c_double()
            scalar_data = byref(scalar)
            self.shared_lib.sim_out_get.argtypes = [c_void_p, c_void_p, c_void_p, c_char_p, c_void_p]
            self.shared_lib.sim_out_get(self.sim_config, self.sim_dims, self.sim_out, field, scalar_data)

            out = scalar.value
        else:
            raise Exception(f'AcadosSimSolver.get(): Unknown field {field_},' \
                f' available fields are {", ".join(self.gettable_vectors+self.gettable_matrices)}, {", ".join(self.gettable_scalars)}')

        return out
def get_cache(self, cache_id: int, cache_group: int = 0) -> torch.Tensor:
        
        
        
        return self._kv_cache.get_cache(cache_id, cache_group=cache_group)
def get_stacks(
        template_file: Optional[str] = None,
        stack_path: str = "",
        name: str = "",
        parameter_overrides: Optional[Dict] = None,
        global_parameter_overrides: Optional[Dict] = None,
        metadata: Optional[Dict] = None,
        template_dictionary: Optional[Dict] = None,
    ) -> Tuple[List[Stack], List[str]]:
        
        
        
        template_dict: dict
        if template_file:
            template_dict = get_template_data(template_file)
        elif template_dictionary:
            template_file = ""
            template_dict = template_dictionary
        else:
            raise TemplateNotFoundException(
                message="A template file or a template dict is required but both are missing."
            )

        stacks = [
            Stack(
                stack_path,
                name,
                template_file,
                SamLocalStackProvider.merge_parameter_overrides(parameter_overrides, global_parameter_overrides),
                template_dict,
                metadata,
            )
        ]
        remote_stack_full_paths: List[str] = []

        current = SamLocalStackProvider(
            template_file, stack_path, template_dict, parameter_overrides, global_parameter_overrides
        )
        remote_stack_full_paths.extend(current.remote_stack_full_paths)

        for child_stack in current.get_all():
            stacks_in_child, remote_stack_full_paths_in_child = SamLocalStackProvider.get_stacks(
                child_stack.location,
                os.path.join(stack_path, stacks[0].stack_id),
                child_stack.name,
                child_stack.parameters,
                global_parameter_overrides,
                child_stack.metadata,
            )
            stacks.extend(stacks_in_child)
            remote_stack_full_paths.extend(remote_stack_full_paths_in_child)

        return stacks, remote_stack_full_paths
def __init__(
        self,
        template_file: str,
        stack_path: str,
        template_dict: Dict,
        parameter_overrides: Optional[Dict] = None,
        global_parameter_overrides: Optional[Dict] = None,
        use_sam_transform: bool = True,
    ):
        
        
        

        self._template_file = template_file
        self._stack_path = stack_path
        self._template_dict = self.get_template(
            template_dict,
            SamLocalStackProvider.merge_parameter_overrides(parameter_overrides, global_parameter_overrides),
            use_sam_transform=use_sam_transform,
        )
        self._resources = self._template_dict.get("Resources", {})
        self._global_parameter_overrides = global_parameter_overrides

        # Store a map of stack name to stack information for quick reference -> self._stacks
        # and detect remote stacks -> self._remote_stack_full_paths
        self._stacks: Dict[str, Stack] = {}
        self.remote_stack_full_paths: List[str] = []
        self._extract_stacks()

        LOG.debug("%d stacks found in the template", len(self._stacks))
def _extract_stacks(self) -> None:
        
        
        

        for name, resource in self._resources.items():

            resource_type = resource.get("Type")
            resource_properties = resource.get("Properties", {})
            resource_metadata = resource.get("Metadata", None)
            # Add extra metadata information to properties under a separate field.
            if resource_metadata:
                resource_properties["Metadata"] = resource_metadata

            stack: Optional[Stack] = None
            try:
                if resource_type == SamLocalStackProvider.SERVERLESS_APPLICATION:
                    stack = SamLocalStackProvider._convert_sam_application_resource(
                        self._template_directory, self._stack_path, name, resource_properties
                    )
                if resource_type == SamLocalStackProvider.CLOUDFORMATION_STACK:
                    stack = SamLocalStackProvider._convert_cfn_stack_resource(
                        self._template_directory, self._stack_path, name, resource_properties
                    )
            except RemoteStackLocationNotSupported:
                self.remote_stack_full_paths.append(get_full_path(self._stack_path, name))

            if stack:
                self._stacks[name] = stack
def normalize_resource_path(stack_file_path: str, path: str) -> str:
        
        

        
        if os.path.isabs(path):
            return path

        if os.path.islink(stack_file_path):
            # os.path.realpath() always returns an absolute path while
            # the return value of this method will show up in build artifacts,
            # in case customers move the build artifacts among different machines (e.g., git or file sharing)
            # absolute paths are not robust as relative paths. So here prefer to use relative path.
            stack_file_path = os.path.relpath(os.path.realpath(stack_file_path))

        return os.path.normpath(os.path.join(os.path.dirname(stack_file_path), path))
def get_lc_namespace(cls) -> List[str]:
        
        return ["langchain", "schema", "runnable"]
def print_reverse(head_node: Node) -> None:
    
    
    if head_node is not None and isinstance(head_node, Node):
        print_reverse(head_node.next)
        print(head_node.data)
def __rpc_analysed_dataframe_raw(
        self,
        pair: str,
        timeframe: str,
        limit: Optional[int]
    ) -> Tuple[DataFrame, datetime]:
        
        
        
        _data, last_analyzed = self._freqtrade.dataprovider.get_analyzed_dataframe(
            pair, timeframe)
        _data = _data.copy()

        if limit:
            _data = _data.iloc[-limit:]

        return _data, last_analyzed
def delete_front(self) -> None:
        
        
        
        if not self.head:
            raise IndexError("Deleting from an empty list")

        current_node = self.head

        if current_node.next_ptr == current_node:
            self.head = None
        else:
            while current_node.next_ptr != self.head:
                current_node = current_node.next_ptr

            current_node.next_ptr = self.head.next_ptr
            self.head = self.head.next_ptr

        self.length -= 1
        if not self.head:
            assert self.length == 0
def async_handle_event(self, sia_event: SIAEvent) -> None:
        
        
        _LOGGER.debug("Received event: %s", sia_event)
        if int(sia_event.ri) not in (self.zone, SIA_HUB_ZONE):
            return

        relevant_event = self.update_state(sia_event)

        if relevant_event:
            self._attr_extra_state_attributes.update(get_attr_from_sia_event(sia_event))

        if relevant_event or sia_event.code == AVAILABILITY_EVENT_CODE:
            self._attr_available = True
            self._cancel_post_interval_update_cb()
            self.async_create_post_interval_update_cb()

        self.async_write_ha_state()
def __init__(self, env: gym.Env, epsilon: float = 1e-8):
        
        
        super().__init__(env)
        self.num_envs = getattr(env, "num_envs", 1)
        self.is_vector_env = getattr(env, "is_vector_env", False)
        if self.is_vector_env:
            self.obs_rms = RunningMeanStd(shape=self.single_observation_space.shape)
        else:
            self.obs_rms = RunningMeanStd(shape=self.observation_space.shape)
        self.epsilon = epsilon
def state(self) -> Union[None, str, int, float]:
        
        return self._state_data
def cancel(self):
        
        
        if not self._assign_request_task.done():
            self._assign_request_task.cancel()
        elif self._assign_request_task.exception() is None:
            ray.cancel(self._assign_request_task.result())
def __call__(self, x, layers=['prob'], **kwargs):
        

        

        argument.check_unexpected_kwargs(
            kwargs, train='train argument is not supported anymore. '
            'Use chainer.using_config')
        argument.assert_kwargs_empty(kwargs)

        h = x
        activations = {}
        inception_4a_cache = None
        inception_4d_cache = None
        target_layers = set(layers)
        for key, funcs in self.functions.items():
            if len(target_layers) == 0:
                break

            if key == 'loss1_fc2':
                h = inception_4a_cache
            elif key == 'loss2_fc2':
                h = inception_4d_cache

            for func in funcs:
                h = func(h)
            if key in target_layers:
                activations[key] = h
                target_layers.remove(key)

            if key == 'inception_4a':
                inception_4a_cache = h
            elif key == 'inception_4d':
                inception_4d_cache = h

        return activations
def extract(self, images, layers=['pool5'], size=(224, 224),
                volatile=flag.OFF):
        

        

        x = concat_examples([prepare(img, size=size) for img in images])
        x = Variable(self.xp.asarray(x), volatile=volatile)
        return self(x, layers=layers)
def extract(self, images, layers=['pool5'], size=(224, 224)):
        

        

        x = concat_examples([prepare(img, size=size) for img in images])
        x = Variable(self.xp.asarray(x))
        return self(x, layers=layers)
def cosine_decay_with_warmup(global_step,
                             learning_rate_base,
                             total_steps,
                             warmup_learning_rate=0.0,
                             warmup_steps=0,
                             hold_base_rate_steps=0):
  
  
  if learning_rate_base < warmup_learning_rate:
    raise ValueError('learning_rate_base must be larger '
                     'or equal to warmup_learning_rate.')
  if total_steps < warmup_steps:
    raise ValueError('total_steps must be larger or equal to '
                     'warmup_steps.')
  learning_rate = 0.5 * learning_rate_base * (1 + tf.cos(
      np.pi *
      (tf.cast(global_step, tf.float32) - warmup_steps - hold_base_rate_steps
      ) / float(total_steps - warmup_steps - hold_base_rate_steps)))
  if hold_base_rate_steps > 0:
    learning_rate = tf.where(global_step > warmup_steps + hold_base_rate_steps,
                             learning_rate, learning_rate_base)
  if warmup_steps > 0:
    slope = (learning_rate_base - warmup_learning_rate) / warmup_steps
    warmup_rate = slope * tf.cast(global_step,
                                  tf.float32) + warmup_learning_rate
    learning_rate = tf.where(global_step < warmup_steps, warmup_rate,
                             learning_rate)
  return tf.where(global_step > total_steps, 0.0, learning_rate,
                  name='learning_rate')
def test_model_validation_with_condition(self):
        
        
        
        obj1 = UniqueConstraintConditionProduct.objects.create(name="p1", color="red")
        obj2 = UniqueConstraintConditionProduct.objects.create(name="p2")
        UniqueConstraintConditionProduct(
            name=obj1.name, color="blue"
        ).validate_constraints()
        msg = "Constraint “name_without_color_uniq” is violated."
        with self.assertRaisesMessage(ValidationError, msg):
            UniqueConstraintConditionProduct(name=obj2.name).validate_constraints()
def summaries_with_matching_keyword(keyword, summary_dir):
  
  matches = []
  event_paths = tf.io.gfile.glob(os.path.join(summary_dir, "events*"))
  for event in tf.compat.v1.train.summary_iterator(event_paths[-1]):
    if event.summary is not None:
      for value in event.summary.value:
        if keyword in value.tag:
          matches.append(event.summary)
  return matches
def from_env(cls, values: dict) -> dict:
        
        settings = {}
        settings.update(dotenv_values(ENV_FILE_SETTINGS))
        settings.update(values)
        filtered = {k.replace("OPENBB_", ""): v for k, v in settings.items()}
        return filtered
def test_num_peaks():
    

    img_corners = corner_harris(rgb2gray(data.astronaut()))

    for i in range(20):
        n = np.random.random_integers(20)
        results = peak_local_max(img_corners, num_peaks=n)
        assert (results.shape[0] == n)
def filter_options(
        args,  # type: EnvironmentConfig
        argv,  # type: t.List[str]
        options,  # type: t.Dict[str, int]
        exclude,  # type: t.List[str]
        require,  # type: t.List[str]
):  # type: (...) -> t.Iterable[str]
    
    options = options.copy()

    options['--truncate'] = 1
    options['--redact'] = 0
    options['--no-redact'] = 0

    if isinstance(args, TestConfig):
        options.update({
            '--changed': 0,
            '--tracked': 0,
            '--untracked': 0,
            '--ignore-committed': 0,
            '--ignore-staged': 0,
            '--ignore-unstaged': 0,
            '--changed-from': 1,
            '--changed-path': 1,
            '--metadata': 1,
            '--exclude': 1,
            '--require': 1,
        })
    elif isinstance(args, SanityConfig):
        options.update({
            '--base-branch': 1,
        })

    if isinstance(args, IntegrationConfig):
        options.update({
            '--no-temp-unicode': 0,
        })

    for arg in filter_args(argv, options):
        yield arg

    for arg in args.delegate_args:
        yield arg

    for target in exclude:
        yield '--exclude'
        yield target

    for target in require:
        yield '--require'
        yield target

    if isinstance(args, TestConfig):
        if args.metadata_path:
            yield '--metadata'
            yield args.metadata_path

    yield '--truncate'
    yield '%d' % args.truncate

    if not args.redact:
        yield '--no-redact'

    if isinstance(args, IntegrationConfig):
        if args.no_temp_unicode:
            yield '--no-temp-unicode'
def _reset_account_id(data):
        
        return aws_stack.fix_account_id_in_arns(
            data,
            colon_delimiter="%3A",
            existing=TEST_AWS_ACCOUNT_ID,
            replace=MOTO_ACCOUNT_ID,
        )
def create_segmentation_test_example(
    image_height: int,
    image_width: int,
    image_channel: int,
    output_serialized_example: bool = False,
    dense_features: Optional[Mapping[str, int]] = None) -> tf.train.Example:
  
  
  image = fake_feature_generator.generate_image_np(image_height, image_width,
                                                   image_channel)
  mask = fake_feature_generator.generate_semantic_mask_np(
      image_height, image_width, 3)
  builder = tf_example_builder.TfExampleBuilder()
  builder.add_image_matrix_feature(
      image,
      image_source_id=DUMP_SOURCE_ID).add_semantic_mask_matrix_feature(mask)

  if dense_features:
    for prefix, channel in dense_features.items():
      dense_feature = fake_feature_generator.generate_semantic_mask_np(
          image_height, image_width, channel)
      builder.add_semantic_mask_matrix_feature(
          dense_feature, feature_prefix=prefix)

  example = builder.example

  if output_serialized_example:
    return example.SerializeToString()
  return example
def l1_loss(pred: Tensor, target: Tensor) -> Tensor:
    
    
    if target.numel() == 0:
        return pred.sum() * 0

    assert pred.size() == target.size()
    loss = torch.abs(pred - target)
    return loss
def test_registry_conflict(ray_start_4_cpus, tmpdir, exit_same):
    
    
    # Create file markers
    run_1_running = tmpdir / "run_1_running"
    run_1_finished = tmpdir / "run_1_finished"
    run_2_finished = tmpdir / "run_2_finished"

    run_1_running.write_text("", encoding="utf-8")
    run_1_finished.write_text("", encoding="utf-8")
    run_2_finished.write_text("", encoding="utf-8")

    ray_address = ray_start_4_cpus.address_info["address"]

    run_1_env = {
        "RAY_ADDRESS": ray_address,
        "FIXED_VAL": str(1),
        "VAL_1": str(2),
        "VAL_2": str(3),
        # Run 1 can start immediately
        "HANG_RUN_MARKER": "",
        # Allow second run to start once first trial of first run is started
        "DELETE_TRIAL_MARKER": str(run_1_running),
        # Hang in first trial until the second run finished
        "HANG_TRIAL_MARKER": str(run_2_finished),
        # Mark run 1 as completed
        "DELETE_RUN_MARKER": str(run_1_finished),
        # Do not wait at end
        "HANG_END_MARKER": "",
    }

    run_2_env = {
        "RAY_ADDRESS": ray_address,
        "FIXED_VAL": str(4),
        "VAL_1": str(5),
        "VAL_2": str(6),
        # Wait until first trial of first run is running
        "HANG_RUN_MARKER": str(run_1_running),
        # Don't delete during run
        "DELETE_TRIAL_MARKER": "",
        # No need to hang in trial
        "HANG_TRIAL_MARKER": "",
        # After full run finished, allow first run to continue
        "DELETE_RUN_MARKER": str(run_2_finished),
        # Wait until first run finished
        # If we don't do this, we actually don't die because of parameter conflict
        # but because of "The object's owner has exited" - so we test this
        # separately
        "HANG_END_MARKER": str(run_1_finished) if exit_same else "",
    }

    script_path = Path(__file__).parent / "_test_multi_tenancy_run.py"

    run_1 = subprocess.Popen(
        [sys.executable, script_path], env=run_1_env, stderr=subprocess.PIPE
    )
    print("Started run 1:", run_1.pid)

    run_2 = subprocess.Popen([sys.executable, script_path], env=run_2_env)
    print("Started run 2:", run_2.pid)

    assert run_2.wait() == 0
    assert run_1.wait() == 0
def compute_outputs(self, data):
        
        
        
        if self._exec_order is None:
            self._prepare_run(data)
        intermediate = {}
        for compute_key, edges, conf in self._exec_order:
            if compute_key in data:
                continue
            # It is a dynamic_item, so conf is a DynamicItemConf, which we can unpack:
            func, argkeys = conf
            args = [
                data[argkey] if argkey in data else intermediate[argkey]
                for argkey in argkeys
            ]
            intermediate[compute_key] = func(*args)
        return {
            outkey: data[inkey] if inkey in data else intermediate[inkey]
            for outkey, inkey in self.output_mapping.items()
        }
def set_output_keys(self, keys):
        
        
        self.output_keys = keys
        self._exec_order = None
def add_dynamic_item(self, func, takes=None, provides=None):
        
        
        if isinstance(func, DynamicItem):
            if takes is not None or provides is not None:
                raise ValueError(
                    "If providing a DynamicItem directly, don't "
                    "specify takes or provides"
                )
            else:
                self._add_dynamic_item_object(func)
        if isinstance(takes, str):
            takes = [takes]
        if isinstance(provides, str):
            provides = [provides]
        di = takes_decorator(*takes)(provides_decorator(*provides)(func))
        self._add_dynamic_item_object(di)
def create_model(self,
                   model_input,
                   vocab_size,
                   num_mixtures: int = 2,
                   use_input_context_gate: bool = False,
                   use_output_context_gate: bool = False,
                   normalizer_fn=None,
                   normalizer_params: Optional[Dict[str, Any]] = None,
                   vocab_as_last_dim: bool = False,
                   l2_penalty: float = 1e-5,
                   **kwargs):
    
    
    del kwargs  # Unused.
    if use_input_context_gate:
      model_input = utils.context_gate(
          model_input,
          normalizer_fn=normalizer_fn,
          normalizer_params=normalizer_params,
      )

    gate_activations = layers.Dense(
        vocab_size * (num_mixtures + 1),
        activation=None,
        bias_initializer=None,
        kernel_regularizer=tf.keras.regularizers.l2(l2_penalty))(
            model_input)
    expert_activations = layers.Dense(
        vocab_size * num_mixtures,
        activation=None,
        kernel_regularizer=tf.keras.regularizers.l2(l2_penalty))(
            model_input)

    if vocab_as_last_dim:
      # Batch x (num_mixtures + 1) x #Labels
      gate_activations = tf.reshape(
          gate_activations, [-1, num_mixtures + 1, vocab_size])
      # Batch x num_mixtures x #Labels
      expert_activations = tf.reshape(
          expert_activations, [-1, num_mixtures, vocab_size])
    else:
      # (Batch * #Labels) x (num_mixtures + 1)
      gate_activations = tf.reshape(gate_activations, [-1, num_mixtures + 1])
      # (Batch * #Labels) x num_mixtures
      expert_activations = tf.reshape(expert_activations, [-1, num_mixtures])

    gating_distribution = tf.nn.softmax(gate_activations, axis=1)
    expert_distribution = tf.nn.sigmoid(expert_activations)
    final_probabilities = tf.reduce_sum(
        gating_distribution[:, :num_mixtures] * expert_distribution, axis=1)

    if not vocab_as_last_dim:
      final_probabilities = tf.reshape(final_probabilities, [-1, vocab_size])

    if use_output_context_gate:
      final_probabilities = utils.context_gate(
          final_probabilities,
          normalizer_fn=normalizer_fn,
          normalizer_params=normalizer_params,
      )
    return {"predictions": final_probabilities}
def cancel_all_orders(self, symbol=None, params={}):
        
        
        
        self.load_markets()
        market = None
        request = {}
        if symbol is not None:
            market = self.market(symbol)
            request['symbol'] = market['id']
        marketType = None
        marketType, params = self.handle_market_type_and_params('cancelAllOrders', market, params)
        method = self.get_supported_mapping(marketType, {
            'spot': 'privateDeleteSpotOrder',
            'swap': 'privateDeleteFuturesOrder',
            'margin': 'privateDeleteMarginOrder',
        })
        marginMode, query = self.handle_margin_mode_and_params('cancelAllOrders', params)
        if marginMode is not None:
            method = 'privateDeleteMarginOrder'
        response = getattr(self, method)(self.extend(request, query))
        return self.parse_orders(response, market)
def safe_open(
    path,
    *args,
    allow_symlinks=False,
    base_path=None,
    max_size_bytes=MAX_FILE_SIZE_BYTES,
    **kwargs
):
    
    
    
    if allow_symlinks and not base_path:
        raise ValueError("base_path must be given if symlinks are allowed.")

    path = Path(path).absolute()

    log.bind(
        path_resolved=str(path.absolute().resolve()),
    )

    if path.exists() and not path.is_file():
        raise FileIsNotRegularFile(path)

    if not allow_symlinks and path.is_symlink():
        log.info("Skipping file becuase it's a symlink.")
        raise UnsupportedSymlinkFileError(path)

    # Expand symlinks.
    resolved_path = path.resolve()

    if resolved_path.exists():
        file_size = resolved_path.stat().st_size
        if file_size > max_size_bytes:
            log.info("File is too large.", size_bytes=file_size)
            raise FileTooLarge(path)

    if allow_symlinks and base_path:
        base_path = Path(base_path).absolute()
        if not resolved_path.is_relative_to(base_path):
            # Trying to path traversal via a symlink, sneaky!
            log.info("Path traversal via symlink.")
            raise SymlinkOutsideBasePath(path)

    _assert_path_is_inside_docroot(resolved_path)

    return resolved_path.open(*args, **kwargs)
def cancel_order(self, id: str, symbol: Str = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' cancelOrder() requires a symbol argument')
        self.load_markets()
        market = self.market(symbol)
        origClientOrderId = self.safe_value(params, 'origClientOrderId')
        request = {
            'symbol': market['id'],
            # 'orderId': int(id),
            # 'origClientOrderId': id,
        }
        if origClientOrderId is None:
            request['orderId'] = id
        else:
            request['origClientOrderId'] = origClientOrderId
        response = self.privateDeleteV2Order(self.extend(request, params))
        #
        #     {
        #         "symbol": "DOGE/USD",
        #         "orderId": "00000000-0000-0003-0000-000006db764c",
        #         "price": "0.13",
        #         "origQty": "30.0",
        #         "executedQty": "0.0",  # positive for BUY, negative for SELL
        #         "status": "CANCELED",
        #         "timeInForce": "GTC",
        #         "type": "LIMIT",
        #         "side": "BUY",
        #     }
        #
        return self.parse_order(response, market)
def _depth_event(self, msg):
        

        

        if self._first_update_id is None:
            # Initial depth snapshot fetch not yet performed, buffer messages
            self._depth_message_buffer.append(msg)
        else:
            self._process_depth_message(msg)
def get_asks(self):
        

        
        return DepthCache.sort_depth(self._asks, reverse=False)
def __init__(
        self, client, symbol, loop=None, refresh_interval=None, bm=None, limit=500, conv_type=float, ws_interval=None
    ):
        

        
        super().__init__(client, symbol, loop, refresh_interval, bm, limit, conv_type)
        self._ws_interval = ws_interval
def __init__(self, client, loop, symbol, refresh_interval=None, bm=None, limit=10, conv_type=float):
        

        

        self._client = client
        self._depth_cache = None
        self._loop = loop or asyncio.get_event_loop()
        self._symbol = symbol
        self._limit = limit
        self._last_update_id = None
        self._bm = bm or BinanceSocketManager(self._client, self._loop)
        self._refresh_interval = refresh_interval or self.DEFAULT_REFRESH
        self._conn_key = None
        self._conv_type = conv_type
def main(wheel_dirname):
    
    if not op.exists(VCOMP140_SRC_PATH):
        raise ValueError(f"Could not find {VCOMP140_SRC_PATH}.")

    if not op.exists(MSVCP140_SRC_PATH):
        raise ValueError(f"Could not find {MSVCP140_SRC_PATH}.")

    if not op.isdir(wheel_dirname):
        raise RuntimeError(f"Could not find {wheel_dirname} file.")

    vcomp140_dll_filename = op.basename(VCOMP140_SRC_PATH)
    msvcp140_dll_filename = op.basename(MSVCP140_SRC_PATH)

    target_folder = op.join(wheel_dirname, TARGET_FOLDER)
    distributor_init = op.join(wheel_dirname, DISTRIBUTOR_INIT)

    # Create the "sklearn/.libs" subfolder
    if not op.exists(target_folder):
        os.mkdir(target_folder)

    print(f"Copying {VCOMP140_SRC_PATH} to {target_folder}.")
    shutil.copy2(VCOMP140_SRC_PATH, target_folder)

    print(f"Copying {MSVCP140_SRC_PATH} to {target_folder}.")
    shutil.copy2(MSVCP140_SRC_PATH, target_folder)

    # Generate the _distributor_init file in the source tree
    print("Generating the '_distributor_init.py' file.")
    make_distributor_init_64_bits(
        distributor_init,
        vcomp140_dll_filename,
        msvcp140_dll_filename,
    )
def register(router: Router[Handler]):
    
    
    
    router.add(standard_strategy_handler)
    router.add(path_strategy_handler)
    router.add(domain_strategy_handler)
    router.add(legacy_handler)
def name(self) -> str:
        
        
        return PurePosixPath(_as_posix(self).split("::")[0]).name
def extract(self, url_or_urls):
        
        
        urlpaths = map_nested(self._extract, url_or_urls, map_tuple=True)
        return urlpaths
def xwalk(urlpath, use_auth_token: Optional[Union[str, bool]] = None, **kwargs):
    
    
    main_hop, *rest_hops = _as_str(urlpath).split("::")
    if is_local_path(main_hop):
        yield from os.walk(main_hop, **kwargs)
    else:
        # walking inside a zip in a private repo requires authentication
        if not rest_hops and (main_hop.startswith("http://") or main_hop.startswith("https://")):
            raise NotImplementedError("os.walk is not extended to support URLs in streaming mode")
        elif rest_hops and (rest_hops[0].startswith("http://") or rest_hops[0].startswith("https://")):
            url = rest_hops[0]
            url, kwargs = _prepare_http_url_kwargs(url, use_auth_token=use_auth_token)
            storage_options = {"https": kwargs}
            urlpath = "::".join([main_hop, url, *rest_hops[1:]])
        else:
            storage_options = None
        fs, *_ = fsspec.get_fs_token_paths(urlpath, storage_options=storage_options)
        inner_path = main_hop.split("://")[1]
        if inner_path.strip("/") and not fs.isdir(inner_path):
            return []
        protocol = fs.protocol if isinstance(fs.protocol, str) else fs.protocol[-1]
        for dirpath, dirnames, filenames in fs.walk(inner_path, **kwargs):
            yield "::".join([f"{protocol}://{dirpath}"] + rest_hops), dirnames, filenames
def exists(self, download_config: Optional[DownloadConfig] = None):
        
        
        return xexists(str(self), download_config=download_config)
def _prepare_single_hop_path_and_storage_options(
    urlpath: str, download_config: Optional[DownloadConfig] = None
) -> Tuple[str, Dict[str, Dict[str, Any]]]:
    
    
    
    token = None if download_config is None else download_config.token
    protocol = urlpath.split("://")[0] if "://" in urlpath else "file"
    if download_config is not None and protocol in download_config.storage_options:
        storage_options = download_config.storage_options[protocol]
    elif download_config is not None and protocol not in download_config.storage_options:
        storage_options = {
            option_name: option_value
            for option_name, option_value in download_config.storage_options.items()
            if option_name not in fsspec.available_protocols()
        }
    else:
        storage_options = {}
    if storage_options:
        storage_options = {protocol: storage_options}
    if protocol in ["http", "https"]:
        storage_options[protocol] = {
            "headers": {
                **get_authentication_headers_for_url(urlpath, token=token),
                "user-agent": get_datasets_user_agent(),
            },
            "client_kwargs": {"trust_env": True},  # Enable reading proxy env variables.
            **(storage_options.get(protocol, {})),
        }
        if "drive.google.com" in urlpath:
            response = http_head(urlpath)
            cookies = None
            for k, v in response.cookies.items():
                if k.startswith("download_warning"):
                    urlpath += "&confirm=" + v
                    cookies = response.cookies
                    storage_options[protocol] = {"cookies": cookies, **storage_options.get(protocol, {})}
        # Fix Google Drive URL to avoid Virus scan warning
        if "drive.google.com" in urlpath and "confirm=" not in urlpath:
            urlpath += "&confirm=t"
        if urlpath.startswith("https://raw.githubusercontent.com/"):
            # Workaround for served data with gzip content-encoding: https://github.com/fsspec/filesystem_spec/issues/389
            storage_options[protocol] = {"block_size": 0, **storage_options.get(protocol, {})}
    elif protocol == "hf":
        storage_options[protocol] = {
            "token": token,
            "endpoint": config.HF_ENDPOINT,
            **storage_options.get(protocol, {}),
        }
    return urlpath, storage_options
def _get_include_attributes(cls, args, extra_attributes=None):
        
        
        
        extra_attributes = extra_attributes or []

        include_attributes = []

        if extra_attributes:
            include_attributes.extend(extra_attributes)

        # If user specifies which attributes to retrieve via CLI --attr / -a argument, take that
        # into account

        # Special case for "all"
        if 'all' in args.attr:
            return None

        for attr in args.attr:
            include_attributes.append(attr)

        if include_attributes:
            return include_attributes

        display_attributes = getattr(cls, 'display_attributes', [])

        if display_attributes:
            include_attributes += display_attributes

        include_attributes = list(OrderedSet(include_attributes))

        return include_attributes
def set_runtime_env(self,
                        runtime_env: Optional[Dict[str, Any]],
                        validate: bool = False) -> None:
        
        
        self.runtime_env = runtime_env if runtime_env is not None else {}
        if validate:
            self.runtime_env = self._validate_runtime_env()[0]
        self._cached_pb = None
def get_proto_job_config(self):
        
        if self._cached_pb is None:
            pb = gcs_utils.JobConfig()
            if self.ray_namespace is None:
                pb.ray_namespace = str(uuid.uuid4())
            else:
                pb.ray_namespace = self.ray_namespace
            pb.num_java_workers_per_process = self.num_java_workers_per_process
            pb.jvm_options.extend(self.jvm_options)
            pb.code_search_path.extend(self.code_search_path)
            for k, v in self.metadata.items():
                pb.metadata[k] = v

            parsed_env, eager_install = self._validate_runtime_env()
            pb.runtime_env.uris[:] = parsed_env.get_uris()
            pb.runtime_env.serialized_runtime_env = parsed_env.serialize()
            pb.runtime_env.runtime_env_eager_install = eager_install

            self._cached_pb = pb

        return self._cached_pb
def test_slugify(self):
        
        self.assertEqual(
            slugify('This is a test'),
            'this-is-a-test',
        )
        self.assertEqual(
            slugify('project_with_underscores-v.1.0'),
            'project-with-underscores-v10',
        )
        self.assertEqual(
            slugify('project_with_underscores-v.1.0', dns_safe=False),
            'project_with_underscores-v10',
        )
        self.assertEqual(
            slugify('A title_-_with separated parts'),
            'a-title-with-separated-parts',
        )
        self.assertEqual(
            slugify('A title_-_with separated parts', dns_safe=False),
            'a-title_-_with-separated-parts',
        )
def kruskal(self) -> GraphUndirectedWeighted[T]:
        # Kruskal's Algorithm to generate a Minimum Spanning Tree (MST) of a graph
        
        
        

        # getting the edges in ascending order of weights
        edges = []
        seen = set()
        for start in self.connections:
            for end in self.connections[start]:
                if (start, end) not in seen:
                    seen.add((end, start))
                    edges.append((start, end, self.connections[start][end]))
        edges.sort(key=lambda x: x[2])

        # creating the disjoint set
        disjoint_set = DisjointSetTree[T]()
        for node in self.connections:
            disjoint_set.make_set(node)

        # MST generation
        num_edges = 0
        index = 0
        graph = GraphUndirectedWeighted[T]()
        while num_edges < len(self.connections) - 1:
            u, v, w = edges[index]
            index += 1
            parent_u = disjoint_set.find_set(u)
            parent_v = disjoint_set.find_set(v)
            if parent_u != parent_v:
                num_edges += 1
                graph.add_edge(u, v, w)
                disjoint_set.union(u, v)
        return graph
def unique_id(self):
        
        tmp_id = (
            f"{self._device_config.getConfig().serial}-{self.entity_description.key}"
        )
        if hasattr(self._api, "id"):
            return f"{tmp_id}-{self._api.id}"
        return tmp_id
def get_dir_path_handler(dir_path_str: str, ignore_regexes: Optional[List[str]] = None) -> PathHandler:
        
        
        dir_path = Path(dir_path_str).resolve()

        case_sensitive = platform.system().lower() != "windows"

        file_handler = RegexMatchingEventHandler(
            regexes=["^.*$"],
            ignore_regexes=ignore_regexes,
            ignore_directories=False,
            case_sensitive=case_sensitive,
        )
        return PathHandler(path=dir_path, event_handler=file_handler, recursive=True, static_folder=True)
def __init__(
        self,
        resource_identifier: ResourceIdentifier,
        stacks: List[Stack],
        base_dir: Path,
        on_code_change: OnChangeCallback,
    ):
        
        
        
        super().__init__()
        self._resource_identifier = resource_identifier
        resource = get_resource_by_id(stacks, resource_identifier)
        if not resource:
            raise ResourceNotFound()
        self._resource = resource
        self._on_code_change = on_code_change
        self.base_dir = base_dir
def __init__(self, template_file: str, stack_name: str, on_template_change: OnChangeCallback) -> None:
        
        
        
        super().__init__()
        self._template_file = template_file
        self._stack_name = stack_name
        self._on_template_change = on_template_change
        self._validator = DefinitionValidator(Path(self._template_file))
def _handle_long_word(self, chunks, cur_line, cur_len):
        
        
        space_left = self.width - cur_len

        # If we're allowed to break long words, then do so: put as much
        # of the next chunk onto the current line as will fit.
        if self.break_long_words:
            cur_line.append(chunks[0][0:space_left])
            chunks[0] = chunks[0][space_left:]

        # Otherwise, we have to preserve the long word intact.  Only add
        # it to the current line if there's nothing already there --
        # that minimizes how much we violate the width constraint.
        elif not cur_line:
            cur_line.append(chunks.pop(0))
def dedent(text):
    
    
    # Look for the longest leading string of spaces and tabs common to
    # all lines.
    margin = None
    text = _whitespace_only_re.sub('', text)
    indents = _leading_whitespace_re.findall(text)
    for indent in indents:
        if margin is None:
            margin = indent

        # Current line more deeply indented than previous winner:
        # no change (previous winner is still on top).
        elif indent.startswith(margin): 
            pass                        

        # Current line consistent with and no deeper than previous winner:
        # it's the new winner.
        elif margin.startswith(indent): 
            margin = indent             

        # Current line and previous winner have no common whitespace:
        # there is no margin.
        else:
            margin = ""
            break

    # sanity check (testing/debugging only)
    if 0 and margin:
        for line in text.split("\n"):
            assert not line or line.startswith(margin), \
                   "line = %r, margin = %r" % (line, margin)

    if margin:
        text = re.sub(r'(?m)^' + margin, '', text)
    return text
def _split(self, text):
        
        
        if self.break_on_hyphens is True:
            chunks = self.wordsep_re.split(text)
        else:
            chunks = self.wordsep_simple_re.split(text)
        chunks = filter(None, chunks)  # remove empty chunks
        return chunks
def __truediv__(self, other):
        
        raise NotImplementedError
def __float__(self):
        
        return float(int(self))
def __init__(self, num_stages, channel_dims, blocks_per_stage,
               num_hourglasses, downsample=True):
    
    

    super(HourglassNetwork, self).__init__()

    self.num_hourglasses = num_hourglasses
    self.downsample = downsample
    if downsample:
      self.downsample_input = InputDownsampleBlock(
          out_channels_initial_conv=channel_dims[0],
          out_channels_residual_block=channel_dims[1]
      )
    else:
      self.conv_input = InputConvBlock(
          out_channels_initial_conv=channel_dims[0],
          out_channels_residual_block=channel_dims[1]
      )

    self.hourglass_network = []
    self.output_conv = []
    for _ in range(self.num_hourglasses):
      self.hourglass_network.append(
          EncoderDecoderBlock(
              num_stages=num_stages, channel_dims=channel_dims[1:],
              blocks_per_stage=blocks_per_stage)
      )
      self.output_conv.append(
          ConvolutionalBlock(kernel_size=3, out_channels=channel_dims[1])
      )

    self.intermediate_conv1 = []
    self.intermediate_conv2 = []
    self.intermediate_residual = []

    for _ in range(self.num_hourglasses - 1):
      self.intermediate_conv1.append(
          ConvolutionalBlock(
              kernel_size=1, out_channels=channel_dims[1], relu=False)
      )
      self.intermediate_conv2.append(
          ConvolutionalBlock(
              kernel_size=1, out_channels=channel_dims[1], relu=False)
      )
      self.intermediate_residual.append(
          ResidualBlock(out_channels=channel_dims[1])
      )

    self.intermediate_relu = tf.keras.layers.ReLU()
def __init__(self, num_stages, channel_dims, blocks_per_stage,
               stagewise_downsample=True, encoder_decoder_shortcut=True):
    
    

    super(EncoderDecoderBlock, self).__init__()

    out_channels = channel_dims[0]
    out_channels_downsampled = channel_dims[1]

    self.encoder_decoder_shortcut = encoder_decoder_shortcut

    if encoder_decoder_shortcut:
      self.merge_features = tf.keras.layers.Add()
      self.encoder_block1 = _make_repeated_residual_blocks(
          out_channels=out_channels, num_blocks=blocks_per_stage[0],
          initial_stride=1)

    initial_stride = 2 if stagewise_downsample else 1
    self.encoder_block2 = _make_repeated_residual_blocks(
        out_channels=out_channels_downsampled,
        num_blocks=blocks_per_stage[0], initial_stride=initial_stride,
        initial_skip_conv=out_channels != out_channels_downsampled)

    if num_stages > 1:
      self.inner_block = [
          EncoderDecoderBlock(num_stages - 1, channel_dims[1:],
                              blocks_per_stage[1:],
                              stagewise_downsample=stagewise_downsample,
                              encoder_decoder_shortcut=encoder_decoder_shortcut)
      ]
    else:
      self.inner_block = _make_repeated_residual_blocks(
          out_channels=out_channels_downsampled,
          num_blocks=blocks_per_stage[1])

    self.decoder_block = _make_repeated_residual_blocks(
        residual_channels=out_channels_downsampled,
        out_channels=out_channels, num_blocks=blocks_per_stage[0])

    self.upsample = tf.keras.layers.UpSampling2D(initial_stride)
def observe_lr(optimizer_name='main', observation_key='lr'):
    
    
    return observe_value(
        observation_key,
        lambda trainer: trainer.updater.get_optimizer(optimizer_name).lr)
def _mvdr(Xs, NNs, As, eps=1e-20):
        
        

        # Get unique covariance values to reduce the number of computations
        NNs_val, NNs_idx = torch.unique(NNs, return_inverse=True, dim=1)

        # Inverse covariance matrices
        NNs_inv = eig.inv(NNs_val)

        # Capture real and imaginary parts, and restore time steps
        NNs_inv_re = NNs_inv[..., 0][:, NNs_idx]
        NNs_inv_im = NNs_inv[..., 1][:, NNs_idx]

        # Decompose steering vector
        AsC_re = As[..., 0, :].unsqueeze(4)
        AsC_im = 1.0 * As[..., 1, :].unsqueeze(4)
        AsT_re = AsC_re.transpose(3, 4)
        AsT_im = -1.0 * AsC_im.transpose(3, 4)

        # Project
        NNs_inv_AsC_re = torch.matmul(NNs_inv_re, AsC_re) - torch.matmul(
            NNs_inv_im, AsC_im
        )
        NNs_inv_AsC_im = torch.matmul(NNs_inv_re, AsC_im) + torch.matmul(
            NNs_inv_im, AsC_re
        )

        # Compute the gain
        alpha = 1.0 / (
            torch.matmul(AsT_re, NNs_inv_AsC_re)
            - torch.matmul(AsT_im, NNs_inv_AsC_im)
        )

        # Get the unmixing coefficients
        Ws_re = torch.matmul(NNs_inv_AsC_re, alpha).squeeze(4)
        Ws_im = -torch.matmul(NNs_inv_AsC_im, alpha).squeeze(4)

        # Applying MVDR
        Xs_re = Xs[..., 0, :]
        Xs_im = Xs[..., 1, :]

        Ys_re = torch.sum((Ws_re * Xs_re - Ws_im * Xs_im), dim=3, keepdim=True)
        Ys_im = torch.sum((Ws_re * Xs_im + Ws_im * Xs_re), dim=3, keepdim=True)

        Ys = torch.stack((Ys_re, Ys_im), -2)

        return Ys
def delete_package(pkg_uri: str, base_directory: str) -> Tuple[bool, int]:
    
    

    deleted = False
    path = Path(_get_local_path(base_directory, pkg_uri))
    with FileLock(str(path) + ".lock"):
        path = path.with_suffix("")
        if path.exists():
            if path.is_dir() and not path.is_symlink():
                shutil.rmtree(str(path))
            else:
                path.unlink()
            deleted = True

    return deleted
def parse_uri(pkg_uri: str) -> Tuple[Protocol, str]:
    
    

    
    uri = urlparse(pkg_uri)
    try:
        protocol = Protocol(uri.scheme)
    except ValueError as e:
        raise ValueError(
            f'Invalid protocol for runtime_env URI "{pkg_uri}". '
            f"Supported protocols: {Protocol._member_names_}. Original error: {e}"
        )

    if protocol in Protocol.remote_protocols():
        if pkg_uri.endswith(".whl"):
            # Don't modify the .whl filename. See
            # https://peps.python.org/pep-0427/#file-name-convention
            # for more information.
            package_name = pkg_uri.split("/")[-1]
        else:
            package_name = f"{protocol.value}_{uri.netloc}{uri.path}"

            disallowed_chars = ["/", ":", "@", "+"]
            for disallowed_char in disallowed_chars:
                package_name = package_name.replace(disallowed_char, "_")

            # Remove all periods except the last, which is part of the
            # file extension
            package_name = package_name.replace(".", "_", package_name.count(".") - 1)
    else:
        package_name = uri.netloc

    return (protocol, package_name)
def estimate_blur(cls, image, metadata=None):
         
        
        if metadata is not None:
            alignments = metadata["alignments"]
            det_face = DetectedFace()
            det_face.from_png_meta(alignments)
            aln_face = AlignedFace(np.array(alignments["landmarks_xy"], dtype="float32"),
                                   image=image,
                                   centering="legacy",
                                   size=256,
                                   is_aligned=True)
            mask = det_face.mask["components"]
            mask.set_sub_crop(aln_face.pose.offset["face"] * -1)
            mask = cv2.resize(mask.mask, (256, 256), interpolation=cv2.INTER_CUBIC)[..., None]
            image = np.minimum(aln_face.face, mask)
        if image.ndim == 3:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        blur_map = cv2.Laplacian(image, cv2.CV_32F)
        score = np.var(blur_map) / np.sqrt(image.shape[0] * image.shape[1])
        return score
def setSyncState(self, service_name: str, state: SyncState) -> None:
         
        

        prev_state = self._sync_state

        self._sync_services[service_name] = state

        if any(val == SyncState.SYNCING for val in self._sync_services.values()):
            self._sync_state = SyncState.SYNCING
        elif any(val == SyncState.ERROR for val in self._sync_services.values()):
            self._sync_state = SyncState.ERROR
        else:
            self._sync_state = SyncState.SUCCESS

        if self._sync_state != prev_state:
            self.syncStateChanged.emit(self._sync_state)

            if self._sync_state == SyncState.SUCCESS:
                self._last_sync_str = datetime.now().strftime("%d/%m/%Y %H:%M")
                self.lastSyncDateTimeChanged.emit()

            if self._sync_state != SyncState.SYNCING:
                # schedule new auto update after syncing completed (for whatever reason)
                if not self._update_timer.isActive():
                    self._update_timer.start()
def sync(self) -> None:
        
        

        if self._update_timer.isActive():
            self._update_timer.stop()
        self.syncRequested.emit()
def find_last(self):
        
        
        # Get directory names. Each directory corresponds to a model
        dir_names = next(os.walk(self.model_dir))[1]
        key = self.config.NAME.lower()
        dir_names = filter(lambda f: f.startswith(key), dir_names)
        dir_names = sorted(dir_names)
        if not dir_names:
            import errno
            raise FileNotFoundError(
                errno.ENOENT,
                "Could not find model directory under {}".format(self.model_dir))
        # Pick last directory
        dir_name = os.path.join(self.model_dir, dir_names[-1])
        # Find the last checkpoint
        checkpoints = next(os.walk(dir_name))[2]
        checkpoints = filter(lambda f: f.startswith("mask_rcnn"), checkpoints)
        checkpoints = sorted(checkpoints)
        if not checkpoints:
            import errno
            raise FileNotFoundError(
                errno.ENOENT, "Could not find weight files in {}".format(dir_name))
        checkpoint = os.path.join(dir_name, checkpoints[-1])
        return checkpoint
def pprint_columns(columns, width, margin, format_string, format_args, columns_dict, color="yellow", **textwrap_kwargs):
    

    
    
    for columns_text in zip_longest(*wrapped_text_generator(columns, width, margin, **textwrap_kwargs), fillvalue=""):
        counter = count()
        # Generate columnar data that correspond to the column names and update them.
        for k, _ in columns_dict.items():
            columns_dict[k] = columns_text[next(counter)]

        click.secho(format_string.format(*format_args, **columns_dict), fg=color)
def count_frames_and_secs(filename, timeout=90):
     
    
    # https://stackoverflow.com/questions/2017843/fetch-frame-count-with-ffmpeg

    assert isinstance(filename, str), "Video path must be a string"
    exe = im_ffm.get_ffmpeg_exe()
    iswin = sys.platform.startswith("win")
    logger.debug("iswin: '%s'", iswin)
    cmd = [exe, "-i", filename, "-map", "0:v:0", "-c", "copy", "-f", "null", "-"]
    logger.debug("FFMPEG Command: '%s'", " ".join(cmd))
    attempts = 3
    for attempt in range(attempts):
        try:
            logger.debug("attempt: %s of %s", attempt + 1, attempts)
            out = subprocess.check_output(cmd,
                                          stderr=subprocess.STDOUT,
                                          shell=iswin,
                                          timeout=timeout)
            logger.debug("Succesfully communicated with FFMPEG")
            break
        except subprocess.CalledProcessError as err:
            out = err.output.decode(errors="ignore")
            raise RuntimeError("FFMEG call failed with {}:\n{}".format(err.returncode, out))
        except subprocess.TimeoutExpired as err:
            this_attempt = attempt + 1
            if this_attempt == attempts:
                msg = ("FFMPEG hung while attempting to obtain the frame count. "
                       "Sometimes this issue resolves itself, so you can try running again. "
                       "Otherwise use the Effmpeg Tool to extract the frames from your video into "
                       "a folder, and then run the requested Faceswap process on that folder.")
                raise FaceswapError(msg) from err
            logger.warning("FFMPEG hung while attempting to obtain the frame count. "
                           "Retrying %s of %s", this_attempt + 1, attempts)
            continue

    # Note that other than with the sub-process calls below, ffmpeg wont hang here.
    # Worst case Python will stop/crash and ffmpeg will continue running until done.

    nframes = nsecs = None
    for line in reversed(out.splitlines()):
        if not line.startswith(b"frame="):
            continue
        line = line.decode(errors="ignore")
        logger.debug("frame line: '%s'", line)
        idx = line.find("frame=")
        if idx >= 0:
            splitframes = line[idx:].split("=", 1)[-1].lstrip().split(" ", 1)[0].strip()
            nframes = int(splitframes)
        idx = line.find("time=")
        if idx >= 0:
            splittime = line[idx:].split("=", 1)[-1].lstrip().split(" ", 1)[0].strip()
            nsecs = convert_to_secs(*splittime.split(":"))
        logger.debug("nframes: %s, nsecs: %s", nframes, nsecs)
        return nframes, nsecs

    raise RuntimeError("Could not get number of frames")
def _from_folder(self):
         
        
        logger.debug("Loading images from folder: '%s'", self.location)
        for idx, filename in enumerate(self.file_list):
            if idx in self._skip_list:
                logger.trace("Skipping face %s due to skip list")
                continue
            image_read = read_image(filename, raise_error=False, with_metadata=True)
            retval = filename, *image_read
            if retval[1] is None:
                logger.warning("Face not loaded: '%s'", filename)
                continue
            yield retval
def count_frames(filename, fast=False):
     
    
    assert isinstance(filename, str), "Video path must be a string"

    cmd = [im_ffm.get_ffmpeg_exe(), "-i", filename, "-map", "0:v:0"]
    if fast:
        cmd.extend(["-c", "copy"])
    cmd.extend(["-f", "null", "-"])

    logger.debug("FFMPEG Command: '%s'", " ".join(cmd))
    process = subprocess.Popen(cmd,
                               stderr=subprocess.STDOUT,
                               stdout=subprocess.PIPE,
                               universal_newlines=True)
    pbar = None
    duration = None
    init_tqdm = False
    update = 0
    frames = 0
    while True:
        output = process.stdout.readline().strip()
        if output == "" and process.poll() is not None:
            break

        if output.startswith("Duration:"):
            logger.debug("Duration line: %s", output)
            idx = output.find("Duration:") + len("Duration:")
            duration = int(convert_to_secs(*output[idx:].split(",", 1)[0].strip().split(":")))
            logger.debug("duration: %s", duration)
        if output.startswith("frame="):
            logger.debug("frame line: %s", output)
            if not init_tqdm:
                logger.debug("Initializing tqdm")
                pbar = tqdm(desc="Counting Video Frames", total=duration, unit="secs")
                init_tqdm = True
            time_idx = output.find("time=") + len("time=")
            frame_idx = output.find("frame=") + len("frame=")
            frames = int(output[frame_idx:].strip().split(" ")[0].strip())
            vid_time = int(convert_to_secs(*output[time_idx:].split(" ")[0].strip().split(":")))
            logger.debug("frames: %s, vid_time: %s", frames, vid_time)
            prev_update = update
            update = vid_time
            pbar.update(update - prev_update)
    if pbar is not None:
        pbar.close()
    return_code = process.poll()
    logger.debug("Return code: %s, frames: %s", return_code, frames)
    return frames
def _from_folder(self):
         
        
        logger.debug("Loading frames from folder: '%s'", self.location)
        for idx, filename in enumerate(self.file_list):
            if idx in self._skip_list:
                logger.trace("Skipping frame %s due to skip list")
                continue
            image_read = read_image(filename, raise_error=False, with_hash=False)
            retval = filename, image_read
            if retval[1] is None:
                logger.warning("Frame not loaded: '%s'", filename)
                continue
            yield retval
def _save(self,
              filename: str,
              image: Union[bytes, np.ndarray],
              sub_folder: Optional[str]) -> None:
         
        
        location = os.path.join(self.location, sub_folder) if sub_folder else self._location
        if sub_folder and not os.path.exists(location):
            os.makedirs(location)

        filename = os.path.join(location, os.path.basename(filename))
        try:
            if self._as_bytes:
                assert isinstance(image, bytes)
                with open(filename, "wb") as out_file:
                    out_file.write(image)
            else:
                cv2.imwrite(filename, image)
            logger.trace("Saved image: '%s'", filename)  # type:ignore
        except Exception as err:  # pylint: disable=broad-except
            logger.error("Failed to save image '%s'. Original Error: %s", filename, str(err))
def read_image_batch(filenames):
     
    
    logger.trace("Requested batch: '%s'", filenames)
    executor = futures.ThreadPoolExecutor()
    with executor:
        images = {executor.submit(read_image, filename, raise_error=True): filename
                  for filename in filenames}
        batch = []
        filenames = []
        for future in futures.as_completed(images):
            batch.append(future.result())
            filenames.append(images[future])
        batch = np.array(batch)
    logger.trace("Returning images: (filenames: %s, batch shape: %s)", filenames, batch.shape)
    return filenames, batch
def _dummy_video_framename(self, index):
          
        vidname, ext = os.path.splitext(os.path.basename(self.location))
        return f"{vidname}_{index + 1:06d}{ext}"
def read_image_hash(filename, output_shape=False):
     
    
    img = read_image(filename, raise_error=True)
    retval = sha1(img).hexdigest()
    if output_shape:
        retval = (retval, img.shape)
    logger.trace("filename: '%s', retval: %s", filename, retval)
    return retval
def encode_image(image: np.ndarray,
                 extension: str,
                 encoding_args: tuple[int, ...] | None = None,
                 metadata: PNGHeaderDict | dict[str, T.Any] | bytes | None = None) -> bytes:
     
    
    if metadata and extension.lower() not in (".png", ".tif"):
        raise ValueError("Metadata is only supported for .png and .tif images")
    args = tuple() if encoding_args is None else encoding_args

    retval = cv2.imencode(extension, image, args)[1]
    if metadata:
        func = {".png": png_write_meta, ".tif": tiff_write_meta}[extension]
        retval = func(retval.tobytes(), metadata)  # type:ignore[arg-type]
    return retval
def NMF_separate_spectra(Whats, Xmix):
    
    
    W1, W2 = Whats

    nmixtures = Xmix.shape[0]
    Xmix = Xmix.permute(0, 2, 1).reshape(-1, Xmix.size(-1)).t()
    n = Xmix.shape[1]
    eps = 1e-20

    # Normalize input
    g = Xmix.sum(dim=0) + eps
    z = Xmix / g

    # initialize
    w = torch.cat([W1, W2], dim=1)
    K = w.size(1)
    K1 = W1.size(1)

    h = 0.1 * torch.rand(K, n)
    h /= torch.sum(h, dim=0) + eps

    for ep in range(1000):
        v = z / (torch.matmul(w, h) + eps)

        nh = h * torch.matmul(w.t(), v)
        h = nh / (torch.sum(nh, dim=0) + eps)

    h *= g
    Xhat1 = torch.matmul(w[:, :K1], h[:K1, :])
    Xhat1 = torch.split(Xhat1.unsqueeze(0), Xhat1.size(1) // nmixtures, dim=2)
    Xhat1 = torch.cat(Xhat1, dim=0)

    Xhat2 = torch.matmul(w[:, K1:], h[K1:, :])
    Xhat2 = torch.split(Xhat2.unsqueeze(0), Xhat2.size(1) // nmixtures, dim=2)
    Xhat2 = torch.cat(Xhat2, dim=0)

    return Xhat1, Xhat2
def reconstruct_results(
    X1hat, X2hat, X_stft, sample_rate, win_length, hop_length,
):

    
    

    ISTFT = spf.ISTFT(
        sample_rate=sample_rate, win_length=win_length, hop_length=hop_length
    )

    phase_mix = spectral_phase(X_stft)
    mag_mix = spectral_magnitude(X_stft, power=2)

    x1hats, x2hats = [], []
    eps = 1e-25
    for i in range(X1hat.shape[0]):
        X1hat_stft = (
            (X1hat[i] / (eps + X1hat[i] + X2hat[i])).unsqueeze(-1)
            * mag_mix[i].unsqueeze(-1)
            * torch.cat(
                [
                    torch.cos(phase_mix[i].unsqueeze(-1)),
                    torch.sin(phase_mix[i].unsqueeze(-1)),
                ],
                dim=-1,
            )
        )

        X2hat_stft = (
            (X2hat[i] / (eps + X1hat[i] + X2hat[i])).unsqueeze(-1)
            * mag_mix[i].unsqueeze(-1)
            * torch.cat(
                [
                    torch.cos(phase_mix[i].unsqueeze(-1)),
                    torch.sin(phase_mix[i].unsqueeze(-1)),
                ],
                dim=-1,
            )
        )
        shat1 = ISTFT(X1hat_stft.unsqueeze(0).permute(0, 2, 1, 3))
        shat2 = ISTFT(X2hat_stft.unsqueeze(0).permute(0, 2, 1, 3))

        div_factor = 10
        x1 = shat1 / (div_factor * shat1.std())
        x2 = shat2 / (div_factor * shat2.std())

        x1hats.append(x1)
        x2hats.append(x2)
    return x1hats, x2hats
def _status(self, hide_status):
         
        
        if hide_status:
            return

        statusframe = ttk.Frame(self)
        statusframe.pack(side=tk.LEFT, anchor=tk.W, fill=tk.X, expand=False)

        lbltitle = ttk.Label(statusframe, text="Status:", width=6, anchor=tk.W)
        lbltitle.pack(side=tk.LEFT, expand=False)

        lblstatus = ttk.Label(statusframe,
                              width=40,
                              textvariable=self._message,
                              anchor=tk.W)
        lblstatus.pack(side=tk.LEFT, anchor=tk.W, fill=tk.X, expand=True)
def _progress_bar(self) -> ttk.Progressbar:
         
        
        progressframe = ttk.Frame(self._frame)
        progressframe.pack(side=tk.RIGHT, anchor=tk.E, fill=tk.X)

        lblmessage = ttk.Label(progressframe, textvariable=self._pbar_message)
        lblmessage.pack(side=tk.LEFT, padx=3, fill=tk.X, expand=True)

        pbar = ttk.Progressbar(progressframe,
                               length=200,
                               variable=self._pbar_position,
                               maximum=100,
                               mode=self._mode)
        pbar.pack(side=tk.LEFT, padx=2, fill=tk.X, expand=True)
        pbar.pack_forget()
        return pbar
def get_skill_name(self, skill_id):
        
        
        
        return self.skill_names.get(int(skill_id), skill_id)
def handle_utterance(self, message):
         
        
        try:
            # Get language of the utterance
            lang = message.data.get('lang', "en-us")
            utterances = message.data.get('utterances', '')

            stopwatch = Stopwatch()
            with stopwatch:
                # Give active skills an opportunity to handle the utterance
                converse = self._converse(utterances, lang)

                if not converse:
                    # No conversation, use intent system to handle utterance
                    intent = self._adapt_intent_match(utterances, lang)
                    padatious_intent = PadatiousService.instance.calc_intent(
                                        utterances[0])

            if converse:
                # Report that converse handled the intent and return
                ident = message.context['ident'] if message.context else None
                report_timing(ident, 'intent_service', stopwatch,
                              {'intent_type': 'converse'})
                return
            elif intent and not (padatious_intent and
                                 padatious_intent.conf >= 0.95):
                # Send the message to the Adapt intent's handler unless
                # Padatious is REALLY sure it was directed at it instead.
                reply = message.reply(intent.get('intent_type'), intent)
            else:
                # Allow fallback system to handle utterance
                # NOTE: Padatious intents are handled this way, too
                reply = message.reply('intent_failure',
                                      {'utterance': utterances[0],
                                       'lang': lang})
            self.bus.emit(reply)
            self.send_metrics(intent, message.context, stopwatch)
        except Exception as e:
            LOG.exception(e)
def _converse(self, utterances, lang, message):
        
        

        # check for conversation time-out
        self.active_skills = [skill for skill in self.active_skills
                              if time.time() - skill[
                                  1] <= self.converse_timeout * 60]

        # check if any skill wants to handle utterance
        for skill in copy(self.active_skills):
            if self.do_converse(utterances, skill[0], lang, message):
                # update timestamp, or there will be a timeout where
                # intent stops conversing whether its being used or not
                self.add_active_skill(skill[0])
                return True
        return False
def _converse(self, utterances, lang, message):
        
        
        # check for conversation time-out
        self.active_skills = [skill for skill in self.active_skills
                              if time.time() - skill[
                                  1] <= self.converse_timeout * 60]

        # check if any skill wants to handle utterance
        for skill in copy(self.active_skills):
            if self.do_converse(utterances, skill[0], lang, message):
                # update timestamp, or there will be a timeout where
                # intent stops conversing whether its being used or not
                return IntentMatch('Converse', None, None, skill[0])
        return None
def parse_utterances(self, utterances, lang):
        
            
        
        # check for conversation time-out
        self.active_skills = [skill for skill in self.active_skills
                              if time.time() - skill[
                                  1] <= self.converse_timeout * 60]

        # check if any skill wants to handle utterance
        for skill in self.active_skills:
            if self.do_converse(utterances, skill[0], lang):
                # update timestamp, or there will be a timeout where
                # intent stops conversing whether its being used or not
                self.add_active_skill(skill[0])
                return

        # no skill wants to handle utterance
        best_intent = None
        for utterance in utterances:
            try:
                # normalize() changes "it's a boy" to "it is boy", etc.
                best_intent = next(self.engine.determine_intent(
                    normalize(utterance, lang), 100,
                    include_tags=True,
                    context_manager=self.context_manager))
                # TODO - Should Adapt handle this?
                best_intent['utterance'] = utterance
            except StopIteration:
                # don't show error in log
                continue
            except Exception as e:
                LOG.exception(e)
                continue

        if best_intent and best_intent.get('confidence', 0.0) > 0.0:
            self.update_context(best_intent)
            # update active skills
            skill_id = int(best_intent['intent_type'].split(":")[0])
            self.add_active_skill(skill_id)
            return best_intent
def parse_utterances(self, utterances, lang):
        
            
        
        best_intent = None
        for utterance in utterances:
            try:
                # normalize() changes "it's a boy" to "it is boy", etc.
                best_intent = next(self.engine.determine_intent(
                    normalize(utterance, lang), 100,
                    include_tags=True,
                    context_manager=self.context_manager))
                # TODO - Should Adapt handle this?
                best_intent['utterance'] = utterance
            except StopIteration:
                # don't show error in log
                continue
            except Exception as e:
                LOG.exception(e)
                continue

        if best_intent and best_intent.get('confidence', 0.0) > 0.0:
            self.update_context(best_intent)
            # update active skills
            skill_id = int(best_intent['intent_type'].split(":")[0])
            self.add_active_skill(skill_id)
            return best_intent
def track_start(self, track):
        
        
        if track:
            # Inform about the track about to start.
            LOG.debug('New track coming up!')
            self.bus.emit(Message('mycroft.audio.playing_track',
                                  data={'track': track}))
        else:
            # If no track is about to start last track of the queue has been
            # played.
            LOG.debug('End of playlist!')
            self.bus.emit(Message('mycroft.audio.queue_end'))
def _restore_volume(self, _=None):
        
        current = self.current
        if current:
            LOG.debug('restoring volume')
            self.volume_is_low = False
            current.restore_volume()
def _build_content_dict(self):
        
        
        content = self.dict()
        if hasattr(self, '_attributes_in_str') and isinstance(
            self._attributes_in_str, list
        ):
            content = {k: content[k] for k in self._attributes_in_str}
        return content
def swipe(self, fx, fy, tx, ty, duration=0.5):
        
        
        
        return self.jsonrpc.swipe(fx, fy, tx, ty, int(duration*200))
def connect(addr=None):
    
    
    
    if not addr:
        addr = os.getenv('ANDROID_DEVICE_IP') or '127.0.0.1'
    if '://' not in addr:
        addr = 'http://' + addr
    if addr.startswith('http://'):
        u = urlparse.urlparse(addr)
        host = u.hostname
        port = u.port or 7912
        return AutomatorServer(host, port)
    else:
        raise RuntimeError("address should startswith http://")
def click(self, x, y):
        
        
        
        x, y = self.pos_rel2abs(x, y)
        ret = self.jsonrpc.click(x, y)
        if self.server.click_post_delay:  # click code delay
            time.sleep(self.server.click_post_delay)
def app_info(self, package_name: str) -> Dict[str, Any]:
        
        
        
        info = self.adb_device.app_info(package_name)
        if not info:
            raise BaseException("App not installed")
        return {
            "versionName": info.version_name,
            "versionCode": info.version_code,
        }
def click(self, timeout=None):
        
        
        
        self.wait(timeout=timeout)
        x, y = self.center()
        # ext.htmlreport need to comment bellow code
        # if info['clickable']:
        #     return self.jsonrpc.click(self.selector)
        self.session.click(x, y)
        delay = self.session.server.click_post_delay
        if delay:
            time.sleep(delay)
def app_start(self, pkg_name, activity=None, stop=False):
         
        
        if activity:
            # -D: enable debugging
            # -W: wait for launch to complete
            # -S: force stop the target app before starting the activity
            # --user <USER_ID> | current: Specify which user to run as; if not
            #    specified then run as the current user.
            args = ['am', 'start', '-W']
            if stop:
                args.append('-S')
            args += ['-n', '{}/{}'.format(pkg_name, activity)]
            self.adb_shell(*args) #'am', 'start', '-W', '-n', '{}/{}'.format(pkg_name, activity))
        else:
            self.adb_shell('monkey', '-p', pkg_name, '-c', 'android.intent.category.LAUNCHER', '1')
def app_install(self, url):
        
        
        
        r = self._reqsess.post(self.path2url('/install'), data={'url': url})
        if r.status_code != 200:
            raise RuntimeError("app install error:", r.text)
        id = r.text.strip()
        interval = 1.0 # 2.0s
        next_refresh = time.time()
        while True:
            if time.time() < next_refresh:
                time.sleep(.2)
                continue
            ret = self._reqsess.get(self.path2url('/install/'+id))
            progress = None
            try:
                progress = ret.json()
            except:
                raise RuntimeError("invalid json response:", ret.text)
            total_size = progress.get('totalSize') or progress.get('titalSize')
            copied_size = progress.get('copiedSize')
            message = progress.get('message')
            if message == 'downloading':
                next_refresh = time.time() + interval
            elif message == 'installing':
                next_refresh = time.time() + interval*2
            log_print("{} {} / {}".format(
                progress.get('message'),
                humanize.naturalsize(copied_size),
                humanize.naturalsize(total_size)))
            if progress.get('error'):
                raise RuntimeError(progress.get('error'), progress.get('message'))
            if message == 'success installed':
                break
        return True
def app_install(self, url, installing_callback=None, server=None):
        
        
        
        r = self._reqsess.post(self.path2url('/install'), data={'url': url})
        if r.status_code != 200:
            raise RuntimeError("app install error:", r.text)
        id = r.text.strip()
        print(time.strftime('%H:%M:%S'), "id:", id)
        return self._wait_install_finished(id, installing_callback)
def healthcheck(self):
        
        
        
        self.app_start('com.github.uiautomator', '.MainActivity')
        self.adb_shell('input', 'keyevent', 'HOME')
        self._reqsess.post(self.path2url('/uiautomator'))
        start = time.time()
        while time.time() - start < 10.0:
            if self.alive:
                return True
            time.sleep(.5)
        raise RuntimeError("Uiautomator started failed.")
def healthcheck(self):
        
        
        
        if not self.info['screenOn']:
            self.screen_on()
            self.press("home")
            self.swipe(0.1, 0.9, 0.9, 0.1, 0.3)  # swipe to unlock

        self.press("home")
        self.press("back")
        self.reset_uiautomator()
def screenshot(self, filename=None, format='pillow'):
        
        
        
        r = requests.get(self.server.screenshot_uri)
        if filename:
            with open(filename, 'wb') as f:
                f.write(r.content)
            return filename
        elif format == 'pillow':
            from PIL import Image
            buff = io.BytesIO(r.content)
            return Image.open(buff)
        elif format == 'opencv':
            import cv2
            import numpy as np
            nparr = np.fromstring(r.content, np.uint8)
            return cv2.imdecode(nparr, cv2.IMREAD_COLOR)
def screenshot(self, filename: Optional[str] = None, format="pillow"):
        
        
        
        r = self.http.get("/screenshot/0", timeout=10)
        if filename:
            with open(filename, 'wb') as f:
                f.write(r.content)
            return filename
        elif format == 'pillow':
            buff = io.BytesIO(r.content)
            try:
                return Image.open(buff).convert("RGB")
            except Exception as ex:
                raise IOError("PIL.Image.open IOError", ex)
        elif format == 'opencv':
            import cv2
            import numpy as np
            nparr = np.fromstring(r.content, np.uint8)
            return cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        elif format == 'raw':
            return r.content
        else:
            raise ValueError("Invalid format {}".format(format))
def wait(self, exists=True, timeout=None):
        
        
        
        timeout = timeout or self.wait_timeout
        http_wait = timeout + 20
        if exists:
            return self.jsonrpc.waitForExists(self.selector, int(timeout*1000), http_timeout=http_wait)
        else:
            return self.jsonrpc.waitUntilGone(self.selector, int(timeout*1000), http_timeout=http_wait)
def current_app(self):
        
        
        
        # try: adb shell dumpsys window windows
        _focusedRE = re.compile(
            r'mFocusedApp=.*ActivityRecord{\w+ \w+ (?P<package>.*)/(?P<activity>.*) .*'
        )
        m = _focusedRE.search(self.shell(['dumpsys', 'window', 'windows'])[0])
        if m:
            return dict(
                package=m.group('package'), activity=m.group('activity'))

        # try: adb shell dumpsys activity top
        _activityRE = re.compile(
            r'ACTIVITY (?P<package>[^/]+)/(?P<activity>[^/\s]+) \w+ pid=(?P<pid>\d+)'
        )
        output, _ = self.shell(['dumpsys', 'activity', 'top'])
        ms = _activityRE.finditer(output)
        ret = None
        for m in ms:
            ret = dict(
                package=m.group('package'),
                activity=m.group('activity'),
                pid=int(m.group('pid')))
        if ret:
            return ret
        # empty result
        warnings.warn("Couldn't get focused app", stacklevel=2)
        return dict(package=None, activity=None)
def app_start(self,
                  package_name,
                  activity=None,
                  extras={},
                  wait=False,
                  stop=False,
                  unlock=False, launch_timeout=None, use_monkey=False):
         
        
        if unlock:
            self.unlock()

        if stop:
            self.app_stop(package_name)

        if use_monkey:
            self.shell([
                'monkey', '-p', package_name, '-c',
                'android.intent.category.LAUNCHER', '1'
            ])
            if wait:
                self.app_wait(package_name)
            return
        
        if not activity:
            info = self.app_info(package_name)
            activity = info['mainActivity']
            if activity.find(".") == -1:
                activity = "." + activity

        # -D: enable debugging
        # -W: wait for launch to complete
        # -S: force stop the target app before starting the activity
        # --user <USER_ID> | current: Specify which user to run as; if not
        #    specified then run as the current user.
        # -e <EXTRA_KEY> <EXTRA_STRING_VALUE>
        # --ei <EXTRA_KEY> <EXTRA_INT_VALUE>
        # --ez <EXTRA_KEY> <EXTRA_BOOLEAN_VALUE>
        args = [
            'am', 'start', '-a', 'android.intent.action.MAIN', '-c',
            'android.intent.category.LAUNCHER'
        ]
        args += ['-n', '{}/{}'.format(package_name, activity)]
        # -e --ez
        extra_args = []
        for k, v in extras.items():
            if isinstance(v, bool):
                extra_args.extend(['--ez', k, 'true' if v else 'false'])
            elif isinstance(v, int):
                extra_args.extend(['--ei', k, str(v)])
            else:
                extra_args.extend(['-e', k, v])
        args += extra_args
        self.shell(args)

        if wait:
            self.app_wait(package_name)
def adb_shell(self, *args):
        
        
        
        cmdline = args[0] if len(args) == 1 else list2cmdline(args)
        ret = self._reqsess.post(self.path2url('/shell'), data={'command': cmdline})
        if ret.status_code != 200:
            raise RuntimeError("device agent responds with an error code %d" % ret.status_code)
        return ret.json().get('output')
def app_start(self,
                  pkg_name,
                  activity=None,
                  extras={},
                  wait=True,
                  stop=False,
                  unlock=False, launch_timeout=None):
         
        
        if unlock:
            self.unlock()

        if activity:
            # -D: enable debugging
            # -W: wait for launch to complete
            # -S: force stop the target app before starting the activity
            # --user <USER_ID> | current: Specify which user to run as; if not
            #    specified then run as the current user.
            # -e <EXTRA_KEY> <EXTRA_STRING_VALUE>
            # --ei <EXTRA_KEY> <EXTRA_INT_VALUE>
            # --ez <EXTRA_KEY> <EXTRA_BOOLEAN_VALUE>
            args = [
                'am', 'start', '-a', 'android.intent.action.MAIN', '-c',
                'android.intent.category.LAUNCHER'
            ]
            if wait:
                args.append('-W')
            if stop:
                args.append('-S')
            args += ['-n', '{}/{}'.format(pkg_name, activity)]
            # -e --ez
            extra_args = []
            for k, v in extras.items():
                if isinstance(v, bool):
                    extra_args.extend(['--ez', k, 'true' if v else 'false'])
                elif isinstance(v, int):
                    extra_args.extend(['--ei', k, str(v)])
                else:
                    extra_args.extend(['-e', k, v])
            args += extra_args
            # 'am', 'start', '-W', '-n', '{}/{}'.format(pkg_name, activity))
            self.shell(args)
        else:
            if stop:
                self.app_stop(pkg_name)
            
            # launch with atx-agent
            data = {"flags": "-W -S"}
            if launch_timeout:
                data["timeout"] = str(launch_timeout)
            resp = self._reqsess.post(
                self.path2url("/session/" + pkg_name), data=data)
            if resp.status_code != 200: # 410: Gone
                raise SessionBrokenError(pkg_name, resp.text)
            jsondata = resp.json()
            if not jsondata["success"]:
                raise SessionBrokenError(pkg_name,
                                         jsondata["error"], jsondata["output"])
def connect_wifi(addr:str) -> "Device":
    
    
    
    if not re.match(r"^https?://", addr):
        addr = "http://" + addr
    # fixed_addr = fix_wifi_addr(addr)
    # if fixed_addr is None:
        # raise ConnectError("addr is invalid or atx-agent is not running", addr)
    u = urlparse.urlparse(addr)
    host = u.hostname
    port = u.port or 7912
    return Device(host, port)
def long_click(self, x, y, duration=None):
        '''
        '''
        if not duration:
            duration = 0.5
        x, y = self.pos_rel2abs(x, y)
        self.touch.down(x, y)
        time.sleep(duration)
        self.touch.up(x, y)
        return self
def disable_popups(self, enable=True):
        
        
        
        raise NotImplementedError()

        if enable:
            self.jsonrpc.setAccessibilityPatterns({
                "com.android.packageinstaller": [u"确定", u"安装", u"下一步", u"好", u"允许", u"我知道"],
                "com.miui.securitycenter": [u"继续安装"], # xiaomi
                "com.lbe.security.miui": [u"允许"], # xiaomi
                "android": [u"好", u"安装"], # vivo
                "com.huawei.systemmanager": [u"立即删除"], # huawei
            })
        else:
            self.jsonrpc.setAccessibilityPatterns({})
def connect_usb(serial=None, healthcheck=True):
    
    
    
    adb = adbutils.AdbClient()
    if not serial:
        device = adb.must_one_device()
    else:
        device = adbutils.AdbDevice(adb, serial)
    # adb = adbutils.Adb(serial)
    lport = device.forward_port(7912)
    d = connect_wifi('127.0.0.1:' + str(lport))
    if healthcheck:
        if not d.agent_alive:
            warnings.warn("backend atx-agent is not alive, start again ...",
                        RuntimeWarning)
            # TODO: /data/local/tmp might not be execuable and atx-agent can be somewhere else
            device.shell_output("/data/local/tmp/atx-agent", "server", "-d")
            deadline = time.time() + 3
            while time.time() < deadline:
                if d.alive:
                    break
        elif not d.alive:
            warnings.warn("backend uiautomator2 is not alive, start again ...",
                        RuntimeWarning)
            d.reset_uiautomator()
    return d
def xpath(self, xpath, source=None):
        
        
        
        return XPathSelector(xpath, self.server, source)
def app_uninstall(self, pkg_name) -> bool:
         
        
        ret = self.shell(["pm", "uninstall", pkg_name])
        return ret.exit_code == 0
def _wait_for_device(self, timeout=None) -> adbutils.AdbDevice:
        
        
        
        if not timeout:
            timeout = WAIT_FOR_DEVICE_TIMEOUT if _is_production() else 3.0

        for d in adbutils.adb.device_list():
            if d.serial == self._serial:
                return d

        _RE_remote_adb = re.compile(r"^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d+$")
        _is_remote = _RE_remote_adb.match(self._serial) is not None

        adb = adbutils.adb
        deadline = time.time() + timeout
        while time.time() < deadline:
            title = "device reconnecting" if _is_remote else "wait-for-device"
            logger.info("%s, time left(%.1fs)", title, deadline - time.time())
            if _is_remote:
                try:
                    adb.disconnect(self._serial)
                    adb.connect(self._serial, timeout=1)
                except (adbutils.AdbError, adbutils.AdbTimeout) as e:
                    logger.debug("adb reconnect error: %s", str(e))
                    time.sleep(1.0)
                    continue
            try:
                adb.wait_for(self._serial, timeout=1)
            except adbutils.AdbTimeout:
                continue
            
            return adb.device(self._serial)
        return None
def register(self, domain, service, service_func, description=None,
                 schema=None):
        
        
        
        description = description or {}
        service_obj = Service(service_func, description.get('description'),
                              description.get('fields', {}), schema)
        with self._lock:
            if domain in self._services:
                self._services[domain][service] = service_obj
            else:
                self._services[domain] = {service: service_obj}

            self._bus.fire(
                EVENT_SERVICE_REGISTERED,
                {ATTR_DOMAIN: domain, ATTR_SERVICE: service})
def async_register(self, domain, service, service_func, schema=None):
        
        
        
        domain = domain.lower()
        service = service.lower()
        service_obj = Service(service_func, schema)

        if domain in self._services:
            self._services[domain][service] = service_obj
        else:
            self._services[domain] = {service: service_obj}

        if self._async_unsub_call_event is None:
            self._async_unsub_call_event = self._hass.bus.async_listen(
                EVENT_CALL_SERVICE, self._event_to_service_call)

        self._hass.bus.async_fire(
            EVENT_SERVICE_REGISTERED,
            {ATTR_DOMAIN: domain, ATTR_SERVICE: service}
        )
def distance(self: object, lat: float, lon: float) -> float:
        
        return self.units.length(
            location.distance(self.latitude, self.longitude, lat, lon), 'm')
def as_dict(self):
        
        
        time_zone = self.time_zone or dt_util.UTC

        return {
            'latitude': self.latitude,
            'longitude': self.longitude,
            'unit_system': self.units.as_dict(),
            'location_name': self.location_name,
            'time_zone': time_zone.zone,
            'components': self.components,
            'config_dir': self.config_dir,
            'version': __version__
        }
def get(self, entity_id):
        
        
        return self._states.get(entity_id.lower())
def is_state(self, entity_id: str, state: str) -> bool:
        
        
        state_obj = self.get(entity_id)
        return state_obj is not None and state_obj.state == state
def valid_entity_id(entity_id: str) -> bool:
    
    
    return ('.' in entity_id and
            slugify(entity_id) == entity_id.replace('.', '_', 1))
def add_job(self, target: Callable[..., None], *args: Any) -> None:
        
        
        run_callback_threadsafe(
            self.loop, self.async_add_job, target, *args).result()
def async_add_job(
            self,
            target: Callable[..., Any],
            *args: Any) -> Optional[asyncio.Future]:
        
        
        task = None

        if asyncio.iscoroutine(target):
            task = self.loop.create_task(target)
        elif is_callback(target):
            self.loop.call_soon(target, *args)
        elif asyncio.iscoroutinefunction(target):
            task = self.loop.create_task(target(*args))
        else:
            task = self.loop.run_in_executor(None, target, *args)

        # If a task is scheduled
        if self._track_task and task is not None:
            self._pending_tasks.append(task)

        return task
def async_stop_track_tasks(self):
        
        self._track_task = False
def as_dict(self) -> ReadOnlyDict[str, str | None]:
        
        return self._as_read_only_dict
def async_all(
        self, domain_filter: Optional[Union[str, Iterable]] = None
    ) -> List[State]:
        
        
        if domain_filter is None:
            return list(self._states.values())

        if isinstance(domain_filter, str):
            domain_filter = (domain_filter.lower(),)

        return [
            state for state in self._states.values() if state.domain in domain_filter
        ]
def async_listen(
        self,
        event_type: str,
        listener: Callable,
        event_filter: Optional[Callable] = None,
    ) -> CALLBACK_TYPE:
        
        
        if event_filter is not None and not is_callback(event_filter):
            raise HomeAssistantError(f"Event filter {event_filter} is not a callback")
        return self._async_listen_filterable_job(
            event_type, (HassJob(listener), event_filter)
        )
def __new__(cls) -> HomeAssistant:
        
        hass = super().__new__(cls)
        _hass.hass = hass
        return hass
def get_lock_keys(self) -> Set[str]:
        
        
        lock_keys = set()
        for resource_api_calls in self._get_resource_api_calls():
            for api_call in resource_api_calls.api_calls:
                lock_keys.add(SyncFlow._get_lock_key(resource_api_calls.shared_resource, api_call))
        return lock_keys
def __init__(
        self,
        build_context: "BuildContext",
        deploy_context: "DeployContext",
        sync_context: "SyncContext",
        physical_id_mapping: Dict[str, str],
        log_name: str,
        stacks: Optional[List[Stack]] = None,
        application_build_result: Optional[ApplicationBuildResult] = None,
    ):
        
        
        
        self._build_context = build_context
        self._deploy_context = deploy_context
        self._sync_context = sync_context
        self._log_name = log_name
        self._stacks = stacks
        self._session = None
        self._physical_id_mapping = physical_id_mapping
        self._locks = None
        self._local_sha = None
        self._application_build_result = application_build_result
def _rel_message_callback(self, message):
        

        if self._relay_addr == message.address and self._relay_chan == message.channel:
            _LOGGER.debug(
                "%s %d:%d value:%d",
                "Relay" if message.type == message.RELAY else "ZoneExpander",
                message.address,
                message.channel,
                message.value,
            )
            self._state = message.value
            self.schedule_update_ha_state()
def test_early_stopping_log_info(trainer, log_rank_zero_only, world_size, global_rank, expected_log):
    
    # set the global_rank and world_size if trainer is not None
    # or else always expect the simple logging message
    if trainer:
        trainer.strategy.global_rank = global_rank
        trainer.strategy.world_size = world_size
    else:
        expected_log = "bar"

    with mock.patch("pytorch_lightning.callbacks.early_stopping.log.info") as log_mock:
        EarlyStopping._log_info(trainer, "bar", log_rank_zero_only)

    # check log.info() was called or not with expected arg
    if expected_log:
        log_mock.assert_called_once_with(expected_log)
    else:
        log_mock.assert_not_called()
def cert_time_to_seconds(cert_time):
    
    
    from time import strptime
    from calendar import timegm

    months = (
        "Jan","Feb","Mar","Apr","May","Jun",
        "Jul","Aug","Sep","Oct","Nov","Dec"
    )
    time_format = ' %d %H:%M:%S %Y GMT' # NOTE: no month, fixed GMT
    try:
        month_number = months.index(cert_time[:3].title()) + 1
    except ValueError:
        raise ValueError('time data %r does not match '
                         'format "%%b%s"' % (cert_time, time_format))
    else:
        # found valid month
        tt = strptime(cert_time[3:], time_format)
        # return an integer, the previous mktime()-based implementation
        # returned a float (fractional seconds are always zero here).
        return timegm((tt[0], month_number) + tt[2:6])
def match_hostname(cert, hostname):
    
    
    if not cert:
        raise ValueError("empty or no certificate, match_hostname needs a "
                         "SSL socket or SSL context with either "
                         "CERT_OPTIONAL or CERT_REQUIRED")
    try:
        host_ip = _inet_paton(hostname)
    except ValueError:
        # Not an IP address (common case)
        host_ip = None
    dnsnames = []
    san = cert.get('subjectAltName', ())
    for key, value in san:
        if key == 'DNS':
            if host_ip is None and _dnsname_match(value, hostname):
                return
            dnsnames.append(value)
        elif key == 'IP Address':
            if host_ip is not None and _ipaddress_match(value, host_ip):
                return
            dnsnames.append(value)
    if not dnsnames:
        # The subject is only checked when there is no dNSName entry
        # in subjectAltName
        for sub in cert.get('subject', ()):
            for key, value in sub:
                # XXX according to RFC 2818, the most specific Common Name
                # must be used.
                if key == 'commonName':
                    if _dnsname_match(value, hostname):
                        return
                    dnsnames.append(value)
    if len(dnsnames) > 1:
        raise CertificateError("hostname %r "
            "doesn't match either of %s"
            % (hostname, ', '.join(map(repr, dnsnames))))
    elif len(dnsnames) == 1:
        raise CertificateError("hostname %r "
            "doesn't match %r"
            % (hostname, dnsnames[0]))
    else:
        raise CertificateError("no appropriate commonName or "
            "subjectAltName fields were found")
def get_server_certificate(addr, ssl_version=PROTOCOL_TLS_CLIENT,
                           ca_certs=None, timeout=_GLOBAL_DEFAULT_TIMEOUT):
    
    

    host, port = addr
    if ca_certs is not None:
        cert_reqs = CERT_REQUIRED
    else:
        cert_reqs = CERT_NONE
    context = _create_stdlib_context(ssl_version,
                                     cert_reqs=cert_reqs,
                                     cafile=ca_certs)
    with create_connection(addr, timeout=timeout) as sock:
        with context.wrap_socket(sock, server_hostname=host) as sslsock:
            dercert = sslsock.getpeercert(True)
    return DER_cert_to_PEM_cert(dercert)
def Z0(self) -> NumberLike:
        
        
        
        omega = self.frequency.w
        impedance_dict = {'te':   1j*omega*self.mu/(self.gamma),
                          'tm':   -1j*self.gamma/(omega*self.ep),\
                         }

        return impedance_dict[self.mode_type]
def from_Z0(cls, frequency: 'Frequency', z0: NumberLike, f: Number,
                ep_r=1, mu_r=1, **kw) -> Media:
        
        
        

        mu = mu_0*mu_r
        ep = epsilon_0*ep_r
        w = 2*pi*f
        a =pi/(w*mu) * 1./sqrt(1/(z0*1j)**2+ep/mu)

        kw.update(dict(frequency=frequency,a=a, m=1, n=0, ep_r=ep_r, mu_r=mu_r))

        return cls(**kw)
def __init__(
        self,
        client: XboxLiveClient,
        console: SmartglassConsole,
        coordinator: XboxUpdateCoordinator,
    ) -> None:
        
        super().__init__(coordinator)
        self.client: XboxLiveClient = client
        self._console: SmartglassConsole = console
def callback(
        self,
        obs_t: ObsType,
        obs_tp1: ObsType,
        action: ActType,
        rew: float,
        terminated: bool,
        truncated: bool,
        info: dict,
    ):
        
        
        points = self.data_callback(
            obs_t, obs_tp1, action, rew, terminated, truncated, info
        )
        for point, data_series in zip(points, self.data):
            data_series.append(point)
        self.t += 1

        xmin, xmax = max(0, self.t - self.horizon_timesteps), self.t

        for i, plot in enumerate(self.cur_plot):
            if plot is not None:
                plot.remove()
            self.cur_plot[i] = self.ax[i].scatter(
                range(xmin, xmax), list(self.data[i]), c="blue"
            )
            self.ax[i].set_xlim(xmin, xmax)

        if plt is None:
            raise DependencyNotInstalled(
                "matplotlib is not installed, run `pip install gym[other]`"
            )
        plt.pause(0.000001)
def gt_roidb(self):
        
        
        
        gt_roidb = [self._load_gta_annotation(index)
                    for index in self._image_index]

        return gt_roidb
def _play(self, message):
        
        
        LOG.info('SimpleAudioService._play')

        # Stop any existing audio playback
        self._stop_running_process()

        repeat = message.data.get('repeat', False)
        self._is_playing = True
        self._paused = False
        with self.track_lock:
            if len(self.tracks) > self.index:
                track, mime = self._get_track(self.tracks[self.index])
            else:
                return

        LOG.debug('Mime info: {}'.format(mime))

        # Indicate to audio service which track is being played
        if self._track_start_callback:
            self._track_start_callback(track)

        # Replace file:// uri's with normal paths
        track = track.replace('file://', '')
        try:
            if 'mpeg' in mime[1]:
                self.process = play_mp3(track)
            elif 'ogg' in mime[1]:
                self.process = play_ogg(track)
            elif 'wav' in mime[1]:
                self.process = play_wav(track)
            else:
                # If no mime info could be determined guess mp3
                self.process = play_mp3(track)
        except FileNotFoundError as e:
            LOG.error('Couldn\'t play audio, {}'.format(repr(e)))
            self.process = None
        except Exception as e:
            LOG.exception(repr(e))
            self.process = None

        # Wait for completion or stop request
        while (self._is_process_running() and not self._stop_signal):
            sleep(0.25)

        if self._stop_signal:
            self._stop_running_process()
            self._is_playing = False
            self._paused = False
            return
        else:
            self.process = None

        # if there are more tracks available play next
        self.index += 1
        with self.track_lock:
            if self.index < len(self.tracks) or repeat:
                if self.index >= len(self.tracks):
                    self.index = 0
                self.bus.emit(Message('SimpleAudioServicePlay',
                                      {'repeat': repeat}))
            else:
                self._track_start_callback(None)
                self._is_playing = False
                self._paused = False
def exec_cmd(cmd: List[str],
             throw_on_error: bool = True,
             logger: Optional[logging.Logger] = None
             ) -> Union[int, Tuple[int, str, str]]:
    
    
    
    child = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stdin=subprocess.PIPE,
        stderr=subprocess.PIPE,
        universal_newlines=True)
    (stdout, stderr) = child.communicate()
    exit_code = child.wait()
    if throw_on_error and exit_code != 0:
        raise ShellCommandException(
            "Non-zero exit code: %s\n\nSTDOUT:\n%s\n\nSTDERR:%s" %
            (exit_code, stdout, stderr))
    return exit_code, stdout, stderr
def get_conda_bin_executable(executable_name: str) -> str:
    
    
    
    conda_home = os.environ.get(RAY_CONDA_HOME)
    if conda_home:
        if _WIN32:
            candidate = os.path.join(conda_home, "%s.exe" % executable_name)
            if os.path.exists(candidate):
                return candidate
            candidate = os.path.join(conda_home, "%s.bat" % executable_name)
            if os.path.exists(candidate):
                return candidate
        else:
            return os.path.join(conda_home, "bin/%s" % executable_name)
    else:
        conda_home = "."
    # Use CONDA_EXE as per https://github.com/conda/conda/issues/7126
    if "CONDA_EXE" in os.environ:
        conda_bin_dir = os.path.dirname(os.environ["CONDA_EXE"])
        if _WIN32:
            candidate = os.path.join(conda_home, "%s.exe" % executable_name)
            if os.path.exists(candidate):
                return candidate
            candidate = os.path.join(conda_home, "%s.bat" % executable_name)
            if os.path.exists(candidate):
                return candidate
        else:
            return os.path.join(conda_bin_dir, executable_name)
    if _WIN32:
        return executable_name + ".bat"
    return executable_name
def protocol(self):
        
        
        
        protocol = getattr(self.args, 'protocol', 'grpc')
        return str(protocol) + ('s' if self.tls_enabled else '')
def get_worker_host(pod_args, pod_is_container, head_is_container):
        
        
        
        # Check if the current pod and head are both containerized on the same host
        # If so __docker_host__ needs to be advertised as the worker's address to the head
        worker_host = (
            __docker_host__
            if (pod_is_container and (head_is_container or in_docker()))
            and host_is_local(pod_args.host)
            else pod_args.host
        )
        return worker_host
def all_args(self) -> List[Namespace]:
        
        
        all_args = (
            ([self.pod_args['uses_before']] if self.pod_args['uses_before'] else [])
            + ([self.pod_args['uses_after']] if self.pod_args['uses_after'] else [])
            + ([self.pod_args['head']] if self.pod_args['head'] else [])
            + ([self.pod_args['gateway']] if self.pod_args['gateway'] else [])
        )
        for shard_id in self.pod_args['pods']:
            all_args += self.pod_args['pods'][shard_id]
        return all_args
def ports(self) -> List[int]:
        
        
        if self.head_port:
            return [self.head_port]
        else:
            ports = []
            for replica in self.pod_args['pods'][0]:
                if isinstance(replica.port, list):
                    ports.extend(replica.port)
                else:
                    ports.append(replica.port)
            return ports
def to_docker_compose_yaml(
        self,
        output_path: Optional[str] = None,
        network_name: Optional[str] = None,
    ):
        
        
        
        import yaml

        output_path = output_path or 'docker-compose.yml'
        network_name = network_name or 'jina-network'

        docker_compose_dict = {
            'version': '3.3',
            'networks': {network_name: {'driver': 'bridge'}},
        }
        services = {}

        service_configs = self._to_docker_compose_config()

        for service_name, service in service_configs:
            service['networks'] = [network_name]
            services[service_name] = service

        if self._include_gateway:
            service_configs = self._inner_gateway_to_docker_compose_config()

            for service_name, service in service_configs:
                service['networks'] = [network_name]
                services[service_name] = service

        docker_compose_dict['services'] = services
        with open(output_path, 'w+') as fp:
            yaml.dump(docker_compose_dict, fp, sort_keys=False)

        command = (
            'docker-compose up'
            if output_path is None
            else f'docker-compose -f {output_path} up'
        )

        self.logger.info(
            f'Docker Compose file has been created under [b]{output_path}[/b]. You can use it by running [b]{command}[/b]'
        )
def initialize(engine_list):
    
    for engine_data in engine_list:
        engine_name = engine_data['name']
        engine = engines.engines.get(engine_name)
        if engine:
            processor = get_processor(engine, engine_name)
            initialize_processor(processor)
            if processor is None:
                logger.error('Error get processor for engine %s', engine_name)
            else:
                PROCESSORS[engine_name] = processor
def collect(self, properties: ResourceTranslationProperties):
        
        
        
        resolved_config_address = get_configuration_address(properties.resource_full_address)
        matched_gateway_methods = self.cfn_resources.get(resolved_config_address, [])
        matched_gateway_methods.append(properties.translated_resource)
        self.cfn_resources[resolved_config_address] = matched_gateway_methods
        self.terraform_config[resolved_config_address] = properties.config_resource
        self.terraform_resources[properties.logical_id] = properties.resource
def fetch_deposit_address(self, code, params={}):
        
        
        
        self.load_markets()
        currency = self.currency(code)
        request = {
            'asset_symbol': currency['id'],
        }
        networkCode = self.safe_string_upper(params, 'network')
        if networkCode is not None:
            request['network'] = self.network_code_to_id(networkCode, code)
            params = self.omit(params, 'network')
        response = self.privateGetDepositsAddress(self.extend(request, params))
        #
        #    {
        #        "success": True,
        #        "result": {
        #            "id": 1915615,
        #            "user_id": 27854758,
        #            "address": "TXYB4GdKsXKEWbeSNPsmGZu4ZVCkhVh1Zz",
        #            "memo": "",
        #            "status": "active",
        #            "updated_at": "2023-01-12T06:03:46.000Z",
        #            "created_at": "2023-01-12T06:03:46.000Z",
        #            "asset_symbol": "USDT",
        #            "network": "TRC20(TRON)",
        #            "custodian": "fireblocks"
        #        }
        #    }
        #
        result = self.safe_value(response, 'result', {})
        return self.parse_deposit_address(result, currency)
def fetch_ohlcv(self, symbol: str, timeframe='1m', since: Optional[int] = None, limit: Optional[int] = None, params={}):
        
        
        
        self.load_markets()
        market = self.market(symbol)
        request = {
            'symbol': market['id'],
            'resolution': self.safe_string(self.timeframes, timeframe, timeframe),
        }
        duration = self.parse_timeframe(timeframe)
        limit = limit if limit else 2000  # max 2000
        if since is None:
            end = self.seconds()
            request['end'] = end
            request['start'] = end - limit * duration
        else:
            start = self.parse_to_int(since / 1000)
            request['start'] = start
            request['end'] = self.sum(start, limit * duration)
        response = self.publicGetHistoryCandles(self.extend(request, params))
        #
        #     {
        #         "success":true,
        #         "result":[
        #             {"time":1605393120,"open":15989,"high":15989,"low":15987.5,"close":15987.5,"volume":565},
        #             {"time":1605393180,"open":15966,"high":15966,"low":15959,"close":15959,"volume":24},
        #             {"time":1605393300,"open":15973,"high":15973,"low":15973,"close":15973,"volume":1288},
        #         ]
        #     }
        #
        result = self.safe_value(response, 'result', [])
        return self.parse_ohlcvs(result, market, timeframe, since, limit)
def cancel_all_orders(self, symbol: Str = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' cancelAllOrders() requires a symbol argument')
        self.load_markets()
        market = self.market(symbol)
        request = {
            'product_id': market['numericId'],
            # 'cancel_limit_orders': 'true',
            # 'cancel_stop_orders': 'true',
        }
        response = self.privateDeleteOrdersAll(self.extend(request, params))
        #
        #     {
        #         "result":{},
        #         "success":true
        #     }
        #
        return response
def write(self, filename: str, image) -> None:
         
        
        logger.trace("Received frame: (filename: '%s', shape: %s", filename, image.shape)
        if not self._output_dimensions:
            self._set_dimensions(image.shape[:2])
            self._writer = self._get_writer()
        self.cache_frame(filename, image)
        self._save_from_cache()
def _get_writer(self, frame_dims: Tuple[int]) -> Generator[None, np.ndarray, None]:
         
        
        audio_codec = self._audio_codec
        audio_path = None if audio_codec is None else self._source_video
        logger.debug("writer config: %s, audio_path: '%s'", self.config, audio_path)

        retval = im_ffm.write_frames(self._output_filename,
                                     size=(frame_dims[1], frame_dims[0]),
                                     fps=self._video_fps,
                                     quality=None,
                                     codec=self.config["codec"],
                                     macro_block_size=8,
                                     ffmpeg_log_level="error",
                                     ffmpeg_timeout=10,
                                     output_params=self._output_params,
                                     audio_path=audio_path,
                                     audio_codec=audio_codec)
        logger.debug("FFMPEG Writer created: %s", retval)
        retval.send(None)

        return retval
def multiscale_basic_features(
    image,
    intensity=True,
    edges=True,
    texture=True,
    sigma_min=0.5,
    sigma_max=16,
    num_sigma=None,
    num_workers=None,
    *,
    channel_axis=None,
):
    
    
    if not any([intensity, edges, texture]):
        raise ValueError(
            "At least one of `intensity`, `edges` or `textures`"
            "must be True for features to be computed."
        )
    if channel_axis is None:
        image = image[..., np.newaxis]
        channel_axis = -1
    elif channel_axis != -1:
        image = np.moveaxis(image, channel_axis, -1)

    all_results = (
        _mutiscale_basic_features_singlechannel(
            image[..., dim],
            intensity=intensity,
            edges=edges,
            texture=texture,
            sigma_min=sigma_min,
            sigma_max=sigma_max,
            num_sigma=num_sigma,
            num_workers=num_workers,
        )
        for dim in range(image.shape[-1])
    )
    features = list(itertools.chain.from_iterable(all_results))
    out = np.stack(features, axis=-1)
    return out
def _match(self, similarity_matrix, valid_rows):
    
    
    pass
def on_fit_epoch_end(trainer):
    
    if mlflow:
        sanitized_metrics = {k.replace('(', '').replace(')', ''): float(v) for k, v in trainer.metrics.items()}
        mlflow.log_metrics(metrics=sanitized_metrics, step=trainer.epoch)
def as_completed(fs, timeout=None):
    
    
    if timeout is not None:
        end_time = timeout + time.time()

    fs = set(fs)
    with _AcquireFutures(fs):
        finished = set(
                f for f in fs
                if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])
        pending = fs - finished
        waiter = _create_and_install_waiters(fs, _AS_COMPLETED)

    try:
        yield from finished

        while pending:
            if timeout is None:
                wait_timeout = None
            else:
                wait_timeout = end_time - time.time()
                if wait_timeout < 0:
                    raise TimeoutError(
                            '%d (of %d) futures unfinished' % (
                            len(pending), len(fs)))

            waiter.event.wait(wait_timeout)

            with waiter.lock:
                finished = waiter.finished_futures
                waiter.finished_futures = []
                waiter.event.clear()

            for future in finished:
                yield future
                pending.remove(future)

    finally:
        for f in fs:
            f._waiters.remove(waiter)
def wait(fs, timeout=None, return_when=ALL_COMPLETED):
    
    
    fs = set(fs)
    with _AcquireFutures(fs):
        done = {f for f in fs
                   if f._state in [CANCELLED_AND_NOTIFIED, FINISHED]}
        not_done = fs - done
        if (return_when == FIRST_COMPLETED) and done:
            return DoneAndNotDoneFutures(done, not_done)
        elif (return_when == FIRST_EXCEPTION) and done:
            if any(f for f in done
                   if not f.cancelled() and f.exception() is not None):
                return DoneAndNotDoneFutures(done, not_done)

        if len(done) == len(fs):
            return DoneAndNotDoneFutures(done, not_done)

        waiter = _create_and_install_waiters(fs, return_when)

    waiter.event.wait(timeout)
    for f in fs:
        with f._condition:
            f._waiters.remove(waiter)

    done.update(waiter.finished_futures)
    return DoneAndNotDoneFutures(done, fs - done)
def __init__(self, assets, account_username=None, account_policy='smart', host_var_callback=None):
        
        
        
        self.assets = self.clean_assets(assets)
        self.account_username = account_username
        self.account_policy = account_policy
        self.host_var_callback = host_var_callback
def trigger_webhooks_async(
    data,  # deprecated, legacy_data_generator should be used instead
    event_type,
    webhooks,
    subscribable_object=None,
    requestor=None,
    legacy_data_generator=None,
    allow_replica=False,
    pre_save_payloads=None,
    request_time=None,
):
    
    
    regular_webhooks, subscription_webhooks = group_webhooks_by_subscription(webhooks)
    deliveries = []
    if regular_webhooks:
        if legacy_data_generator:
            data = legacy_data_generator()
        elif data is None:
            raise NotImplementedError("No payload was provided for regular webhooks.")

        payload = EventPayload.objects.create(payload=data)
        deliveries.extend(
            create_event_delivery_list_for_webhooks(
                webhooks=regular_webhooks,
                event_payload=payload,
                event_type=event_type,
            )
        )
    if subscription_webhooks:
        deliveries.extend(
            create_deliveries_for_subscriptions(
                event_type=event_type,
                subscribable_object=subscribable_object,
                webhooks=subscription_webhooks,
                requestor=requestor,
                allow_replica=allow_replica,
                pre_save_payloads=pre_save_payloads,
                request_time=request_time,
            )
        )

    for delivery in deliveries:
        send_webhook_request_async.delay(delivery.id)
def async_send_command(self, **kwargs):
        
        
        return self.hass.loop.run_in_executor(
            None, ft.partial(self.send_command, **kwargs))
def buildDMG():
    
    
    
    outdir = os.path.join(WORKDIR, 'diskimage')
    if os.path.exists(outdir):
        shutil.rmtree(outdir)

    imagepath = os.path.join(outdir,
                    'python-%s-macosx'%(getFullVersion(),))
    if INCLUDE_TIMESTAMP:
        imagepath = imagepath + '%04d-%02d-%02d'%(time.localtime()[:3])
    imagepath = imagepath + '.dmg'

    os.mkdir(outdir)
    runCommand("hdiutil create -volname 'Universal MacPython %s' -srcfolder %s %s"%(
            getFullVersion(),
            shellQuote(os.path.join(WORKDIR, 'installer')),
            shellQuote(imagepath)))

    return imagepath
def setIcon(filePath, icnsPath):
    
    
    

    toolPath = os.path.join(os.path.dirname(__file__), "seticon.app/Contents/MacOS/seticon")
    dirPath = os.path.dirname(__file__)
    if not os.path.exists(toolPath) or os.stat(toolPath).st_mtime < os.stat(dirPath + '/seticon.m').st_mtime:
        # NOTE: The tool is created inside an .app bundle, otherwise it won't work due
        # to connections to the window server.
        if not os.path.exists('seticon.app/Contents/MacOS'):
            os.makedirs('seticon.app/Contents/MacOS')
        runCommand("cc -o %s %s/seticon.m -framework Cocoa"%(
            shellQuote(toolPath), shellQuote(dirPath)))

    runCommand("%s %s %s"%(shellQuote(os.path.abspath(toolPath)), shellQuote(icnsPath),
        shellQuote(filePath)))
def extractArchive(builddir, archiveName):
    
    
    
    curdir = os.getcwd()
    try:
        os.chdir(builddir)
        if archiveName.endswith('.tar.gz'):
            retval = os.path.basename(archiveName[:-7])
            if ((retval.startswith('tcl') or retval.startswith('tk'))
                    and retval.endswith('-src')):
                retval = retval[:-4]
            if os.path.exists(retval):
                shutil.rmtree(retval)
            fp = os.popen("tar zxf %s 2>&1"%(shellQuote(archiveName),), 'r')

        elif archiveName.endswith('.tar.bz2'):
            retval = os.path.basename(archiveName[:-8])
            if os.path.exists(retval):
                shutil.rmtree(retval)
            fp = os.popen("tar jxf %s 2>&1"%(shellQuote(archiveName),), 'r')

        elif archiveName.endswith('.tar'):
            retval = os.path.basename(archiveName[:-4])
            if os.path.exists(retval):
                shutil.rmtree(retval)
            fp = os.popen("tar xf %s 2>&1"%(shellQuote(archiveName),), 'r')

        elif archiveName.endswith('.zip'):
            retval = os.path.basename(archiveName[:-4])
            if os.path.exists(retval):
                shutil.rmtree(retval)
            fp = os.popen("unzip %s 2>&1"%(shellQuote(archiveName),), 'r')

        data = fp.read()
        xit = fp.close()
        if xit is not None:
            sys.stdout.write(data)
            raise RuntimeError("Cannot extract %s"%(archiveName,))

        return os.path.join(builddir, retval)

    finally:
        os.chdir(curdir)
def DatumToArray(datum):
  
  
  if datum.HasField('float_list'):
    return np.array(datum.float_list.value).astype('float32').reshape(
        datum.shape.dim)
  elif datum.HasField('uint32_list'):
    return np.array(datum.uint32_list.value).astype('uint32').reshape(
        datum.shape.dim)
  else:
    raise ValueError('Input DatumProto does not have float_list or uint32_list')
def forward(
            self,
            inputs: torch.Tensor,
            input_lengths: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        
        
        batch = inputs.size(0)

        if len(inputs.size()) == 1:  # validate, evaluation
            inputs = inputs.unsqueeze(1)
            target_lengths = inputs.size(1)

            outputs = self.forward_step(
                decoder_inputs=inputs,
                decoder_input_lengths=input_lengths,
                positional_encoding_length=target_lengths,
            )

        else:  # train
            inputs = inputs[inputs != self.eos_id].view(batch, -1)
            target_lengths = inputs.size(1)

            outputs = self.forward_step(
                decoder_inputs=inputs,
                decoder_input_lengths=input_lengths,
                positional_encoding_length=target_lengths,
            )

        return outputs, input_lengths
def find_set(x: Node) -> Node:
    
    
    
    if x != x.parent:
        x.parent = find_set(x.parent)
    return x.parent
def write(
        self,
        example: Dict[str, Any],
        key: Optional[Union[str, int, bytes]] = None,
        writer_batch_size: Optional[int] = None,
    ):
        
        
        # Utilize the keys and duplicate checking when `self._check_duplicates` is passed True
        if self._check_duplicates:
            # Create unique hash from key and store as (key, example) pairs
            hash = self._hasher.hash(key)
            self.current_examples.append((example, hash))
            # Maintain record of keys and their respective hashes for checking duplicates
            self.hkey_record.append((hash, key))
        else:
            # Store example as a tuple so as to keep the structure of `self.current_examples` uniform
            self.current_examples.append((example, ""))

        if writer_batch_size is None:
            writer_batch_size = self.writer_batch_size
        if writer_batch_size is not None and len(self.current_examples) >= writer_batch_size:
            if self._check_duplicates:
                self.check_duplicate_keys()
                # Re-intializing to empty list for next batch
                self.hkey_record = []

            self.write_examples_on_file()
def write_batch(
        self,
        batch_examples: Dict[str, List[Any]],
        writer_batch_size: Optional[int] = None,
    ):
        
        
        if batch_examples and len(next(iter(batch_examples.values()))) == 0:
            return
        schema = None if self.pa_writer is None and self.update_features else self._schema
        try_schema = self._schema if self.pa_writer is None and self.update_features else None
        typed_sequence_examples = {}
        for col in sorted(batch_examples.keys()):
            col_type = schema.field(col).type if schema is not None else None
            col_try_type = try_schema.field(col).type if try_schema is not None and col in try_schema.names else None
            typed_sequence = OptimizedTypedSequence(batch_examples[col], type=col_type, try_type=col_try_type, col=col)
            typed_sequence_examples[col] = typed_sequence
        pa_table = pa.Table.from_pydict(typed_sequence_examples)
        self.write_table(pa_table, writer_batch_size)
def bp_commands(self, frame):
        
        # self.currentbp is set in bdb in Bdb.break_here if a breakpoint was hit
        if getattr(self, "currentbp", False) and \
               self.currentbp in self.commands:
            currentbp = self.currentbp
            self.currentbp = 0
            lastcmd_back = self.lastcmd
            self.setup(frame, None)
            for line in self.commands[currentbp]:
                self.onecmd(line)
            self.lastcmd = lastcmd_back
            if not self.commands_silent[currentbp]:
                self.print_stack_entry(self.stack[self.curindex])
            if self.commands_doprompt[currentbp]:
                self.cmdloop()
            self.forget()
            return
        return 1
def do_list(self, arg):
        
        
        self.lastcmd = 'list'
        last = None
        if arg and arg != '.':
            try:
                if ',' in arg:
                    first, last = arg.split(',')
                    first = int(first.strip())
                    last = int(last.strip())
                    if last < first:
                        # assume it's a count
                        last = first + last
                else:
                    first = int(arg.strip())
                    first = max(1, first - 5)
            except ValueError:
                self.error('Error in argument: %r' % arg)
                return
        elif self.lineno is None or arg == '.':
            first = max(1, self.curframe.f_lineno - 5)
        else:
            first = self.lineno + 1
        if last is None:
            last = first + 10
        filename = self.curframe.f_code.co_filename
        breaklist = self.get_file_breaks(filename)
        try:
            lines = linecache.getlines(filename, self.curframe.f_globals)
            self._print_lines(lines[first-1:last], first, breaklist,
                              self.curframe)
            self.lineno = min(last, len(lines))
            if len(lines) < last:
                self.message('[EOF]')
        except KeyboardInterrupt:
            pass
def lookupmodule(self, filename):
        
        
        if not filename.endswith('.py'):
            # A module is passed in so convert it to equivalent file
            filename = filename.replace('.', os.sep) + '.py'

        if os.path.isabs(filename):
            if os.path.exists(filename):
                return filename
            return None

        for dirname in sys.path:
            while os.path.islink(dirname):
                dirname = os.readlink(dirname)
            fullname = os.path.join(dirname, filename)
            if os.path.exists(fullname):
                return fullname
        return None
def pm():
    
    post_mortem(sys.last_exc)
def can_fast_delete(self, objs, from_field=None):
        
        
        
        if from_field and from_field.remote_field.on_delete is not CASCADE:
            return False
        if hasattr(objs, '_meta'):
            model = type(objs)
        elif hasattr(objs, 'model') and hasattr(objs, '_raw_delete'):
            model = objs.model
        else:
            return False
        if (signals.pre_delete.has_listeners(model) or
                signals.post_delete.has_listeners(model) or
                signals.m2m_changed.has_listeners(model)):
            return False
        # The use of from_field comes from the need to avoid cascade back to
        # parent when parent delete is cascading to child.
        opts = model._meta
        return (
            all(link == from_field for link in opts.concrete_model._meta.parents.values()) and
            # Foreign keys pointing to this model.
            all(
                related.field.remote_field.on_delete is DO_NOTHING
                for related in get_candidate_relations_to_delete(opts)
            ) and (
                # Something like generic foreign key.
                not any(hasattr(field, 'bulk_related_objects') for field in opts.private_fields)
            )
        )
def related_objects(self, related, objs):
        
        
        
        return related.related_model._base_manager.using(self.using).filter(
            **{"%s__in" % related.field.name: objs}
        )
def related_objects(self, related_model, related_fields, objs):
        
        
        
        predicate = reduce(operator.or_, (
            query_utils.Q(**{'%s__in' % related_field.name: objs})
            for related_field in related_fields
        ))
        return related_model._base_manager.using(self.using).filter(predicate)
def _wait_for_function(self, function_descriptor, job_id, timeout=10):
        
        
        start_time = time.time()
        # Only send the warning once.
        warning_sent = False
        while True:
            with self.lock:
                if (self._worker.actor_id.is_nil()
                        and (function_descriptor.function_id in
                             self._function_execution_info)):
                    break
                elif not self._worker.actor_id.is_nil() and (
                        self._worker.actor_id in self._worker.actors):
                    break
            if time.time() - start_time > timeout:
                warning_message = ("This worker was asked to execute a "
                                   "function that it does not have "
                                   "registered. You may have to restart "
                                   "Ray.")
                if not warning_sent:
                    ray._private.utils.push_error_to_driver(
                        self._worker,
                        ray_constants.WAIT_FOR_FUNCTION_PUSH_ERROR,
                        warning_message,
                        job_id=job_id)
                warning_sent = True
            time.sleep(0.001)
def test_container_deepcopy():
    
    
    
    t = theano.tensor.scalar()
    v = numpy.asarray(0.)
    c = Container(t, [v])
    assert isinstance(c.storage[0], numpy.ndarray)
    d = deepcopy(c)
    assert isinstance(d.storage[0], numpy.ndarray)
def list_metrics(with_community_metrics=True, with_details=False):
    
    
    metrics = huggingface_hub.list_metrics()
    if not with_community_metrics:
        metrics = [metric for metric in metrics if "/" not in metric.id]
    if not with_details:
        metrics = [metric.id for metric in metrics]
    return metrics
def get_dataset_config_info(
    path: str,
    config_name: Optional[str] = None,
    data_files: Optional[Union[str, Sequence[str], Mapping[str, Union[str, Sequence[str]]]]] = None,
    download_config: Optional[DownloadConfig] = None,
    download_mode: Optional[Union[DownloadMode, str]] = None,
    revision: Optional[Union[str, Version]] = None,
    token: Optional[Union[bool, str]] = None,
    use_auth_token="deprecated",
    **config_kwargs,
) -> DatasetInfo:
    

    
    if use_auth_token != "deprecated":
        warnings.warn(
            "'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n"
            f"You can remove this warning by passing 'token={use_auth_token}' instead.",
            FutureWarning,
        )
        token = use_auth_token

    builder = load_dataset_builder(
        path,
        name=config_name,
        data_files=data_files,
        download_config=download_config,
        download_mode=download_mode,
        revision=revision,
        token=token,
        **config_kwargs,
    )
    info = builder.info
    if info.splits is None:
        download_config = download_config.copy() if download_config else DownloadConfig()
        if token is not None:
            download_config.token = token
        builder._check_manual_download(
            StreamingDownloadManager(base_path=builder.base_path, download_config=download_config)
        )
        try:
            info.splits = {
                split_generator.name: {"name": split_generator.name, "dataset_name": path}
                for split_generator in builder._split_generators(
                    StreamingDownloadManager(base_path=builder.base_path, download_config=download_config)
                )
            }
        except Exception as err:
            raise SplitsNotFoundError("The split names could not be parsed from the dataset config.") from err
    return info
def __init__(
      self,
      d_ff: int,
      *,
      inner_dropout: float = 0.0,
      output_dropout: float = 0.0,
      activation: Callable[[tf.Tensor], tf.Tensor] = tf.keras.activations.gelu,
      kernel_initializer: _InitializerType = _DEFAULT_KERNEL_INITIALIZER,
      bias_initializer: _InitializerType = _DEFAULT_BIAS_INITIALIZER,
      name: str = "feed_forward",
      **kwargs):
    
    
    super().__init__(name=name, **kwargs)
    self.activation = activation
    self.kernel_initializer = kernel_initializer
    self.bias_initializer = bias_initializer

    self.intermediate_layer = tf.keras.layers.Dense(
        d_ff,
        kernel_initializer=tf_utils.clone_initializer(self.kernel_initializer),
        bias_initializer=tf_utils.clone_initializer(self.bias_initializer),
        name="intermediate")
    self.inner_dropout_layer = tf.keras.layers.Dropout(
        inner_dropout)
    self.output_dropout_layer = tf.keras.layers.Dropout(output_dropout)
def __init__(
      self,
      experts: FeedForwardExperts,
      router: MaskedRouter,
      *,
      train_capacity_factor: float = 1.0,
      eval_capacity_factor: float = 1.0,
      examples_per_group: float = 1.0,
      name: str = "moe",
      **kwargs):
    
    
    super().__init__(name=name, **kwargs)
    self._experts = experts
    self._router = router

    self.num_experts = experts.num_experts
    assert experts.num_experts == router.num_experts

    self._train_capacity_factor = train_capacity_factor
    self._eval_capacity_factor = eval_capacity_factor
    self._examples_per_group = examples_per_group
def call(self,
           inputs: tf.Tensor,
           *,
           training: Optional[bool] = None) -> tf.Tensor:
    
    
    if training is None:
      training = tf_keras.backend.learning_phase()

    # inputs shape [batch_size, seq_length, hidden_dim]
    batch_size, seq_length, hidden_dim = inputs.shape
    if batch_size is not None:
      if self._examples_per_group > batch_size:
        raise ValueError(
            f"examples_per_group={self._examples_per_group} is larger than the "
            "number of examples available in the local (per-device) batch_size="
            f"{batch_size}. Either decrease examples_per_group or increase the "
            "batch_size.")
    tokens_per_group = int(seq_length * self._examples_per_group)

    if training:
      capacity_factor = self._train_capacity_factor
    else:
      capacity_factor = self._eval_capacity_factor
    # Each group will send expert_capacity tokens to each expert.
    expert_capacity = int(
        round(capacity_factor * tokens_per_group / self.num_experts))

    # Reshape batch and sequence/token dimensions for expert routing.
    x = tf.reshape(inputs, (-1, tokens_per_group, hidden_dim))

    x = self._mask_and_dispatch_to_experts(x, expert_capacity, training)

    # Return to original input shape.
    x = tf.reshape(x, (-1, seq_length, hidden_dim))
    return x
def cuirfft(inp, norm=None, is_odd=False):
    
    
    

    if is_odd not in (True, False):
        raise ValueError("Invalid value %s for id_odd, must be True or False" % is_odd)

    s = inp.shape[1:-1]
    if is_odd:
        s = T.set_subtensor(s[-1], (s[-1] - 1) * 2 + 1)
    else:
        s = T.set_subtensor(s[-1], (s[-1] - 1) * 2)

    cond_norm = _unitary(norm)
    scaling = 1
    if cond_norm is None:
        scaling = s.prod().astype('float32')
    elif cond_norm == "ortho":
        scaling = T.sqrt(s.prod().astype('float32'))

    return cuirfft_op(inp, s) / scaling
def get_current_user_id(token: str = Security(oauth2_scheme)) -> str | None:
    
    if not settings.use_auth:
        return None
    if token is None:
        raise HTTPException(status_code=HTTP_403_FORBIDDEN, detail="Not authenticated")

    # Generate a key from the auth secret
    key: bytes = derive_key()

    # Decrypt the JWE token
    try:
        token: bytes = jwe.decrypt(token, key)
    except JWEError:
        raise HTTPException(status_code=401, detail="Invalid token")

    payload: dict = json.loads(token.decode())
    user_id = payload.get("user_id")
    exp = payload.get("exp")

    if not user_id or not exp:
        raise HTTPException(status_code=401, detail="Invalid token")
    if datetime.utcnow() >= datetime.fromtimestamp(exp):
        raise HTTPException(status_code=401, detail="Token expired")

    return user_id
def create_access_token(user_id: str) -> str:
    
    expires_delta = timedelta(minutes=settings.auth_access_token_expire_minutes)
    expire = datetime.utcnow() + expires_delta

    to_encode = {
        "user_id": user_id,
        "exp": expire.timestamp(),
        "type": "access",
    }

    # Generate a key from the auth secret
    key = derive_key()

    # Encrypt the payload using JWE
    to_encode_bytes: bytes = json.dumps(to_encode).encode()
    token: bytes = jwe.encrypt(to_encode_bytes, key)
    return token.decode()
def async_store_trace(hass, trace, stored_traces):
    
    key = trace.key
    if key:
        traces = hass.data[DATA_TRACE]
        if key not in traces:
            traces[key] = LimitedSizeDict(size_limit=stored_traces)
        else:
            traces[key].size_limit = stored_traces
        traces[key][trace.run_id] = trace
def ldflags(libs=True, flags=False, libs_dir=False, include_dir=False):
    
    
    ldflags_str = theano.config.blas.ldflags
    return _ldflags(ldflags_str=ldflags_str,
                    libs=libs,
                    flags=flags,
                    libs_dir=libs_dir,
                    include_dir=include_dir)
def dumps(self):
         
        
        result = []
        for (name, value) in self.values_list:
            # It is important to discard None values, so migrations in settings can be done
            # without breaking all existing packages SHAs, by adding a first "None" option
            # that doesn't change the final sha
            if value != "None":
                result.append("%s=%s" % (name, value))
        return '\n'.join(result)
def _async_internal_update_from_websocket_event(self, event):
        
        if event.event_type == EVENT_CONNECTION_LOST:
            self._online = False
        elif event.event_type == EVENT_CONNECTION_RESTORED:
            self._online = True

        # It's uncertain whether SimpliSafe events will still propagate down the
        # websocket when the base station is offline. Just in case, we guard against
        # further action until connection is restored:
        if not self._online:
            return

        if event.sensor_type:
            sensor_type = event.sensor_type.name
        else:
            sensor_type = None

        self._attrs.update(
            {
                ATTR_LAST_EVENT_INFO: event.info,
                ATTR_LAST_EVENT_SENSOR_NAME: event.sensor_name,
                ATTR_LAST_EVENT_SENSOR_TYPE: sensor_type,
                ATTR_LAST_EVENT_TIMESTAMP: event.timestamp,
            }
        )

        self.async_update_from_websocket_event(event)
def dilated_convolution_2d(x, W, b=None, stride=1, pad=0, dilate=1,
                           cover_all=False):
    

    
    no_data_grad = not (isinstance(x, variable.Variable) and x.requires_grad)
    func = DilatedConvolution2DFunction(stride, pad, dilate, cover_all,
                                        no_data_grad)
    if b is None:
        return func(x, W)
    else:
        return func(x, W, b)
def deconvolution_2d(x, W, b=None, stride=1, pad=0, outsize=None, **kwargs):
    


    
    argument.check_unexpected_kwargs(
        kwargs, deterministic="deterministic argument is not "
        "supported anymore. "
        "Use chainer.using_config('cudnn_deterministic', value) "
        "context where value is either `True` or `False`.")
    dilate, groups = argument.parse_kwargs(kwargs,
                                           ('dilate', 1), ('groups', 1))

    func = Deconvolution2DFunction(stride, pad, outsize, dilate=dilate,
                                   groups=groups)
    if b is None:
        args = x, W
    else:
        args = x, W, b
    y, = func.apply(args)
    return y
def softmax_cross_entropy(
        x, t, use_cudnn=True, normalize=True, cache_score=True):
    

    
    return SoftmaxCrossEntropy(use_cudnn, normalize, cache_score)(x, t)
def roi_average_align_2d(
        x, rois, roi_indices, outsize, spatial_scale, sampling_ratio=None
):
    

    
    return ROIAverageAlign2D(outsize, spatial_scale, sampling_ratio)(
        x, rois, roi_indices)
def roi_max_align_2d(
        x, rois, roi_indices, outsize, spatial_scale, sampling_ratio=None
):
    

    
    return ROIMaxAlign2D(outsize, spatial_scale, sampling_ratio)(
        x, rois, roi_indices)
def convolution_2d(x, W, b=None, stride=1, pad=0, cover_all=False, **kwargs):
    

      # NOQA
    dilate, groups = argument.parse_kwargs(
        kwargs, ('dilate', 1), ('groups', 1),
        deterministic="deterministic argument is not supported anymore. "
        "Use chainer.using_config('cudnn_deterministic', value) "
        "context where value is either `True` or `False`.")

    fnode = Convolution2DFunction(stride, pad, cover_all, dilate=dilate,
                                  groups=groups)
    if b is None:
        args = x, W
    else:
        args = x, W, b
    y, = fnode.apply(args)
    return y
def resize_images(x, output_shape, mode='bilinear', align_corners=True):
    

    
    return ResizeImages(output_shape, mode, align_corners).apply((x,))[0]
def rsqrt(x):
    
    
    xp = backend.get_array_module(x)
    if xp is numpy or xp is chainerx:
        return 1.0 / sqrt(x)

    # CuPy provides `rsqrt` which is faster than `1.0 / sqrt(x)`.
    return RsqrtGPU().apply((x,))[0]
def backward(self, indices, grad_outputs):
        inputs = self.get_retained_inputs()
        
        
        

        g, = grad_outputs

        fwd_in_subs = self.in_subs.split(',')
        fwd_out_sub = self.out_sub
        return tuple(
            inputs[i] * 0. +  # it seems a bug of gradient_check
            DiagEinSum(
                in_subs=','.join([
                    (fwd_out_sub if j == i else s)
                    for j, s in enumerate(fwd_in_subs)
                ]),
                out_sub=fwd_in_subs[i],
                out_shape=inputs[i].shape,
            ).apply(tuple(
                (g if j == i else x)
                for j, x in enumerate(inputs)
            ))[0]
            for i in indices
        )
def simplified_dropconnect(x, W, b=None, ratio=.5, train=True, mask=None,
                           use_batchwise_mask=True):
    
    
    if not train:
        ratio = 0
    if b is None:
        return SimplifiedDropconnect(ratio, mask, use_batchwise_mask)(x, W)
    else:
        return SimplifiedDropconnect(ratio, mask, use_batchwise_mask)(x, W, b)
def batch_normalization(x, gamma, beta, **kwargs):
    

      # NOQA

    argument.check_unexpected_kwargs(
        kwargs, train='train argument is not supported anymore. '
        'Use chainer.using_config')
    eps, running_mean, running_var, decay, axis = argument.parse_kwargs(
        kwargs, ('eps', 2e-5), ('running_mean', None),
        ('running_var', None), ('decay', 0.9), ('axis', None))

    return BatchNormalization(eps, running_mean, running_var, decay,
                              axis).apply((x, gamma, beta))[0]
def fixed_batch_normalization(x, gamma, beta, fixed_mean, fixed_var, eps=2e-5):
    

    
    return BatchNormalizationFunction(fixed_mean, fixed_var, False, 0.0,
                                      eps)(x, gamma, beta)
def batch_renormalization(x, gamma, beta, rmax, dmax, eps=2e-5,
                          running_mean=None, running_var=None, decay=0.9,
                          update_statistics=False):
    

    
    if running_mean is None:
        raise TypeError('running_mean is required')
    if running_var is None:
        raise TypeError('running_var is required')
    return BatchRenormalizationFunction(
        eps, running_mean, running_var, decay, rmax, dmax, update_statistics
    )(x, gamma, beta)
def _parse_einsum_input(operands):
    
    

    if len(operands) == 0:
        raise ValueError("No input operands")

    if isinstance(operands[0], str):
        subscripts = operands[0].replace(" ", "")
        operands = operands[1:]

        # Ensure all characters are valid
        for s in subscripts:
            if s in '.,->':
                continue
            if s not in einsum_symbols:
                raise ValueError("Character %s is not a valid symbol." % s)

        # Check for proper "->"
        if ("-" in subscripts) or (">" in subscripts):
            invalid = (subscripts.count("-") > 1) or (subscripts.count(">") > 1)
            if invalid or (subscripts.count("->") != 1):
                raise ValueError("Subscripts can only contain one '->'.")

        # Parse "..."
        subscripts = subscripts.replace("...", "@")
        if "." in subscripts:
            raise ValueError("Invalid Ellipses.")

    else:
        tmp_operands = list(operands)
        operand_list = []
        subscript_list = []
        for p in range(len(operands) // 2):
            operand_list.append(tmp_operands.pop(0))
            subscript_list.append(tmp_operands.pop(0))

        output_list = tmp_operands[-1] if len(tmp_operands) else None
        operands = operand_list
        subscripts = ""
        last = len(subscript_list) - 1
        for num, sub in enumerate(subscript_list):
            for s in sub:
                if s is Ellipsis:
                    subscripts += "@"
                elif isinstance(s, int):
                    subscripts += einsum_symbols[s]
                else:
                    raise TypeError("For this input type lists must contain "
                                    "either int or Ellipsis")
            if num != last:
                subscripts += ","

        if output_list is not None:
            subscripts += "->"
            for s in output_list:
                if s is Ellipsis:
                    subscripts += "@"
                elif isinstance(s, int):
                    subscripts += einsum_symbols[s]
                else:
                    raise TypeError("For this input type lists must contain "
                                    "either int or Ellipsis")

    # Build output string if does not exist
    if "->" in subscripts:
        input_subscripts, output_subscript = subscripts.split("->")

        # Make sure output subscripts are in the input
        for char in output_subscript:
            if char not in input_subscripts:
                raise ValueError("Output character %s did not appear in the input"
                                 % ('...' if char == '@' else char))

    else:
        input_subscripts = subscripts
        # Build output subscripts
        tmp_subscripts = subscripts.replace(",", "")
        output_subscript = ""
        for s in sorted(set(tmp_subscripts)):
            if s == '@' or tmp_subscripts.count(s) == 1:
                output_subscript += s

    # Make sure number operands is equivalent to the number of terms
    if len(input_subscripts.split(',')) != len(operands):
        raise ValueError("Number of einsum subscripts must be equal to the "
                         "number of operands.")

    return (input_subscripts, output_subscript, operands)
def ifft(x):
    

    
    real, imag = x
    return FFT('ifft').apply((real, imag))
def broadcast_to(x, shape):
    

    
    if x.shape == shape:
        return chainer.as_variable(x)

    y, = BroadcastTo(shape).apply((x,))
    return y
def permutate(x, indices, axis=0, inv=False):
    

    
    return Permutate(axis=axis, inv=inv)(x, indices)
def unpooling_1d(x, ksize, stride=None, pad=0):
    

    
    if len(x.shape[2:]) != 1:
        raise ValueError(
            'The number of dimensions under channel dimension of the input '
            '\'x\' should be 1. But the actual ndim was {}.'.fromat(
                len(x.shape[2:])))
    return UnpoolingND(1, ksize, stride=stride, pad=pad)(x)
def squeeze(x, axis=None):
    

    
    y, = Squeeze(axis).apply((x,))
    return y
def hinge(x, t, norm='L1', reduce='mean'):
    

    
    return Hinge(norm, reduce)(x, t)
def prelu(x, W):
    

    
    return PReLUFunction().apply((x, W))[0]
def log_softmax(x, axis=1):
    

    
    return LogSoftmax(axis=axis).apply((x,))[0]
def decorrelated_batch_normalization(x, **kwargs):
    

    
    groups, eps, running_mean, running_projection, decay = \
        argument.parse_kwargs(
            kwargs, ('groups', 16), ('eps', 2e-5), ('running_mean', None),
            ('running_projection', None), ('decay', 0.9))

    f = DecorrelatedBatchNormalization(
        groups, eps, running_mean, running_projection, decay)
    return f.apply((x,))[0]
def matmul(a, b, transa=False, transb=False):
    

    
    if backend.get_array_module(a, b) is chainerx:
        # TODO(sonots): Support transa and transb in ChainerX
        # TODO(sonots): Support dtype promotion in ChainerX
        # TODO(sonots): Support ndim > 2 in ChainerX
        if (not transa and not transb
                and a.dtype == b.dtype
                and a.ndim == 2 and b.ndim == 2):
            return function._chainerx_op(chainerx.dot, a, b)

    return MatMul(transa=transa, transb=transb).apply((a, b))[0]
def transpose_sequence(xs):
    
    
    ys = TransposeSequence()(*xs)
    if not isinstance(ys, tuple):
        ys = (ys,)
    return ys
def leaky_relu(x, slope=0.2):
    

    
    return LeakyReLU(slope)(x)
def negative_sampling(x, t, W, sampler, sample_size, reduce='sum', **kwargs):
    

      # NOQA
    return_samples = False
    if kwargs:
        return_samples, = argument.parse_kwargs(
            kwargs, ('return_samples', return_samples))

    func = NegativeSamplingFunction(sampler, sample_size, reduce)
    out = func.apply((x, t, W))[0]

    if return_samples:
        return out, func._samples
    return out
def rrelu(x, l=1. / 8, u=1. / 3, **kwargs):
    
    
    r = None
    return_r = False
    if kwargs:
        r, return_r = argument.parse_kwargs(
            kwargs, ('r', r), ('return_r', r),
            train='train argument is not supported anymore.'
                  'Use chainer.using_config')

    func = RReLU(l, u, r)
    out, = func.apply((x,))
    r = func.r

    if return_r:
        return out, r
    return out
def bernoulli_nll(x, y, reduce='sum'):
    

    
    if reduce not in ('sum', 'mean', 'no'):
        raise ValueError(
            'only \'sum\', \'mean\' and \'no\' are valid for \'reduce\', but '
            '\'%s\' is given' % reduce)

    loss = softplus.softplus(y) - x * y
    if reduce == 'sum':
        return sum.sum(loss)
    elif reduce == 'mean':
        return average.average(loss)
    else:
        return loss
def gaussian_nll(x, mean, ln_var):
    

    
    assert isinstance(x, variable.Variable)
    assert isinstance(mean, variable.Variable)
    assert isinstance(ln_var, variable.Variable)

    D = x.size
    x_prec = exponential.exp(-ln_var)
    x_diff = x - mean
    x_power = (x_diff * x_diff) * x_prec * -0.5
    return (sum.sum(ln_var) + D * math.log(2 * math.pi)) / 2 - sum.sum(x_power)
def sparse_matmul(a, b, transa=False, transb=False):
    

    
    if (isinstance(a, utils.CooMatrix) and
            isinstance(b, (chainer.Variable, numpy.ndarray, cuda.ndarray))):
        return CooMatMul(a.row, a.col, a.shape, a.order,
                         transa=transa,
                         transb=transb,
                         transc=False).apply((a.data, b))[0]
    elif (isinstance(a, (chainer.Variable, numpy.ndarray, cuda.ndarray)) and
          isinstance(b, utils.CooMatrix)):
        return CooMatMul(b.row, b.col, b.shape, b.order,
                         transa=not transb,
                         transb=not transa,
                         transc=True).apply((b.data, a))[0]
    else:
        msg = 'This combination of type of inputs is not supported.\n'
        msg += '    a: {}\n'.format(type(a))
        msg += '    b: {}\n'.format(type(b))
        raise ValueError(msg)
def max_pooling_1d(x, ksize, stride=None, pad=0):
    

    
    if len(x.shape[2:]) != 1:
        raise ValueError(
            'The number of dimensions under channel dimension of the input '
            '\'x\' should be 1. But the actual ndim was {}.'.fromat(
                len(x.shape[2:])))
    return MaxPoolingND(1, ksize, stride=stride, pad=pad)(x)
def inv(a):
    
    
    return Inv()(a)
def linear(x, W, b=None):
    

    
    if b is None:
        return LinearFunction()(x, W)
    else:
        return LinearFunction()(x, W, b)
def deconvolution_1d(x, W, b=None, stride=1, pad=0, outsize=None):
    
    
    
    if len(x.shape[2:]) != 1:
        raise ValueError(
            'The number of dimensions under channel dimension of the input '
            '\'x\' should be 1. But the actual ndim was {}.'.fromat(
                len(x.shape[2:])))

    func = DeconvolutionND(1, stride, pad, outsize)
    if b is None:
        return func(x, W)
    else:
        return func(x, W, b)
def local_response_normalization(x, n=5, k=2, alpha=1e-4, beta=.75):
    

    
    return LocalResponseNormalization(n, k, alpha, beta).apply((x,))[0]
def convolution_1d(x, W, b=None, stride=1, pad=0, cover_all=False):
    

    
    if len(x.shape[2:]) != 1:
        raise ValueError(
            'The number of dimensions under channel dimension of the input '
            '\'x\' should be 1. But the actual ndim was {}.'.fromat(
                len(x.shape[2:])))

    func = ConvolutionND(1, stride, pad, cover_all)
    if b is None:
        return func(x, W)
    else:
        return func(x, W, b)
def average_pooling_nd(x, ksize, stride=None, pad=0, pad_value=0):
    

    
    if backend.get_array_module(x) is chainerx:
        if pad_value == 0:
            pad_mode = 'zero'
        elif pad_value is None:
            pad_mode = 'ignore'
        else:
            raise ValueError(
                'pad_value must be either 0 or None, not {}.'.format(
                    pad_value))
        return function._chainerx_op(
            lambda a: chainerx.average_pool(a, ksize, stride, pad, pad_mode), x)

    ndim = len(x.shape[2:])
    return AveragePoolingND(
        ndim, ksize, stride=stride, pad=pad, pad_value=pad_value
    ).apply((x,))[0]
def average_pooling_1d(x, ksize, stride=None, pad=0):
    

    
    if len(x.shape[2:]) != 1:
        raise ValueError(
            'The number of dimensions under channel dimension of the input '
            '\'x\' should be 1. But the actual ndim was {}.'.fromat(
                len(x.shape[2:])))
    return AveragePoolingND(1, ksize, stride=stride, pad=pad)(x)
def im2col(x, ksize, stride=1, pad=0, cover_all=False, dilate=1):
    

    
    return Im2Col(ksize, stride, pad, cover_all, dilate).apply((x,))[0]
def reshape(x, shape):
    

    
    if x.shape == shape:
        return chainer.as_variable(x)
    y, = Reshape(shape).apply((x,))
    return y
def size(self):
        
        
        return Variable(len(self), '{0}.size'.format(self.name))
def expect_broadcast_shapes(*shape_types, **kwargs):
    

    
    ignore_tail, = argument.parse_kwargs(kwargs, ('ignore_tail', 0))
    shapes = [eval(s) for s in shape_types]
    error = None
    try:
        # simulate the shape calculation using zero-sized arrays
        numpy.broadcast(*[
            numpy.empty(s[:len(s) - ignore_tail] + (0,)) for s in shapes
        ])
    except ValueError:
        if ignore_tail > 0:
            msg = ('cannot broadcast inputs of the following shapes '
                   'along all axes but the last {}:'.format(ignore_tail))
        else:
            msg = 'cannot broadcast inputs of the following shapes:'
        msgs = [msg]
        for shape_type, shape in six.moves.zip(shape_types, shapes):
            msgs.append('{} = {}'.format(shape_type, shape))
        error = InvalidType('', '', msg='\n'.join(msgs))
    if error is not None:
        raise error
def expect_broadcast_shapes(*shape_types):
    

    
    shapes = [eval(s) for s in shape_types]
    error = None
    try:
        # simulate the shape calculation using zero-sized arrays
        numpy.broadcast(*[numpy.empty(s + (0,)) for s in shapes])
    except ValueError:
        msgs = ['cannot broadcast inputs of the following shapes:']
        for shape_type, shape in six.moves.zip(shape_types, shapes):
            msgs.append('{} = {}'.format(shape_type, shape))
        error = InvalidType('', '', msg='\n'.join(msgs))
    if error is not None:
        raise error
def CPS_send_status(self, artist='', track='', album='', image='',
                        uri='', track_length=None, elapsed_time=None,
                        playlist_position=None,
                        status=CPSTrackStatus.DISAMBIGUATION, **kwargs):
        
        
        data = {'skill': self.name,
                'uri': uri,
                'artist': artist,
                'album': album,
                'track': track,
                'image': image,
                'track_length': track_length,
                'elapsed_time': elapsed_time,
                'playlist_position': playlist_position,
                'status': status
                }
        data = {**data, **kwargs}  # Merge extra arguments
        self.bus.emit(Message('play:status', data))
def CPS_send_tracklist(self, tracklist):
        
        
        tracklist = tracklist or []
        if not isinstance(tracklist, list):
            tracklist = [tracklist]
        for idx, track in enumerate(tracklist):
            self.CPS_send_status(playlist_position=idx, **track)
def _sys_version(sys_version=None):

     

    
    # Get the Python version
    if sys_version is None:
        sys_version = sys.version

    # Try the cache first
    result = _sys_version_cache.get(sys_version, None)
    if result is not None:
        return result

    # Parse it
    if sys_version[:10] == 'IronPython':
        # IronPython
        name = 'IronPython'
        match = _ironpython_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse IronPython sys.version: %s' %
                repr(sys_version))
        version, alt_version, compiler = match.groups()
        branch = ''
        revision = ''
        buildno = ''
        builddate = ''

    elif sys.platform[:4] == 'java':
        # Jython
        name = 'Jython'
        match = _jython_sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse Jython sys.version: %s' %
                repr(sys_version))
        version, = match.groups()
        branch = ''
        revision = ''
        compiler = sys.platform
        buildno = ''
        builddate = ''

    else:
        # CPython
        match = _sys_version_parser.match(sys_version)
        if match is None:
            raise ValueError(
                'failed to parse CPython sys.version: %s' %
                repr(sys_version))
        version, buildno, builddate, buildtime, compiler = \
              match.groups()
        if hasattr(sys, 'subversion'):
            # sys.subversion was added in Python 2.5
            name, branch, revision = sys.subversion
        else:
            name = 'CPython'
            branch = ''
            revision = ''
        builddate = builddate + ' ' + buildtime

    # Add the patchlevel version if missing
    l = version.split('.')
    if len(l) == 2:
        l.append('0')
        version = '.'.join(l)

    # Build and cache the result
    result = (name, version, branch, revision, buildno, builddate, compiler)
    _sys_version_cache[sys_version] = result
    return result
def _syscmd_ver(system='', release='', version='',

               supported_platforms=('win32', 'win16', 'dos')):

     

    
    if sys.platform not in supported_platforms:
        return system, release, version

    # Try some common cmd strings
    for cmd in ('ver', 'command /c ver', 'cmd /c ver'):
        try:
            pipe = popen(cmd)
            info = pipe.read()
            if pipe.close():
                raise OSError('command failed')
            # XXX How can I suppress shell errors from being written
            #     to stderr ?
        except OSError as why:
            #print 'Command %s failed: %s' % (cmd, why)
            continue
        else:
            break
    else:
        return system, release, version

    # Parse the output
    info = info.strip()
    m = _ver_output.match(info)
    if m is not None:
        system, release, version = m.groups()
        # Strip trailing dots from version and release
        if release[-1] == '.':
            release = release[:-1]
        if version[-1] == '.':
            version = version[:-1]
        # Normalize the version and build strings (eliminating additional
        # zeros)
        version = _norm_version(version)
    return system, release, version
def relabel_from_one(label_field):
    
    
    return relabel_sequential(label_field, offset=1)
def relabel_sequential(label_field, offset=1):
    
    
    if offset <= 0:
        raise ValueError("Offset must be strictly positive.")
    if np.min(label_field) < 0:
        raise ValueError("Cannot relabel array that contains negative values.")
    offset = int(offset)
    in_vals = np.unique(label_field)
    if in_vals[0] == 0:
        # always map 0 to 0
        out_vals = np.concatenate(
            [[0], np.arange(offset, offset+len(in_vals)-1)]
        )
    else:
        out_vals = np.arange(offset, offset+len(in_vals))
    input_type = label_field.dtype

    # Some logic to determine the output type:
    #  - we don't want to return a smaller output type than the input type,
    #  ie if we get uint32 as labels input, don't return a uint8 array.
    #  - but, in some cases, using the input type could result in overflow. The
    #  input type could be a signed integer (e.g. int32) but
    #  `np.min_scalar_type` will always return an unsigned type. We check for
    #  that by casting the largest output value to the input type. If it is
    #  unchanged, we use the input type, else we use the unsigned minimum
    #  required type
    required_type = np.min_scalar_type(out_vals[-1])
    if input_type.itemsize < required_type.itemsize:
        output_type = required_type
    else:
        if input_type.type(out_vals[-1]) == out_vals[-1]:
            output_type = input_type
        else:
            output_type = required_type
    out_array = np.empty(label_field.shape, dtype=output_type)
    out_vals = out_vals.astype(output_type)
    map_array(label_field, in_vals, out_vals, out=out_array)
    fw_map = ArrayMap(in_vals, out_vals)
    inv_map = ArrayMap(out_vals, in_vals)
    return out_array, fw_map, inv_map
def cer(self, decode, target):
    
    
    return distance.edit_distance(decode, target)
def pyplot():
    
    
    pyplot = pytest.importorskip('matplotlib.pyplot')
    pyplot.close('all')
    yield pyplot
    pyplot.close('all')
def normalize_image(image: tf.Tensor,
                    offset: Sequence[float] = MEAN_NORM,
                    scale: Sequence[float] = STDDEV_NORM) -> tf.Tensor:
  
  
  with tf.name_scope('normalize_image'):
    image = tf.image.convert_image_dtype(image, dtype=tf.float32)
    return normalize_scaled_float_image(image, offset, scale)
def resize_and_crop_masks(masks, image_scale, output_size, offset):
  
  
  with tf.name_scope('resize_and_crop_masks'):
    mask_size = tf.cast(tf.shape(masks)[1:3], tf.float32)
    num_channels = tf.shape(masks)[3]
    # Pad masks to avoid empty mask annotations.
    masks = tf.concat([
        tf.zeros([1, mask_size[0], mask_size[1], num_channels],
                 dtype=masks.dtype), masks
    ],
                      axis=0)

    scaled_size = tf.cast(image_scale * mask_size, tf.int32)
    scaled_masks = tf.image.resize(
        masks, scaled_size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
    offset = tf.cast(offset, tf.int32)
    scaled_masks = scaled_masks[
        :,
        offset[0]:offset[0] + output_size[0],
        offset[1]:offset[1] + output_size[1],
        :]

    output_masks = tf.image.pad_to_bounding_box(
        scaled_masks, 0, 0, output_size[0], output_size[1])
    # Remove padding.
    output_masks = output_masks[1::]
    return output_masks
def center_crop_image(
    image, center_crop_fraction: float = CENTER_CROP_FRACTION):
  
  
  with tf.name_scope('center_crop_image'):
    image_size = tf.cast(tf.shape(image)[:2], dtype=tf.float32)
    crop_size = (
        center_crop_fraction * tf.math.minimum(image_size[0], image_size[1]))
    crop_offset = tf.cast((image_size - crop_size) / 2.0, dtype=tf.int32)
    crop_size = tf.cast(crop_size, dtype=tf.int32)
    cropped_image = image[
        crop_offset[0]:crop_offset[0] + crop_size,
        crop_offset[1]:crop_offset[1] + crop_size, :]
    return cropped_image
def random_crop_image(
    image,
    aspect_ratio_range=(3.0 / 4.0, 4.0 / 3.0),
    area_range=(0.08, 1.0),
    max_attempts=10,
    seed=1,
):
  
  
  with tf.name_scope('random_crop_image'):
    crop_offset, crop_size, _ = tf.image.sample_distorted_bounding_box(
        tf.shape(image),
        tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4]),
        seed=seed,
        min_object_covered=area_range[0],
        aspect_ratio_range=aspect_ratio_range,
        area_range=area_range,
        max_attempts=max_attempts,
    )
    cropped_image = tf.slice(image, crop_offset, crop_size)
    return cropped_image
def resize_and_crop_image(
    image,
    desired_size,
    padded_size,
    aug_scale_min=1.0,
    aug_scale_max=1.0,
    seed=1,
    method=tf.image.ResizeMethod.BILINEAR,
    keep_aspect_ratio=True,
    centered_crop=False,
):
  
  
  with tf.name_scope('resize_and_crop_image'):
    image_size = tf.cast(tf.shape(image)[0:2], tf.float32)

    random_jittering = (
        isinstance(aug_scale_min, tf.Tensor)
        or isinstance(aug_scale_max, tf.Tensor)
        or not math.isclose(aug_scale_min, 1.0)
        or not math.isclose(aug_scale_max, 1.0)
    )

    if random_jittering:
      random_scale = tf.random.uniform(
          [], aug_scale_min, aug_scale_max, seed=seed
      )
      scaled_size = tf.round(random_scale * tf.cast(desired_size, tf.float32))
    else:
      scaled_size = tf.cast(desired_size, tf.float32)

    if keep_aspect_ratio:
      scale = tf.minimum(
          scaled_size[0] / image_size[0], scaled_size[1] / image_size[1]
      )
      scaled_size = tf.round(image_size * scale)

    # Computes 2D image_scale.
    image_scale = scaled_size / image_size

    # Selects non-zero random offset (x, y) if scaled image is larger than
    # desired_size.
    if random_jittering:
      max_offset = scaled_size - tf.cast(desired_size, tf.float32)
      max_offset = tf.where(
          tf.less(max_offset, 0), tf.zeros_like(max_offset), max_offset
      )
      offset = max_offset * tf.random.uniform(
          [
              2,
          ],
          0,
          1,
          seed=seed,
      )
      offset = tf.cast(offset, tf.int32)
    else:
      offset = tf.zeros((2,), tf.int32)

    scaled_image = tf.image.resize(
        image, tf.cast(scaled_size, tf.int32), method=method
    )

    if random_jittering:
      scaled_image = scaled_image[
          offset[0] : offset[0] + desired_size[0],
          offset[1] : offset[1] + desired_size[1],
          :,
      ]

    output_image = scaled_image
    if padded_size is not None:
      if centered_crop:
        scaled_image_size = tf.cast(tf.shape(scaled_image)[0:2], tf.int32)
        output_image = tf.image.pad_to_bounding_box(
            scaled_image,
            tf.maximum((padded_size[0] - scaled_image_size[0]) // 2, 0),
            tf.maximum((padded_size[1] - scaled_image_size[1]) // 2, 0),
            padded_size[0],
            padded_size[1],
        )
      else:
        output_image = tf.image.pad_to_bounding_box(
            scaled_image, 0, 0, padded_size[0], padded_size[1]
        )

    image_info = tf.stack([
        image_size,
        tf.cast(desired_size, dtype=tf.float32),
        image_scale,
        tf.cast(offset, tf.float32),
    ])
    return output_image, image_info
def uniform(random_state, size=None, low=0.0, high=1.0, ndim=None):
    
    
    
    low = tensor.as_tensor_variable(low)
    high = tensor.as_tensor_variable(high)
    ndim, size = _infer_ndim(ndim, size, low, high)
    op = RandomFunction('uniform',
            tensor.TensorType(dtype = 'float64', broadcastable = (False,)*ndim) )
    return op(random_state, size, low, high)
def multinomial(random_state, size=None, n=1, pvals=[0.5, 0.5],
                ndim=None, dtype='int64'):
    

    
    n = tensor.as_tensor_variable(n)
    pvals = tensor.as_tensor_variable(pvals)
    # until ellipsis is implemented (argh)
    tmp = pvals.T[0].T
    ndim, size, bcast = _infer_ndim_bcast(ndim, size, n, tmp)
    bcast = bcast + (pvals.type.broadcastable[-1],)
    op = RandomFunction(multinomial_helper,
            tensor.TensorType(dtype=dtype, broadcastable=bcast),
            ndim_added=1)
    return op(random_state, size, n, pvals)
def update(self) -> None:
        
        try:
            items, _ = self._calendar_service.list_events(
                self._calendar_id, search=self._search
            )
        except ServerNotFoundError as err:
            _LOGGER.error("Unable to connect to Google: %s", err)
            return

        # Pick the first visible evemt. Make a copy since calculate_offset mutates the event
        valid_items = filter(self._event_filter, items)
        self._event = copy.deepcopy(next(valid_items, None))
        if self._event:
            calculate_offset(self._event, self._offset)
            self._offset_reached = is_offset_reached(self._event)
def fit(self, X, y, sample_weight=None, check_input=True):
        
        

        super().fit(
            X,
            y,
            sample_weight=sample_weight,
            check_input=check_input,
        )
        return self
def _compute_partial_dependence_recursion(self, grid, target_features):
        
        
        grid = np.asarray(grid, dtype=DTYPE, order="C")
        averaged_predictions = np.zeros(
            shape=grid.shape[0], dtype=np.float64, order="C"
        )
        target_features = np.asarray(target_features, dtype=np.intp, order="C")

        self.tree_.compute_partial_dependence(
            grid, target_features, averaged_predictions
        )
        return averaged_predictions
def qr(a, mode="reduced"):
    
    

    

    x = [[2, 1], [3, 4]]
    if isinstance(numpy.linalg.qr(x, mode), tuple):
        return QRFull(mode)(a)
    else:
        return QRIncomplete(mode)(a)
def extract(self, images, layers=['pool5'], size=(224, 224)):
        

        

        x = chainer.dataset.concat_examples(
            [prepare(img, size=size) for img in images])
        x = Variable(self.xp.asarray(x))
        return self(x, layers=layers)
def __call__(self, x, layers=['prob'], test=True):
        

        

        h = x
        activations = {}
        target_layers = set(layers)
        for key, funcs in self.functions.items():
            if len(target_layers) == 0:
                break
            for func in funcs:
                if isinstance(func, BatchNormalization) or \
                        isinstance(func, BuildingBlock):
                    h = func(h, test=test)
                else:
                    h = func(h)
            if key in target_layers:
                activations[key] = h
                target_layers.remove(key)
        return activations
def __call__(self, x, layers=['prob'], **kwargs):
        

        

        argument.check_unexpected_kwargs(
            kwargs, test='test argument is not supported anymore. '
            'Use chainer.using_config')
        argument.assert_kwargs_empty(kwargs)

        h = x
        activations = {}
        target_layers = set(layers)
        for key, funcs in self.functions.items():
            if len(target_layers) == 0:
                break
            for func in funcs:
                h = func(h)
            if key in target_layers:
                activations[key] = h
                target_layers.remove(key)
        return activations
def predict(self, images, oversample=True):
        

        

        x = concat_examples([prepare(img, size=(256, 256)) for img in images])
        if oversample:
            x = imgproc.oversample(x, crop_dims=(224, 224))
        else:
            x = x[:, :, 16:240, 16:240]
        x = Variable(self.xp.asarray(x))
        y = self(x, layers=['prob'])['prob']
        if oversample:
            n = y.data.shape[0] // 10
            y_shape = y.data.shape[1:]
            y = reshape(y, (n, 10) + y_shape)
            y = sum(y, axis=1) / 10
        return y
def get_all(self, **kw):
        
            
        
        ruletype_dbs = RuleType.get_all(**kw)
        ruletype_apis = [RuleTypeAPI.from_model(runnertype_db)
                         for runnertype_db in ruletype_dbs]
        return ruletype_apis
def _fast_hmac(key, msg, digest):
    
    
    
    dig1, dig2 = digest(), digest()
    if len(key) > dig1.block_size:
        key = digest(key).digest()
    key += b'\x00' * (dig1.block_size - len(key))
    dig1.update(key.translate(_trans_36))
    dig1.update(msg)
    dig2.update(key.translate(_trans_5c))
    dig2.update(dig1.digest())
    return dig2
def constant_time_compare(val1, val2):
    
    
    
    if len(val1) != len(val2):
        return False
    result = 0
    if six.PY3 and isinstance(val1, bytes) and isinstance(val2, bytes):
        for x, y in zip(val1, val2):
            result |= x ^ y
    else:
        for x, y in zip(val1, val2):
            result |= ord(x) ^ ord(y)
    return result == 0
def _random_warp(self, batch: np.ndarray) -> np.ndarray:
         
        
        logger.trace("Randomly warping batch")  # type: ignore
        slices = self._constants.warp_slices
        rands = np.random.normal(size=(self._batchsize, 2, 5, 5),
                                 scale=self._warp_scale).astype("float32")
        batch_maps = ne.evaluate("m + r", local_dict=dict(m=self._constants.warp_maps, r=rands))
        batch_interp = np.array([[cv2.resize(map_, self._constants.warp_pad)[slices, slices]
                                  for map_ in maps]
                                 for maps in batch_maps])
        warped_batch = np.array([cv2.remap(image, interp[0], interp[1], cv2.INTER_LINEAR)
                                 for image, interp in zip(batch, batch_interp)])

        logger.trace("Warped image shape: %s", warped_batch.shape)  # type: ignore
        return warped_batch
def is_simple(sum_type):
    
    

    return not (
        sum_type.attributes or
        any(constructor.fields for constructor in sum_type.types)
    )
def test_models_match_migrations(self):
        
        
        call_command(
            "makemigrations", "django_celery_results", "--check", "--dry-run"
        )
def parse_ents(doc: Doc, options: Dict[str, Any] = {}) -> Dict[str, Any]:
    
    
    kb_url_template = options.get("kb_url_template", None)
    ents = [
        {
            "start": ent.start_char,
            "end": ent.end_char,
            "label": ent.label_,
            "kb_id": ent.kb_id_ if ent.kb_id_ else "",
            "kb_url": kb_url_template.format(ent.kb_id_) if kb_url_template else "#",
        }
        for ent in doc.ents
    ]
    if not ents:
        warnings.warn(Warnings.W006)
    title = doc.user_data.get("title", None) if hasattr(doc, "user_data") else None
    settings = get_doc_settings(doc)
    return {"text": doc.text, "ents": ents, "title": title, "settings": settings}
def is_label_indicator_matrix(y):
     

    
    if not (hasattr(y, "shape") and y.ndim == 2 and y.shape[1] > 1):
        return False
    labels = np.unique(y)
    return len(labels) <= 2 and (y.dtype.kind in 'biu'  # bool, int, uint
                                 or _is_integral_float(labels))
def is_multilabel(y):
     

    
    return is_label_indicator_matrix(y)
def check_rerun(
    project_dir: Path,
    command: Dict[str, Any],
    check_spacy_version: bool = True,
    check_spacy_commit: bool = False,
) -> bool:
    
    
    lock_path = project_dir / PROJECT_LOCK
    if not lock_path.exists():  # We don't have a lockfile, run command
        return True
    data = srsly.read_yaml(lock_path)
    if command["name"] not in data:  # We don't have info about this command
        return True
    entry = data[command["name"]]
    # Always run commands with no outputs (otherwise they'd always be skipped)
    if not entry.get("outs", []):
        return True
    # Always rerun if spaCy version or commit hash changed
    spacy_v = entry.get("spacy_version")
    commit = entry.get("spacy_git_version")
    if check_spacy_version and not is_minor_version_match(spacy_v, about.__version__):
        info = f"({spacy_v} in {PROJECT_LOCK}, {about.__version__} current)"
        msg.info(f"Re-running '{command['name']}': spaCy minor version changed {info}")
        return True
    if check_spacy_commit and commit != GIT_VERSION:
        info = f"({commit} in {PROJECT_LOCK}, {GIT_VERSION} current)"
        msg.info(f"Re-running '{command['name']}': spaCy commit changed {info}")
        return True
    # If the entry in the lockfile matches the lockfile entry that would be
    # generated from the current command, we don't rerun because it means that
    # all inputs/outputs, hashes and scripts are the same and nothing changed
    lock_entry = get_lock_entry(project_dir, command)
    exclude = ["spacy_version", "spacy_git_version"]
    return get_hash(lock_entry, exclude=exclude) != get_hash(entry, exclude=exclude)
def close(self) -> None:
          
        return
def call(self, features, training=False, return_covmat=False):
    
    
    logits = super().call(features)

    # Extracts logits and covariance matrix from model output.
    if self.use_gp_layer:
      logits, covmat = logits
    else:
      covmat = None

    # Computes the uncertainty-adjusted logits during evaluation.
    if not training:
      logits = gaussian_process.mean_field_logits(
          logits, covmat, mean_field_factor=self.temperature)

    if return_covmat and covmat is not None:
      return logits, covmat
    return logits
def get_sandbox_python_path_for_python_action(pack, inherit_from_parent=True,
                                              inherit_parent_virtualenv=True):
    
    
    
    sandbox_python_path = get_sandbox_python_path(
        inherit_from_parent=inherit_from_parent,
        inherit_parent_virtualenv=inherit_parent_virtualenv)

    pack_base_path = get_pack_base_path(pack_name=pack)
    virtualenv_path = get_sandbox_virtualenv_path(pack=pack)

    if not virtualenv_path:
        return sandbox_python_path

    uses_python3, virtualenv_directories = is_pack_virtualenv_using_python3(pack=pack)
    if uses_python3:
        # Add Python 3 lib directory (lib/python3.x) in front of the PYTHONPATH. This way we avoid
        # issues with scripts trying to use packages / modules from Python 2.7 site-packages
        # directory instead of the versions from Python 3 stdlib.
        pack_actions_lib_paths = os.path.join(pack_base_path, 'actions/lib/')
        pack_virtualenv_lib_path = os.path.join(virtualenv_path, 'lib')
        python3_lib_directory = os.path.join(pack_virtualenv_lib_path, virtualenv_directories[0])

        # Add Python 3 site-packages directory (lib/python3.x/site-packages) in front of the Python
        # 2.7 system site-packages This is important because we want Python 3 compatible libraries
        # to be used from the pack virtual environment and not system ones.
        python3_site_packages_directory = os.path.join(pack_virtualenv_lib_path,
                                                       virtualenv_directories[0],
                                                       'site-packages')
        sandbox_python_path = (python3_lib_directory + ':' + python3_site_packages_directory + ':' +
                               pack_actions_lib_paths + ':' + sandbox_python_path)

    return sandbox_python_path
def is_pack_virtualenv_using_python3(pack):
    
    
    
    # If python3.? directory exists in pack virtualenv lib/ path it means Python 3 is used by
    # that virtual environment and we take that in to account when constructing PYTHONPATH
    virtualenv_path = get_sandbox_virtualenv_path(pack=pack)

    if virtualenv_path and os.path.isdir(virtualenv_path):
        pack_virtualenv_lib_path = os.path.join(virtualenv_path, 'lib')

        virtualenv_directories = os.listdir(pack_virtualenv_lib_path)
        virtualenv_directories = [dir_name for dir_name in virtualenv_directories if
                                  fnmatch.fnmatch(dir_name, 'python3*')]
        uses_python3 = bool(virtualenv_directories)
    else:
        uses_python3 = False
        virtualenv_directories = None

    return uses_python3, virtualenv_directories
def solution(n: int = 600851475143) -> int:
    
    
    

    try:
        n = int(n)
    except (TypeError, ValueError):
        raise TypeError("Parameter n must be int or castable to int.")
    if n <= 0:
        raise ValueError("Parameter n must be greater than or equal to one.")
    max_number = 0
    if isprime(n):
        return n
    while n % 2 == 0:
        n //= 2
    if isprime(n):
        return n
    for i in range(3, int(math.sqrt(n)) + 1, 2):
        if n % i == 0:
            if isprime(n / i):
                max_number = n / i
                break
            elif isprime(i):
                max_number = i
    return max_number
def summarize(self, just_avg=True):
        
        
        self.summary = wer_summary(self.scores)

        if just_avg:
            return self.summary["WER"]
        else:
            return self.summary
def summarize(self, field=None):
        
        
        min_index = torch.argmin(torch.tensor(self.scores))
        max_index = torch.argmax(torch.tensor(self.scores))
        self.summary = {
            "average": sum(self.scores) / len(self.scores),
            "min_score": self.scores[min_index],
            "min_id": self.ids[min_index],
            "max_score": self.scores[max_index],
            "max_id": self.ids[max_index],
        }

        if field is not None:
            return self.summary[field]
        else:
            return self.summary
def summarize(self, field=None, threshold=None, beta=1, eps=1e-8):
        
        

        if isinstance(self.scores, list):
            self.scores = torch.stack(self.scores)
            self.labels = torch.stack(self.labels)

        if threshold is None:
            positive_scores = self.scores[self.labels.nonzero(as_tuple=True)]
            negative_scores = self.scores[
                self.labels[self.labels == 0].nonzero(as_tuple=True)
            ]

            eer, threshold = EER(positive_scores, negative_scores)

        pred = (self.scores >= threshold).float()
        true = self.labels

        TP = self.summary["TP"] = float(pred.mul(true).sum())
        TN = self.summary["TN"] = float((1.0 - pred).mul(1.0 - true).sum())
        FP = self.summary["FP"] = float(pred.mul(1.0 - true).sum())
        FN = self.summary["FN"] = float((1.0 - pred).mul(true).sum())

        self.summary["FAR"] = FP / (FP + TN + eps)
        self.summary["FRR"] = FN / (TP + FN + eps)
        self.summary["DER"] = (FP + FN) / (TP + TN + eps)

        self.summary["precision"] = TP / (TP + FP + eps)
        self.summary["recall"] = TP / (TP + FN + eps)
        self.summary["F-score"] = (
            (1.0 + beta ** 2.0)
            * TP
            / ((1.0 + beta ** 2.0) * TP + beta ** 2.0 * FN + FP)
        )

        self.summary["MCC"] = (TP * TN - FP * FN) / (
            (TP + FP) * (TP + FN) * (TN + FP) * (TN + FN) + eps
        ) ** 0.5

        if field is not None:
            return self.summary[field]
        else:
            return self.summary
def EER(positive_scores, negative_scores):
    
    

    # Computing candidate thresholds
    thresholds, _ = torch.sort(torch.cat([positive_scores, negative_scores]))
    thresholds = torch.unique(thresholds)

    # Adding intermediate thresholds
    interm_thresholds = (thresholds[0:-1] + thresholds[1:]) / 2
    thresholds, _ = torch.sort(torch.cat([thresholds, interm_thresholds]))

    # Computing False Rejection Rate (miss detection)
    positive_scores = torch.cat(
        len(thresholds) * [positive_scores.unsqueeze(0)]
    )
    pos_scores_threshold = positive_scores.transpose(0, 1) <= thresholds
    FRR = (pos_scores_threshold.sum(0)).float() / positive_scores.shape[1]
    del positive_scores
    del pos_scores_threshold

    # Computing False Aceptance Rate (false alarm)
    negative_scores = torch.cat(
        len(thresholds) * [negative_scores.unsqueeze(0)]
    )
    neg_scores_threshold = negative_scores.transpose(0, 1) > thresholds
    FAR = (neg_scores_threshold.sum(0)).float() / negative_scores.shape[1]
    del negative_scores
    del neg_scores_threshold

    # Finding the threshold for EER
    min_index = (FAR - FRR).abs().argmin()

    # It is possible that eer != fpr != fnr. We return (FAR  + FRR) / 2 as EER.
    EER = (FAR[min_index] + FRR[min_index]) / 2

    return float(EER), float(thresholds[min_index])
def parse_query(self, query: str) -> (GraphQLDocument, ExecutionResult):
        
        
        if not query or not isinstance(query, str):
            return (
                None,
                ExecutionResult(
                    errors=[ValueError("Must provide a query string.")], invalid=True
                ),
            )

        # Attempt to parse the query, if it fails, return the error
        try:
            return self.backend.document_from_string(self.schema, query), None
        except (ValueError, GraphQLSyntaxError) as e:
            return None, ExecutionResult(errors=[e], invalid=True)
def tamper(payload, **kwargs):
    
    
    

    retVal = payload

    if payload:
        match = re.search(r"('[^']+'|CHAR\(\d+\))\+.*(?<=\+)('[^']+'|CHAR\(\d+\))", retVal)
        if match:
            part = match.group(0)

            chars = [char for char in part]
            for index in zeroDepthSearch(part, '+'):
                chars[index] = ','

            replacement = "CONCAT(%s)" % "".join(chars)
            retVal = retVal.replace(part, replacement)

    return retVal
def parse_docker_exec_output(self, logs: bytes) -> tuple[bytes, bytes]:
        
            
        
        res = b''
        tail = b''
        i = 0
        byte_order = sys.byteorder
        while i < len(logs):
            prefix = logs[i : i + 8]
            if len(prefix) < 8:
                msg_type = prefix[0:1]
                if msg_type in [b'\x00', b'\x01', b'\x02', b'\x03']:
                    tail = prefix
                break

            msg_type = prefix[0:1]
            padding = prefix[1:4]
            if (
                msg_type in [b'\x00', b'\x01', b'\x02', b'\x03']
                and padding == b'\x00\x00\x00'
            ):
                msg_length = int.from_bytes(prefix[4:8], byteorder=byte_order)
                res += logs[i + 8 : i + 8 + msg_length]
                i += 8 + msg_length
            else:
                res += logs[i : i + 1]
                i += 1
        return res, tail
def median(image, selem=None, out=None, mask=None,
           shift_x=False, shift_y=False):
    

    

    return _apply_scalar_per_pixel(generic_cy._median, image, selem,
                                   out=out, mask=mask,
                                   shift_x=shift_x, shift_y=shift_y)
def majority_filter(image, selem, out=None, mask=None, shift_x=False,
                    shift_y=False):
    

    

    return _apply_scalar_per_pixel(generic_cy._majority, image, selem,
                                   out=out, mask=mask,
                                   shift_x=shift_x, shift_y=shift_y)
def mkstemp(suffix=None, prefix=None, dir=None, text=False):
    
    

    prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)

    if text:
        flags = _text_openflags
    else:
        flags = _bin_openflags

    return _mkstemp_inner(dir, prefix, suffix, flags, output_type)
def NamedTemporaryFile(mode='w+b', bufsize=-1, suffix="",
                       prefix=template, dir=None, delete=True):
    
    

    if dir is None:
        dir = gettempdir()

    if 'b' in mode:
        flags = _bin_openflags
    else:
        flags = _text_openflags

    # Setting O_TEMPORARY in the flags causes the OS to delete
    # the file when it is closed.  This is only supported by Windows.
    if _os.name == 'nt' and delete:
        flags |= _os.O_TEMPORARY

    (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)
    file = _os.fdopen(fd, mode, bufsize)
    return _TemporaryFileWrapper(file, name, delete)
def fetch_open_interest_history(self, symbol, timeframe='1h', since=None, limit=None, params={}):
        
        
        
        if timeframe == '1m':
            raise BadRequest(self.id + 'fetchOpenInterestHistory cannot use the 1m timeframe')
        self.load_markets()
        market = self.market(symbol)
        if market['spot'] or market['option']:
            raise BadRequest(self.id + ' fetchOpenInterestHistory() symbol does not support market ' + symbol)
        request = {
            'symbol': market['id'],
        }
        if limit is not None:
            request['limit'] = limit
        return self.fetch_derivatives_open_interest_history(symbol, timeframe, since, limit, params)
def borrow_margin(self, code: str, amount, symbol: Optional[str] = None, params={}):
        
        
        
        self.load_markets()
        currency = self.currency(code)
        marginMode, query = self.handle_margin_mode_and_params('borrowMargin', params)
        if marginMode == 'isolated':
            raise NotSupported(self.id + ' borrowMargin() cannot use isolated margin')
        request = {
            'coin': currency['id'],
            'qty': self.currency_to_precision(code, amount),
        }
        response = self.privatePostV5SpotCrossMarginTradeLoan(self.extend(request, query))
        #
        #     {
        #         "retCode": 0,
        #         "retMsg": "success",
        #         "result": {
        #             "transactId": "14143"
        #         },
        #         "retExtInfo": null,
        #         "time": 1662617848970
        #     }
        #
        result = self.safe_value(response, 'result', {})
        transaction = self.parse_margin_loan(result, currency)
        return self.extend(transaction, {
            'symbol': symbol,
            'amount': amount,
        })
def fetch_currencies(self, params={}):
        
        
        
        if not self.check_required_credentials(False):
            return None
        response = self.privateGetV5AssetCoinQueryInfo(params)
        #
        #     {
        #         "retCode": 0,
        #         "retMsg": "",
        #         "result": {
        #             "rows": [
        #                 {
        #                     "name": "BTC",
        #                     "coin": "BTC",
        #                     "remainAmount": "150",
        #                     "chains": [
        #                         {
        #                             "chainType": "BTC",
        #                             "confirmation": "10000",
        #                             "withdrawFee": "0.0005",
        #                             "depositMin": "0.0005",
        #                             "withdrawMin": "0.001",
        #                             "chain": "BTC",
        #                             "chainDeposit": "1",
        #                             "chainWithdraw": "1",
        #                             "minAccuracy": "8"
        #                         }
        #                     ]
        #                 }
        #             ]
        #         },
        #         "retExtInfo": {},
        #         "time": 1672194582264
        #     }
        #
        data = self.safe_value(response, 'result', [])
        rows = self.safe_value(data, 'rows', [])
        result = {}
        for i in range(0, len(rows)):
            currency = rows[i]
            currencyId = self.safe_string(currency, 'coin')
            code = self.safe_currency_code(currencyId)
            name = self.safe_string(currency, 'name')
            chains = self.safe_value(currency, 'chains', [])
            networks = {}
            minPrecision = None
            for j in range(0, len(chains)):
                chain = chains[j]
                networkId = self.safe_string(chain, 'chain')
                networkCode = self.network_id_to_code(networkId)
                precision = self.parse_number(self.parse_precision(self.safe_string(chain, 'minAccuracy')))
                minPrecision = precision if (minPrecision is None) else min(minPrecision, precision)
                depositAllowed = self.safe_integer(chain, 'chainDeposit') == 1
                withdrawAllowed = self.safe_integer(chain, 'chainWithdraw') == 1
                networks[networkCode] = {
                    'info': chain,
                    'id': networkId,
                    'network': networkCode,
                    'active': None,
                    'deposit': depositAllowed,
                    'withdraw': withdrawAllowed,
                    'fee': self.safe_number(chain, 'withdrawFee'),
                    'precision': precision,
                    'limits': {
                        'withdraw': {
                            'min': self.safe_number(chain, 'withdrawMin'),
                            'max': None,
                        },
                        'deposit': {
                            'min': self.safe_number(chain, 'depositMin'),
                            'max': None,
                        },
                    },
                }
            result[code] = {
                'info': currency,
                'code': code,
                'id': currencyId,
                'name': name,
                'active': None,
                'deposit': None,
                'withdraw': None,
                'fee': None,
                'precision': minPrecision,
                'limits': {
                    'amount': {
                        'min': None,
                        'max': None,
                    },
                },
                'networks': networks,
            }
        return result
def fetch_deposits(self, code: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):
        
        
       
        self.load_markets()
        paginate = False
        paginate, params = self.handle_option_and_params(params, 'fetchDeposits', 'paginate')
        if paginate:
            return self.fetch_paginated_call_cursor('fetchDeposits', code, since, limit, params, 'nextPageCursor', 'nextPageCursor', None, 50)
        request = {
            # 'coin': currency['id'],
            # 'limit': 20,  # max 50
            # 'cursor': '',
        }
        currency = None
        if code is not None:
            currency = self.currency(code)
            request['coin'] = currency['id']
        if since is not None:
            request['startTime'] = since
        if limit is not None:
            request['limit'] = limit
        request, params = self.handle_until_option('endTime', request, params)
        response = self.privateGetV5AssetDepositQueryRecord(self.extend(request, params))
        #
        #     {
        #         "retCode": 0,
        #         "retMsg": "success",
        #         "result": {
        #             "rows": [
        #                 {
        #                     "coin": "USDT",
        #                     "chain": "ETH",
        #                     "amount": "10000",
        #                     "txID": "skip-notification-scene-test-amount-202212270944-533285-USDT",
        #                     "status": 3,
        #                     "toAddress": "test-amount-address",
        #                     "tag": "",
        #                     "depositFee": "",
        #                     "successAt": "1672134274000",
        #                     "confirmations": "10000",
        #                     "txIndex": "",
        #                     "blockHash": ""
        #                 }
        #             ],
        #             "nextPageCursor": "eyJtaW5JRCI6MTA0NjA0MywibWF4SUQiOjEwNDYwNDN9"
        #         },
        #         "retExtInfo": {},
        #         "time": 1672191992512
        #     }
        #
        data = self.add_pagination_cursor_to_result(response)
        return self.parse_transactions(data, currency, since, limit)
def fetch_order_trades(self, id: str, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):
        
        
        
        request = {}
        clientOrderId = self.safe_string_2(params, 'clientOrderId', 'orderLinkId')
        if clientOrderId is not None:
            request['orderLinkId'] = clientOrderId
        else:
            request['orderId'] = id
        params = self.omit(params, ['clientOrderId', 'orderLinkId'])
        return self.fetch_my_trades(symbol, since, limit, self.extend(request, params))
def fetch_canceled_orders(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}):
        
        
        
        self.load_markets()
        request = {
            'orderStatus': 'Cancelled',
        }
        return self.fetch_canceled_and_closed_orders(symbol, since, limit, self.extend(request, params))
def create_order(self, symbol: str, type: OrderType, side: OrderSide, amount, price=None, params={}):
        
        
        
        self.load_markets()
        market = self.market(symbol)
        enableUnifiedMargin, enableUnifiedAccount = self.is_unified_enabled()
        isUnifiedAccount = (enableUnifiedMargin or enableUnifiedAccount)
        isUsdcSettled = market['settle'] == 'USDC'
        if isUsdcSettled and not isUnifiedAccount:
            return self.create_usdc_order(symbol, type, side, amount, price, params)
        trailingAmount = self.safe_string_2(params, 'trailingAmount', 'trailingStop')
        isTrailingAmountOrder = trailingAmount is not None
        orderRequest = self.create_order_request(symbol, type, side, amount, price, params)
        response = None
        if isTrailingAmountOrder:
            response = self.privatePostV5PositionTradingStop(orderRequest)
        else:
            response = self.privatePostV5OrderCreate(orderRequest)  # already extended inside createOrderRequest
        #
        #     {
        #         "retCode": 0,
        #         "retMsg": "OK",
        #         "result": {
        #             "orderId": "1321003749386327552",
        #             "orderLinkId": "spot-test-postonly"
        #         },
        #         "retExtInfo": {},
        #         "time": 1672211918471
        #     }
        #
        order = self.safe_value(response, 'result', {})
        return self.parse_order(order, market)
def edit_order(self, id: str, symbol, type, side, amount=None, price=None, params={}):
        
        
        
        self.check_required_symbol('editOrder', symbol)
        self.load_markets()
        market = self.market(symbol)
        enableUnifiedMargin, enableUnifiedAccount = self.is_unified_enabled()
        isUnifiedAccount = (enableUnifiedMargin or enableUnifiedAccount)
        isUsdcSettled = market['settle'] == 'USDC'
        if isUsdcSettled and not isUnifiedAccount:
            return self.edit_usdc_order(id, symbol, type, side, amount, price, params)
        request = {
            'symbol': market['id'],
            'orderId': id,
            # 'orderLinkId': 'string',  # unique client order id, max 36 characters
            # 'takeProfit': 123.45,  # take profit price, only take effect upon opening the position
            # 'stopLoss': 123.45,  # stop loss price, only take effect upon opening the position
            # 'triggerPrice': 123.45,  # trigger price, required for conditional orders
            # 'triggerBy': 'MarkPrice',  # IndexPrice, MarkPrice, LastPrice
            # 'tpTriggerby': 'MarkPrice',  # IndexPrice, MarkPrice, LastPrice
            # 'slTriggerBy': 'MarkPrice',  # IndexPrice, MarkPrice, LastPrice
            # Valid for option only.
            # 'orderIv': '0',  # Implied volatility; parameters are passed according to the real value; for example, for 10%, 0.1 is passed
        }
        if market['linear']:
            request['category'] = 'linear'
        elif market['inverse']:
            request['category'] = 'inverse'
        elif market['option']:
            request['category'] = 'option'
        if amount is not None:
            request['qty'] = self.amount_to_precision(symbol, amount)
        if price is not None:
            request['price'] = self.price_to_precision(symbol, price)
        if amount is not None:
            request['qty'] = self.amount_to_precision(symbol, amount)
        triggerPrice = self.safe_string_2(params, 'triggerPrice', 'stopPrice')
        stopLossTriggerPrice = self.safe_string(params, 'stopLossPrice')
        takeProfitTriggerPrice = self.safe_string(params, 'takeProfitPrice')
        stopLoss = self.safe_value(params, 'stopLoss')
        takeProfit = self.safe_value(params, 'takeProfit')
        isStopLossTriggerOrder = stopLossTriggerPrice is not None
        isTakeProfitTriggerOrder = takeProfitTriggerPrice is not None
        isStopLoss = stopLoss is not None
        isTakeProfit = takeProfit is not None
        if isStopLossTriggerOrder or isTakeProfitTriggerOrder:
            triggerPrice = stopLossTriggerPrice if isStopLossTriggerOrder else takeProfitTriggerPrice
        if triggerPrice is not None:
            triggerPriceRequest = triggerPrice if (triggerPrice == '0') else self.price_to_precision(symbol, triggerPrice)
            request['triggerPrice'] = triggerPriceRequest
            triggerBy = self.safe_string(params, 'triggerBy', 'LastPrice')
            request['triggerBy'] = triggerBy
        if isStopLoss or isTakeProfit:
            if isStopLoss:
                slTriggerPrice = self.safe_string_2(stopLoss, 'triggerPrice', 'stopPrice', stopLoss)
                stopLossRequest = slTriggerPrice if (slTriggerPrice == '0') else self.price_to_precision(symbol, slTriggerPrice)
                request['stopLoss'] = stopLossRequest
                slTriggerBy = self.safe_string(params, 'slTriggerBy', 'LastPrice')
                request['slTriggerBy'] = slTriggerBy
            if isTakeProfit:
                tpTriggerPrice = self.safe_string_2(takeProfit, 'triggerPrice', 'stopPrice', takeProfit)
                takeProfitRequest = tpTriggerPrice if (tpTriggerPrice == '0') else self.price_to_precision(symbol, tpTriggerPrice)
                request['takeProfit'] = takeProfitRequest
                tpTriggerBy = self.safe_string(params, 'tpTriggerBy', 'LastPrice')
                request['tpTriggerBy'] = tpTriggerBy
        clientOrderId = self.safe_string(params, 'clientOrderId')
        if clientOrderId is not None:
            request['orderLinkId'] = clientOrderId
        params = self.omit(params, ['stopPrice', 'stopLossPrice', 'takeProfitPrice', 'triggerPrice', 'clientOrderId', 'stopLoss', 'takeProfit'])
        response = self.privatePostV5OrderAmend(self.extend(request, params))
        #
        #     {
        #         "retCode": 0,
        #         "retMsg": "OK",
        #         "result": {
        #             "orderId": "c6f055d9-7f21-4079-913d-e6523a9cfffa",
        #             "orderLinkId": "linear-004"
        #         },
        #         "retExtInfo": {},
        #         "time": 1672217093461
        #     }
        #
        result = self.safe_value(response, 'result', {})
        return self.safe_order({
            'info': response,
            'id': self.safe_string(result, 'orderId'),
        })
def fetch_my_trades(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}):
        
        
        
        self.load_markets()
        paginate = False
        paginate, params = self.handle_option_and_params(params, 'fetchMyTrades', 'paginate')
        if paginate:
            return self.fetch_paginated_call_cursor('fetchMyTrades', symbol, since, limit, params, 'nextPageCursor', 'nextPageCursor', None, 100)
        enableUnifiedMargin, enableUnifiedAccount = self.is_unified_enabled()
        isUnifiedAccount = (enableUnifiedMargin or enableUnifiedAccount)
        request = {}
        market = None
        isUsdcSettled = False
        if symbol is not None:
            market = self.market(symbol)
            isUsdcSettled = market['settle'] == 'USDC'
            request['symbol'] = market['id']
        type = None
        type, params = self.get_bybit_type('fetchMyTrades', market, params)
        if ((type == 'option') or isUsdcSettled) and not isUnifiedAccount:
            return self.fetch_my_usdc_trades(symbol, since, limit, params)
        request['category'] = type
        if limit is not None:
            request['limit'] = limit
        if since is not None:
            request['startTime'] = since
        request, params = self.handle_until_option('endTime', request, params)
        response = self.privateGetV5ExecutionList(self.extend(request, params))
        #
        #     {
        #         "retCode": 0,
        #         "retMsg": "OK",
        #         "result": {
        #             "nextPageCursor": "132766%3A2%2C132766%3A2",
        #             "category": "linear",
        #             "list": [
        #                 {
        #                     "symbol": "ETHPERP",
        #                     "orderType": "Market",
        #                     "underlyingPrice": "",
        #                     "orderLinkId": "",
        #                     "side": "Buy",
        #                     "indexPrice": "",
        #                     "orderId": "8c065341-7b52-4ca9-ac2c-37e31ac55c94",
        #                     "stopOrderType": "UNKNOWN",
        #                     "leavesQty": "0",
        #                     "execTime": "1672282722429",
        #                     "isMaker": False,
        #                     "execFee": "0.071409",
        #                     "feeRate": "0.0006",
        #                     "execId": "e0cbe81d-0f18-5866-9415-cf319b5dab3b",
        #                     "tradeIv": "",
        #                     "blockTradeId": "",
        #                     "markPrice": "1183.54",
        #                     "execPrice": "1190.15",
        #                     "markIv": "",
        #                     "orderQty": "0.1",
        #                     "orderPrice": "1236.9",
        #                     "execValue": "119.015",
        #                     "execType": "Trade",
        #                     "execQty": "0.1"
        #                 }
        #             ]
        #         },
        #         "retExtInfo": {},
        #         "time": 1672283754510
        #     }
        #
        trades = self.add_pagination_cursor_to_result(response)
        return self.parse_trades(trades, market, since, limit)
def fetch_leverage_tiers(self, symbols: Strings = None, params={}):
        
        
        
        self.load_markets()
        market = None
        symbol = None
        if symbols is not None:
            market = self.market(symbols[0])
            if market['spot']:
                raise NotSupported(self.id + ' fetchLeverageTiers() is not supported for spot market')
            symbol = market['symbol']
        data = self.get_leverage_tiers_paginated(symbol, self.extend({'paginate': True, 'paginationCalls': 20}, params))
        symbols = self.market_symbols(symbols)
        return self.parse_leverage_tiers(data, symbols, 'symbol')
def fetch(self):
        
        
        examples = self.get_examples(None, None)
        if self.mode is tuple:
            return examples
        elif self.mode is dict:
            return dict(six.moves.zip(self.keys, examples))
        elif self.mode is None:
            return examples[0]
def convert(self, *args, **kwargs):
        
        
        if args and not kwargs:
            return tuple(_as_array(d) for d in args)
        elif not args and kwargs:
            return {k: _as_array(v) for k, v in kwargs.items()}
def convert(self, data):
        
        
        if isinstance(data, tuple):
            return tuple(_as_array(d) for d in data)
        elif isinstance(data, dict):
            return {k: _as_array(v) for k, v in data.items()}
def get_lambda_output(
        stdout_stream_str: io.StringIO, stdout_stream_bytes: Optional[io.BytesIO] = None
    ) -> Tuple[Union[str, bytes], bool]:
        
        
        
        lambda_response: Union[str, bytes] = stdout_stream_str.getvalue()
        if stdout_stream_bytes and not lambda_response:
            lambda_response = stdout_stream_bytes.getvalue()

        # When the Lambda Function returns an Error/Exception, the output is added to the stdout of the container. From
        # our perspective, the container returned some value, which is not always true. Since the output is the only
        # information we have, we need to inspect this to understand if the container returned a some data or raised an
        # error
        is_lambda_user_error_response = LambdaOutputParser.is_lambda_error_response(lambda_response)

        return lambda_response, is_lambda_user_error_response
def mean_squared_error(self, labels, prediction):
        
        

        
        if labels.ndim != 1:
            print("Error: Input labels must be one dimensional")

        return np.mean((labels - prediction) ** 2)
def guess_extension(type):
    
    
    global inited
    if not inited:
        init()
    type = string.lower(type)
    for ext, stype in types_map.items():
        if type == stype:
            return ext
    return None
def guess_extension(self, type, strict=1):
        
        
        type = type.lower()
        for ext, stype in self.types_map.items():
            if type == stype:
                return ext
        if not strict:
            for ext, stype in common_types.items():
                if type == stype:
                    return ext
        return None
def md_expand_links(text, cwd_rel_path, repo_url=""):
    
    

    
    link_text, link, anchor = md_link_break_up(text)

    #print(link_text, link, anchor)

    # skip:
    # - empty links (i.e. just local anchor to the same doc)
    # - skip explicit .md links
    # - external links like https://...
    if len(link) == 0 or link.endswith(".md") or re.search(r'^\w+://', link):
        return text

    link = Path(link)
    try_link = link / "README.md"

    full_path = cwd_rel_path / try_link
    if full_path.exists():
        link = str(try_link)
    else:
        link = str(link)

        # leave the images local for pdf rendering, but for the rest of the file (scripts,
        # reports, etc.)
        # prepend the repo base url, while removing ./ relative prefix if any
        if not re.search(re_link_images, link):
            link = resolve_rel_link(link, cwd_rel_path)
            link = repo_url + "/" + link

    return md_link_build(link_text, link, anchor)
def from_data(
        cls,
        data: Union[str, bytes],
        *,
        encoding: str = "utf-8",
        mime_type: Optional[str] = None,
        path: Optional[str] = None,
        metadata: Optional[dict] = None,
    ) -> Blob:
        
        
        return cls(
            data=data,
            mimetype=mime_type,
            encoding=encoding,
            path=path,
            metadata=metadata if metadata is not None else {},
        )
def rotate(
    document: vp.Document,
    angle: float,
    layer: Union[int, List[int]],
    origin_coords: Union[Tuple[()], Tuple[float, float]],
):
    
    

    try:
        origin, layer_ids, _ = _compute_origin(document, layer, origin_coords)
    except ValueError:
        return document

    for vid in layer_ids:
        lc = document[vid]
        lc.translate(-origin[0], -origin[1])
        lc.rotate(angle * math.pi / 180.0)
        lc.translate(origin[0], origin[1])

    return document
def skip_exceptions(exc: Optional[Exception]) -> Exception:
    
    
    should_not_shorten = bool(int(os.environ.get("RAY_AIR_FULL_TRACEBACKS", "0")))

    if should_not_shorten:
        return exc

    if isinstance(exc, StartTraceback):
        # If this is a StartTraceback, skip
        return skip_exceptions(exc.__cause__)

    # Perform a shallow copy to prevent recursive __cause__/__context__.
    new_exc = copy.copy(exc).with_traceback(exc.__traceback__)

    # Make sure nested exceptions are properly skipped.
    cause = getattr(exc, "__cause__", None)
    if cause:
        new_exc.__cause__ = skip_exceptions(cause)

    return new_exc
def info(name=None, show_only_latest=True, name_only=False):
    

    
    information = json.loads(urlopen(DATA_LIST_URL).read().decode("utf-8"))

    if name is not None:
        corpora = information['corpora']
        models = information['models']
        if name in corpora:
            return information['corpora'][name]
        elif name in models:
            return information['models'][name]
        else:
            raise ValueError("Incorrect model/corpus name")

    if not show_only_latest:
        return information

    if name_only:
        return {"corpora": list(information['corpora'].keys()), "models": list(information['models'])}

    return {
        "corpora": {name: data for (name, data) in information['corpora'].items() if data.get("latest", True)},
        "models": {name: data for (name, data) in information['models'].items() if data.get("latest", True)}
    }
def create_server(self, protocol_factory, host=None, port=None, *,
                      family=socket.AF_UNSPEC, flags=socket.AI_PASSIVE,
                      sock=None, backlog=100, ssl=None, reuse_address=None,
                      reuse_port=None):
        
        
        raise NotImplementedError
def get_event_loop():
    
    
    current_loop = _get_running_loop()
    if current_loop is not None:
        return current_loop
    return get_event_loop_policy().get_event_loop()
def async_aiohttp_proxy_stream(hass, request, stream, content_type,
                               buffer_size=102400, timeout=10):
    
    response = web.StreamResponse()
    response.content_type = content_type
    yield from response.prepare(request)

    try:
        while True:
            with async_timeout.timeout(timeout, loop=hass.loop):
                data = yield from stream.read(buffer_size)

            if not data:
                yield from response.write_eof()
                break

            response.write(data)

    except (asyncio.TimeoutError, aiohttp.ClientError):
        pass

    yield from response.write_eof()
def _alter_field_type_workaround(self, model, old_field, new_field):
        
        
        
        # Make a new field that's like the new one but with a temporary
        # column name.
        new_temp_field = copy.deepcopy(new_field)
        new_temp_field.null = (new_field.get_internal_type() not in ('AutoField', 'BigAutoField'))
        new_temp_field.column = self._generate_temp_name(new_field.column)
        # Add it
        self.add_field(model, new_temp_field)
        # Explicit data type conversion
        # https://docs.oracle.com/database/121/SQLRF/sql_elements002.htm#SQLRF51054
        new_value = self.quote_name(old_field.column)
        old_type = old_field.db_type(self.connection)
        if re.match('^N?CLOB', old_type):
            new_value = "TO_CHAR(%s)" % new_value
            old_type = 'VARCHAR2'
        if re.match('^N?VARCHAR2', old_type):
            new_internal_type = new_field.get_internal_type()
            if new_internal_type == 'DateField':
                new_value = "TO_DATE(%s, 'YYYY-MM-DD')" % new_value
            elif new_internal_type == 'DateTimeField':
                new_value = "TO_TIMESTAMP(%s, 'YYYY-MM-DD HH24:MI:SS.FF')" % new_value
            elif new_internal_type == 'TimeField':
                # TimeField are stored as TIMESTAMP with a 1900-01-01 date part.
                new_value = "TO_TIMESTAMP(CONCAT('1900-01-01 ', %s), 'YYYY-MM-DD HH24:MI:SS.FF')" % new_value
        # Transfer values across
        self.execute("UPDATE %s set %s=%s" % (
            self.quote_name(model._meta.db_table),
            self.quote_name(new_temp_field.column),
            new_value,
        ))
        # Drop the old field
        self.remove_field(model, old_field)
        # Rename and possibly make the new field NOT NULL
        super().alter_field(model, new_temp_field, new_field)
def build_pkg(dist_path: str, app_filename: str, component_filename: str, cura_version: str, installer_filename: str) -> None:
     
    
    pkg_build_executable = os.environ.get("PKG_BUILD_EXECUTABLE", "pkgbuild")
    product_build_executable = os.environ.get("PRODUCT_BUILD_EXECUTABLE", "productbuild")
    codesign_identity = os.environ.get("CODESIGN_IDENTITY")

    # This builds the component package that contains UltiMaker-Cura.app. This component package will be bundled in a distribution package.
    pkg_build_arguments = [
        pkg_build_executable,
        "--identifier", ULTIMAKER_CURA_DOMAIN,
        "--version", cura_version,
        "--component",
        Path(dist_path, app_filename),
        Path(dist_path, component_filename),
        "--install-location", "/Applications",
    ]

    if codesign_identity:
        pkg_build_arguments.extend(["--sign", codesign_identity])
    else:
        print("CODESIGN_IDENTITY missing. The installer is not being signed")

    subprocess.run(pkg_build_arguments)

    # This automatically generates a distribution.xml file that is used to build the installer.
    # If you want to make any changes to how the installer functions, this file should be changed to do that.
    # TODO: Use --product {property_list_file} to pull keys out of file for distribution.xml. This can be used to set min requirements
    distribution_creation_arguments = [
        product_build_executable,
        "--synthesize",
        "--package", Path(dist_path, component_filename),  # Package that will be inside installer
        Path(dist_path, "distribution.xml"),  # Output location for sythesized distributions file
    ]
    subprocess.run(distribution_creation_arguments)

    # This creates the distributable package (Installer)
    installer_creation_arguments = [
        product_build_executable,
        "--distribution", Path(dist_path, "distribution.xml"),
        "--package-path", dist_path,  # Where to find the component packages mentioned in distribution.xml (UltiMaker-Cura.pkg)
        Path(dist_path, installer_filename),
    ]

    if codesign_identity:
        installer_creation_arguments.extend(["--sign", codesign_identity])

    subprocess.run(installer_creation_arguments)
def is_on(self) -> bool:
        
        return cast(bool, getattr(self._device, self.entity_description.key))
def push_data(self, data: dict[str, Any], raw_data: dict[Any, Any]) -> None:
        
        self.hass.loop.call_soon(self.async_push_data, data, raw_data)
def test_special_prefix(self):
        
        self.assertEqual(static('http://example.org'), [])
        self.assertEqual(static('//example.org'), [])
def _get_deconv_activation_layer(params: Dict) -> Any:
    
        
    
    deconv_activation: str = params.get('deconv_activation')
    if deconv_activation == 'LeakyReLU':
        return LeakyReLU(0.2)
    elif deconv_activation == 'ELU':
        return ELU()
    return ReLU()
def __init__(
      self,
      *,  # Makes all args keyword only.
      global_step: tf.Variable,
      trainer: Optional[runner.AbstractTrainer] = None,
      evaluator: Optional[runner.AbstractEvaluator] = None,
      strategy: Optional[tf.distribute.Strategy] = None,
      # Actions
      train_actions: Optional[Iterable[Action]] = None,
      eval_actions: Optional[Iterable[Action]] = None,
      # Train related
      steps_per_loop: Optional[Union[int, Callable[[int], int]]] = None,
      checkpoint_manager: Optional[tf.train.CheckpointManager] = None,
      enable_async_checkpointing: bool = False,
      # Summary related
      summary_interval: Optional[int] = None,
      summary_dir: Optional[str] = None,
      # Evaluation related
      eval_summary_dir: Optional[str] = None,
      summary_manager: Optional[utils.SummaryManagerInterface] = None,
      eval_summary_manager: Optional[utils.SummaryManagerInterface] = None):
    
    
    if trainer is None and evaluator is None:
      raise ValueError("`trainer` and `evaluator` should not both be `None`.")

    if trainer is not None:
      if steps_per_loop is None:
        raise ValueError(
            "`steps_per_loop` is required when `trainer` is provided.")
      elif not callable(steps_per_loop) and (
          not isinstance(steps_per_loop, int) or steps_per_loop < 1):
        raise ValueError(
            f"`steps_per_loop` ({steps_per_loop}) must be a positive integer "
            "or a callable.")

      if summary_interval is not None:
        if summary_interval <= 0:
          raise ValueError(
              f"`summary_interval` ({summary_interval}) must be larger than 0.")
        elif not callable(steps_per_loop) and (summary_interval % steps_per_loop
                                               != 0):
          raise ValueError(
              f"`summary interval` ({summary_interval}) must be a multiple "
              f"of `steps_per_loop` ({steps_per_loop}).")

    if not isinstance(global_step, tf.Variable):
      raise ValueError("`global_step` must be a `tf.Variable`.")

    self.trainer = trainer
    self.evaluator = evaluator

    self.strategy = strategy or tf.distribute.get_strategy()

    self.train_actions = () if train_actions is None else tuple(train_actions)
    self.eval_actions = () if eval_actions is None else tuple(eval_actions)

    self.global_step = global_step
    self.checkpoint_manager = checkpoint_manager
    self._enable_async_checkpoint_saving = enable_async_checkpointing
    self._checkpoint_options = tf.train.CheckpointOptions(
        enable_async=enable_async_checkpointing
    )

    if self.trainer is not None:
      self.step_timer = None
      self.summary_interval = summary_interval
      if summary_manager:
        self.summary_manager = summary_manager
      else:
        self.summary_manager = utils.SummaryManager(
            summary_dir, tf.summary.scalar, global_step=self.global_step)
      self._steps_per_loop = steps_per_loop

    if self.evaluator is not None:
      eval_summary_dir = eval_summary_dir or summary_dir
      if eval_summary_dir == summary_dir and self.trainer is not None:
        # Reuse the summary writer if train and evaluation summary directory
        # are the same.
        self.eval_summary_manager = self.summary_manager
      else:
        if eval_summary_manager:
          self.eval_summary_manager = eval_summary_manager
        else:
          self.eval_summary_manager = utils.SummaryManager(
              eval_summary_dir, tf.summary.scalar, global_step=self.global_step)

    tf.summary.experimental.set_step(self.global_step)

    # Restores the model if needed.
    if self.checkpoint_manager is not None:
      restored_path = self.restore_checkpoint()
      if restored_path:
        _log(f"restored from checkpoint: {restored_path}")
def evaluate_continuously(
      self,
      steps: int = -1,
      timeout: Optional[Union[int, float]] = None,
      timeout_fn: Optional[Callable[[], bool]] = None,
  ) -> Optional[runner.Output]:
    
    
    self._require("evaluator", for_method="evaluate_continuously")
    self._require("checkpoint_manager", for_method="evaluate_continuously")

    output = None
    for checkpoint_path in tf.train.checkpoints_iterator(
        self.checkpoint_manager.directory,
        timeout=timeout,
        timeout_fn=timeout_fn):
      self.restore_checkpoint(checkpoint_path)
      output = self.evaluate(steps)
    return output
def get_label_map_dict(label_map_path, use_display_name=False):
  
  
  label_map = load_labelmap(label_map_path)
  label_map_dict = {}
  for item in label_map.item:
    if use_display_name:
      label_map_dict[item.display_name] = item.id
    else:
      label_map_dict[item.name] = item.id
  return label_map_dict
def test_checkpoint(ray_start_4_cpus):
    
    trainer = DataParallelTrainer(
        train_loop_per_worker=checkpoint_train_func,
        scaling_config=scale_config,
    )
    result = trainer.fit()
    assert load_dict_checkpoint(result.checkpoint)["epoch"] == NUM_EPOCHS - 1
def get(self):
        
        
        context = {
            "job_id": self.job_id,
            "node_id": self.node_id,
            "namespace": self.namespace,
        }
        if self.worker.mode == ray.worker.WORKER_MODE:
            if self.task_id is not None:
                context["task_id"] = self.task_id
            if self.actor_id is not None:
                context["actor_id"] = self.actor_id

        return context
def parse_multi(choice, end=None):
    
    

    
    end = end or str(len(g.model))
    pattern = r'(?<![-\d\[\]])(\d+-\d+|-\d+|\d+-|\d+)(?:\[(\d+)\])?(?![-\d\[\]])'
    items = re.findall(pattern, choice)
    alltracks = []

    for x, nreps in items:
        # nreps is in the inclusive range [1,100]
        nreps = min(int(nreps), 100) if nreps else 1
        for _ in range(nreps):

            if x.startswith("-"):
                x = "1" + x

            elif x.endswith("-"):
                x = x + str(end)

            if "-" in x:
                nrange = x.split("-")
                startend = map(int, nrange)
                alltracks += _bi_range(*startend)

            else:
                alltracks.append(int(x))

    return alltracks
def yt_datetime(yt_date_time):
      
    time_obj = time.strptime(yt_date_time, "%Y-%m-%dT%H:%M:%S.%fZ")
    locale_date = time.strftime("%x", time_obj)
    locale_time = time.strftime("%X", time_obj)
    # strip first two digits of four digit year
    short_date = re.sub(r"(\d\d\D\d\d\D)20(\d\d)$", r"\1\2", locale_date)
    return time_obj, short_date, locale_time
def on_off_states(self, on_states: Set, off_state: str) -> None:
        
        for on_state in on_states:
            if on_state not in self.on_off_mapping:
                self.on_off_mapping[on_state] = off_state

        if len(on_states) == 1 and off_state not in self.off_on_mapping:
            self.off_on_mapping[off_state] = list(on_states)[0]

        self.on_states_by_domain[current_domain.get()] = set(on_states)
def _create_shell_command(self) -> Tuple[List, Dict]:
        
        
        
        env_vars = {"KINESIS_MOCK_PLAIN_PORT": self.port, "SHARD_LIMIT": config.KINESIS_SHARD_LIMIT}

        latency_params = [
            "CREATE_STREAM_DURATION",
            "DELETE_STREAM_DURATION",
            "REGISTER_STREAM_CONSUMER_DURATION",
            "START_STREAM_ENCRYPTION_DURATION",
            "STOP_STREAM_ENCRYPTION_DURATION",
            "DEREGISTER_STREAM_CONSUMER_DURATION",
            "MERGE_SHARDS_DURATION",
            "SPLIT_SHARD_DURATION",
            "UPDATE_SHARD_COUNT_DURATION",
        ]
        for param in latency_params:
            env_vars[param] = self._latency

        if self._data_dir:
            env_vars["SHOULD_PERSIST_DATA"] = "true"
            env_vars["PERSIST_PATH"] = self._data_dir
            env_vars["PERSIST_INTERVAL"] = config.KINESIS_MOCK_PERSIST_INTERVAL

        env_vars["LOG_LEVEL"] = self._log_level
        if self._initialize_streams:
            env_vars["INITIALIZE_STREAMS"] = self._initialize_streams

        if self._bin_path.endswith(".jar"):
            cmd = ["java", "-XX:+UseG1GC", "-jar", self._bin_path]
        else:
            chmod_r(self._bin_path, 0o777)
            cmd = [self._bin_path, "--gc=G1"]
        return cmd, env_vars
def _create_kinesis_mock_server(self, account_id: str) -> KinesisMockServer:
        
        
        
        port = get_free_tcp_port()
        kinesismock_package.install()
        kinesis_mock_js_path = Path(kinesismock_package.get_installer().get_executable_path())

        # kinesis-mock stores state in json files <account_id>.json, so we can dump everything into `kinesis/`
        persist_path = os.path.join(config.dirs.data, "kinesis")
        mkdir(persist_path)
        if config.KINESIS_MOCK_LOG_LEVEL:
            log_level = config.KINESIS_MOCK_LOG_LEVEL.upper()
        elif config.LS_LOG:
            if config.LS_LOG == "warning":
                log_level = "WARN"
            else:
                log_level = config.LS_LOG.upper()
        else:
            log_level = "INFO"
        latency = config.KINESIS_LATENCY + "ms"

        server = KinesisMockServer(
            port=port,
            js_path=kinesis_mock_js_path,
            log_level=log_level,
            latency=latency,
            data_dir=persist_path,
            account_id=account_id,
        )
        return server
def suggest_name(cls, ops):
        
        
        
        if len(ops) == 1:
            if isinstance(ops[0], operations.CreateModel):
                return ops[0].name.lower()
            elif isinstance(ops[0], operations.DeleteModel):
                return "delete_%s" % ops[0].name.lower()
            elif isinstance(ops[0], operations.AddField):
                return "%s_%s" % (ops[0].model_name.lower(), ops[0].name.lower())
            elif isinstance(ops[0], operations.RemoveField):
                return "remove_%s_%s" % (ops[0].model_name.lower(), ops[0].name.lower())
        elif all(isinstance(o, operations.CreateModel) for o in ops):
            return "_".join(sorted(o.name.lower() for o in ops))
        return "auto_%s" % datetime.datetime.now().strftime("%Y%m%d_%H%M")
def check_dependency(self, operation, dependency):
        
        
        
        # Created model
        if dependency[2] is None and dependency[3] is True:
            return (
                isinstance(operation, operations.CreateModel) and
                operation.name.lower() == dependency[1].lower()
            )
        # Created field
        elif dependency[2] is not None and dependency[3] is True:
            return (
                (
                    isinstance(operation, operations.CreateModel) and
                    operation.name.lower() == dependency[1].lower() and
                    any(dependency[2] == x for x, y in operation.fields)
                ) or
                (
                    isinstance(operation, operations.AddField) and
                    operation.model_name.lower() == dependency[1].lower() and
                    operation.name.lower() == dependency[2].lower()
                )
            )
        # Removed field
        elif dependency[2] is not None and dependency[3] is False:
            return (
                isinstance(operation, operations.RemoveField) and
                operation.model_name.lower() == dependency[1].lower() and
                operation.name.lower() == dependency[2].lower()
            )
        # Removed model
        elif dependency[2] is None and dependency[3] is False:
            return (
                isinstance(operation, operations.DeleteModel) and
                operation.name.lower() == dependency[1].lower()
            )
        # Field being altered
        elif dependency[2] is not None and dependency[3] == "alter":
            return (
                isinstance(operation, operations.AlterField) and
                operation.model_name.lower() == dependency[1].lower() and
                operation.name.lower() == dependency[2].lower()
            )
        # order_with_respect_to being unset for a field
        elif dependency[2] is not None and dependency[3] == "order_wrt_unset":
            return (
                isinstance(operation, operations.AlterOrderWithRespectTo) and
                operation.name.lower() == dependency[1].lower() and
                (operation.order_with_respect_to or "").lower() != dependency[2].lower()
            )
        # Field is removed and part of an index/unique_together
        elif dependency[2] is not None and dependency[3] == "foo_together_change":
            if operation.name.lower() == dependency[1].lower():
                return (
                    (
                        isinstance(operation, operations.AlterUniqueTogether) and
                        any(dependency[2] not in t for t in operation.unique_together)
                    ) or
                    (
                        isinstance(operation, operations.AlterIndexTogether) and
                        any(dependency[2] not in t for t in operation.index_together)
                    )
                )
        # Unknown dependency. Raise an error.
        else:
            raise ValueError("Can't handle dependency %r" % (dependency, ))
def parse_number(cls, name):
        
        
        
        if squashed_match := re.search(r'.*_squashed_(\d+)', name):
            return int(squashed_match[1])
        match = re.match(r'^\d+', name)
        if match:
            return int(match[0])
        return None
def generate_renamed_fields(self):
        
        for (
            rem_app_label,
            rem_model_name,
            rem_db_column,
            rem_field_name,
            app_label,
            model_name,
            field,
            field_name,
        ) in self.renamed_operations:
            # A db_column mismatch requires a prior noop AlterField for the
            # subsequent RenameField to be a noop on attempts at preserving the
            # old name.
            if rem_db_column != field.db_column:
                altered_field = field.clone()
                altered_field.name = rem_field_name
                self.add_operation(
                    app_label,
                    operations.AlterField(
                        model_name=model_name,
                        name=rem_field_name,
                        field=altered_field,
                    ),
                )
            self.add_operation(
                app_label,
                operations.RenameField(
                    model_name=model_name,
                    old_name=rem_field_name,
                    new_name=field_name,
                ),
            )
            self.old_field_keys.remove((rem_app_label, rem_model_name, rem_field_name))
            self.old_field_keys.add((app_label, model_name, field_name))
def _assert_is_current_event_loop(self):
        
        
        try:
            current = events.get_event_loop()
        except AssertionError:
            return
        if current is not self:
            raise RuntimeError(
                "Non-thread-safe operation invoked on an event loop other "
                "than the current one")
def stop(self):
        
        
        self._stopping = True
def DER(
    ref_rttm,
    sys_rttm,
    ignore_overlap=False,
    collar=0.25,
    individual_file_scores=False,
):
    
    

    curr = os.path.abspath(os.path.dirname(__file__))
    mdEval = os.path.join(curr, "../../tools/der_eval/md-eval.pl")

    cmd = [
        mdEval,
        "-af",
        "-r",
        ref_rttm,
        "-s",
        sys_rttm,
        "-c",
        str(collar),
    ]
    if ignore_overlap:
        cmd.append("-1")

    try:
        stdout = subprocess.check_output(cmd, stderr=subprocess.STDOUT)

    except subprocess.CalledProcessError as ex:
        stdout = ex.output

    else:
        stdout = stdout.decode("utf-8")

        # Get all recording IDs
        file_ids = [m.strip() for m in FILE_IDS.findall(stdout)]
        file_ids = [
            file_id[2:] if file_id.startswith("f=") else file_id
            for file_id in file_ids
        ]

        scored_speaker_times = np.array(
            [float(m) for m in SCORED_SPEAKER_TIME.findall(stdout)]
        )

        miss_speaker_times = np.array(
            [float(m) for m in MISS_SPEAKER_TIME.findall(stdout)]
        )

        fa_speaker_times = np.array(
            [float(m) for m in FA_SPEAKER_TIME.findall(stdout)]
        )

        error_speaker_times = np.array(
            [float(m) for m in ERROR_SPEAKER_TIME.findall(stdout)]
        )

        with np.errstate(invalid="ignore", divide="ignore"):
            tot_error_times = (
                miss_speaker_times + fa_speaker_times + error_speaker_times
            )
            miss_speaker_frac = miss_speaker_times / scored_speaker_times
            fa_speaker_frac = fa_speaker_times / scored_speaker_times
            sers_frac = error_speaker_times / scored_speaker_times
            ders_frac = tot_error_times / scored_speaker_times

        # Values in percentage of scored_speaker_time
        miss_speaker = rectify(miss_speaker_frac)
        fa_speaker = rectify(fa_speaker_frac)
        sers = rectify(sers_frac)
        ders = rectify(ders_frac)

        if individual_file_scores:
            return miss_speaker, fa_speaker, sers, ders
        else:
            return miss_speaker[-1], fa_speaker[-1], sers[-1], ders[-1]
def for_provider(
        provider: ServiceProvider,
        dispatch_table_factory: Callable[[ServiceProvider], DispatchTable] = None,
        service_lifecycle_hook: ServiceLifecycleHook = None,
    ) -> "Service":
        
        
        
        # determine the service_lifecycle_hook
        if service_lifecycle_hook is None:
            if isinstance(provider, ServiceLifecycleHook):
                service_lifecycle_hook = provider

        # determine the delegate for injecting into the skeleton
        delegate = dispatch_table_factory(provider) if dispatch_table_factory else provider
        service = Service(
            name=provider.service,
            skeleton=Skeleton(load_service(provider.service), delegate),
            lifecycle_hook=service_lifecycle_hook,
        )
        service._provider = provider

        return service
def delete(
        self,
        inputs: InputDeleteType,
        on_done: CallbackFnType = None,
        on_error: CallbackFnType = None,
        on_always: CallbackFnType = None,
        **kwargs
    ) -> None:
        
        
        self.mode = RequestType.DELETE
        return run_async(
            self._get_results, inputs, on_done, on_error, on_always, **kwargs
        )
def reload(
        self,
        targets: Union[str, List[str]],
        on_done: CallbackFnType = None,
        on_error: CallbackFnType = None,
        on_always: CallbackFnType = None,
        **kwargs
    ):
        
        

        if isinstance(targets, str):
            targets = [targets]
        kwargs['targets'] = targets

        self.mode = RequestType.CONTROL
        return run_async(
            self._get_results,
            [],
            on_done,
            on_error,
            on_always,
            command='RELOAD',
            **kwargs
        )
def get_stack_template(self, stack_name: str, stage: str) -> str:
        
        
        
        try:
            resp = self._client.get_template(StackName=stack_name, TemplateStage=stage)
            template = resp.get("TemplateBody", "")

            # stack may not have template, check the change set
            if not template:
                change_set_name = self._get_change_set_name(stack_name)

                if change_set_name:
                    # the stack has a change set, use the template from this
                    resp = self._client.get_template(
                        StackName=stack_name, TemplateStage=stage, ChangeSetName=change_set_name
                    )
                    template = resp.get("TemplateBody", "")

            return str(template)

        except (ClientError, BotoCoreError) as e:
            # If there are credentials, environment errors,
            # catch that and throw a delete failed error.

            LOG.error("Failed to fetch template for the stack : %s", str(e))
            raise FetchTemplateFailedError(stack_name=stack_name, msg=str(e)) from e
        except FetchChangeSetError as ex:
            raise FetchTemplateFailedError(stack_name=stack_name, msg=str(ex)) from ex
        except NoChangeSetFoundError as ex:
            msg = "Failed to find a change set to fetch the template"
            raise FetchTemplateFailedError(stack_name=stack_name, msg=msg) from ex
        except Exception as e:
            # We don't know anything about this exception. Don't handle
            LOG.error("Unable to get stack details.", exc_info=e)
            raise e
def _similarity_search_with_score(
        self,
        embeddings: List[float],
        k: int = 4,
        kind: CosmosDBVectorSearchType = CosmosDBVectorSearchType.VECTOR_IVF,
        ef_search: int = 40,
        score_threshold: float = 0.0,
    ) -> List[Tuple[Document, float]]:
        
        
        pipeline: List[dict[str, Any]] = []
        if kind == CosmosDBVectorSearchType.VECTOR_IVF:
            pipeline = self._get_pipeline_vector_ivf(embeddings, k)
        elif kind == CosmosDBVectorSearchType.VECTOR_HNSW:
            pipeline = self._get_pipeline_vector_hnsw(embeddings, k, ef_search)

        cursor = self._collection.aggregate(pipeline)

        docs = []
        for res in cursor:
            score = res.pop("similarityScore")
            if score < score_threshold:
                continue
            document_object_field = (
                res.pop("document")
                if kind == CosmosDBVectorSearchType.VECTOR_IVF
                else res
            )
            text = document_object_field.pop(self._text_key)
            docs.append(
                (Document(page_content=text, metadata=document_object_field), score)
            )
        return docs
def create_virtual_environment(
    args: EnvironmentConfig,
    python: PythonConfig,
    path: str,
    system_site_packages: bool = False,
    pip: bool = False,
) -> bool:
    
    if not os.path.exists(python.path):
        # the requested python version could not be found
        return False

    # creating a virtual environment using 'venv' when running in a virtual environment created by 'virtualenv' results
    # in a copy of the original virtual environment instead of creation of a new one
    # avoid this issue by only using "real" python interpreters to invoke 'venv'
    for real_python in iterate_real_pythons(python.version):
        if run_venv(args, real_python, system_site_packages, pip, path):
            display.info('Created Python %s virtual environment using "venv": %s' % (python.version, path), verbosity=1)
            return True

    # something went wrong, most likely the package maintainer for the Python installation removed ensurepip
    # which will prevent creation of a virtual environment without installation of other OS packages
    return False
def test_cors_options(self):
        
        
        
        response = requests.options(
            self.url + "/anypath/anypath", headers={"Origin": "https://example.com"}, timeout=300
        )

        self.assertEqual(response.status_code, 200)

        self.assertEqual(response.headers.get("Access-Control-Allow-Origin"), "https://example.com")
        self.assertEqual(response.headers.get("Access-Control-Allow-Headers"), "x-apigateway-header")
        self.assertEqual(response.headers.get("Access-Control-Allow-Methods"), "GET,OPTIONS")
        self.assertEqual(response.headers.get("Access-Control-Allow-Credentials"), "true")
        self.assertEqual(response.headers.get("Access-Control-Max-Age"), "600")
def check_changes(self, args, results):  # type: (SanityConfig, Results) -> None
        
        integration_targets = list(walk_integration_targets())
        module_targets = list(walk_module_targets())

        integration_targets_by_name = dict((target.name, target) for target in integration_targets)
        module_names_by_path = dict((target.path, target.module) for target in module_targets)

        disabled_targets = []
        unstable_targets = []
        unsupported_targets = []

        for command in [command for command in args.metadata.change_description.focused_command_targets if 'integration' in command]:
            for target in args.metadata.change_description.focused_command_targets[command]:
                if self.DISABLED in integration_targets_by_name[target].aliases:
                    disabled_targets.append(target)
                elif self.UNSTABLE in integration_targets_by_name[target].aliases:
                    unstable_targets.append(target)
                elif self.UNSUPPORTED in integration_targets_by_name[target].aliases:
                    unsupported_targets.append(target)

        untested_modules = []

        for path in args.metadata.change_description.no_integration_paths:
            module = module_names_by_path.get(path)

            if module:
                untested_modules.append(module)

        comments = [
            self.format_comment(self.TEMPLATE_DISABLED, disabled_targets),
            self.format_comment(self.TEMPLATE_UNSTABLE, unstable_targets),
            self.format_comment(self.TEMPLATE_UNSUPPORTED, unsupported_targets),
            self.format_comment(self.TEMPLATE_UNTESTED, untested_modules),
        ]

        comments = [comment for comment in comments if comment]

        labels = dict(
            needs_tests=bool(untested_modules),
            disabled_tests=bool(disabled_targets),
            unstable_tests=bool(unstable_targets),
            unsupported_tests=bool(unsupported_targets),
        )

        results.comments += comments
        results.labels.update(labels)
def dividend(
        self,
        start_date: Annotated[
            Union[datetime.date, None, str],
            OpenBBCustomParameter(
                description="Start date of the data, in YYYY-MM-DD format."
            ),
        ] = None,
        end_date: Annotated[
            Union[datetime.date, None, str],
            OpenBBCustomParameter(
                description="End date of the data, in YYYY-MM-DD format."
            ),
        ] = None,
        provider: Optional[Literal["fmp"]] = None,
        **kwargs
    ) -> OBBject:
        
          # noqa: E501

        return self._run(
            "/equity/calendar/dividend",
            **filter_inputs(
                provider_choices={
                    "provider": self._get_provider(
                        provider,
                        "/equity/calendar/dividend",
                        ("fmp",),
                    )
                },
                standard_params={
                    "start_date": start_date,
                    "end_date": end_date,
                },
                extra_params=kwargs,
            )
        )
def _fetch_brute_kddcup99(data_home=None, download_if_missing=True, percent10=True):
    

    

    data_home = get_data_home(data_home=data_home)
    dir_suffix = "-py3"

    if percent10:
        kddcup_dir = join(data_home, "kddcup99_10" + dir_suffix)
        archive = ARCHIVE_10_PERCENT
    else:
        kddcup_dir = join(data_home, "kddcup99" + dir_suffix)
        archive = ARCHIVE

    samples_path = join(kddcup_dir, "samples")
    targets_path = join(kddcup_dir, "targets")
    available = exists(samples_path)

    dt = [
        ("duration", int),
        ("protocol_type", "S4"),
        ("service", "S11"),
        ("flag", "S6"),
        ("src_bytes", int),
        ("dst_bytes", int),
        ("land", int),
        ("wrong_fragment", int),
        ("urgent", int),
        ("hot", int),
        ("num_failed_logins", int),
        ("logged_in", int),
        ("num_compromised", int),
        ("root_shell", int),
        ("su_attempted", int),
        ("num_root", int),
        ("num_file_creations", int),
        ("num_shells", int),
        ("num_access_files", int),
        ("num_outbound_cmds", int),
        ("is_host_login", int),
        ("is_guest_login", int),
        ("count", int),
        ("srv_count", int),
        ("serror_rate", float),
        ("srv_serror_rate", float),
        ("rerror_rate", float),
        ("srv_rerror_rate", float),
        ("same_srv_rate", float),
        ("diff_srv_rate", float),
        ("srv_diff_host_rate", float),
        ("dst_host_count", int),
        ("dst_host_srv_count", int),
        ("dst_host_same_srv_rate", float),
        ("dst_host_diff_srv_rate", float),
        ("dst_host_same_src_port_rate", float),
        ("dst_host_srv_diff_host_rate", float),
        ("dst_host_serror_rate", float),
        ("dst_host_srv_serror_rate", float),
        ("dst_host_rerror_rate", float),
        ("dst_host_srv_rerror_rate", float),
        ("labels", "S16"),
    ]

    column_names = [c[0] for c in dt]
    target_names = column_names[-1]
    feature_names = column_names[:-1]

    if available:
        try:
            X = joblib.load(samples_path)
            y = joblib.load(targets_path)
        except Exception as e:
            raise OSError(
                "The cache for fetch_kddcup99 is invalid, please delete "
                f"{str(kddcup_dir)} and run the fetch_kddcup99 again"
            ) from e

    elif download_if_missing:
        _mkdirp(kddcup_dir)
        logger.info("Downloading %s" % archive.url)
        _fetch_remote(archive, dirname=kddcup_dir)
        DT = np.dtype(dt)
        logger.debug("extracting archive")
        archive_path = join(kddcup_dir, archive.filename)
        file_ = GzipFile(filename=archive_path, mode="r")
        Xy = []
        for line in file_.readlines():
            line = line.decode()
            Xy.append(line.replace("\n", "").split(","))
        file_.close()
        logger.debug("extraction done")
        os.remove(archive_path)

        Xy = np.asarray(Xy, dtype=object)
        for j in range(42):
            Xy[:, j] = Xy[:, j].astype(DT[j])

        X = Xy[:, :-1]
        y = Xy[:, -1]
        # XXX bug when compress!=0:
        # (error: 'Incorrect data length while decompressing[...] the file
        #  could be corrupted.')

        joblib.dump(X, samples_path, compress=0)
        joblib.dump(y, targets_path, compress=0)
    else:
        raise OSError("Data not found and `download_if_missing` is False")

    return Bunch(
        data=X,
        target=y,
        feature_names=feature_names,
        target_names=[target_names],
    )
def is_project_user(user, project):
    
    return user in AdminPermission.members(project)
def __init__(
        self,
        file_path: Union[str, Path, List[str], List[Path]],
        output_type: Union[OutputType, dict] = "html",
        split: SplitType = "none",
        api_key: Optional[str] = None,
        use_ocr: bool = False,
        exclude: list = ["header", "footer"],
    ):
        
        
        
        self.file_path = file_path
        self.output_type = output_type
        self.split = split
        self.api_key = get_from_param_or_env(
            "UPSTAGE_DOCUMENT_AI_API_KEY", api_key, "UPSTAGE_DOCUMENT_AI_API_KEY"
        )
        self.use_ocr = use_ocr
        self.exclude = exclude

        validate_file_path(self.file_path)
        validate_api_key(self.api_key)
def _split_relationship_line(line: str) -> DotClassRelationship:
        
        
        
        splitters = [" -> ", " [", "];"]
        idxs = []
        for tag in splitters:
            if tag not in line:
                return None
            idxs.append(line.find(tag))
        ret = DotClassRelationship()
        ret.src = line[0 : idxs[0]].strip('"')
        ret.dest = line[idxs[0] + len(splitters[0]) : idxs[1]].strip('"')
        properties = line[idxs[1] + len(splitters[1]) : idxs[2]].strip(" ")
        mappings = {
            'arrowhead="empty"': GENERALIZATION,
            'arrowhead="diamond"': COMPOSITION,
            'arrowhead="odiamond"': AGGREGATION,
        }
        for k, v in mappings.items():
            if k in properties:
                ret.relationship = v
                if v != GENERALIZATION:
                    ret.label = RepoParser._get_label(properties)
                break
        return ret
def inc_rstate(self):
        
        self.rstate = ff_2p134(self.rstate)
        assert self.rstate.dtype == numpy.int32
def uniform(self, size, low=0.0, high=1.0, ndim=None, dtype=None,
                nstreams=None):
        
        
        
        low = as_tensor_variable(low)
        high = as_tensor_variable(high)
        if dtype is None:
            dtype = scal.upcast(config.floatX, low.dtype, high.dtype)

        low = cast(low, dtype=dtype)
        high = cast(high, dtype=dtype)

        if isinstance(size, tuple):
            msg = "size must be a tuple of int or a Theano variable"
            assert all([isinstance(i, (numpy.integer, int, Variable))
                        for i in size]), msg
            if any([isinstance(i, (numpy.integer, int)) and i <= 0
                    for i in size]):
                raise ValueError(
                    "The specified size contains a dimension with value <= 0",
                    size)

        else:
            if not (isinstance(size, Variable) and size.ndim == 1):
                raise TypeError("size must be a tuple of int or a Theano "
                                "Variable with 1 dimension, got " + str(size) +
                                " of type " + str(type(size)))

        if nstreams is None:
            nstreams = self.n_streams(size)

        if self.use_cuda and dtype == 'float32':
            rstates = self.get_substream_rstates(nstreams)
            rstates = rstates.flatten()
            # HACK - we use fact that int32 and float32 have same size to
            # sneak ints into the CudaNdarray type.
            # these *SHOULD NEVER BE USED AS FLOATS*
            tmp_float_buf = numpy.frombuffer(rstates.data, dtype='float32')
            assert tmp_float_buf.shape == rstates.shape
            assert (tmp_float_buf.view('int32') == rstates).all()
            # transfer to device
            node_rstate = float32_shared_constructor(tmp_float_buf)
            assert isinstance(node_rstate.type, CudaNdarrayType)

            # we can't use the normal mrg_uniform constructor + later
            # optimization
            # because of the tmp_float_buf hack above.  There is
            # currently no Theano node that will do a frombuffer
            # reinterpretation.
            u = self.pretty_return(node_rstate,
                                   *GPU_mrg_uniform.new(node_rstate,
                                                        ndim, dtype, size))
        else:
            node_rstate = shared(self.get_substream_rstates(nstreams))
            u = self.pretty_return(node_rstate,
                                   *mrg_uniform.new(node_rstate,
                                                    ndim, dtype, size))
        r = u * (high - low) + low

        if u.type.broadcastable != r.type.broadcastable:
            raise NotImplementedError(
                'Increase the size to match the broadcasting pattern of '
                '`low` and `high` arguments')

        assert r.dtype == dtype
        return r
def __init__(self, seed=12345, use_cuda=None):
        
        

        
        super(MRG_RandomStreams, self).__init__()
        if isinstance(seed, int):
            if seed == 0:
                raise ValueError('seed should not be 0', seed)
            elif seed >= M2:
                raise ValueError('seed should be less than %i' % M2, seed)
            self.rstate = numpy.asarray([seed]*6, dtype='int32')
        elif len(seed)==6:
            if seed[0] == 0 and seed[1] == 0 and seed[2] == 0:
                raise ValueError('The first 3 values of seed should not be all 0', seed)
            if seed[3] == 0 and seed[4] == 0 and seed[5] == 0:
                raise ValueError('The last 3 values of seed should not be all 0', seed)
            if seed[0] >= M1 or seed[1] >= M1 or seed[2] >= M1:
                raise ValueError('The first 3 values of seed should be less than %i' % M1, seed)
            if seed[3] >= M2 or seed[4] >= M2 or seed[5] >= M2:
                raise ValueError('The last 3 values of seed should be less than %i' % M2, seed)
            self.rstate = numpy.asarray(seed, dtype='int32')
        else:
            raise TypeError("seed should be 1 integer or 6 integers")
        if use_cuda is None:
            self.use_cuda = cuda_enabled
        else:
            self.use_cuda = use_cuda
def normal(self, size=None, avg=0.0, std=1.0, ndim=None,
               dtype=None, nstreams=None):
        
        
        
        # We need an even number of ]0,1[ samples. Then we split them
        # in two halves. First half becomes our U1's for Box-Muller,
        # second half our U2's. See Wikipedia page:
        # http://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform
        avg = as_tensor_variable(avg)
        std = as_tensor_variable(std)

        if dtype is None:
            dtype = scal.upcast(config.floatX, avg.dtype, std.dtype)

        avg = cast(avg, dtype)
        std = cast(std, dtype)

        evened = False
        constant = False
        if isinstance(size, tuple) and all([isinstance(i,int) for i in size]):
            constant = True
            n_samples = numpy.prod(size)

            if n_samples % 2 == 1:
                n_samples += 1
                evened = True
        else:
            #if even, don't change, if odd, +1
            n_samples = prod(size)+(prod(size)%2)
        flattened = self.uniform(size=(n_samples,), dtype=dtype,
                                 nstreams=nstreams)

        if constant:
            U1 = flattened[:n_samples // 2]
            U2 = flattened[n_samples // 2:]
        else:
            U1 = flattened[:prod(flattened.shape) // 2]
            U2 = flattened[prod(flattened.shape) // 2:]

        #normal_samples = zeros_like(flattened)
        sqrt_ln_U1 = sqrt(-2.0 * log(U1))
        # TypeError: 'TensorVariable' object does not support item assignment
        # so this doesn't work...
        #normal_samples[:n_samples/2] = sqrt_ln_U1 * cos(2.0*numpy.pi*U2)
        #normal_samples[n_samples/2:] = sqrt_ln_U1 * sin(2.0*numpy.pi*U2)

        # so trying this instead
        first_half = sqrt_ln_U1 * cos(numpy.array(2.0 * numpy.pi, dtype=dtype) * U2)
        second_half = sqrt_ln_U1 * sin(numpy.array(2.0 * numpy.pi, dtype=dtype)*U2)
        normal_samples = join(0, first_half, second_half)

        final_samples = None
        if evened:
            final_samples = normal_samples[:-1]
        elif constant:
            final_samples = normal_samples
        else:
            final_samples = normal_samples[:prod(size)]

        if size:
            final_samples = final_samples.reshape(size)

        final_samples = avg + std * final_samples

        assert final_samples.dtype == dtype
        return final_samples
def guess_n_streams(size, warn=True):
    
    
    
    # TODO: a smart way of choosing the number of streams, see #612.
    # Note that this code was moved out of `MRG_RandomStreams` so that it can
    # be easily accessed from tests, where we want to disable the warning.
    if (isinstance(size, (tuple, list)) and
        all([isinstance(i, int) for i in size])):
        # We can make a guess.
        r = 1
        for s in size:
            r *= s
        if r > 6:
            r = r/6 # chosen as fastest for rbm_benchmark

        # The purpose of sampling from many streams is to be able to use
        # the GPU to its full capacity.  It just wastes RAM and stream-initialization time to
        # allocate more streams than necessary for the GPU.
        # XXX: This number is chosen to be good for 280 and 480 architectures,
        #      Better would be to use pycuda to query the number of
        #      processors on the GPU device,
        #      rather than guessing 60.
        return min(r, 60 * 256)
    else:
        if warn:
            warnings.warn((
                    "MRG_RandomStreams Can't determine #streams from "
                    "size (%s), guessing 60*256") % str(size),
                    stacklevel=3)
        return 60 * 256
def do_include(parser, token):
    
    
    
    bits = token.split_contents()
    if len(bits) < 2:
        raise TemplateSyntaxError("%r tag takes at least one argument: the name of the template to be included." % bits[0])
    options = {}
    remaining_bits = bits[2:]
    while remaining_bits:
        option = remaining_bits.pop(0)
        if option in options:
            raise TemplateSyntaxError('The %r option was specified more '
                                      'than once.' % option)
        if option == 'with':
            value = token_kwargs(remaining_bits, parser, support_legacy=False)
            if not value:
                raise TemplateSyntaxError('"with" in %r tag needs at least '
                                          'one keyword argument.' % bits[0])
        elif option == 'only':
            value = True
        else:
            raise TemplateSyntaxError('Unknown argument for %r tag: %r.' %
                                      (bits[0], option))
        options[option] = value
    isolated_context = options.get('only', False)
    namemap = options.get('with', {})
    path = bits[1]
    if path[0] in ('"', "'") and path[-1] == path[0]:
        return ConstantIncludeNode(path[1:-1], extra_context=namemap,
                                   isolated_context=isolated_context)
    return IncludeNode(parser.compile_filter(bits[1]), extra_context=namemap,
                       isolated_context=isolated_context)
def multi_gpu_train_one_step(algorithm, train_batch) -> Dict:
    
    
    config = algorithm.config
    workers = algorithm.workers
    local_worker = workers.local_worker()
    num_sgd_iter = config.get("num_sgd_iter", 1)
    sgd_minibatch_size = config.get("sgd_minibatch_size", config["train_batch_size"])

    # Determine the number of devices (GPUs or 1 CPU) we use.
    num_devices = int(math.ceil(config["num_gpus"] or 1))

    # Make sure total batch size is dividable by the number of devices.
    # Batch size per tower.
    per_device_batch_size = sgd_minibatch_size // num_devices
    # Total batch size.
    batch_size = per_device_batch_size * num_devices
    assert batch_size % num_devices == 0
    assert batch_size >= num_devices, "Batch size too small!"

    # Handle everything as if multi-agent.
    train_batch = train_batch.as_multi_agent()

    # Load data into GPUs.
    load_timer = algorithm._timers[LOAD_BATCH_TIMER]
    with load_timer:
        num_loaded_samples = {}
        for policy_id, batch in train_batch.policy_batches.items():
            # Not a policy-to-train.
            if not local_worker.is_policy_to_train(policy_id, train_batch):
                continue

            # Decompress SampleBatch, in case some columns are compressed.
            batch.decompress_if_needed()

            # Load the entire train batch into the Policy's only buffer
            # (idx=0). Policies only have >1 buffers, if we are training
            # asynchronously.
            num_loaded_samples[policy_id] = local_worker.policy_map[
                policy_id
            ].load_batch_into_buffer(batch, buffer_index=0)

    # Execute minibatch SGD on loaded data.
    learn_timer = algorithm._timers[LEARN_ON_BATCH_TIMER]
    with learn_timer:
        # Use LearnerInfoBuilder as a unified way to build the final
        # results dict from `learn_on_loaded_batch` call(s).
        # This makes sure results dicts always have the same structure
        # no matter the setup (multi-GPU, multi-agent, minibatch SGD,
        # tf vs torch).
        learner_info_builder = LearnerInfoBuilder(num_devices=num_devices)

        for policy_id, samples_per_device in num_loaded_samples.items():
            policy = local_worker.policy_map[policy_id]
            num_batches = max(1, int(samples_per_device) // int(per_device_batch_size))
            logger.debug("== sgd epochs for {} ==".format(policy_id))
            for _ in range(num_sgd_iter):
                permutation = np.random.permutation(num_batches)
                for batch_index in range(num_batches):
                    # Learn on the pre-loaded data in the buffer.
                    # Note: For minibatch SGD, the data is an offset into
                    # the pre-loaded entire train batch.
                    results = policy.learn_on_loaded_batch(
                        permutation[batch_index] * per_device_batch_size, buffer_index=0
                    )

                    learner_info_builder.add_learn_on_batch_results(results, policy_id)

        # Tower reduce and finalize results.
        learner_info = learner_info_builder.finalize()

    load_timer.push_units_processed(train_batch.count)
    learn_timer.push_units_processed(train_batch.count)

    # TODO: Move this into Trainer's `training_iteration` method for
    #  better transparency.
    algorithm._counters[NUM_ENV_STEPS_TRAINED] += train_batch.count
    algorithm._counters[NUM_AGENT_STEPS_TRAINED] += train_batch.agent_steps()

    return learner_info
def fit(self, X, y=None):
        
        
        feature_names = []
        vocab = {}

        for x in X:
            for f, v in x.items():
                if isinstance(v, str):
                    feature_name = "%s%s%s" % (f, self.separator, v)
                    v = 1
                elif isinstance(v, Number) or (v is None):
                    feature_name = f
                elif isinstance(v, Mapping):
                    raise TypeError(f'Unsupported value type {type(v)} '
                                    f'for {f}: {v}.\n'
                                    'Mapping objects are not supported.')
                elif isinstance(v, Iterable):
                    feature_name = None
                    self._add_iterable_element(f, v, feature_names, vocab)

                if feature_name is not None:
                    if feature_name not in vocab:
                        vocab[feature_name] = len(feature_names)
                        feature_names.append(feature_name)

        if self.sort:
            feature_names.sort()
            vocab = {f: i for i, f in enumerate(feature_names)}

        self.feature_names_ = feature_names
        self.vocabulary_ = vocab

        return self
def append_conf(self):
        
        
        
        if self.config_file is None:
            raise ProjectConfigurationError(ProjectConfigurationError.NOT_FOUND)

        self.config_file = self.config_file or self.project.conf_file(self.version.slug)

        if not os.path.exists(self.config_file):
            raise UserFileNotFound(
                UserFileNotFound.FILE_NOT_FOUND.format(self.config_file)
            )

        # Allow symlinks, but only the ones that resolve inside the base directory.
        # NOTE: if something goes wrong,
        # `safe_open` raises an exception that's clearly communicated to the user.
        outfile = safe_open(
            self.config_file, "a", allow_symlinks=True, base_path=self.project_path
        )

        # Append config to project conf file
        tmpl = template_loader.get_template('doc_builder/conf.py.tmpl')
        rendered = tmpl.render(self.get_config_params())

        with outfile:
            outfile.write('\n')
            outfile.write(rendered)

        # Print the contents of conf.py in order to make the rendered
        # configfile visible in the build logs
        self.run(
            'cat',
            os.path.relpath(
                self.config_file,
                self.project_path,
            ),
            cwd=self.project_path,
        )
def _is_dense_result(x):
    
    
    
    if not isinstance(x.type, SparseType) and not isinstance(x.type, tensor.NDArrayType):
        raise NotImplementedError("this function should only be called on *results* (of type sparse.SparseType or tensor.NDArrayType), not,", x)
    return isinstance(x.type, tensor.NDArrayType)
def _is_dense_variable(x):
    
    
    
    if not isinstance(x, gof.Variable):
        raise NotImplementedError("this function should only be called on "
                                  "*variables* (of type sparse.SparseType or "
                                  "tensor.TensorType, for instance), not ", x)
    return isinstance(x.type, tensor.TensorType)
def blob_dog(image, min_sigma=1, max_sigma=50, sigma_ratio=1.6, threshold=0.5,
             overlap=.5, *, threshold_rel=None, exclude_border=False):
    
    
    image = img_as_float(image)
    float_dtype = _supported_float_type(image.dtype)
    image = image.astype(float_dtype, copy=False)

    # if both min and max sigma are scalar, function returns only one sigma
    scalar_sigma = np.isscalar(max_sigma) and np.isscalar(min_sigma)

    # Gaussian filter requires that sequence-type sigmas have same
    # dimensionality as image. This broadcasts scalar kernels
    if np.isscalar(max_sigma):
        max_sigma = np.full(image.ndim, max_sigma, dtype=float_dtype)
    if np.isscalar(min_sigma):
        min_sigma = np.full(image.ndim, min_sigma, dtype=float_dtype)

    # Convert sequence types to array
    min_sigma = np.asarray(min_sigma, dtype=float_dtype)
    max_sigma = np.asarray(max_sigma, dtype=float_dtype)

    if sigma_ratio <= 1.0:
        raise ValueError('sigma_ratio must be > 1.0')

    # k such that min_sigma*(sigma_ratio**k) > max_sigma
    k = int(np.mean(np.log(max_sigma / min_sigma) / np.log(sigma_ratio) + 1))

    # a geometric progression of standard deviations for gaussian kernels
    sigma_list = np.array([min_sigma * (sigma_ratio ** i)
                           for i in range(k + 1)])

    gaussian_images = [ndi.gaussian_filter(image, s) for s in sigma_list]

    # normalization factor for consistency in DoG magnitude
    sf = 1 / (sigma_ratio - 1)

    # computing difference between two successive Gaussian blurred images
    # to obtain an approximation of the scale invariant Laplacian of the
    # Gaussian operator
    dog_images = [
        (gaussian_images[i] - gaussian_images[i + 1]) * sf for i in range(k)
    ]

    image_cube = np.stack(dog_images, axis=-1)

    exclude_border = _format_exclude_border(image.ndim, exclude_border)
    local_maxima = peak_local_max(
        image_cube,
        threshold_abs=threshold,
        threshold_rel=threshold_rel,
        exclude_border=exclude_border,
        footprint=np.ones((3,) * (image.ndim + 1)),
    )

    # Catch no peaks
    if local_maxima.size == 0:
        return np.empty((0, 3))

    # Convert local_maxima to float64
    lm = local_maxima.astype(float_dtype)

    # translate final column of lm, which contains the index of the
    # sigma that produced the maximum intensity value, into the sigma
    sigmas_of_peaks = sigma_list[local_maxima[:, -1]]

    if scalar_sigma:
        # select one sigma column, keeping dimension
        sigmas_of_peaks = sigmas_of_peaks[:, 0:1]

    # Remove sigma index and replace with sigmas
    lm = np.hstack([lm[:, :-1], sigmas_of_peaks])

    sigma_dim = sigmas_of_peaks.shape[1]

    return _prune_blobs(lm, overlap, sigma_dim=sigma_dim)
def blob_log(image, min_sigma=1, max_sigma=50, num_sigma=10, threshold=.2,
             overlap=.5, log_scale=False):
    

    

    if image.ndim != 2:
        raise ValueError("'image' must be a grayscale ")

    image = img_as_float(image)

    if log_scale:
        start, stop = log(min_sigma, 10), log(max_sigma, 10)
        sigma_list = np.logspace(start, stop, num_sigma)
    else:
        sigma_list = np.linspace(min_sigma, max_sigma, num_sigma)

    gl_images = [-gaussian_laplace(image, s) * s ** 2 for s in sigma_list]
    image_cube = np.dstack(gl_images)

    local_maxima = peak_local_max(image_cube, threshold_abs=threshold,
                                  footprint=np.ones((3, 3, 3)),
                                  threshold_rel=0.0,
                                  exclude_border=False)

    # Convert the last index to its corresponding scale value
    local_maxima[:, 2] = sigma_list[local_maxima[:, 2]]
    ret_val = _prune_blobs(local_maxima, overlap)

    if len(ret_val) > 0:
        ret_val[:, 2] = math.pi * \
            ((ret_val[:, 2] * math.sqrt(2)) ** 2).astype(int)
        return ret_val
    else:
        return []
def blob_doh(image, min_sigma=1, max_sigma=30, num_sigma=10, threshold=0.01,
             overlap=.5, log_scale=False, *, threshold_rel=None):
    
      # noqa: E501
    check_nD(image, 2)

    image = img_as_float(image)
    float_dtype = _supported_float_type(image.dtype)
    image = image.astype(float_dtype, copy=False)

    image = integral_image(image)

    if log_scale:
        start, stop = math.log(min_sigma, 10), math.log(max_sigma, 10)
        sigma_list = np.logspace(start, stop, num_sigma)
    else:
        sigma_list = np.linspace(min_sigma, max_sigma, num_sigma)

    image_cube = np.empty(shape=image.shape + (len(sigma_list),), dtype=float_dtype)
    for j, s in enumerate(sigma_list):
        image_cube[..., j] = _hessian_matrix_det(image, s)

    local_maxima = peak_local_max(image_cube,
                                  threshold_abs=threshold,
                                  threshold_rel=threshold_rel,
                                  exclude_border=False,
                                  footprint=np.ones((3,) * image_cube.ndim))

    # Catch no peaks
    if local_maxima.size == 0:
        return np.empty((0, 3))
    # Convert local_maxima to float64
    lm = local_maxima.astype(np.float64)
    # Convert the last index to its corresponding scale value
    lm[:, -1] = sigma_list[local_maxima[:, -1]]
    return _prune_blobs(lm, overlap)
def _prune_blobs(blobs_array, overlap):
    
    
    sigma = blobs_array[:, -1].max()
    distance = 2 * sigma * sqrt(blobs_array.shape[1] - 1)
    tree = spatial.cKDTree(blobs_array[:, :-1])
    pairs = np.array(list(tree.query_pairs(distance)))
    if len(pairs) == 0:
        return blobs_array
    else:
        for (i, j) in pairs:
            blob1, blob2 = blobs_array[i], blobs_array[j]
            if _blob_overlap(blob1, blob2) > overlap:
                if blob1[-1] > blob2[-1]:
                    blob2[-1] = 0
                else:
                    blob1[-1] = 0

    return np.array([b for b in blobs_array if b[-1] > 0])
def _blob_overlap(blob1, blob2, *, sigma_dim=1):
    
    
    ndim = len(blob1) - sigma_dim
    if ndim > 3:
        return 0.0
    root_ndim = sqrt(ndim)

    # we divide coordinates by sigma * sqrt(ndim) to rescale space to isotropy,
    # giving spheres of radius = 1 or < 1.
    if blob1[-1] > blob2[-1]:
        max_sigma = blob1[-sigma_dim:]
        r1 = 1
        r2 = blob2[-1] / blob1[-1]
    else:
        max_sigma = blob2[-sigma_dim:]
        r2 = 1
        r1 = blob1[-1] / blob2[-1]
    pos1 = blob1[:ndim] / (max_sigma * root_ndim)
    pos2 = blob2[:ndim] / (max_sigma * root_ndim)

    d = np.sqrt(np.sum((pos2 - pos1)**2))
    if d > r1 + r2:  # centers farther than sum of radii, so no overlap
        return 0.0

    # one blob is inside the other
    if d <= abs(r1 - r2):
        return 1.0

    if ndim == 2:
        return _compute_disk_overlap(d, r1, r2)

    else:  # ndim=3 http://mathworld.wolfram.com/Sphere-SphereIntersection.html
        return _compute_sphere_overlap(d, r1, r2)
def list(
        self, resource: StateResource, options: ListApiOptions, _explain: bool = False
    ) -> Union[Dict, List]:
        

        
        endpoint = f"/api/v0/{resource.value}"
        params = self._make_param(options)
        list_api_response = self._make_http_get_request(
            endpoint=endpoint,
            params=params,
            timeout=options.timeout,
            resource=resource,
            _explain=_explain,
        )
        if _explain:
            self._print_list_api_warning(resource, list_api_response)
        return list_api_response["result"]
def _print_api_warning(self, resource: StateResource, api_response: dict):
        
        
        # Print warnings if anything was given.
        warning_msgs = api_response.get("partial_failure_warning", None)
        if warning_msgs:
            warnings.warn(warning_msgs)

        # Print warnings if data is truncated at the data source.
        num_after_truncation = api_response["num_after_truncation"]
        total = api_response["total"]
        if total > num_after_truncation:
            # NOTE(rickyyx): For now, there's not much users could do (neither can we),
            # with hard truncation. Unless we allow users to set a higher
            # `RAY_MAX_LIMIT_FROM_DATA_SOURCE`, the data will always be truncated at the
            # data source.
            warnings.warn(
                (
                    f"{num_after_truncation} ({total} total) {resource.value} "
                    "are retrieved from the data source. "
                    f"{total - num_after_truncation} entries have been truncated. "
                    f"Max of {num_after_truncation} entries are retrieved from data "
                    "source to prevent over-sized payloads."
                ),
            )

        # Print warnings if return data is limited at the API server due to
        # limit enforced at the server side
        num_filtered = api_response["num_filtered"]
        data = api_response["result"]
        if num_filtered > len(data):
            warnings.warn(
                (
                    f"{len(data)}/{num_filtered} {resource.value} returned. "
                    "Use `--filter` to reduce the amount of data to return or "
                    "setting a higher limit with `--limit` to see all data. "
                ),
            )

        # Print the additional warnings.
        warnings_to_print = api_response.get("warnings", [])
        if warnings_to_print:
            for warning_to_print in warnings_to_print:
                warnings.warn(warning_to_print)
def __init__(self, corpus=None, num_topics=100, id2word=None,
                 distributed=False, chunksize=2000, passes=1, update_every=1,
                 alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10,
                 iterations=50, gamma_threshold=0.001, minimum_probability=0.01,
                 random_state=None, ns_conf=None, minimum_phi_value=0.01,
                 per_word_topics=False, callbacks=None, dtype=np.float32):
        

        

        
        self.dtype = np.finfo(dtype).dtype

        # store user-supplied parameters
        self.id2word = id2word
        if corpus is None and self.id2word is None:
            raise ValueError(
                'at least one of corpus/id2word must be specified, to establish input space dimensionality'
            )

        if self.id2word is None:
            logger.warning("no word id mapping provided; initializing from corpus, assuming identity")
            self.id2word = utils.dict_from_corpus(corpus)
            self.num_terms = len(self.id2word)
        elif len(self.id2word) > 0:
            self.num_terms = 1 + max(self.id2word.keys())
        else:
            self.num_terms = 0

        if self.num_terms == 0:
            raise ValueError("cannot compute LDA over an empty collection (no terms)")

        self.distributed = bool(distributed)
        self.num_topics = int(num_topics)
        self.chunksize = chunksize
        self.decay = decay
        self.offset = offset
        self.minimum_probability = minimum_probability
        self.num_updates = 0

        self.passes = passes
        self.update_every = update_every
        self.eval_every = eval_every
        self.minimum_phi_value = minimum_phi_value
        self.per_word_topics = per_word_topics
        self.callbacks = callbacks

        self.alpha, self.optimize_alpha = self.init_dir_prior(alpha, 'alpha')
        assert self.alpha.shape == (self.num_topics,), \
            "Invalid alpha shape. Got shape %s, but expected (%d, )" % (str(self.alpha.shape), self.num_topics)

        self.eta, self.optimize_eta = self.init_dir_prior(eta, 'eta')
        assert self.eta.shape == (self.num_terms,) or self.eta.shape == (self.num_topics, self.num_terms), (
            "Invalid eta shape. Got shape %s, but expected (%d, 1) or (%d, %d)" %
            (str(self.eta.shape), self.num_terms, self.num_topics, self.num_terms))

        self.random_state = utils.get_random_state(random_state)

        # VB constants
        self.iterations = iterations
        self.gamma_threshold = gamma_threshold

        # set up distributed environment if necessary
        if not distributed:
            logger.info("using serial LDA version on this node")
            self.dispatcher = None
            self.numworkers = 1
        else:
            if self.optimize_alpha:
                raise NotImplementedError("auto-optimizing alpha not implemented in distributed LDA")
            # set up distributed version
            try:
                import Pyro4
                if ns_conf is None:
                    ns_conf = {}

                with utils.getNS(**ns_conf) as ns:
                    from gensim.models.lda_dispatcher import LDA_DISPATCHER_PREFIX
                    self.dispatcher = Pyro4.Proxy(ns.list(prefix=LDA_DISPATCHER_PREFIX)[LDA_DISPATCHER_PREFIX])
                    logger.debug("looking for dispatcher at %s" % str(self.dispatcher._pyroUri))
                    self.dispatcher.initialize(
                        id2word=self.id2word, num_topics=self.num_topics, chunksize=chunksize,
                        alpha=alpha, eta=eta, distributed=False
                    )
                    self.numworkers = len(self.dispatcher.getworkers())
                    logger.info("using distributed version with %i workers", self.numworkers)
            except Exception as err:
                logger.error("failed to initialize distributed LDA (%s)", err)
                raise RuntimeError("failed to initialize distributed LDA (%s)" % err)

        # Initialize the variational distribution q(beta|lambda)
        self.state = LdaState(self.eta, (self.num_topics, self.num_terms), dtype=self.dtype)
        self.state.sstats[...] = self.random_state.gamma(100., 1. / 100., (self.num_topics, self.num_terms))
        self.expElogbeta = np.exp(dirichlet_expectation(self.state.sstats))

        # Check that we haven't accidentally fallen back to np.float64
        assert self.eta.dtype == self.dtype
        assert self.expElogbeta.dtype == self.dtype

        # if a training corpus was provided, start estimating the model right away
        if corpus is not None:
            use_numpy = self.dispatcher is not None
            start = time.time()
            self.update(corpus, chunks_as_numpy=use_numpy)
            self.add_lifecycle_event(
                "created",
                msg=f"trained {self} in {time.time() - start:.2f}s",
            )
def clear(self):
        
        self.state = None
        self.Elogbeta = None
def update(self, corpus, chunksize=None, decay=None, passes=None, update_every=None):
        
        
        
        # use parameters given in constructor, unless user explicitly overrode them
        if chunksize is None:
            chunksize = self.chunksize
        if decay is None:
            decay = self.decay
        if passes is None:
            passes = self.passes
        if update_every is None:
            update_every = self.update_every

        # rho is the "speed" of updating; TODO try other fncs
        rho = lambda: pow(1.0 + self.num_updates, -decay)

        try:
            lencorpus = len(corpus)
        except:
            logger.warning("input corpus stream has no len(); counting documents")
            lencorpus = sum(1 for _ in corpus)
        if lencorpus == 0:
            logger.warning("LdaModel.update() called with an empty corpus")
            return

        self.state.numdocs += lencorpus

        if update_every > 0:
            updatetype = "online"
            updateafter = min(lencorpus, update_every * self.numworkers * chunksize)
        else:
            updatetype = "batch"
            updateafter = lencorpus

        updates_per_pass = max(1, lencorpus / updateafter)
        logger.info("running %s LDA training, %s topics, %i passes over "
                    "the supplied corpus of %i documents, updating model once "
                    "every %i documents" %
                    (updatetype, self.num_topics, passes, lencorpus, updateafter))
        if updates_per_pass * passes < 10:
            logger.warning("too few updates, training might not converge; consider "
                           "increasing the number of passes to improve accuracy")

        for iteration in xrange(passes):
            if self.dispatcher:
                logger.info('initializing %s workers' % self.numworkers)
                self.dispatcher.reset(self.state)
            else:
                other = LdaState(self.eta, self.state.sstats.shape)
            dirty = False

            reallen = 0
            for chunk_no, chunk in enumerate(utils.grouper(corpus, chunksize, as_numpy=True)):
                reallen += len(chunk)  # keep track of how many documents we've processed so far
                if self.dispatcher:
                    # add the chunk to dispatcher's job queue, so workers can munch on it
                    logger.info('PROGRESS: iteration %i, dispatching documents up to #%i/%i' %
                                (iteration, chunk_no * chunksize + len(chunk), lencorpus))
                    # this will eventually block until some jobs finish, because the queue has a small finite length
                    self.dispatcher.putjob(chunk)
                else:
                    logger.info('PROGRESS: iteration %i, at document #%i/%i' %
                                (iteration, chunk_no * chunksize + len(chunk), lencorpus))
                    self.do_estep(chunk, other)
                dirty = True
                del chunk

                # perform an M step. determine when based on update_every, don't do this after every chunk
                if update_every and (chunk_no + 1) % (update_every * self.numworkers) == 0:
                    if self.dispatcher:
                        # distributed mode: wait for all workers to finish
                        logger.info("reached the end of input; now waiting for all remaining jobs to finish")
                        other = self.dispatcher.getstate()
                    self.do_mstep(rho(), other)
                    del other # free up some mem

                    if self.dispatcher:
                        logger.info('initializing workers')
                        self.dispatcher.reset(self.state)
                    else:
                        other = LdaState(self.eta, self.state.sstats.shape)
                    dirty = False
            #endfor single corpus iteration
            if reallen != lencorpus:
                raise RuntimeError("input corpus size changed during training (don't use generators as input)")

            if dirty:
                # finish any remaining updates
                if self.dispatcher:
                    # distributed mode: wait for all workers to finish
                    logger.info("reached the end of input; now waiting for all remaining jobs to finish")
                    other = self.dispatcher.getstate()
                self.do_mstep(rho(), other)
                del other
                dirty = False
def update_alpha(self, gammat, rho):
        
        
        
        N = float(len(gammat))
        logphat = sum(dirichlet_expectation(gamma) for gamma in gammat) / N

        self.alpha = update_dir_prior(self.alpha, N, logphat, rho)
        logger.info("optimized alpha %s", list(self.alpha))

        return self.alpha
def show_topics(self, num_topics=10, num_words=10, log=False, formatted=True):
        
        

        
        if num_topics < 0 or num_topics >= self.num_topics:
            num_topics = self.num_topics
            chosen_topics = range(num_topics)
        else:
            num_topics = min(num_topics, self.num_topics)
            sort_alpha = self.alpha + 0.0001 * numpy.random.rand(len(self.alpha)) # add a little random jitter, to randomize results around the same alpha
            sorted_topics = list(numpy.argsort(sort_alpha))
            chosen_topics = sorted_topics[:num_topics//2] + sorted_topics[-num_topics//2:]
        shown = []
        for i in chosen_topics:
            if formatted:
                topic = self.print_topic(i, topn=num_words)
            else:
                topic = self.show_topic(i, topn=num_words)
            shown.append(topic)
            if log:
                logger.info("topic #%i (%.3f): %s" % (i, self.alpha[i], topic))
        return shown
def inference(self, chunk, collect_sstats=False):
        
        

        
        try:
            _ = len(chunk)
        except:
            chunk = list(chunk) # convert iterators/generators to plain list, so we have len() etc.
        if len(chunk) > 1:
            logger.debug("performing inference on a chunk of %i documents" % len(chunk))

        # Initialize the variational distribution q(theta|gamma) for the chunk
        gamma = numpy.random.gamma(100., 1. / 100., (len(chunk), self.num_topics))
        Elogtheta = dirichlet_expectation(gamma)
        expElogtheta = numpy.exp(Elogtheta)
        if collect_sstats:
            sstats = numpy.zeros_like(self.expElogbeta)
        else:
            sstats = None
        converged = 0

        # Now, for each document d update that document's gamma and phi
        # Inference code copied from Hoffman's `onlineldavb.py` (esp. the
        # Lee&Seung trick which speeds things up by an order of magnitude, compared
        # to Blei's original LDA-C code, cool!).
        for d, doc in enumerate(chunk):
            ids = [id for id, _ in doc]
            cts = numpy.array([cnt for _, cnt in doc])
            gammad = gamma[d, :]
            Elogthetad = Elogtheta[d, :]
            expElogthetad = expElogtheta[d, :]
            expElogbetad = self.expElogbeta[:, ids]

            # The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.
            # phinorm is the normalizer.
            phinorm = numpy.dot(expElogthetad, expElogbetad) + 1e-100 # TODO treat zeros explicitly, instead of adding eps?

            # Iterate between gamma and phi until convergence
            for _ in xrange(self.iterations):
                lastgamma = gammad
                # We represent phi implicitly to save memory and time.
                # Substituting the value of the optimal phi back into
                # the update for gamma gives this update. Cf. Lee&Seung 2001.
                gammad = self.alpha + expElogthetad * numpy.dot(cts / phinorm, expElogbetad.T)
                Elogthetad = dirichlet_expectation(gammad)
                expElogthetad = numpy.exp(Elogthetad)
                phinorm = numpy.dot(expElogthetad, expElogbetad) + 1e-100
                # If gamma hasn't changed much, we're done.
                meanchange = numpy.mean(abs(gammad - lastgamma))
                if (meanchange < self.gamma_threshold):
                    converged += 1
                    break
            gamma[d, :] = gammad
            if collect_sstats:
                # Contribution of document d to the expected sufficient
                # statistics for the M step.
                sstats[:, ids] += numpy.outer(expElogthetad.T, cts / phinorm)

        if len(chunk) > 1:
            logger.debug("%i/%i documents converged within %i iterations" %
                (converged, len(chunk), self.iterations))

        if collect_sstats:
            # This step finishes computing the sufficient statistics for the
            # M step, so that
            # sstats[k, w] = \sum_d n_{dw} * phi_{dwk}
            # = \sum_d n_{dw} * exp{Elogtheta_{dk} + Elogbeta_{kw}} / phinorm_{dw}.
            sstats *= self.expElogbeta
        return gamma, sstats
def show_topic(self, topicid, topn=10):
        
        

        
        topic = self.state.get_lambda()[topicid]
        topic = topic / topic.sum()  # normalize to probability distribution
        bestn = matutils.argsort(topic, topn, reverse=True)
        beststr = [(topic[id], self.id2word[id]) for id in bestn]
        return beststr
def load(cls, fname, *args, **kwargs):
        
        

        
        kwargs['mmap'] = kwargs.get('mmap', None)
        result = super(LdaModel, cls).load(fname, *args, **kwargs)
        state_fname = utils.smart_extension(fname, '.state')
        try:
            result.state = super(LdaModel, cls).load(state_fname, *args, **kwargs)
        except Exception as e:
            logging.warning("failed to load state from %s: %s" % (state_fname, e))
        return result
def save(self, fname, *args, **kwargs):
        
        

        
        if self.state is not None:
            self.state.save(utils.smart_extension(fname, '.state'), *args, **kwargs)
        super(LdaModel, self).save(fname, *args, ignore=['state', 'dispatcher'], **kwargs)
def get_document_topics(self, bow, minimum_probability=None, minimum_phi_probability=None, per_word_topics=False):
        
        

        
        if minimum_probability is None:
            minimum_probability = self.minimum_probability
        minimum_probability = max(minimum_probability, 1e-8)  # never allow zero values in sparse output

        if minimum_phi_probability is None:
            minimum_phi_probability = self.minimum_probability
        minimum_phi_probability = max(minimum_phi_probability, 1e-8)  # never allow zero values in sparse output

        # if the input vector is a corpus, return a transformed corpus
        is_corpus, corpus = utils.is_corpus(bow)
        if is_corpus:
            return self._apply(corpus)

        gamma, phis = self.inference([bow], collect_sstats=True)
        topic_dist = gamma[0] / sum(gamma[0])  # normalize distribution

        document_topics = [(topicid, topicvalue) for topicid, topicvalue in enumerate(topic_dist)
                    if topicvalue >= minimum_probability]
     
        if not per_word_topics:
            return document_topics
        else:
            word_topic = [] # contains word and corresponding topic
            word_phi = [] # contains word and phi values
            for word_type, weight in bow:
                phi_values = [] # contains (phi_value, topic) pairing to later be sorted
                phi_topic = [] # contains topic and corresponding phi value to be returned 'raw' to user
                for topic_id in range(0, self.num_topics):
                    if phis[topic_id][word_type] >= minimum_phi_probability:
                        # appends phi values for each topic for that word
                        # these phi values are scaled by feature length 
                        phi_values.append((phis[topic_id][word_type], topic_id))
                        phi_topic.append((topic_id, phis[topic_id][word_type]))
               
                # list with ({word_id => [(topic_0, phi_value), (topic_1, phi_value) ...]).
                word_phi.append((word_type, phi_topic))
                # sorts the topics based on most likely topic
                # returns a list like ({word_id => [topic_id_most_probable, topic_id_second_most_probable, ...]).
                sorted_phi_values = sorted(phi_values, reverse=True)
                topics_sorted = [x[1] for x in sorted_phi_values]
                word_topic.append((word_type, topics_sorted))
            return (document_topics, word_topic, word_phi)
def __getitem__(self, bow, eps=None):
        
        
        
        return self.get_document_topics(bow, eps, self.minimum_phi_value, self.per_word_topics)
def sync_state(self, current_Elogbeta=None):
        
        

        if current_Elogbeta is None:
            current_Elogbeta = self.state.get_Elogbeta()
        self.expElogbeta = np.exp(current_Elogbeta)
        assert self.expElogbeta.dtype == self.dtype
def init_dir_prior(self, prior, name):
        
        
        if prior is None:
            prior = 'symmetric'

        if name == 'alpha':
            prior_shape = self.num_topics
        elif name == 'eta':
            prior_shape = self.num_terms
        else:
            raise ValueError("'name' must be 'alpha' or 'eta'")

        is_auto = False

        if isinstance(prior, six.string_types):
            if prior == 'symmetric':
                logger.info("using symmetric %s at %s", name, 1.0 / self.num_topics)
                init_prior = np.fromiter(
                    (1.0 / self.num_topics for i in range(prior_shape)),
                    dtype=self.dtype, count=prior_shape,
                )
            elif prior == 'asymmetric':
                init_prior = np.fromiter(
                    (1.0 / (i + np.sqrt(prior_shape)) for i in range(prior_shape)),
                    dtype=self.dtype, count=prior_shape,
                )
                init_prior /= init_prior.sum()
                logger.info("using asymmetric %s %s", name, list(init_prior))
            elif prior == 'auto':
                is_auto = True
                init_prior = np.fromiter((1.0 / self.num_topics for i in range(prior_shape)),
                    dtype=self.dtype, count=prior_shape)
                if name == 'alpha':
                    logger.info("using autotuned %s, starting with %s", name, list(init_prior))
            else:
                raise ValueError("Unable to determine proper %s value given '%s'" % (name, prior))
        elif isinstance(prior, list):
            init_prior = np.asarray(prior, dtype=self.dtype)
        elif isinstance(prior, np.ndarray):
            init_prior = prior.astype(self.dtype, copy=False)
        elif isinstance(prior, (np.number, numbers.Real)):
            init_prior = np.fromiter((prior for i in range(prior_shape)), dtype=self.dtype)
        else:
            raise ValueError("%s must be either a np array of scalars, list of scalars, or scalar" % name)

        return init_prior, is_auto
def __init__(self,
                 base_iterator: Callable[[], Iterable[T]],
                 shared_metrics: SharedMetrics,
                 local_transforms: List[Callable[[Iterable], Any]] = None,
                 timeout: int = None,
                 name=None):
        
        
        assert isinstance(shared_metrics, SharedMetrics)
        self.base_iterator = base_iterator
        self.built_iterator = None
        self.local_transforms = local_transforms or []
        self.shared_metrics = shared_metrics
        self.timeout = timeout
        self.name = name or "unknown"
def __init__(self,
               is_training,
               depth_multiplier,
               min_depth,
               pad_to_multiple,
               conv_hyperparams,
               freeze_batchnorm,
               inplace_batchnorm_update,
               use_explicit_padding=False,
               use_depthwise=False,
               num_layers=6,
               override_base_feature_extractor_hyperparams=False,
               name=None):
    
    
    super(SSDKerasFeatureExtractor, self).__init__(name=name)

    self._is_training = is_training
    self._depth_multiplier = depth_multiplier
    self._min_depth = min_depth
    self._pad_to_multiple = pad_to_multiple
    self._conv_hyperparams = conv_hyperparams
    self._freeze_batchnorm = freeze_batchnorm
    self._inplace_batchnorm_update = inplace_batchnorm_update
    self._use_explicit_padding = use_explicit_padding
    self._use_depthwise = use_depthwise
    self._num_layers = num_layers
    self._override_base_feature_extractor_hyperparams = (
        override_base_feature_extractor_hyperparams)
def git_clone(repo, checkout=None):
    
    
    

    # Return repo dir
    tail = os.path.split(repo)[1]
    repo_dir = tail.rsplit('.git')[0]
    logging.debug('repo_dir is {0}'.format(repo_dir))

    if os.path.isdir(repo_dir):
        delete_repo(repo_dir)

    os.system('git clone {0}'.format(repo))

    if checkout is not None:
        subprocess.check_call(['git', 'checkout', checkout], cwd=repo_dir)

    return repo_dir
def identify_repo(repo_url):
    
    
    
    repo_url_values = repo_url.split('+')
    if len(repo_url_values) == 2:
        repo_type = repo_url_values[0]
        if repo_type in ["git", "hg"]:
            return repo_type, repo_url_values[1]
        else:
            raise UnknownRepoType
    else:
        if 'git' in repo_url:
            return 'git', repo_url
        elif 'bitbucket' in repo_url:
            return 'hg', repo_url
        else:
            raise UnknownRepoType
def clone(repo_url, checkout=None, clone_to_dir='.', no_input=False):
    
    
    # Ensure that clone_to_dir exists
    clone_to_dir = os.path.expanduser(clone_to_dir)
    make_sure_path_exists(clone_to_dir)

    # identify the repo_type
    repo_type, repo_url = identify_repo(repo_url)

    # check that the appropriate VCS for the repo_type is installed
    if not is_vcs_installed(repo_type):
        msg = f"'{repo_type}' is not installed."
        raise VCSNotInstalled(msg)

    repo_url = repo_url.rstrip('/')
    repo_name = os.path.split(repo_url)[1]
    if repo_type == 'git':
        repo_name = repo_name.split(':')[-1].rsplit('.git')[0]
        repo_dir = os.path.normpath(os.path.join(clone_to_dir, repo_name))
    if repo_type == 'hg':
        repo_dir = os.path.normpath(os.path.join(clone_to_dir, repo_name))
    logger.debug(f'repo_dir is {repo_dir}')

    if os.path.isdir(repo_dir):
        clone = prompt_and_delete(repo_dir, no_input=no_input)
    else:
        clone = True

    if clone:
        try:
            subprocess.check_output(  # nosec
                [repo_type, 'clone', repo_url],
                cwd=clone_to_dir,
                stderr=subprocess.STDOUT,
            )
            if checkout is not None:
                checkout_params = [checkout]
                # Avoid Mercurial "--config" and "--debugger" injection vulnerability
                if repo_type == "hg":
                    checkout_params.insert(0, "--")
                subprocess.check_output(  # nosec
                    [repo_type, 'checkout', *checkout_params],
                    cwd=repo_dir,
                    stderr=subprocess.STDOUT,
                )
        except subprocess.CalledProcessError as clone_error:
            output = clone_error.output.decode('utf-8')
            if 'not found' in output.lower():
                raise RepositoryNotFound(
                    f'The repository {repo_url} could not be found, '
                    'have you made a typo?'
                ) from clone_error
            if any(error in output for error in BRANCH_ERRORS):
                raise RepositoryCloneFailed(
                    f'The {checkout} branch of repository '
                    f'{repo_url} could not found, have you made a typo?'
                ) from clone_error
            logger.error('git clone failed with error: %s', output)
            raise

    return repo_dir
def get_discovery_info(platform_setup, groups, controller_id):
    
    discovery_data = {}
    for group in groups:
        groupname = group.attrib["name"]
        for product_cfg in platform_setup:
            products = group.findall(product_cfg[CONF_XPATH])
            for product in products:
                nodes = product.findall(product_cfg[CONF_NODE])
                for node in nodes:
                    if "setting" in node.attrib and node.attrib["setting"] == "yes":
                        continue
                    ihc_id = int(node.attrib["id"].strip("_"), 0)
                    name = f"{groupname}_{ihc_id}"
                    device = {
                        "ihc_id": ihc_id,
                        "ctrl_id": controller_id,
                        "product": {
                            "name": product.get("name") or "",
                            "note": product.get("note") or "",
                            "position": product.get("position") or "",
                        },
                        "product_cfg": product_cfg,
                    }
                    discovery_data[name] = device
    return discovery_data
def search(
        self,
        query: Annotated[str, OpenBBCustomParameter(description="Search query.")] = "",
        is_symbol: Annotated[
            bool,
            OpenBBCustomParameter(description="Whether to search by ticker symbol."),
        ] = False,
        use_cache: Annotated[
            Optional[bool],
            OpenBBCustomParameter(description="Whether to use the cache or not."),
        ] = True,
        provider: Optional[Literal["intrinio", "sec"]] = None,
        **kwargs
    ) -> OBBject:
        
          # noqa: E501

        return self._run(
            "/equity/search",
            **filter_inputs(
                provider_choices={
                    "provider": provider,
                },
                standard_params={
                    "query": query,
                    "is_symbol": is_symbol,
                    "use_cache": use_cache,
                },
                extra_params=kwargs,
            )
        )
def profile(
        self,
        symbol: Annotated[
            Union[str, List[str]],
            OpenBBCustomParameter(
                description="Symbol to get data for. Multiple items allowed for provider(s): fmp, intrinio, yfinance."
            ),
        ],
        provider: Optional[Literal["fmp", "intrinio", "yfinance"]] = None,
        **kwargs
    ) -> OBBject:
        
          # noqa: E501

        return self._run(
            "/equity/profile",
            **filter_inputs(
                provider_choices={
                    "provider": self._get_provider(
                        provider,
                        "/equity/profile",
                        ("fmp", "intrinio", "yfinance"),
                    )
                },
                standard_params={
                    "symbol": symbol,
                },
                extra_params=kwargs,
                extra_info={
                    "symbol": {
                        "multiple_items_allowed": ["fmp", "intrinio", "yfinance"]
                    }
                },
            )
        )
def _clear_cache(self) -> None:
        
        
        
        self._price_ranges.clear()
        self._volume_ranges.clear()
def prompt_choice_for_config(cookiecutter_dict, env, key, options, no_input):
    
    
    rendered_options = [render_variable(env, raw, cookiecutter_dict) for raw in options]
    if no_input:
        return rendered_options[0]
    return read_user_choice(key, rendered_options)
def query(self, *tags, **kwtags):
        
        
        
        opts = super(SequenceDB, self).query(*tags, **kwtags)

        position_cutoff = kwtags.pop('position_cutoff', config.optdb.position_cutoff)
        if len(tags)>=1 and isinstance(tags[0],Query):
#the call to super should have raise an error with a good message
            assert len(tags)==1
            if getattr(tags[0],'position_cutoff', None):
                position_cutoff = tags[0].position_cutoff

        opts = [o for o in opts if self.__position__[o.name] < position_cutoff]
        opts.sort(key = lambda obj: self.__position__[obj.name])
        return opt.SeqOptimizer(opts, failure_callback = self.failure_callback)
def query(self, *tags, **kwtags):
        
        
        
        opts = super(SequenceDB, self).query(*tags, **kwtags)

        position_cutoff = kwtags.pop('position_cutoff',
                                     config.optdb.position_cutoff)
        if len(tags) >= 1 and isinstance(tags[0], Query):
#the call to super should have raise an error with a good message
            assert len(tags) == 1
            if getattr(tags[0], 'position_cutoff', None):
                position_cutoff = tags[0].position_cutoff

        opts = [o for o in opts if self.__position__[o.name] < position_cutoff]
        opts.sort(key=lambda obj: self.__position__[obj.name])
        return opt.SeqOptimizer(opts, failure_callback=self.failure_callback)
def _DetectVisualStudioVersions(versions_to_check, force_express):
  
  
  version_to_year = {
      '8.0': '2005',
      '9.0': '2008',
      '10.0': '2010',
      '11.0': '2012',
      '12.0': '2013',
      '14.0': '2015',
  }
  versions = []
  for version in versions_to_check:
    # Old method of searching for which VS version is installed
    # We don't use the 2010-encouraged-way because we also want to get the
    # path to the binaries, which it doesn't offer.
    keys = [r'HKLM\Software\Microsoft\VisualStudio\%s' % version,
            r'HKLM\Software\Wow6432Node\Microsoft\VisualStudio\%s' % version,
            r'HKLM\Software\Microsoft\VCExpress\%s' % version,
            r'HKLM\Software\Wow6432Node\Microsoft\VCExpress\%s' % version]
    for index in range(len(keys)):
      path = _RegistryGetValue(keys[index], 'InstallDir')
      if not path:
        continue
      path = _ConvertToCygpath(path)
      # Check for full.
      full_path = os.path.join(path, 'devenv.exe')
      express_path = os.path.join(path, '*express.exe')
      if not force_express and os.path.exists(full_path):
        # Add this one.
        versions.append(_CreateVersion(version_to_year[version],
            os.path.join(path, '..', '..')))
      # Check for express.
      elif glob.glob(express_path):
        # Add this one.
        versions.append(_CreateVersion(version_to_year[version] + 'e',
            os.path.join(path, '..', '..')))

    # The old method above does not work when only SDK is installed.
    keys = [r'HKLM\Software\Microsoft\VisualStudio\SxS\VC7',
            r'HKLM\Software\Wow6432Node\Microsoft\VisualStudio\SxS\VC7']
    for index in range(len(keys)):
      path = _RegistryGetValue(keys[index], version)
      if not path:
        continue
      path = _ConvertToCygpath(path)
      if version != '14.0':  # There is no Express edition for 2015.
        versions.append(_CreateVersion(version_to_year[version] + 'e',
            os.path.join(path, '..'), sdk_based=True))

  return versions
def _RegistryGetValueUsingWinReg(key, value):
    
  
    from winreg import HKEY_LOCAL_MACHINE, OpenKey, QueryValueEx
    try:
        root, subkey = key.split("\\", 1)
        assert root == "HKLM"  # Only need HKLM for now.
        with OpenKey(HKEY_LOCAL_MACHINE, subkey) as hkey:
            return QueryValueEx(hkey, value)[0]
    except OSError:
        return None
def sync_device_config(self):
        
        _LOGGER.debug('Device %s settings payload: %s', self.device_id,
                      self.desired_settings_payload())
        if self.desired_settings_payload() != self.current_settings_payload():
            _LOGGER.info('pushing settings to device %s', self.device_id)
            self.client.put_settings(**self.desired_settings_payload())
def dlimport(fullpath, suffix=None):
    

    
    if suffix is None:
        if fullpath.endswith('.so'):
            suffix = '.so'
        elif fullpath.endswith('.pyd'):
            suffix = '.pyd'
        elif fullpath.endswith('.dll'):
            suffix = '.dll'
        elif fullpath.endswith('.py'):
            suffix = '.py'
        else:
            suffix = ''
    rval = None
    if fullpath.endswith(suffix):
        module_name = '.'.join(fullpath.split(os.path.sep)[-2:])[:-len(suffix)]
    else:
        raise ValueError('path has wrong suffix', (fullpath, suffix))
    workdir = fullpath[:-len(module_name)- 1 - len(suffix)]
    #debug("WORKDIR", workdir)
    #debug("module_name", module_name)

    pathcopy = list(sys.path)
    sys.path = [workdir]
    try:
        rval = __import__(module_name, {}, {}, [module_name])
        if not rval:
            error('__import__ failed', fullpath)
    finally:
        sys.path = pathcopy

    assert fullpath.startswith(rval.__file__)
    return rval
def refresh(self):
        
        
        compilelock.get_lock()
        try:
            # add entries that are not in the entry_from_key dictionary
            for root, dirs, files in os.walk(self.dirname):
                if os.path.join(root, 'key.pkl') in self.loaded_key_pkl:
                    continue
                if 'key.pkl' in files:
                    key_pkl = os.path.join(root, 'key.pkl')
                    debug('refresh adding', key_pkl)
                    try:
                        key = cPickle.load(file(key_pkl))
                    except:
                        error("ModuleCache.refresh() Failed to unpickle cache key", key_pkl)
                        info("Erasing broken file", key_pkl)
                        os.remove(key_pkl)
                        continue
                    if not key[0]: #if the version is False
                        warning("ModuleCache.refresh() Found unversioned key in cache, deleting it.", key_pkl)
                        info("Erasing broken file", key_pkl)
                        os.remove(key_pkl)
                        continue
                    if key not in self.entry_from_key:
                        entry = module_name_from_dir(root)
                        self.entry_from_key[key] = entry
                        # assert that we haven't already got this entry somehow
                        assert entry not in self.module_from_name
                        self.loaded_key_pkl.add(key_pkl)

            # remove entries that are not in the filesystem
            items_copy = list(self.entry_from_key.iteritems())
            for key, entry in items_copy:
                try:
                    # test to see that the file is [present and] readable
                    open(entry).close()
                    gone = False
                except IOError:
                    gone = True
                if gone:
                    # assert that we didn't have one of the deleted files
                    # loaded up and in use.  
                    # If so, it should not have been deleted.  This should be considered a
                    # failure of the OTHER process, that deleted it.
                    if entry in self.module_from_name:
                        error("The module %s that was loaded by this ModuleCache can no longer be read from file... this could lead to problems." % name)
                        del self.module_from_name[entry]

                    info("deleting ModuleCache entry", entry)
                    del self.entry_from_key[key]
                    if key[0]: 
                        #this is a versioned entry, so should have been on disk
                        self.loaded_key_pkl.remove(os.path.join(os.path.dirname(entry), 'key.pkl'))

        finally:
            compilelock.release_lock()
def clear(self, unversioned_min_age=None, clear_base_files=False,
              delete_if_problem=False):
        
        
        
        compilelock.get_lock()
        try:
            self.clear_old(
                    age_thresh_del=-1.0,
                    delete_if_problem=delete_if_problem)
            self.clear_unversioned(min_age=unversioned_min_age)
            if clear_base_files:
                self.clear_base_files()
        finally:
            compilelock.release_lock()
def clear_base_files(self):
        
        

        
        with compilelock.lock_ctx():
            for base_dir in ('cutils_ext', 'lazylinker_ext', 'scan_perform'):
                to_delete = os.path.join(self.dirname, base_dir + '.delete.me')
                if os.path.isdir(to_delete):
                    try:
                        shutil.rmtree(to_delete)
                        _logger.debug('Deleted: %s', to_delete)
                    except Exception:
                        _logger.warning('Could not delete %s', to_delete)
                        continue
                to_rename = os.path.join(self.dirname, base_dir)
                if os.path.isdir(to_rename):
                    try:
                        shutil.move(to_rename, to_delete)
                    except Exception:
                        _logger.warning('Could not move %s to %s',
                                        to_rename, to_delete)
def clear_old(self, age_thresh_del=None, delete_if_problem=False):
        

        
        if age_thresh_del is None:
            age_thresh_del = self.age_thresh_del

        # Ensure that the too_old_to_use list return by refresh() will
        # contain all modules older than age_thresh_del.
        if age_thresh_del < self.age_thresh_use:
            if age_thresh_del > 0:
                _logger.warning("Clearing modules that were not deemed "
                                "too old to use: age_thresh_del=%d, "
                                "self.age_thresh_use=%d",
                                age_thresh_del,
                                self.age_thresh_use)
            else:
                _logger.info("Clearing all modules.")
            age_thresh_use = age_thresh_del
        else:
            age_thresh_use = None

        too_old_to_use = self.refresh(
            age_thresh_use=age_thresh_use,
            delete_if_problem=delete_if_problem,
            # The clean up is done at init, no need to trigger it again
            cleanup=False)
        if not too_old_to_use:
            return
        with compilelock.lock_ctx():
            # Update the age of modules that have been accessed by other
            # processes and get all module that are too old to use
            # (not loaded in self.entry_from_key).

            for entry in too_old_to_use:
                # TODO: we are assuming that modules that haven't been
                # accessed in over age_thresh_del are not currently in
                # use by other processes, but that could be false for
                # long-running jobs, or if age_thresh_del < 0.
                assert entry not in self.module_from_name
                parent = os.path.dirname(entry)
                assert parent.startswith(os.path.join(self.dirname, 'tmp'))
                _rmtree(parent, msg='old cache directory', level=logging.INFO,
                        ignore_nocleanup=True)
def save_pkl(self):
        
        
        
        # Note that writing in binary mode is important under Windows.
        try:
            cPickle.dump(self, open(self.key_pkl, 'wb'),
                         protocol=cPickle.HIGHEST_PROTOCOL)
        except cPickle.PicklingError:
            warning("Cache leak due to unpickle-able key data", self.keys)
            os.remove(self.key_pkl)
            raise
def add_key(self, key, save_pkl=True):
        
        assert key not in self.keys
        self.keys.add(key)
        if save_pkl:
            self.save_pkl()
def is_same_entry(entry_1, entry_2):
    
    
    
    if entry_1 == entry_2:
        return True
    if os.path.realpath(entry_1) == os.path.realpath(entry_2):
        return True
    if (os.path.basename(entry_1) == os.path.basename(entry_2) and
        (os.path.basename(os.path.dirname(entry_1)) ==
         os.path.basename(os.path.dirname(entry_2))) and
        os.path.basename(os.path.dirname(entry_1)).startswith('tmp')):
        return True
    return False
def gcc_llvm():
     
    
    if gcc_llvm.is_llvm is None:
        pass
        p = None
        try:
            p = call_subprocess_Popen(['g++', '--version'],
                                      stdout=subprocess.PIPE,
                                      stderr=subprocess.PIPE)
            p.wait()
            output = p.stdout.read() + p.stderr.read()
        except OSError:
            # Typically means g++ cannot be found.
            # So it is not an llvm compiler.

            # Normally this should not happen as we should not try to
            # compile when g++ is not available. If this happen, it
            # will crash later so supposing it is not llvm is "safe".
            output = ''
        del p
        gcc_llvm.is_llvm = "llvm" in output
    return gcc_llvm.is_llvm
def gcc_llvm():
     
    
    if gcc_llvm.is_llvm is None:
        p = None
        try:
            p = call_subprocess_Popen(['g++', '--version'],
                                      stdout=subprocess.PIPE,
                                      stderr=subprocess.PIPE)
            p.wait()
            output = p.stdout.read() + p.stderr.read()
        except OSError:
            # Typically means g++ cannot be found.
            # So it is not an llvm compiler.

            # Normally this should not happen as we should not try to
            # compile when g++ is not available. If this happens, it
            # will crash later so supposing it is not llvm is "safe".
            output = ''
        del p
        gcc_llvm.is_llvm = "llvm" in output
    return gcc_llvm.is_llvm
def blueprints(self) -> List["sanic.Blueprint"]:
        
        
        
        return self._blueprints
def cancel_order(self, id: str, symbol: Str = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' cancelOrder() requires a symbol argument')
        self.load_markets()
        stop = self.safe_value_2(params, 'stop', 'trigger')
        advanced = self.safe_value(params, 'advanced')
        if stop or advanced:
            orderInner = self.cancel_orders([id], symbol, params)
            return self.safe_value(orderInner, 0)
        market = self.market(symbol)
        request = {
            'instId': market['id'],
            # 'ordId': id,  # either ordId or clOrdId is required
            # 'clOrdId': clientOrderId,
        }
        clientOrderId = self.safe_string_2(params, 'clOrdId', 'clientOrderId')
        if clientOrderId is not None:
            request['clOrdId'] = clientOrderId
        else:
            request['ordId'] = str(id)
        query = self.omit(params, ['clOrdId', 'clientOrderId'])
        response = self.privatePostTradeCancelOrder(self.extend(request, query))
        # {"code":"0","data":[{"clOrdId":"","ordId":"317251910906576896","sCode":"0","sMsg":""}],"msg":""}
        data = self.safe_value(response, 'data', [])
        order = self.safe_value(data, 0)
        return self.parse_order(order, market)
def _build_score_converter(score_converter_config, logit_scale):
  
  
  if score_converter_config == post_processing_pb2.PostProcessing.IDENTITY:
    return _score_converter_fn_with_logit_scale(tf.identity, logit_scale)
  if score_converter_config == post_processing_pb2.PostProcessing.SIGMOID:
    return _score_converter_fn_with_logit_scale(tf.sigmoid, logit_scale)
  if score_converter_config == post_processing_pb2.PostProcessing.SOFTMAX:
    return _score_converter_fn_with_logit_scale(tf.nn.softmax, logit_scale)
  raise ValueError('Unknown score converter.')
def from_llm(
        cls,
        llm: BaseLanguageModel,
        *,
        prompt: Optional[PromptTemplate] = None,
        criteria: Optional[Union[CRITERIA_TYPE, str]] = None,
        **kwargs: Any,
    ) -> PairwiseStringEvalChain:
        

          # noqa: E501
        expected_input_vars = {
            "prediction",
            "prediction_b",
            "input",
            "reference",
            "criteria",
        }
        prompt_ = prompt or PROMPT_WITH_REFERENCE
        if expected_input_vars != set(prompt_.input_variables):
            raise ValueError(
                f"Input variables should be {expected_input_vars}, "
                f"but got {prompt_.input_variables}"
            )
        criteria_ = resolve_pairwise_criteria(criteria)
        criteria_str = "\n".join(f"{k}: {v}" for k, v in criteria_.items())
        return cls(llm=llm, prompt=prompt_.partial(criteria=criteria_str), **kwargs)
def parse(self, text: str) -> Dict[str, Any]:
        

        
        parsed = text.strip().rsplit("\n", maxsplit=1)
        if len(parsed) == 1:
            reasoning = ""
            verdict = parsed[0]
        else:
            reasoning, verdict = parsed
        verdict = verdict.strip("[").strip("]")
        if verdict not in {"A", "B", "C"}:
            raise ValueError(
                f"Invalid verdict: {verdict}. "
                "Verdict must be one of 'A', 'B', or 'C'."
            )
        # C means the models are tied. Return 'None' meaning no preference
        verdict_ = None if verdict == "C" else verdict
        score = {
            "A": 1,
            "B": 0,
            None: 0.5,
        }.get(verdict_)
        return {
            "reasoning": reasoning,
            "value": verdict_,
            "score": score,
        }
def _get_filesystems_and_globs(datetime_to_task, datetime_to_re):
    
    
    
    # probe some scattered datetimes unlikely to all occur in paths, other than by being sincere datetime parameter's representations
    # TODO limit to [self.start, self.stop) so messages are less confusing? Done trivially it can kill correctness
    sample_datetimes = [datetime(y, m, d, h) for y in range(2000, 2050, 10) for m in range(1, 4) for d in range(5, 8) for h in range(21, 24)]
    regexes = [re.compile(datetime_to_re(d)) for d in sample_datetimes]
    sample_tasks = [datetime_to_task(d) for d in sample_datetimes]
    sample_outputs = [flatten_output(t) for t in sample_tasks]

    for o, t in zip(sample_outputs, sample_tasks):
        if len(o) != len(sample_outputs[0]):
            raise NotImplementedError("Outputs must be consistent over time, sorry; was %r for %r and %r for %r" % (o, t, sample_outputs[0], sample_tasks[0]))
            # TODO fall back on requiring last couple of days? to avoid astonishing blocking when changes like that are deployed
            # erm, actually it's not hard to test entire hours_back..hours_forward and split into consistent subranges FIXME?
        for target in o:
            if not isinstance(target, FileSystemTarget):
                raise NotImplementedError("Output targets must be instances of FileSystemTarget; was %r for %r" % (target, t))

    for o in zip(*sample_outputs):  # transposed, so here we're iterating over logical outputs, not datetimes
        glob = _get_per_location_glob(sample_tasks, o, regexes)
        yield o[0].fs, glob
def parse_tags(self, data):
        
        
        
        # parse the lines into a list of tuples (commit-hash, tag ref name)
        squashed_data = re.sub(r' +', ' ', data)
        raw_tags = csv.reader(StringIO(squashed_data), delimiter=' ')
        vcs_tags = []
        for row in raw_tags:
            name = ' '.join(row[:-1])
            commit = row[-1]
            if commit != '?':
                vcs_tags.append(VCSVersion(self, commit, name))
        return vcs_tags
def post(self):
        
        
        
        params = request.get_json(force=True)

        query = params['query']
        max_age = int(params.get('max_age', -1))
        query_id = params.get('query_id', 'adhoc')
        parameters = params.get('parameters', collect_parameters_from_request(request.args))

        data_source = models.DataSource.get_by_id_and_org(params.get('data_source_id'), self.current_org)

        if not has_access(data_source.groups, self.current_user, not_view_only):
            return {'job': {'status': 4, 'error': 'You do not have permission to run queries with this data source.'}}, 403

        self.record_event({
            'action': 'execute_query',
            'object_id': data_source.id,
            'object_type': 'data_source',
            'query': query,
            'query_id': query_id,
            'parameters': parameters
        })
        return run_query(data_source, parameters, query, query_id, max_age)
def _submit_task(
        self, task_idx: int
    ) -> Tuple[
        ObjectRef[MaybeBlockPartition], Union[None, ObjectRef[BlockPartitionMetadata]]
    ]:
        
        
        if self._stats_actor is None:
            self._stats_actor = _get_or_create_stats_actor()
        stats_actor = self._stats_actor
        if not self._execution_started:
            stats_actor.record_start.remote(self._stats_uuid)
            self._execution_started = True
        task = self._tasks[task_idx]
        context = DatasetContext.get_current()
        if context.block_splitting_enabled:
            return (
                cached_remote_fn(_execute_read_task_split)
                .options(num_returns="dynamic", **self._remote_args)
                .remote(
                    i=task_idx,
                    task=task,
                    context=DatasetContext.get_current(),
                    stats_uuid=self._stats_uuid,
                    stats_actor=stats_actor,
                ),
                None,
            )
        else:
            return (
                cached_remote_fn(_execute_read_task_nosplit)
                .options(num_returns=2, **self._remote_args)
                .remote(
                    i=task_idx,
                    task=task,
                    context=DatasetContext.get_current(),
                    stats_uuid=self._stats_uuid,
                    stats_actor=stats_actor,
                )
            )
def __init__(
        self,
        tasks: List[ReadTask],
        read_op_name: Optional[str] = None,
        block_partition_refs: Optional[List[ObjectRef[MaybeBlockPartition]]] = None,
        block_partition_meta_refs: Optional[List[ObjectRef[BlockMetadata]]] = None,
        cached_metadata: Optional[List[BlockPartitionMetadata]] = None,
        ray_remote_args: Optional[Dict[str, Any]] = None,
        stats_uuid: str = None,
        *,
        owned_by_consumer: bool,
    ):
        
        
        self._tasks = tasks
        self._read_op_name = read_op_name
        self._num_blocks = len(self._tasks)
        if stats_uuid is None:
            stats_uuid = uuid.uuid4()
        self._stats_uuid = stats_uuid
        self._execution_started = False
        self._remote_args = ray_remote_args or {}
        # Block partition metadata that have already been computed and fetched.
        if cached_metadata is not None:
            self._cached_metadata = cached_metadata
        else:
            self._cached_metadata = [None] * len(tasks)
        # Block partition metadata that have already been computed.
        if block_partition_meta_refs is not None:
            self._block_partition_meta_refs = block_partition_meta_refs
        else:
            self._block_partition_meta_refs = [None] * len(tasks)
        # Block partitions that have already been computed.
        if block_partition_refs is not None:
            self._block_partition_refs = block_partition_refs
        else:
            self._block_partition_refs = [None] * len(tasks)
        assert len(tasks) == len(self._block_partition_refs), (
            tasks,
            self._block_partition_refs,
        )
        assert len(tasks) == len(self._block_partition_meta_refs), (
            tasks,
            self._block_partition_meta_refs,
        )
        assert len(tasks) == len(self._cached_metadata), (
            tasks,
            self._cached_metadata,
        )
        # Whether the block list is owned by consuming APIs, and if so it can be
        # eagerly deleted after read by the consumer.
        self._owned_by_consumer = owned_by_consumer
        self._stats_actor = _get_or_create_stats_actor()
        # This field can be set to indicate the number of estimated output blocks,
        # since each read task may produce multiple output blocks after splitting.
        self._estimated_num_blocks = None
def stats(self) -> DatasetStats:
        
        return DatasetStats(
            # Make a copy of metadata, as the DatasetStats may mutate it in-place.
            stages={"Read": self.get_metadata(fetch_if_missing=False).copy()},
            parent=None,
            needs_stats_actor=True,
            stats_uuid=self._stats_uuid,
        )
def pre_fill(self, filenames: list[str], side: T.Literal["a", "b"]) -> None:
         
        
        with self._lock:
            for filename, meta in tqdm(read_image_meta_batch(filenames),
                                       desc=f"WTL: Caching Landmarks ({side.upper()})",
                                       total=len(filenames),
                                       leave=False):
                if "itxt" not in meta or "alignments" not in meta["itxt"]:
                    raise FaceswapError(f"Invalid face image found. Aborting: '{filename}'")

                meta = meta["itxt"]
                key = os.path.basename(filename)
                # Version Check
                self._validate_version(meta, filename)
                detected_face = self._load_detected_face(filename, meta["alignments"])

                aligned = detected_face.aligned
                assert aligned is not None
                if aligned.landmark_type != LandmarkType.LM_2D_68:
                    raise FaceswapError("68 Point facial Landmarks are required for Warp-to-"
                                        f"landmarks. The face that failed was: '{filename}'")

                self._cache[key] = detected_face
                self._partially_loaded.append(key)
def load_from_checkpoint(
        cls,
        checkpoint_path: Union[_PATH, IO],
        map_location: _MAP_LOCATION_TYPE = None,
        hparams_file: Optional[_PATH] = None,
        **kwargs: Any,
    ) -> Self:
        

        
        loaded = _load_from_checkpoint(
            cls,  # type: ignore[arg-type]
            checkpoint_path,
            map_location=map_location,
            hparams_file=hparams_file,
            strict=None,
            **kwargs,
        )
        return cast(Self, loaded)
def _stream_handler(loglevel: int, is_gui: bool) -> Union[logging.StreamHandler, TqdmHandler]:
     
    
    # Don't set stdout to lower than verbose
    loglevel = max(loglevel, 15)
    log_format = FaceswapFormatter("%(asctime)s %(levelname)-8s %(message)s",
                                   datefmt="%m/%d/%Y %H:%M:%S")

    if is_gui:
        # tqdm.write inserts extra lines in the GUI, so use standard output as
        # it is not needed there.
        log_console = logging.StreamHandler(sys.stdout)
    else:
        log_console = TqdmHandler(sys.stdout)
    log_console.setFormatter(log_format)
    log_console.setLevel(loglevel)
    return log_console
def _crash_handler(log_format):
     
    
    log_crash = logging.StreamHandler(_DEBUG_BUFFER)
    log_crash.setFormatter(log_format)
    log_crash.setLevel(logging.DEBUG)
    return log_crash
def _rewrite_warnings(cls, record):
         

        
        if record.levelno == 30 and record.funcName == "warn" and record.module == "ag_logging":
            # TF 2.3 in Conda is imported with the wrong gast(0.4 when 0.3.3 should be used). This
            # causes warnings in autograph. They don't appear to impact performance so de-elevate
            # warning to debug
            record.levelno = 10
            record.levelname = "DEBUG"

        if record.levelno == 30 and (record.funcName == "_tfmw_add_deprecation_warning" or
                                     record.module in ("deprecation", "deprecation_wrapper")):
            # Keras Deprecations.
            record.levelno = 10
            record.levelname = "DEBUG"

        return record
def test_to_python(self):
        
        
        
        good_inputs = [
            'POINT(5 23)',
            'MULTIPOLYGON(((0 0, 0 1, 1 1, 1 0, 0 0)))',
            'LINESTRING(0 0, 1 1)',
        ]
        bad_inputs = [
            'POINT(5)',
            'MULTI   POLYGON(((0 0, 0 1, 1 1, 1 0, 0 0)))',
            'BLAH(0 0, 1 1)',
            '{"type": "FeatureCollection", "features": ['
            '{"geometry": {"type": "Point", "coordinates": [508375, 148905]}, "type": "Feature"}]}',
        ]
        fld = forms.GeometryField()
        # to_python returns the same GEOSGeometry for a WKT
        for geo_input in good_inputs:
            with self.subTest(geo_input=geo_input):
                self.assertEqual(GEOSGeometry(geo_input, srid=fld.widget.map_srid), fld.to_python(geo_input))
        # but raises a ValidationError for any other string
        for geo_input in bad_inputs:
            with self.subTest(geo_input=geo_input):
                with self.assertRaises(forms.ValidationError):
                    fld.to_python(geo_input)
def wrap_deepmind(env, dim=84, framestack=True):
    
    
    env = MonitorEnv(env)
    env = NoopResetEnv(env, noop_max=30)
    if env.spec is not None and "NoFrameskip" in env.spec.id:
        env = MaxAndSkipEnv(env, skip=4)
    env = EpisodicLifeEnv(env)
    if "FIRE" in env.unwrapped.get_action_meanings():
        env = FireResetEnv(env)
    env = WarpFrame(env, dim)
    # env = ScaledFloatFrame(env)  # TODO: use for dqn?
    # env = ClipRewardEnv(env)  # reward clipping is handled by policy eval
    # 4x image framestacking.
    if framestack is True:
        env = FrameStack(env, 4)
    return env
def add_worker(self, worker_id, worker_type, worker_info):
        
        
        worker_data = gcs_pb2.WorkerTableData()
        worker_data.is_alive = True
        worker_data.worker_address.worker_id = worker_id
        worker_data.worker_type = worker_type
        for k, v in worker_info.items():
            worker_data.worker_info[k] = bytes(v, encoding="utf-8")
        return self.global_state_accessor.add_worker_info(
            worker_data.SerializeToString()
        )
def get_draining_nodes(self) -> Dict[str, int]:
        
        
        self._check_connected()
        return self.global_state_accessor.get_draining_nodes()
def actor_table(
        self, actor_id: str, job_id: ray.JobID = None, actor_state_name: str = None
    ):
        
        
        self._check_connected()

        if actor_id is not None:
            actor_id = ray.ActorID(hex_to_binary(actor_id))
            actor_info = self.global_state_accessor.get_actor_info(actor_id)
            if actor_info is None:
                return {}
            else:
                actor_table_data = gcs_pb2.ActorTableData.FromString(actor_info)
                return self._gen_actor_info(actor_table_data)
        else:
            validate_actor_state_name(actor_state_name)
            actor_table = self.global_state_accessor.get_actor_table(
                job_id, actor_state_name
            )
            results = {}
            for i in range(len(actor_table)):
                actor_table_data = gcs_pb2.ActorTableData.FromString(actor_table[i])
                results[
                    binary_to_hex(actor_table_data.actor_id)
                ] = self._gen_actor_info(actor_table_data)

            return results
def add_child(self, name: str, data: Calculations) -> None:
         
        
        logger.debug("Adding child: %s", name)
        graph = TrainingGraph(self.subnotebook, data, "Loss")
        graph.build()
        graph = self.subnotebook_add_page(name, widget=graph)
        Tooltip(graph, text=self.helptext, wrap_length=200)
def parse_market_leverage_tiers(self, info, market=None):
        
        
        
        #
        #    {
        #        "symbol":"BTC-PERP",
        #        "status":"Normal",
        #        "displayName":"BTCUSDT",
        #        "settlementAsset":"USDT",
        #        "underlying":"BTC/USDT",
        #        "tradingStartTime":1579701600000,
        #        "priceFilter":{"minPrice":"1","maxPrice":"1000000","tickSize":"1"},
        #        "lotSizeFilter":{"minQty":"0.0001","maxQty":"1000000000","lotSize":"0.0001"},
        #        "commissionType":"Quote",
        #        "commissionReserveRate":"0.001",
        #        "marketOrderPriceMarkup":"0.03",
        #        "marginRequirements":[
        #            {"positionNotionalLowerBound":"0","positionNotionalUpperBound":"50000","initialMarginRate":"0.01","maintenanceMarginRate":"0.006"},
        #            {"positionNotionalLowerBound":"50000","positionNotionalUpperBound":"200000","initialMarginRate":"0.02","maintenanceMarginRate":"0.012"},
        #            {"positionNotionalLowerBound":"200000","positionNotionalUpperBound":"2000000","initialMarginRate":"0.04","maintenanceMarginRate":"0.024"},
        #            {"positionNotionalLowerBound":"2000000","positionNotionalUpperBound":"20000000","initialMarginRate":"0.1","maintenanceMarginRate":"0.06"},
        #            {"positionNotionalLowerBound":"20000000","positionNotionalUpperBound":"40000000","initialMarginRate":"0.2","maintenanceMarginRate":"0.12"},
        #            {"positionNotionalLowerBound":"40000000","positionNotionalUpperBound":"1000000000","initialMarginRate":"0.333333","maintenanceMarginRate":"0.2"}
        #        ]
        #    }
        #
        marginRequirements = self.safe_value(info, 'marginRequirements')
        id = self.safe_string(info, 'symbol')
        market = self.safe_market(id, market)
        tiers = []
        for i in range(0, len(marginRequirements)):
            tier = marginRequirements[i]
            initialMarginRate = self.safe_string(tier, 'initialMarginRate')
            tiers.append({
                'tier': self.sum(i, 1),
                'currency': market['quote'],
                'notionalFloor': self.safe_number(tier, 'positionNotionalLowerBound'),
                'notionalCap': self.safe_number(tier, 'positionNotionalUpperBound'),
                'maintenanceMarginRate': self.safe_number(tier, 'maintenanceMarginRate'),
                'maxLeverage': self.parse_number(Precise.string_div('1', initialMarginRate)),
                'info': tier,
            })
        return tiers
def fetch_tickers(self, symbols=None, params={}):
        
        
        
        self.load_markets()
        request = {}
        market = None
        if symbols is not None:
            symbol = self.safe_value(symbols, 0)
            market = self.market(symbol)
            marketIds = self.market_ids(symbols)
            request['symbol'] = ','.join(marketIds)
        type = None
        type, params = self.handle_market_type_and_params('fetchTickers', market, params)
        response = None
        if type == 'spot':
            response = self.v1PublicGetTicker(self.extend(request, params))
        else:
            response = self.v2PublicGetFuturesTicker(self.extend(request, params))
        #
        #     {
        #         "code":0,
        #         "data":[
        #             {
        #                 "symbol":"QTUM/BTC",
        #                 "open":"0.00016537",
        #                 "close":"0.00019077",
        #                 "high":"0.000192",
        #                 "low":"0.00016537",
        #                 "volume":"846.6",
        #                 "ask":["0.00018698","26.2"],
        #                 "bid":["0.00018408","503.7"],
        #                 "type":"spot"
        #             }
        #         ]
        #     }
        #
        data = self.safe_value(response, 'data', [])
        if not isinstance(data, list):
            return self.parse_tickers([data], symbols)
        return self.parse_tickers(data, symbols)
def set_leverage(self, leverage, symbol: Optional[str] = None, params={}):
        
        
        
        self.check_required_symbol('setLeverage', symbol)
        if (leverage < 1) or (leverage > 100):
            raise BadRequest(self.id + ' leverage should be between 1 and 100')
        self.load_markets()
        self.load_accounts()
        market = self.market(symbol)
        if not market['swap']:
            raise BadSymbol(self.id + ' setLeverage() supports swap contracts only')
        account = self.safe_value(self.accounts, 0, {})
        accountGroup = self.safe_string(account, 'id')
        request = {
            'account-group': accountGroup,
            'symbol': market['id'],
            'leverage': leverage,
        }
        return self.v2PrivateAccountGroupPostFuturesLeverage(self.extend(request, params))
def transfer(self, code: str, amount: float, fromAccount, toAccount, params={}) -> TransferEntry:
        
        
        
        self.load_markets()
        self.load_accounts()
        account = self.safe_value(self.accounts, 0, {})
        accountGroup = self.safe_string(account, 'id')
        currency = self.currency(code)
        amount = self.currency_to_precision(code, amount)
        accountsByType = self.safe_value(self.options, 'accountsByType', {})
        fromId = self.safe_string(accountsByType, fromAccount, fromAccount)
        toId = self.safe_string(accountsByType, toAccount, toAccount)
        if fromId != 'cash' and toId != 'cash':
            raise ExchangeError(self.id + ' transfer() only supports direct balance transfer between spot and swap, spot and margin')
        request = {
            'account-group': accountGroup,
            'amount': amount,
            'asset': currency['id'],
            'fromAccount': fromId,
            'toAccount': toId,
        }
        response = self.v1PrivateAccountGroupPostTransfer(self.extend(request, params))
        #
        #    {"code": "0"}
        #
        transferOptions = self.safe_value(self.options, 'transfer', {})
        fillResponseFromRequest = self.safe_bool(transferOptions, 'fillResponseFromRequest', True)
        transfer = self.parse_transfer(response, currency)
        if fillResponseFromRequest:
            transfer['fromAccount'] = fromAccount
            transfer['toAccount'] = toAccount
            transfer['amount'] = amount
            transfer['currency'] = code
        return transfer
def fetch_deposit_address(self, code: str, params={}):
        
        
        
        self.load_markets()
        currency = self.currency(code)
        networkCode = self.safe_string_2(params, 'network', 'chainName')
        networkId = self.network_code_to_id(networkCode)
        params = self.omit(params, ['chainName'])
        request = {
            'asset': currency['id'],
            'blockchain': networkId,
        }
        response = self.v1PrivateGetWalletDepositAddress(self.extend(request, params))
        #
        #     {
        #         "code":0,
        #         "data":{
        #             "asset":"USDT",
        #             "assetName":"Tether",
        #             "address":[
        #                 {
        #                     "address":"1N22odLHXnLPCjC8kwBJPTayarr9RtPod6",
        #                     "destTag":"",
        #                     "tagType":"",
        #                     "tagId":"",
        #                     "chainName":"Omni",
        #                     "numConfirmations":3,
        #                     "withdrawalFee":4.7,
        #                     "nativeScale":4,
        #                     "tips":[]
        #                 },
        #                 {
        #                     "address":"0xe7c70b4e73b6b450ee46c3b5c0f5fb127ca55722",
        #                     "destTag":"",
        #                     "tagType":"",
        #                     "tagId":"",
        #                     "chainName":"ERC20",
        #                     "numConfirmations":20,
        #                     "withdrawalFee":1.0,
        #                     "nativeScale":4,
        #                     "tips":[]
        #                 }
        #             ]
        #         }
        #     }
        #
        data = self.safe_dict(response, 'data', {})
        addresses = self.safe_list(data, 'address', [])
        numAddresses = len(addresses)
        address = None
        if numAddresses > 1:
            addressesByChainName = self.index_by(addresses, 'chainName')
            if networkId is None:
                chainNames = list(addressesByChainName.keys())
                chains = ', '.join(chainNames)
                raise ArgumentsRequired(self.id + ' fetchDepositAddress() returned more than one address, a chainName parameter is required, one of ' + chains)
            address = self.safe_dict(addressesByChainName, networkId, {})
        else:
            # first address
            address = self.safe_dict(addresses, 0, {})
        result = self.parse_deposit_address(address, currency)
        return self.extend(result, {
            'info': response,
        })
def run_tests(self, test_labels, extra_tests=None, **kwargs):
        
        
        
        if extra_tests is not None:
            warnings.warn(
                'The extra_tests argument is deprecated.',
                RemovedInDjango50Warning,
                stacklevel=2,
            )
        self.setup_test_environment()
        suite = self.build_suite(test_labels, extra_tests)
        databases = self.get_databases(suite)
        serialized_aliases = set(
            alias
            for alias, serialize in databases.items() if serialize
        )
        with self.time_keeper.timed('Total database setup'):
            old_config = self.setup_databases(
                aliases=databases,
                serialized_aliases=serialized_aliases,
            )
        run_failed = False
        try:
            self.run_checks(databases)
            result = self.run_suite(suite)
        except Exception:
            run_failed = True
            raise
        finally:
            try:
                with self.time_keeper.timed('Total database teardown'):
                    self.teardown_databases(old_config)
                self.teardown_test_environment()
            except Exception:
                # Silence teardown exceptions if an exception was raised during
                # runs to avoid shadowing it.
                if not run_failed:
                    raise
        self.time_keeper.print_results()
        return self.suite_result(suite, result)
def log(self, msg, level=None):
        
        
        
        if level is None:
            level = logging.INFO
        if self.logger is None:
            if self.verbosity <= 0 or (
                self.verbosity == 1 and level < logging.INFO
            ):
                return
            print(msg)
        else:
            self.logger.log(level, msg)
def save_formset(self, request, form, formset, change):
        
        
        
        formset.save()
def message_user(self, request, message, level=messages.INFO, extra_tags='',
                     fail_silently=False):
        
        
        

        if not isinstance(level, int):
            # attempt to get the level if passed a string
            try:
                level = getattr(messages.constants, level.upper())
            except AttributeError:
                levels = messages.constants.DEFAULT_TAGS.values()
                levels_repr = ', '.join('`%s`' % l for l in levels)
                raise ValueError('Bad message level string: `%s`. '
                        'Possible values are: %s' % (level, levels_repr))

        messages.add_message(request, level, message, extra_tags=extra_tags,
                fail_silently=fail_silently)
def has_add_permission(self, request):
        
        
        
        opts = self.opts
        return request.user.has_perm(opts.app_label + '.' + opts.get_add_permission())
def response_add(self, request, obj, post_url_continue=None):
        
        
        
        opts = obj._meta
        pk_value = obj._get_pk_val()

        msg_dict = {'name': force_text(opts.verbose_name), 'obj': force_text(obj)}
        # Here, we distinguish between different save types by checking for
        # the presence of keys in request.POST.
        if "_continue" in request.POST:
            msg = _('The %(name)s "%(obj)s" was added successfully. You may edit it again below.') % msg_dict
            self.message_user(request, msg)
            if post_url_continue is None:
                post_url_continue = reverse('admin:%s_%s_change' %
                                            (opts.app_label, opts.module_name),
                                            args=(pk_value,),
                                            current_app=self.admin_site.name)
            else:
                try:
                    post_url_continue = post_url_continue % pk_value
                    warnings.warn(
                        "The use of string formats for post_url_continue "
                        "in ModelAdmin.response_add() is deprecated. Provide "
                        "a pre-formatted url instead.",
                        DeprecationWarning, stacklevel=2)
                except TypeError:
                    pass
            if "_popup" in request.POST:
                post_url_continue += "?_popup=1"
            return HttpResponseRedirect(post_url_continue)

        if "_popup" in request.POST:
            return HttpResponse(
                '<!DOCTYPE html><html><head><title></title></head><body>'
                '<script type="text/javascript">opener.dismissAddAnotherPopup(window, "%s", "%s");</script></body></html>' % \
                # escape() calls force_text.
                (escape(pk_value), escapejs(obj)))
        elif "_addanother" in request.POST:
            msg = _('The %(name)s "%(obj)s" was added successfully. You may add another %(name)s below.') % msg_dict
            self.message_user(request, msg)
            return HttpResponseRedirect(request.path)
        else:
            msg = _('The %(name)s "%(obj)s" was added successfully.') % msg_dict
            self.message_user(request, msg)
            return self.response_post_save(request, obj)
def response_delete(self, request, obj_display, obj_id):
        
        
        
        opts = self.model._meta

        if IS_POPUP_VAR in request.POST:
            popup_response_data = json.dumps({
                'action': 'delete',
                'value': str(obj_id),
            })
            return TemplateResponse(request, self.popup_response_template or [
                'admin/%s/%s/popup_response.html' % (opts.app_label, opts.model_name),
                'admin/%s/popup_response.html' % opts.app_label,
                'admin/popup_response.html',
            ], {
                'popup_response_data': popup_response_data,
            })

        self.message_user(
            request,
            _('The %(name)s "%(obj)s" was deleted successfully.') % {
                'name': force_text(opts.verbose_name),
                'obj': force_text(obj_display),
            },
            messages.SUCCESS,
        )

        if self.has_change_permission(request, None):
            post_url = reverse(
                'admin:%s_%s_changelist' % (opts.app_label, opts.model_name),
                current_app=self.admin_site.name,
            )
            preserved_filters = self.get_preserved_filters(request)
            post_url = add_preserved_filters(
                {'preserved_filters': preserved_filters, 'opts': opts}, post_url
            )
        else:
            post_url = reverse('admin:index', current_app=self.admin_site.name)
        return HttpResponseRedirect(post_url)
def get_model_perms(self, request):
        
        
        
        return {
            'add': self.has_add_permission(request),
            'change': self.has_change_permission(request),
            'delete': self.has_delete_permission(request),
            'view': self.has_view_permission(request),
        }
def _check_backbone(config, print_cfg=True):
    
    
    if print_cfg:
        print('-' * 15 + 'loading ', config)
    cfg = Config.fromfile(config)
    init_cfg = None
    try:
        init_cfg = cfg.model.backbone.init_cfg
        init_flag = True
    except AttributeError:
        init_flag = False
    if init_cfg is None or init_cfg.get('type') != 'Pretrained':
        init_flag = False
    if init_flag:
        checkpoint = CheckpointLoader.load_checkpoint(init_cfg.checkpoint)
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint

        model = MODELS.build(cfg.model)
        model.init_weights()

        checkpoint_layers = state_dict.keys()
        for name, value in model.backbone.state_dict().items():
            if name in checkpoint_layers:
                assert value.equal(state_dict[name])

        if print_cfg:
            print('-' * 10 + 'Successfully load checkpoint' + '-' * 10 +
                  '\n', )
            return None
    else:
        if print_cfg:
            print(config + '\n' + '-' * 10 +
                  'config file do not have init_cfg' + '-' * 10 + '\n')
            return config
def fit(self, X, Y):
        
        
        check_consistent_length(X, Y)
        X = self._validate_data(X, dtype=np.float64, copy=self.copy,
                                ensure_min_samples=2)
        Y = check_array(Y, dtype=np.float64, copy=self.copy, ensure_2d=False)
        if Y.ndim == 1:
            Y = Y.reshape(-1, 1)

        # we'll compute the SVD of the cross-covariance matrix = X.T.dot(Y)
        # This matrix rank is at most min(n_samples, n_features, n_targets) so
        # n_components cannot be bigger than that.
        n_components = self.n_components
        rank_upper_bound = min(X.shape[0], X.shape[1], Y.shape[1])
        if not 1 <= n_components <= rank_upper_bound:
            # TODO: raise an error in 0.26
            warnings.warn(
                f"As of version 0.24, n_components({n_components}) should be "
                f"in [1, min(n_features, n_samples, n_targets)] = "
                f"[1, {rank_upper_bound}]. "
                f"n_components={rank_upper_bound} will be used instead. "
                f"In version 0.26, an error will be raised.",
                FutureWarning
            )
            n_components = rank_upper_bound

        X, Y, self.x_mean_, self.y_mean_, self.x_std_, self.y_std_ = (
            _center_scale_xy(X, Y, self.scale))

        # Compute SVD of cross-covariance matrix
        C = np.dot(X.T, Y)
        U, s, Vt = svd(C, full_matrices=False)
        U = U[:, :n_components]
        Vt = Vt[:n_components]
        U, Vt = svd_flip(U, Vt)
        V = Vt.T

        self._x_scores = np.dot(X, U)  # TODO: remove in 0.26
        self._y_scores = np.dot(Y, V)  # TODO: remove in 0.26
        self.x_weights_ = U
        self.y_weights_ = V
        return self
def fit(self, X, y=None, Y=None):
        
        
        y = _deprecate_Y_when_required(y, Y)

        super().fit(X, y)
        # expose the fitted attributes `x_scores_` and `y_scores_`
        self.x_scores_ = self._x_scores
        self.y_scores_ = self._y_scores
        return self
def inverse_transform(self, X, Y=None):
        
        
        check_is_fitted(self)
        X = check_array(X, dtype=FLOAT_DTYPES)
        # From pls space to original space
        X_reconstructed = np.matmul(X, self.x_loadings_.T)
        # Denormalize
        X_reconstructed *= self._x_std
        X_reconstructed += self._x_mean

        if Y is not None:
            Y = check_array(Y, dtype=FLOAT_DTYPES)
            # From pls space to original space
            Y_reconstructed = np.matmul(Y, self.y_loadings_.T)
            # Denormalize
            Y_reconstructed *= self._y_std
            Y_reconstructed += self._y_mean
            return X_reconstructed, Y_reconstructed

        return X_reconstructed
def add(
        self,
        lines: LineCollectionLike,
        layer_id: Union[None, int] = None,
        with_metadata: bool = False,
    ) -> None:
        
        
        if layer_id is None:
            layer_id = 1
            while layer_id in self._layers:
                layer_id += 1

        if layer_id not in self._layers:
            self._layers[layer_id] = LineCollection()

        self._layers[layer_id].extend(lines)

        if with_metadata and isinstance(lines, LineCollection):
            self._layers[layer_id].metadata.update(lines.metadata)
def __init__(self, lines: LineCollectionLike = ()):
        
        
        self._lines: List[np.ndarray] = []

        self.extend(lines)
def clone(self, keep_layers: bool = False) -> "Document":
        
        
        doc = Document(metadata=self.metadata)
        if keep_layers:
            for layer_id in self.layers:
                doc.layers[layer_id] = self.layers[layer_id].clone()
        return doc
def add_to_sources(self, path) -> None:
        
        
        if (path := pathlib.Path(path)).exists():
            path = path.absolute()
            self.set_property(METADATA_FIELD_SOURCE, path)
            self.sources |= {path}
def delete(
        self,
        ids: Optional[List[str]] = None,
        collection_only: bool = False,
        **kwargs: Any,
    ) -> None:
        
        
        with Session(self._bind) as session:
            if ids is not None:
                self.logger.debug(
                    "Trying to delete vectors by ids (represented by the model "
                    "using the custom ids field)"
                )

                stmt = delete(self.EmbeddingStore)

                if collection_only:
                    collection = self.get_collection(session)
                    if not collection:
                        self.logger.warning("Collection not found")
                        return

                    stmt = stmt.where(
                        self.EmbeddingStore.collection_id == collection.uuid
                    )

                stmt = stmt.where(self.EmbeddingStore.custom_id.in_(ids))
                session.execute(stmt)
            session.commit()
def __init__(self, parser, name):
        
        self._parser = parser
        self._name = name
        self.getint = functools.partial(self._parser.getint,
                                        self._name)
        self.getfloat = functools.partial(self._parser.getfloat,
                                          self._name)
        self.getboolean = functools.partial(self._parser.getboolean,
                                            self._name)
def get(self, section, option, *, raw=False, vars=None, fallback=_UNSET):
        
        
        try:
            d = self._unify_values(section, vars)
        except NoSectionError:
            if fallback is _UNSET:
                raise
            else:
                return fallback
        option = self.optionxform(option)
        try:
            value = d[option]
        except KeyError:
            if fallback is _UNSET:
                raise NoOptionError(option, section)
            else:
                return fallback

        if raw or value is None:
            return value
        else:
            return self._interpolation.before_get(self, section, option, value,
                                                  d)
def read_dict(self, dictionary, source='<dict>'):
        
        
        elements_added = set()
        for section, keys in dictionary.items():
            section = str(section)
            try:
                self.add_section(section)
            except (DuplicateSectionError, ValueError):
                if self._strict and section in elements_added:
                    raise
                elements_added.add(section)
            for key, value in keys.items():
                key = self.optionxform(str(key))
                if value is not None:
                    value = str(value)
                if self._strict and (section, key) in elements_added:
                    raise DuplicateOptionError(section, key, source)
                elements_added.add((section, key))
                self.set(section, key, value)
def _write_section(self, fp, section_name, section_items, delimiter, unnamed=False):
        
        if not unnamed:
            fp.write("[{}]\n".format(section_name))
        for key, value in section_items:
            value = self._interpolation.before_write(self, section_name, key,
                                                     value)
            if value is not None or not self._allow_no_value:
                value = delimiter + str(value).replace('\n', '\n\t')
            else:
                value = ""
            fp.write("{}{}\n".format(key, value))
        fp.write("\n")
def _get_media(self):
        
        
        
        media = Media()
        for w in self.widgets:
            media = media + w.media
        return media
def _img_array(self):
        '''
        '''
        pilIm = self.im
        pilIm.seek(0)

        # Read all images inside
        image_data = []
        try:
            while True:
                img_tmp = pilIm
                img_tmp = self.img_correct(img_tmp)
                image_data.append(ImageData(img_tmp.size[0], img_tmp.size[1],
                                img_tmp.mode.lower(), img_tmp.tostring()))
                pilIm.seek(pilIm.tell()+1)
        except EOFError:
            pass
        # Done
        return image_data
def WriteMakeRule(self, outputs, inputs, actions=None, comment=None,
                    order_only=False, force=False, phony=False, command=None):
    
    
    outputs = map(QuoteSpaces, outputs)
    inputs = map(QuoteSpaces, inputs)

    if comment:
      self.WriteLn('# ' + comment)
    if phony:
      self.WriteLn('.PHONY: ' + ' '.join(outputs))
    if actions:
      self.WriteLn("%s: TOOLSET := $(TOOLSET)" % outputs[0])
    force_append = ' FORCE_DO_CMD' if force else ''

    if order_only:
      # Order only rule: Just write a simple rule.
      # TODO(evanm): just make order_only a list of deps instead of this hack.
      self.WriteLn('%s: | %s%s' %
                   (' '.join(outputs), ' '.join(inputs), force_append))
    elif len(outputs) == 1:
      # Regular rule, one output: Just write a simple rule.
      self.WriteLn('%s: %s%s' % (outputs[0], ' '.join(inputs), force_append))
    else:
      # Regular rule, more than one output: Multiple outputs are tricky in
      # make. We will write three rules:
      # - All outputs depend on an intermediate file.
      # - Make .INTERMEDIATE depend on the intermediate.
      # - The intermediate file depends on the inputs and executes the
      #   actual command.
      # - The intermediate recipe will 'touch' the intermediate file.
      # - The multi-output rule will have an do-nothing recipe.
      intermediate = "%s.intermediate" % (command if command else self.target)
      self.WriteLn('%s: %s' % (' '.join(outputs), intermediate))
      self.WriteLn('\t%s' % '@:');
      self.WriteLn('%s: %s' % ('.INTERMEDIATE', intermediate))
      self.WriteLn('%s: %s%s' %
                   (intermediate, ' '.join(inputs), force_append))
      actions.insert(0, '$(call do_cmd,touch)')

    if actions:
      for action in actions:
        self.WriteLn('\t%s' % action)
    self.WriteLn()
def EscapeShellArgument(s):
    
    
    return "'" + s.replace("'", "'\\''") + "'"
def parse_market_leverage_tiers(self, info, market):
        
         
        
        #
        #    {
        #        "symbol": "SUSHIUSDT",
        #        "brackets": [
        #            {
        #                "bracket": 1,
        #                "initialLeverage": 50,
        #                "notionalCap": 50000,
        #                "notionalFloor": 0,
        #                "maintMarginRatio": 0.01,
        #                "cum": 0.0
        #            },
        #            ...
        #        ]
        #    }
        #
        marketId = self.safe_string(info, 'symbol')
        safeSymbol = self.safe_symbol(marketId)
        market = self.safe_market(safeSymbol, market)
        brackets = self.safe_value(info, 'brackets')
        tiers = []
        for j in range(0, len(brackets)):
            bracket = brackets[j]
            tiers.append({
                'tier': self.safe_number(bracket, 'bracket'),
                'currency': market['quote'],
                'notionalFloor': self.safe_number_2(bracket, 'notionalFloor', 'qtyFloor'),
                'notionalCap': self.safe_number(bracket, 'notionalCap'),
                'maintenanceMarginRate': self.safe_number(bracket, 'maintMarginRatio'),
                'maxLeverage': self.safe_number(bracket, 'initialLeverage'),
                'info': bracket,
            })
        return tiers
def _process_numeric_state(self, entity_observation):
        
        entity = entity_observation["entity_id"]

        should_trigger = condition.async_numeric_state(
            self.hass,
            entity,
            entity_observation.get("below"),
            entity_observation.get("above"),
            None,
            entity_observation,
        )
        return should_trigger
def nav_scale_callback(self, *args, reset_progress=True):  # pylint:disable=unused-argument
         
        
        self._display_frame.pack_threshold_slider()
        if reset_progress:
            self.stop_playback()
        frame_count = self._det_faces.filter.count
        if self._current_nav_frame_count == frame_count:
            logger.trace("Filtered count has not changed. Returning")
        if self._globals.tk_filter_mode.get() == "Misaligned Faces":
            self._det_faces.tk_face_count_changed.set(True)
        self._update_total_frame_count()
        if reset_progress:
            self._globals.tk_transport_index.set(0)
def __get_mandatory_stats(self, proc, procstat):
        
        
        
        procstat['mandatory_stats'] = True

        # Name, cpu_times, status and ppid stats are in the same /proc file
        # Optimisation fir issue #958
        try:
            procstat.update(proc.as_dict(
                attrs=['name', 'cpu_times', 'status', 'ppid'],
                ad_value=''))
        except psutil.NoSuchProcess:
            # Try/catch for issue #432 (process no longer exist)
            return None
        else:
            procstat['status'] = str(procstat['status'])[:1].upper()

        try:
            procstat.update(proc.as_dict(
                attrs=['username', 'cpu_percent', 'memory_percent'],
                ad_value=''))
        except psutil.NoSuchProcess:
            # Try/catch for issue #432 (process no longer exist)
            return None

        if procstat['cpu_percent'] == '' or procstat['memory_percent'] == '':
            # Do not display process if we cannot get the basic
            # cpu_percent or memory_percent stats
            return None

        # Compute the maximum value for cpu_percent and memory_percent
        for k in self._max_values_list:
            if procstat[k] > self.get_max_values(k):
                self.set_max_values(k, procstat[k])

        # Process command line (cached with internal cache)
        try:
            self.cmdline_cache[procstat['pid']]
        except KeyError:
            # Patch for issue #391
            try:
                self.cmdline_cache[procstat['pid']] = proc.cmdline()
            except (AttributeError, UnicodeDecodeError, psutil.AccessDenied, psutil.NoSuchProcess):
                self.cmdline_cache[procstat['pid']] = ""
        procstat['cmdline'] = self.cmdline_cache[procstat['pid']]

        # Process IO
        # procstat['io_counters'] is a list:
        # [read_bytes, write_bytes, read_bytes_old, write_bytes_old, io_tag]
        # If io_tag = 0 > Access denied (display "?")
        # If io_tag = 1 > No access denied (display the IO rate)
        # Note Disk IO stat not available on Mac OS
        if not OSX:
            try:
                # Get the process IO counters
                proc_io = proc.io_counters()
                io_new = [proc_io.read_bytes, proc_io.write_bytes]
            except (psutil.AccessDenied, psutil.NoSuchProcess, NotImplementedError):
                # Access denied to process IO (no root account)
                # NoSuchProcess (process die between first and second grab)
                # Put 0 in all values (for sort) and io_tag = 0 (for
                # display)
                procstat['io_counters'] = [0, 0] + [0, 0]
                io_tag = 0
            else:
                # For IO rate computation
                # Append saved IO r/w bytes
                try:
                    procstat['io_counters'] = io_new + \
                        self.io_old[procstat['pid']]
                except KeyError:
                    procstat['io_counters'] = io_new + [0, 0]
                # then save the IO r/w bytes
                self.io_old[procstat['pid']] = io_new
                io_tag = 1

            # Append the IO tag (for display)
            procstat['io_counters'] += [io_tag]

        return procstat
def _sort_cpu_times(process,
                    sorted_by='cpu_times',
                    sorted_by_secondary='memory_percent'):
     
    
    return process[sorted_by][0] + process[sorted_by][1]
def sort_reverse(self):
        
        if self.sort_key == 'name' or self.sort_key == 'username':
            return False

        return True
def getlist(self, sorted_by=None, as_programs=False):
        
        if as_programs:
            return processes_to_programs(self.processlist)
        else:
            return self.processlist
def export_process_filter(self, value):
        
        self._filter_export.filter = value
def save(self, force_insert=False, force_update=False):
        
        
        
        if force_insert and force_update:
            raise ValueError("Cannot force both insert and updating in "
                    "model saving.")
        self.save_base(force_insert=force_insert, force_update=force_update)
def serializable_value(self, field_name):
        
        
        
        try:
            field = self._meta.get_field_by_name(field_name)[0]
        except FieldDoesNotExist:
            return getattr(self, field_name)
        return getattr(self, field.attname)
def _check_ordering(cls):
          

        from django.db.models import FieldDoesNotExist

        if not cls._meta.ordering:
            return []

        if not isinstance(cls._meta.ordering, (list, tuple)):
            return [
                checks.Error(
                    ("'ordering' must be a tuple or list "
                     "(even if you want to order by only one field)."),
                    hint=None,
                    obj=cls,
                    id='models.E014',
                )
            ]

        errors = []

        fields = cls._meta.ordering

        # Skip '?' fields.
        fields = (f for f in fields if f != '?')

        # Convert "-field" to "field".
        fields = ((f[1:] if f.startswith('-') else f) for f in fields)

        fields = (f for f in fields if
            f != '_order' or not cls._meta.order_with_respect_to)

        # Skip ordering in the format field1__field2 (FIXME: checking
        # this format would be nice, but it's a little fiddly).
        fields = (f for f in fields if '__' not in f)

        # Skip ordering on pk. This is always a valid order_by field
        # but is an alias and therefore won't be found by opts.get_field.
        fields = (f for f in fields if f != 'pk')

        for field_name in fields:
            try:
                cls._meta.get_field(field_name, many_to_many=False)
            except FieldDoesNotExist:
                if field_name.endswith('_id'):
                    try:
                        field = cls._meta.get_field(field_name[:-3], many_to_many=False)
                    except FieldDoesNotExist:
                        pass
                    else:
                        if field.attname == field_name:
                            continue
                errors.append(
                    checks.Error(
                        "'ordering' refers to the non-existent field '%s'." % field_name,
                        hint=None,
                        obj=cls,
                        id='models.E015',
                    )
                )
        return errors
def _check_indexes(cls, databases):
        
        errors = []
        for index in cls._meta.indexes:
            # Index name can't start with an underscore or a number, restricted
            # for cross-database compatibility with Oracle.
            if index.name[0] == '_' or index.name[0].isdigit():
                errors.append(
                    checks.Error(
                        "The index name '%s' cannot start with an underscore "
                        "or a number." % index.name,
                        obj=cls,
                        id='models.E033',
                    ),
                )
            if len(index.name) > index.max_name_length:
                errors.append(
                    checks.Error(
                        "The index name '%s' cannot be longer than %d "
                        "characters." % (index.name, index.max_name_length),
                        obj=cls,
                        id='models.E034',
                    ),
                )
        for db in databases:
            if not router.allow_migrate_model(db, cls):
                continue
            connection = connections[db]
            if (
                connection.features.supports_partial_indexes or
                'supports_partial_indexes' in cls._meta.required_db_features
            ):
                continue
            if any(index.condition is not None for index in cls._meta.indexes):
                errors.append(
                    checks.Warning(
                        '%s does not support indexes with conditions.'
                        % connection.display_name,
                        hint=(
                            "Conditions will be ignored. Silence this warning "
                            "if you don't care about it."
                        ),
                        obj=cls,
                        id='models.W037',
                    )
                )
        fields = [field for index in cls._meta.indexes for field, _ in index.fields_orders]
        errors.extend(cls._check_local_fields(fields, 'indexes'))
        return errors
def _do_insert(self, manager, using, fields, returning_fields, raw):
        
        
        
        return manager._insert(
            [self], fields=fields, returning_fields=returning_fields,
            using=using, raw=raw,
        )
def full_clean(self, exclude=None, validate_unique=True, validate_constraints=True):
        
        
        
        errors = {}
        if exclude is None:
            exclude = set()
        else:
            exclude = set(exclude)

        try:
            self.clean_fields(exclude=exclude)
        except ValidationError as e:
            errors = e.update_error_dict(errors)

        # Form.clean() is run even if other validation fails, so do the
        # same with Model.clean() for consistency.
        try:
            self.clean()
        except ValidationError as e:
            errors = e.update_error_dict(errors)

        # Run unique checks, but only for fields that passed validation.
        if validate_unique:
            for name in errors:
                if name != NON_FIELD_ERRORS and name not in exclude:
                    exclude.add(name)
            try:
                self.validate_unique(exclude=exclude)
            except ValidationError as e:
                errors = e.update_error_dict(errors)

        # Run constraints checks, but only for fields that passed validation.
        if validate_constraints:
            for name in errors:
                if name != NON_FIELD_ERRORS and name not in exclude:
                    exclude.add(name)
            try:
                self.validate_constraints(exclude=exclude)
            except ValidationError as e:
                errors = e.update_error_dict(errors)

        if errors:
            raise ValidationError(errors)
def is_active(self, project):
        
        
        
        any_owner_banned = any(u.profile.banned for u in project.users.all())
        organization = project.organizations.first()
        if (
            project.skip
            or any_owner_banned
            or (organization and organization.disabled)
        ):
            return False

        return True
def external(self):
        
        
        
        return self.filter(version__type=EXTERNAL)
def for_user_and_viewer(self, user, viewer):
        
        
        
        viewer_projects = self._add_user_projects(self.none(), viewer, admin=True, member=True)
        owner_projects = self._add_user_projects(self.none(), user, admin=True, member=True)
        owner_public_projects = owner_projects.filter(privacy_level=constants.PUBLIC)
        queryset = (viewer_projects & owner_projects) | owner_public_projects
        return queryset.distinct()
def max_concurrent_builds(self, project):
        
        
        
        from readthedocs.subscriptions.constants import TYPE_CONCURRENT_BUILDS
        from readthedocs.subscriptions.models import PlanFeature

        max_concurrent_organization = None
        organization = project.organizations.first()
        if organization:
            max_concurrent_organization = organization.max_concurrent_builds

        return (
            project.max_concurrent_builds
            or max_concurrent_organization
            or PlanFeature.objects.get_feature_value(
                project,
                type=TYPE_CONCURRENT_BUILDS,
            )
        )
def execute(self, *args, **options):
        
        
        
        self.stdout = OutputWrapper(options.get('stdout', sys.stdout))
        if options.get('no_color'):
            self.style = no_style()
            self.stderr = OutputWrapper(options.get('stderr', sys.stderr))
        else:
            self.stderr = OutputWrapper(options.get('stderr', sys.stderr), self.style.ERROR)

        if self.can_import_settings:
            from django.conf import settings  # NOQA

        saved_locale = None
        if not self.leave_locale_alone:
            # Only mess with locales if we can assume we have a working
            # settings file, because django.utils.translation requires settings
            # (The final saying about whether the i18n machinery is active will be
            # found in the value of the USE_I18N setting)
            if not self.can_import_settings:
                raise CommandError("Incompatible values of 'leave_locale_alone' "
                                   "(%s) and 'can_import_settings' (%s) command "
                                   "options." % (self.leave_locale_alone,
                                                 self.can_import_settings))
            # Switch to US English, because django-admin.py creates database
            # content like permissions, and those shouldn't contain any
            # translations.
            from django.utils import translation
            saved_locale = translation.get_language()
            translation.activate('en-us')

        try:
            if (self.requires_system_checks and
                    not options.get('skip_validation') and  # This will be removed at the end of deprecation proccess for `skip_validation`.
                    not options.get('skip_checks')):
                self.check()
            output = self.handle(*args, **options)
            if output:
                if self.output_transaction:
                    # This needs to be imported here, because it relies on
                    # settings.
                    from django.db import connections, DEFAULT_DB_ALIAS
                    connection = connections[options.get('database', DEFAULT_DB_ALIAS)]
                    if connection.ops.start_transaction_sql():
                        self.stdout.write(self.style.SQL_KEYWORD(connection.ops.start_transaction_sql()))
                self.stdout.write(output)
                if self.output_transaction:
                    self.stdout.write('\n' + self.style.SQL_KEYWORD("COMMIT;"))
        finally:
            if saved_locale is not None:
                translation.activate(saved_locale)
def validate(self, app_config=None, display_num_errors=False):
        
        

        
        from django.core.management.validation import get_validation_errors
        s = StringIO()
        num_errors = get_validation_errors(s, app_config)
        if num_errors:
            s.seek(0)
            error_text = s.read()
            raise CommandError("One or more models did not validate:\n%s" % error_text)
        if display_num_errors:
            self.stdout.write("%s error%s found" % (num_errors, '' if num_errors == 1 else 's'))
def native_value(self) -> str | None:
        
        if not self.listener.status_localized:
            return None
        if self.listener.listener_kind == ListenerKind.TEMPERATURE:
            # The Notion API only returns a localized string for temperature (e.g.
            # "70°"); we simply remove the degree symbol:
            return self.listener.status_localized.state[:-1]
        return self.listener.status_localized.state
def line_nd(start, stop, *, endpoint=False, integer=True):
    
    
    start = np.asarray(start)
    stop = np.asarray(stop)
    npoints = int(np.ceil(np.max(np.abs(stop - start))))
    if endpoint:
        npoints += 1
    coords = []
    for dim in range(len(start)):
        dimcoords = np.linspace(start[dim], stop[dim], npoints, endpoint)
        if integer:
            dimcoords = _round_safe(dimcoords).astype(int)
        coords.append(dimcoords)
    return coords
def is_on(self):
        
        return self.service.value(CharacteristicsTypes.OCCUPANCY_DETECTED) == 1
def connect(self) -> ClientContext:
        
        
        
        connection_dict = ray.init(
            address=self.address, job_config=self._job_config)
        return ClientContext(
            dashboard_url=connection_dict["webui_url"],
            python_version="{}.{}.{}".format(
                sys.version_info[0], sys.version_info[1], sys.version_info[2]),
            ray_version=ray.__version__,
            ray_commit=ray.__commit__,
            protocol_version=None,
            _num_clients=1)
def client(address: Optional[str] = None,
           _deprecation_warn_enabled: bool = True) -> ClientBuilder:
    
    
    
    env_address = os.environ.get(RAY_ADDRESS_ENVIRONMENT_VARIABLE)
    if env_address and address is None:
        logger.debug(
            f"Using address ({env_address}) instead of auto-detection "
            f"because {RAY_ADDRESS_ENVIRONMENT_VARIABLE} is set.")
        address = env_address

    builder = _get_builder_from_address(address)
    # Disable client deprecation warn when ray.client is used internally
    builder._deprecation_warn_enabled = _deprecation_warn_enabled
    return builder
def findlinestarts(code):
    
    

    lastline = False # None is a valid line number
    for start, end, line in code.co_lines():
        if line is not lastline:
            lastline = line
            yield start, line
    return
def get_instructions(x, *, first_line=None):
    
    
    co = _get_code_object(x)
    cell_names = co.co_cellvars + co.co_freevars
    linestarts = dict(findlinestarts(co))
    if first_line is not None:
        line_offset = first_line - co.co_firstlineno
    else:
        line_offset = 0
    return _get_instructions_bytes(co.co_code, co.co_varnames, co.co_names,
                                   co.co_consts, cell_names, linestarts,
                                   line_offset)
def _get_code_object(x):
    
    # Extract functions from methods.
    if hasattr(x, '__func__'):
        x = x.__func__
    # Extract compiled code objects from...
    if hasattr(x, '__code__'):  # ...a function, or
        x = x.__code__
    elif hasattr(x, 'gi_code'):  #...a generator object, or
        x = x.gi_code
    elif hasattr(x, 'ag_code'):  #...an asynchronous generator object, or
        x = x.ag_code
    elif hasattr(x, 'cr_code'):  #...a coroutine.
        x = x.cr_code
    # Handle source code.
    if isinstance(x, str):
        x = _try_compile(x, "<disassembly>")
    # By now, if we don't have a code object, we can't disassemble x.
    if hasattr(x, 'co_code'):
        return x
    raise TypeError("don't know how to disassemble %s objects" %
                    type(x).__name__)
def _disassemble(self, lineno_width=3, mark_as_current=False, offset_width=0,
                           label_width=0):
        
        
        fields = []
        # Column: Source code line number
        if lineno_width:
            if self.starts_line:
                lineno_fmt = "%%%dd" if self.line_number is not None else "%%%ds"
                lineno_fmt = lineno_fmt % lineno_width
                lineno = self.line_number if self.line_number is not None else '--'
                fields.append(lineno_fmt % lineno)
            else:
                fields.append(' ' * lineno_width)
        # Column: Label
        if self.label is not None:
            lbl = f"L{self.label}:"
            fields.append(f"{lbl:>{label_width}}")
        else:
            fields.append(' ' * label_width)
        # Column: Instruction offset from start of code sequence
        if offset_width > 0:
            fields.append(f"{repr(self.offset):>{offset_width}}  ")
        # Column: Current instruction indicator
        if mark_as_current:
            fields.append('-->')
        else:
            fields.append('   ')
        # Column: Opcode name
        fields.append(self.opname.ljust(_OPNAME_WIDTH))
        # Column: Opcode argument
        if self.arg is not None:
            arg = repr(self.arg)
            # If opname is longer than _OPNAME_WIDTH, we allow it to overflow into
            # the space reserved for oparg. This results in fewer misaligned opargs
            # in the disassembly output.
            opname_excess = max(0, len(self.opname) - _OPNAME_WIDTH)
            fields.append(repr(self.arg).rjust(_OPARG_WIDTH - opname_excess))
            # Column: Opcode argument details
            if self.argrepr:
                fields.append('(' + self.argrepr + ')')
        return ' '.join(fields).rstrip()
def _get_const_info(op, arg, co_consts):
    
    
    argval = _get_const_value(op, arg, co_consts)
    argrepr = repr(argval) if argval is not UNKNOWN else ''
    return argval, argrepr
def __init__(self, file=None, lineno_width=0, offset_width=0, label_width=0,
                       line_offset=0, show_caches=False):
        

        
        self.file = file
        self.lineno_width = lineno_width
        self.offset_width = offset_width
        self.label_width = label_width
        self.show_caches = show_caches
def store_build_artifacts(self):
        
        
        
        log.info('Writing build artifacts to media storage')
        # NOTE: I don't remember why we removed this state from the Build
        # object. I'm re-adding it because I think it's useful, but we can
        # remove it if we want
        self.update_build(state=BUILD_STATE_UPLOADING)

        types_to_copy = []
        types_to_delete = []

        for artifact_type in ARTIFACT_TYPES:
            if os.path.exists(
                self.data.project.artifact_path(
                    version=self.data.version.slug,
                    type_=artifact_type,
                )
            ):
                types_to_copy.append(artifact_type)
            # Never delete HTML nor JSON (search index)
            elif artifact_type not in UNDELETABLE_ARTIFACT_TYPES:
                types_to_delete.append(artifact_type)

        for media_type in types_to_copy:
            # NOTE: here is where we get the correct FROM path to upload
            from_path = self.data.version.project.artifact_path(
                version=self.data.version.slug,
                type_=media_type,
            )

            # Check if there are multiple files on source directories.
            # These output format does not support multiple files yet.
            # In case multiple files are found, the upload for this format is not performed.
            #
            # TODO: we should fail the build for these cases and clearly communicate this.
            # to the user. To do this, we need to call this method (``store_build_artifacts``)
            # since the ``execute`` method.
            # It will allow us to just `raise BuildUserError` and handle it at
            # Celery `on_failure` handler.
            if media_type in ("htmlzip", "epub", "pdf"):
                if len(os.listdir(from_path)) > 1:
                    log.exception(
                        "Multiple files are not supported for this format. "
                        "Skipping this output format.",
                        output_format=media_type,
                    )
                    continue

            to_path = self.data.version.project.get_storage_path(
                type_=media_type,
                version_slug=self.data.version.slug,
                include_file=False,
                version_type=self.data.version.type,
            )
            try:
                build_media_storage.sync_directory(from_path, to_path)
            except Exception:
                # Ideally this should just be an IOError
                # but some storage backends unfortunately throw other errors
                log.exception(
                    'Error copying to storage (not failing build)',
                    media_type=media_type,
                    from_path=from_path,
                    to_path=to_path,
                )

        for media_type in types_to_delete:
            media_path = self.data.version.project.get_storage_path(
                type_=media_type,
                version_slug=self.data.version.slug,
                include_file=False,
                version_type=self.data.version.type,
            )
            try:
                build_media_storage.delete_directory(media_path)
            except Exception:
                # Ideally this should just be an IOError
                # but some storage backends unfortunately throw other errors
                log.exception(
                    'Error deleting from storage (not failing build)',
                    media_type=media_type,
                    media_path=media_path,
                )
def get_valid_artifact_types(self):
        
        
        
        valid_artifacts = []
        for artifact_type in ARTIFACT_TYPES:
            artifact_directory = self.data.project.artifact_path(
                version=self.data.version.slug,
                type_=artifact_type,
            )

            if artifact_type == "html":
                index_html_filepath = os.path.join(artifact_directory, "index.html")
                readme_html_filepath = os.path.join(artifact_directory, "README.html")
                if not os.path.exists(index_html_filepath) and not os.path.exists(
                    readme_html_filepath
                ):
                    log.warning(
                        "Failing the build. "
                        "HTML output does not contain an 'index.html' at its root directory.",
                        index_html=index_html_filepath,
                        readme_html=readme_html_filepath,
                    )
                    # TODO: uncomment this line to fail the build once we have
                    # communicated with projects without an index.html or
                    # README.html
                    #
                    # NOTE: we want to deprecate serving README.html as an
                    # index.html file as well.
                    #
                    # raise BuildUserError(BuildUserError.BUILD_OUTPUT_HTML_NO_INDEX_FILE)

            if not os.path.exists(artifact_directory):
                # There is no output directory.
                # Skip this format.
                continue

            if not os.path.isdir(artifact_directory):
                log.error(
                    "The output path is not a directory.",
                    output_format=artifact_type,
                )
                raise BuildUserError(
                    BuildUserError.BUILD_OUTPUT_IS_NOT_A_DIRECTORY.format(
                        artifact_type=artifact_type
                    )
                )

            # Check if there are multiple files on artifact directories.
            # These output format does not support multiple files yet.
            # In case multiple files are found, the upload for this format is not performed.
            if artifact_type in ARTIFACT_TYPES_WITHOUT_MULTIPLE_FILES_SUPPORT:
                artifact_format_files = len(os.listdir(artifact_directory))
                if artifact_format_files > 1:
                    log.error(
                        "Multiple files are not supported for this format. "
                        "Skipping this output format.",
                        output_format=artifact_type,
                    )
                    raise BuildUserError(
                        BuildUserError.BUILD_OUTPUT_HAS_MULTIPLE_FILES.format(
                            artifact_type=artifact_type
                        )
                    )
                if artifact_format_files == 0:
                    raise BuildUserError(
                        BuildUserError.BUILD_OUTPUT_HAS_0_FILES.format(
                            artifact_type=artifact_type
                        )
                    )

            # If all the conditions were met, the artifact is valid
            valid_artifacts.append(artifact_type)

        return valid_artifacts
def from_documents(
        cls,
        documents: List[Document],
        embedding: Optional[Embeddings] = None,
        user_id: Optional[str] = None,
        app_id: Optional[str] = None,
        number_of_docs: Optional[int] = None,
        pat: Optional[str] = None,
        token: Optional[str] = None,
        **kwargs: Any,
    ) -> Clarifai:
        
        
        texts = [doc.page_content for doc in documents]
        metadatas = [doc.metadata for doc in documents]
        return cls.from_texts(
            user_id=user_id,
            app_id=app_id,
            texts=texts,
            number_of_docs=number_of_docs,
            pat=pat,
            metadatas=metadatas,
            token=token,
            **kwargs,
        )
def test_chained_redirect(redirect_app):
    
    request, response = redirect_app.test_client.get('/1')
    assert request.url.endswith('/1')
    assert response.status == 200
    assert response.text == 'OK'
    assert response.url.endswith('/3')
def rotate_image_by_angle(image, angle,
                              rotated_width=None, rotated_height=None):
          
        height, width = image.shape[:2]
        image_center = (width/2, height/2)
        rotation_matrix = cv2.getRotationMatrix2D(image_center, -1.*angle, 1.)
        if rotated_width is None or rotated_height is None:
            abs_cos = abs(rotation_matrix[0, 0])
            abs_sin = abs(rotation_matrix[0, 1])
            if rotated_width is None:
                rotated_width = int(height*abs_sin + width*abs_cos)
            if rotated_height is None:
                rotated_height = int(height*abs_cos + width*abs_sin)
        rotation_matrix[0, 2] += rotated_width/2 - image_center[0]
        rotation_matrix[1, 2] += rotated_height/2 - image_center[1]
        return cv2.warpAffine(image,
                              rotation_matrix,
                              (rotated_width, rotated_height))
def load(self):
         
        
        data = dict()
        if not self._is_extract:
            if not self.have_alignments_file:
                return data
            data = super().load()
            return data

        skip_existing = hasattr(self._args, 'skip_existing') and self._args.skip_existing
        skip_faces = hasattr(self._args, 'skip_faces') and self._args.skip_faces

        if not skip_existing and not skip_faces:
            logger.debug("No skipping selected. Returning empty dictionary")
            return data

        if not self.have_alignments_file and (skip_existing or skip_faces):
            logger.warning("Skip Existing/Skip Faces selected, but no alignments file found!")
            return data

        data = self.serializer.load(self.file)

        if skip_faces:
            # Remove items from alignments that have no faces so they will
            # be re-detected
            del_keys = [key for key, val in data.items() if not val]
            logger.debug("Frames with no faces selected for redetection: %s", len(del_keys))
            for key in del_keys:
                if key in data:
                    logger.trace("Selected for redetection: '%s'", key)
                    del data[key]
        return data
def process(self, extract_media: "ExtractMedia") -> None:
         
        
        frame = os.path.splitext(os.path.basename(extract_media.filename))[0]
        for idx, face in enumerate(extract_media.detected_faces):
            if not self._face_size:
                self._face_size = get_centered_size(face.aligned.centering,
                                                    "face",
                                                    face.aligned.size)
                logger.debug("set face size: %s", self._face_size)
            if not self._legacy_size:
                self._legacy_size = get_centered_size(face.aligned.centering,
                                                      "legacy",
                                                      face.aligned.size)
                logger.debug("set legacy size: %s", self._legacy_size)

            logger.trace("Drawing Landmarks. Frame: '%s'. Face: %s", frame, idx)  # type: ignore
            # Landmarks
            for (pos_x, pos_y) in face.aligned.landmarks.astype("int32"):
                cv2.circle(face.aligned.face, (pos_x, pos_y), 1, (0, 255, 255), -1)
            # Pose
            center = (face.aligned.size // 2, face.aligned.size // 2)
            points = (face.aligned.pose.xyz_2d * face.aligned.size).astype("int32")
            cv2.line(face.aligned.face, center, tuple(points[1]), (0, 255, 0), 1)
            cv2.line(face.aligned.face, center, tuple(points[0]), (255, 0, 0), 1)
            cv2.line(face.aligned.face, center, tuple(points[2]), (0, 0, 255), 1)
            # Face centering
            roi = face.aligned.get_cropped_roi(face.aligned.size, self._face_size, "face")
            cv2.rectangle(face.aligned.face, tuple(roi[:2]), tuple(roi[2:]), (0, 255, 0), 1)
            # Legacy centering
            roi = face.aligned.get_cropped_roi(face.aligned.size, self._legacy_size, "legacy")
            cv2.rectangle(face.aligned.face, tuple(roi[:2]), tuple(roi[2:]), (0, 0, 255), 1)
def get_input_images(self):
          
        if self.is_video:
            input_images = self.args.input_dir
        else:
            input_images = get_image_paths(self.args.input_dir)

        return input_images
def zip_loader(_filename, **kwargs):
        '''
        '''
        # Read all images inside
        z = zipfile.ZipFile(_filename, 'r')
        image_data = []
        #for each file in zip
        for zfilename in z.namelist():
            try:
                #read file and store it in mem with fileIO struct around it
                tmpfile = SIO.StringIO(z.read(zfilename))
                ext = zfilename.split('.')[-1].lower()
                im = None
                for loader in ImageLoader.loaders:
                    if ext not in loader.extensions():
                        continue
                    im = loader(tmpfile, **kwargs)
                    break
                if im is not None:
                    #append ImageData to local variable before it's overwritten
                    image_data.append(im._data[0])
                #else: if not image file skip to next
            except:
                Logger.warning('Image: Unable to load image' +
                    '<%s> in zip <%s> trying to continue...'
                    % (zfilename,_filename))
                #raise# return the data read till now
                #this should Ideally handle truncated zips
        z.close()
        try:
            if len(image_data):
                pass
        except:
            raise Exception('no images in zip <%s>' % _filename)
        #replace Image.Data with the array of all the images in the zip
        im._data = image_data
        # Done
        return im
def launch(
    main_func,
    # Should be num_processes_per_machine, but kept for compatibility.
    num_gpus_per_machine,
    num_machines=1,
    machine_rank=0,
    dist_url=None,
    args=(),
    timeout=DEFAULT_TIMEOUT,
):
    
    
    
    world_size = num_machines * num_gpus_per_machine
    if world_size > 1:
        # https://github.com/pytorch/pytorch/pull/14391
        # TODO prctl in spawned processes

        if dist_url == "auto":
            assert num_machines == 1, "dist_url=auto not supported in multi-machine jobs."
            port = _find_free_port()
            dist_url = f"tcp://127.0.0.1:{port}"
        if num_machines > 1 and dist_url.startswith("file://"):
            logger = logging.getLogger(__name__)
            logger.warning(
                "file:// is not a reliable init_method in multi-machine jobs. Prefer tcp://"
            )

        mp.start_processes(
            _distributed_worker,
            nprocs=num_gpus_per_machine,
            args=(
                main_func,
                world_size,
                num_gpus_per_machine,
                machine_rank,
                dist_url,
                args,
                timeout,
            ),
            daemon=False,
        )
    else:
        main_func(*args)
def do_list_modules(self, arg):
		  
		for m,d in self.pupsrv.list_modules():
			self.stdout.write("{:<20}	{}\n".format(m, color(d.split("\n",1)[0],'grey')))
def metrics(self) -> _METRICS:
        
        on_step = self._first_loop_iter is not None
        assert self.trainer._results is not None
        return self.trainer._results.metrics(on_step)
def Z0(self):
        
        
        
        return self._Z0*ones(len(self))
def load(self, Gamma0: NumberLike, nports: int = 1,
             z0: Union[NumberLike, None] = None, **kwargs) -> Network:
        
        
        result = self.match(nports, z0 = z0, **kwargs)
        result.s = npy.array(Gamma0).reshape(-1, 1, 1) * \
            npy.eye(nports, dtype=complex).reshape((-1, nports, nports)).\
            repeat(self.frequency.npoints, 0)
        return result
def delay_load(self, Gamma0: NumberLike, d: Number, unit: str = 'deg', **kwargs) -> Network:
        
        
        return self.line(d=d, unit=unit, **kwargs) ** self.load(Gamma0=Gamma0, **kwargs)
def splitter(self, nports: int, **kwargs) -> Network:
        
        
        result = self.match(nports, **kwargs)
        
        y0s = npy.array(1./result.z0)
        y_k = y0s.sum(axis=1)
        s = npy.zeros((self.frequency.npoints, nports, nports),
                      dtype='complex')
        s = 2 *npy.sqrt(npy.einsum('ki,kj->kij', y0s, y0s)) / y_k[:, None, None]
        npy.einsum('kii->ki', s)[:] -= 1  # Sii
        result.s =  s
        return result
def attenuator(self, s21: NumberLike, db: bool = True, d: Number = 0,
                   unit: str = 'deg', name: str = '', **kwargs) -> Network:
        

        

        s21 = npy.array(s21)
        if db:
            s21 = mf.db_2_magnitude(s21)

        result = self.match(nports=2)
        result.s[:, 0, 1] = s21
        result.s[:, 1, 0] = s21
        result = result ** self.line(d=d, unit=unit, **kwargs)
        result.name = name
        return result
def z0(self):
        
        
        
        if self.z0_override is None:
            return self.z0_characteristic
        else:
            return self.z0_override
def capacitor_q(self, C: NumberLike, f_0: NumberLike, q_factor: NumberLike, **kwargs) -> Network:
        

        
        idea_cap = self.shunt(self.capacitor(C=C, **kwargs))
        rac = q_factor / (C * 2 * npy.pi * f_0)
        idea_res = self.shunt(self.resistor(R=rac))

        return innerconnect(connect(idea_cap, 1, idea_res, 2), 1, 3)
def inductor_q(self,
                   L: NumberLike,
                   f_0: NumberLike,
                   q_factor: NumberLike,
                   rdc: NumberLike = 0.0,
                   **kwargs) -> Network:
        

        
        w_q = 2 * npy.pi * f_0

        if rdc == 0.0:
            rdc = 0.05 * w_q * L / q_factor

        rq1 = w_q * L / q_factor
        rq2 = npy.sqrt(rq1**2 - rdc**2)
        qt = w_q * L / rq2
        rac = self.frequency.w * L / qt
        r1 = npy.sqrt(rdc**2 + rac**2)

        return self.inductor(L=L, **kwargs) ** self.resistor(R=r1)
def _make_key(args, kwds, typed,
             kwd_mark = (object(),),
             fasttypes = {int, str, frozenset, type(None)},
             sorted=sorted, tuple=tuple, type=type, len=len):
    

    
    key = args
    if kwds:
        sorted_items = sorted(kwds.items())
        key += kwd_mark
        for item in sorted_items:
            key += item
    if typed:
        key += tuple(type(v) for v in args)
        if kwds:
            key += tuple(type(v) for k, v in sorted_items)
    elif len(key) == 1 and type(key[0]) in fasttypes:
        return key[0]
    return _HashedSeq(key)
def main(argv=None, save_main_session=True):
  
  
  args, pipeline_args = parse_args(argv)

  pipeline_options = beam.options.pipeline_options.PipelineOptions(
            pipeline_args)
  pipeline_options.view_as(
      beam.options.pipeline_options.SetupOptions).save_main_session = (
          save_main_session)

  dirname = os.path.dirname(args.embedding_output_tfrecord)
  tf.io.gfile.makedirs(dirname)

  p = beam.Pipeline(options=pipeline_options)

  construct_pipeline(
      p,
      args.embedding_input_tfrecord,
      args.embedding_output_tfrecord,
      args.embedding_model_dir,
      args.top_k_embedding_count,
      args.bottom_k_embedding_count,
      args.num_shards)

  p.run()
def __init__(self, model_dir, top_k_embedding_count,
               bottom_k_embedding_count, embedding_type='final_box_features'):
    
    
    self._model_dir = model_dir
    self._session = None
    self._num_examples_processed = beam.metrics.Metrics.counter(
        'embedding_data_generation', 'num_tf_examples_processed')
    self._top_k_embedding_count = top_k_embedding_count
    self._bottom_k_embedding_count = bottom_k_embedding_count
    self._embedding_type = embedding_type
def test_deploy_single_package():
     
    
    c = TestClient()
    c.save({"conanfile.py": GenConanfile("pkg", "1.0").with_package_file("include/hi.h", "hi"),
            "consumer/conanfile.txt": "[requires]\npkg/1.0"})
    c.run("create .")

    # if we deploy one --requires, we get that package
    c.run("install  --requires=pkg/1.0 --deploy=direct_deploy --output-folder=output")
    header = c.load("output/direct_deploy/pkg/include/hi.h")
    assert "hi" in header

    # If we deploy a local conanfile.txt, we get deployed its direct dependencies
    c.run("install consumer/conanfile.txt --deploy=direct_deploy --output-folder=output2")
    header = c.load("output2/direct_deploy/pkg/include/hi.h")
    assert "hi" in header
def same_padding(in_size, filter_size, stride_size):
    
    
    in_height, in_width = in_size
    if isinstance(filter_size, int):
        filter_height, filter_width = filter_size, filter_size
    else:
        filter_height, filter_width = filter_size
    stride_height, stride_width = stride_size

    out_height = np.ceil(float(in_height) / float(stride_height))
    out_width = np.ceil(float(in_width) / float(stride_width))

    pad_along_height = int(
        ((out_height - 1) * stride_height + filter_height - in_height))
    pad_along_width = int(
        ((out_width - 1) * stride_width + filter_width - in_width))
    pad_top = pad_along_height // 2
    pad_bottom = pad_along_height - pad_top
    pad_left = pad_along_width // 2
    pad_right = pad_along_width - pad_left
    padding = (pad_left, pad_right, pad_top, pad_bottom)
    output = (out_height, out_width)
    return padding, output
def is_payload_valid(self):
        
        
        
        signature = self.request.headers.get(BITBUCKET_SIGNATURE_HEADER)
        secret = self.get_integration().secret
        if not secret:
            log.debug("Skipping payload signature validation.")
            return True
        if not signature:
            return False
        msg = self.request.body.decode()
        digest = WebhookMixin.get_digest(secret, msg)
        result = hmac.compare_digest(
            b"sha256=" + digest.encode(),
            signature.encode(),
        )
        return result
def _handle_get_static(handler, path_match, data):
      
    req_file = util.sanitize_path(path_match.group('file'))

    # Strip md5 hash out of frontend filename
    if re.match(r'^frontend-[A-Za-z0-9]{32}\.html$', req_file):
        req_file = "frontend.html"

    path = os.path.join(os.path.dirname(__file__), 'www_static', req_file)

    handler.write_file(path)
def websocket_get_panels(hass, connection, msg):
    
    user_is_admin = connection.user.is_admin
    panels = {
        panel_key: panel.to_response()
        for panel_key, panel in connection.hass.data[DATA_PANELS].items()
        if user_is_admin or not panel.require_admin
    }

    connection.send_message(websocket_api.result_message(msg["id"], panels))
def _anonymize(self, text: str, language: Optional[str] = None) -> str:
        
        
        if language is None:
            language = self.supported_languages[0]

        if language not in self.supported_languages:
            raise ValueError(
                f"Language '{language}' is not supported. "
                f"Supported languages are: {self.supported_languages}. "
                "Change your language configuration file to add more languages."
            )

        results = self._analyzer.analyze(
            text,
            entities=self.analyzed_fields,
            language=language,
        )

        return self._anonymizer.anonymize(
            text,
            analyzer_results=results,
            operators=self.operators,
        ).text
def __init__(self, edges):
        
        
        self.edges = edges
def dump(self, format='dot'):
        
        
        if format == 'dot':
            return self._to_dot()
        else:
            NotImplementedError('Currently, only dot format is supported.')
def __init__(self, corpus=None, id2word=None, dictionary=None, wlocal=utils.identity,
                 wglobal=df2idf, normalize=True, smartirs=None, pivot=None, slope=0.25):
        

        
        self.id2word = id2word
        self.wlocal, self.wglobal, self.normalize = wlocal, wglobal, normalize
        self.num_docs, self.num_nnz, self.idfs = None, None, None
        self.smartirs = resolve_weights(smartirs) if smartirs is not None else None
        self.slope = slope
        self.pivot = pivot
        self.eps = 1e-12

        if smartirs:
            n_tf, n_df, n_n = self.smartirs
            self.wlocal = partial(smartirs_wlocal, local_scheme=n_tf)
            self.wglobal = partial(smartirs_wglobal, global_scheme=n_df)

        if dictionary:
            # user supplied a Dictionary object, which already contains all the
            # statistics we need to construct the IDF mapping. we can skip the
            # step that goes through the corpus (= an optimization).
            if corpus:
                logger.warning(
                    "constructor received both corpus and explicit inverse document frequencies; ignoring the corpus"
                )
            self.num_docs, self.num_nnz = dictionary.num_docs, dictionary.num_nnz
            self.cfs = dictionary.cfs.copy()
            self.dfs = dictionary.dfs.copy()
            self.term_lens = {termid: len(term) for termid, term in iteritems(dictionary)}
            self.idfs = precompute_idfs(self.wglobal, self.dfs, self.num_docs)
            if not id2word:
                self.id2word = dictionary
        elif corpus:
            self.initialize(corpus)
        else:
            # NOTE: everything is left uninitialized; presumably the model will
            # be initialized in some other way
            pass

        # If smartirs is not None, override pivot and normalize
        if not smartirs:
            return
        if self.pivot is not None:
            if n_n in 'ub':
                logger.warning("constructor received pivot; ignoring smartirs[2]")
            return
        if n_n in 'ub' and callable(self.normalize):
            logger.warning("constructor received smartirs; ignoring normalize")
        if n_n in 'ub' and not dictionary and not corpus:
            logger.warning("constructor received no corpus or dictionary; ignoring smartirs[2]")
        elif n_n == "u":
            self.pivot = 1.0 * self.num_nnz / self.num_docs
        elif n_n == "b":
            self.pivot = 1.0 * sum(
                self.cfs[termid] * (self.term_lens[termid] + 1.0) for termid in iterkeys(dictionary)
            ) / self.num_docs
def test_create_lock_tool_requires_test(self, client):
         
        
        c = client
        with c.chdir("app"):
            c.run("create . --lockfile-out=conan.lock -tf=")
            lock = c.load("conan.lock")
            assert "cmake/1.0" not in lock
            assert "dep/1.0" in lock
            c.run("test test_package app/1.0 --lockfile-partial --lockfile=conan.lock "
                  "--lockfile-out=conan.lock")
            lock = c.load("conan.lock")
            assert "cmake/1.0" in lock
            assert "dep/1.0" in lock

        c.run("create cmake --version=2.0")
        c.run("create dep --version=2.0")
        with c.chdir("app"):
            c.run("create . --lockfile=conan.lock")
            assert "cmake/1.0" in c.out
            assert "dep/1.0" in c.out
            assert "cmake/2.0" not in c.out
            assert "dep/2.0" not in c.out
            assert "package tested" in c.out
def post(self, api_key_api):
        
        
        
        api_key_db = None
        api_key = None
        try:
            if not getattr(api_key_api, 'user', None):
                api_key_api.user = self._get_user()
            # If key_hash is provided use that and do not create a new key. The assumption
            # is user already has the original api-key
            if not getattr(api_key_api, 'key_hash', None):
                api_key, api_key_hash = auth_util.generate_api_key_and_hash()
                # store key_hash in DB
                api_key_api.key_hash = api_key_hash
            api_key_db = ApiKey.add_or_update(ApiKeyAPI.to_model(api_key_api))
        except (ValidationError, ValueError) as e:
            LOG.exception('Validation failed for api_key data=%s.', api_key_api)
            abort(http_client.BAD_REQUEST, str(e))

        extra = {'api_key_db': api_key_db}
        LOG.audit('ApiKey created. ApiKey.id=%s' % (api_key_db.id), extra=extra)

        api_key_create_response_api = ApiKeyCreateResponseAPI.from_model(api_key_db)
        # Return real api_key back to user. A one-way hash of the api_key is stored in the DB
        # only the real value only returned at create time. Also, no masking of key here since
        # the user needs to see this value atleast once.
        api_key_create_response_api.key = api_key
        return api_key_create_response_api
def get_all(self, requester_user, show_secrets=None, **kw):
        
            
        
        mask_secrets = self._get_mask_secrets(show_secrets=show_secrets,
                                              requester_user=requester_user)
        api_key_dbs = ApiKey.get_all(**kw)
        api_keys = [ApiKeyAPI.from_model(api_key_db, mask_secrets=mask_secrets)
                    for api_key_db in api_key_dbs]

        return api_keys
def test_create_node_cap_at_max(
    attempted_target_replica_count, expected_target_replica_count
):
    
    
    raycluster = get_basic_ray_cr()
    with mock.patch.object(KubeRayNodeProvider, "__init__", return_value=None):
        kr_node_provider = KubeRayNodeProvider(provider_config={}, cluster_name="fake")
        scale_request = ScaleRequest(
            workers_to_delete=set(),
            desired_num_workers={"small-group": attempted_target_replica_count},
        )
        patch = kr_node_provider._scale_request_to_patch_payload(
            scale_request=scale_request, raycluster=raycluster
        )
        assert patch[0]["value"] == expected_target_replica_count
def pretty_print(n):
    
    
    
    if n <= 0:
        return "       ...       ....        nothing printing :("
    upper_half = floyd(n)  # upper half
    lower_half = reverse_floyd(n)  # lower half
    return upper_half + lower_half
def sample(self, mask: Optional[np.ndarray] = None) -> np.ndarray:
        
        
        if mask is not None:
            assert isinstance(
                mask, np.ndarray
            ), f"The expected type of the mask is np.ndarray, actual type: {type(mask)}"
            assert (
                mask.dtype == np.int8
            ), f"The expected dtype of the mask is np.int8, actual dtype: {mask.dtype}"
            assert (
                mask.shape == self.shape
            ), f"The expected shape of the mask is {self.shape}, actual shape: {mask.shape}"
            assert np.all(
                (mask == 0) | (mask == 1) | (mask == 2)
            ), f"All values of a mask should be 0, 1 or 2, actual values: {mask}"

            return np.where(
                mask == 2,
                self.np_random.integers(low=0, high=2, size=self.n, dtype=self.dtype),
                mask.astype(self.dtype),
            )

        return self.np_random.integers(low=0, high=2, size=self.n, dtype=self.dtype)
def __init__(self, name, data):
        
        self._data = data
        self._description = None
        self._name = name
        self._state = None
        self.attrs = {ATTR_ATTRIBUTION: ATTRIBUTION}
def train(self, lang, output_dir, train_data, dev_data=None, n_iter=15,
              parser_L1=0.0, no_tagger=False, no_parser=False, no_ner=False):
        
        
        

        cli_train(lang, output_dir, train_data, dev_data, n_iter,
                  not no_tagger, not no_parser, not no_ner,
                  parser_L1)
def end_logits(self, inputs):
    

    
    if len(tf.shape(inputs)) == 3:
      # inputs: [B, S, H] -> [B, S, 1, H]
      inputs = tf.expand_dims(inputs, axis=2)

    end_logits = self.end_logits_inner_dense(inputs)
    end_logits = self.end_logits_layer_norm(end_logits)
    end_logits = self.end_logits_output_dense(end_logits)
    end_logits = tf.squeeze(end_logits)
    if tf.rank(end_logits) > 2:
      # shape = [B, S, K] -> [B, K, S]
      end_logits = tf.transpose(end_logits, [0, 2, 1])

    return end_logits
def extract_hyps(
        self, ref_seq, hyps, subsequence_phn_start, use_base=False
    ):
        
        
        range_phns = torch.arange(
            ref_seq.size(1), device=ref_seq.device
        ).expand_as(ref_seq)
        target_word_indexes = self._get_target_word_indexes(
            ref_seq,
            range_phns,
            subsequence_phn_start.unsqueeze(-1),
            self.word_separator_base if use_base else self.word_separator,
        )
        separator_indexes = [
            [-1]
            + [
                idx
                for idx, phn in enumerate(item_hyps)
                if phn == self.word_separator
            ]
            + [None]
            for item_hyps in hyps
        ]
        result = [
            self._extract_hyp_word(
                item_hyps, item_separator_indexes, word_index
            )
            for item_hyps, item_separator_indexes, word_index in zip(
                hyps, separator_indexes, target_word_indexes
            )
        ]
        return result
def validate_python(
        self,
        __object: Any,
        *,
        strict: bool | None = None,
        from_attributes: bool | None = None,
        context: dict[str, Any] | None = None,
    ) -> T:
        
        

        
        return self.validator.validate_python(__object, strict=strict, from_attributes=from_attributes, context=context)
def register_from_pack(self, pack_dir):
        
        
        
        pack_dir = pack_dir[:-1] if pack_dir.endswith("/") else pack_dir
        _, pack = os.path.split(pack_dir)
        actions_dir = self._pack_loader.get_content_from_pack(
            pack_dir=pack_dir, content_type="actions"
        )

        # Register pack first
        self.register_pack(pack_name=pack, pack_dir=pack_dir)

        registered_count = 0
        overridden_count = 0
        if not actions_dir:
            return registered_count

        LOG.debug("Registering actions from pack %s:, dir: %s", pack, actions_dir)

        try:
            actions = self._get_actions_from_pack(actions_dir=actions_dir)
            registered_count, overridden_count = self._register_actions_from_pack(
                pack=pack, actions=actions
            )
        except Exception as e:
            if self._fail_on_failure:
                raise e

            LOG.exception("Failed registering all actions from pack: %s", actions_dir)

        return registered_count, overridden_count
def project_manage(request, project_slug):
    
    
    
    return HttpResponseRedirect(reverse('projects_detail', args=[project_slug]))
def project_import_github(request, sync=False):
    ''''''
    github_connected = oauth_utils.import_github(user=request.user, sync=sync)
    repos = GithubProject.objects.filter(users__in=[request.user])

    # Find existing projects that match a repo url
    for repo in repos:
        ghetto_repo = repo.git_url.replace('git://', '').replace('.git', '')
        projects = (Project
                    .objects
                    .public(request.user)
                    .filter(Q(repo__endswith=ghetto_repo) |
                            Q(repo__endswith=ghetto_repo + '.git')))
        if projects:
            repo.matches = [project.slug for project in projects]
        else:
            repo.matches = []

    return render_to_response(
        'projects/project_import_github.html',
        {
            'repos': repos,
            'github_connected': github_connected,
            'sync': sync,
        },
        context_instance=RequestContext(request)
    )
def validate_primary_email(self, user):
        
        
        
        email_qs = user.emailaddress_set.filter(primary=True)
        email = email_qs.first()
        if not email or not email.verified:
            Notification.objects.add(
                attached_to=user,
                message_id=MESSAGE_EMAIL_VALIDATION_PENDING,
                dismissable=True,
                format_values={
                    "account_email_url": reverse("account_email"),
                },
            )
def send_metadata_for_build(build_id: str, timestamp: str):
    
    
    
    # on GitHub the GITHUB_HEAD_REF is only set for pull_request, else we use the GITHUB_REF_NAME
    branch = (
        os.environ.get("CIRCLE_BRANCH", "")
        or os.environ.get("GITHUB_HEAD_REF", "")
        or os.environ.get("GITHUB_REF_NAME", "")
    )
    workflow_id = os.environ.get("CIRCLE_WORKFLOW_ID", "") or os.environ.get("GITHUB_RUN_ID", "")

    build_url = os.environ.get("CIRCLE_BUILD_URL", "")
    if not build_url and os.environ.get("GITHUB_SERVER_URL"):
        # construct the build-url for Github
        server = os.environ.get("GITHUB_SERVER_URL", "")
        repo = os.environ.get("GITHUB_REPOSITORY", "")
        build_url = f"{server}/{repo}/actions/runs/{workflow_id}"

    pull_requests = os.environ.get("CIRCLE_PULL_REQUESTS", "") or os.environ.get("GITHUB_REF", "")
    build_num = os.environ.get(
        "CIRCLE_BUILD_NUM", ""
    )  # TODO could not find equivalent job-id ENV in github

    data = {
        "build_id": build_id,
        "timestamp": timestamp,
        "branch": branch,
        "build_url": build_url,
        "pull_requests": pull_requests,
        "build_num": build_num,
        "workflow_id": workflow_id,
    }
    data_to_send = [json.dumps(data)]
    send_data_to_tinybird(data_to_send, data_name=DATA_SOURCE_RAW_BUILDS)
def exit():
    

    
    if no_exitfunc:
        import atexit
        atexit._clear()
    sys.exit(0)
def s_one_one(topics):
    
    
    
    s_one_one_res = []

    for top_words in topics:
        s_one_one_t = []
        for w_prime_index, w_prime in enumerate(top_words):
            for w_star_index, w_star in enumerate(top_words):
                if w_prime_index == w_star_index:
                    continue
                else:
                    s_one_one_t.append((w_prime, w_star))
        s_one_one_res.append(s_one_one_t)

    return s_one_one_res
def _normalize(self, name, columns, points):
        
        
        ret = []

        # Build initial dict by crossing columns and point
        data_dict = dict(zip(columns, points))

        # issue1871 - Check if a key exist. If a key exist, the value of
        # the key should be used as a tag to identify the measurement.
        keys_list = [k.split('.')[0] for k in columns if k.endswith('.key')]
        if len(keys_list) == 0:
            keys_list = [None]

        for measurement in keys_list:
            # Manage field
            if measurement is not None:
                fields = {
                    k.replace('{}.'.format(measurement), ''): data_dict[k]
                    for k in data_dict
                    if k.startswith('{}.'.format(measurement))
                }
            else:
                fields = data_dict
            # Transform to InfluxDB data model
            # https://docs.influxdata.com/influxdb/v1.8/write_protocols/line_protocol_reference/
            for k in fields:
                #  Do not export empty (None) value
                if fields[k] is None:
                    continue
                # Convert numerical to float
                try:
                    fields[k] = float(fields[k])
                except (TypeError, ValueError):
                    # Convert others to string
                    try:
                        fields[k] = str(fields[k])
                    except (TypeError, ValueError):
                        pass
            # Manage tags
            tags = self.parse_tags(self.tags)
            if 'key' in fields and fields['key'] in fields:
                # Create a tag from the key
                # Tag should be an string (see InfluxDB data model)
                tags[fields['key']] = str(fields[fields['key']])
                # Remove it from the field list (can not be a field and a tag)
                fields.pop(fields['key'])
            # Add the hostname as a tag
            tags['hostname'] = self.hostname
            # Add name as a tag (example for the process list)
            for k in FIELD_TO_TAG:
                if k in fields:
                    tags[k] = str(fields[k])
                    # Remove it from the field list (can not be a field and a tag)
                    if k in fields:
                        fields.pop(fields[k])
            # Add the measurement to the list
            ret.append({'measurement': name, 'tags': tags, 'fields': fields})
        return ret
def filter(names, pat):
    
    result = []
    pat = os.path.normcase(pat)
    match = _compile_pattern(pat)
    if os.path is posixpath:
        # normcase on posix is NOP. Optimize it away from the loop.
        for name in names:
            if match(name):
                result.append(name)
    else:
        for name in names:
            if match(os.path.normcase(name)):
                result.append(name)
    return result
def create_ncf_input_data(params,
                          producer=None,
                          input_meta_data=None,
                          strategy=None):
  
  
  # NCF evaluation metric calculation logic assumes that evaluation data
  # sample size are in multiples of (1 + number of negative samples in
  # evaluation) for each device. As so, evaluation batch size must be a
  # multiple of (number of replicas * (1 + number of negative samples)).
  num_devices = strategy.num_replicas_in_sync if strategy else 1
  if (params["eval_batch_size"] % (num_devices *
                                   (1 + rconst.NUM_EVAL_NEGATIVES))):
    raise ValueError("Evaluation batch size must be divisible by {} "
                     "times {}".format(num_devices,
                                       (1 + rconst.NUM_EVAL_NEGATIVES)))

  if params["train_dataset_path"]:
    assert params["eval_dataset_path"]

    train_dataset = create_dataset_from_tf_record_files(
        params["train_dataset_path"],
        input_meta_data["train_prebatch_size"],
        params["batch_size"],
        is_training=True)
    eval_dataset = create_dataset_from_tf_record_files(
        params["eval_dataset_path"],
        input_meta_data["eval_prebatch_size"],
        params["eval_batch_size"],
        is_training=False)

    num_train_steps = int(input_meta_data["num_train_steps"])
    num_eval_steps = int(input_meta_data["num_eval_steps"])
  else:
    if params["use_tpu"]:
      raise ValueError("TPU training does not support data producer yet. "
                       "Use pre-processed data.")

    assert producer
    # Start retrieving data from producer.
    train_dataset, eval_dataset = create_dataset_from_data_producer(
        producer, params)
    num_train_steps = producer.train_batches_per_epoch
    num_eval_steps = producer.eval_batches_per_epoch

  return train_dataset, eval_dataset, num_train_steps, num_eval_steps
def network_2_dataframe(ntwk, attrs=['s_db'], ports = None):
    
    
    
    from pandas import DataFrame, Series # delayed because its not a requirement
    d = {}
    index =ntwk.frequency.f_scaled

    if ports is None:
        ports = ntwk.port_tuples

    for attr in attrs:
        for m,n in ports:
            d['%s %i%i'%(attr, m+1,n+1)] = \
                Series(ntwk.__getattribute__(attr)[:,m,n], index = index)

    return DataFrame(d)
def read_all(dir: str ='.', sort = True, contains = None, f_unit = None, obj_type=None, files: list=None, recursive=False) -> dict:
    
    
    

    out={}

    filelist = []
    if files is None:
        if recursive:
            if not dir.endswith(os.path.sep):
                dir += os.path.sep
            dir += '**'
        for filename in glob.iglob(os.path.join(dir, '*.s*p'), recursive=recursive):
            filelist.append(filename)
    else:
        filelist.extend(files)
         
    if sort is True:
        filelist.sort()

    for filename in filelist:
        if contains is not None and contains not in filename:
            continue
        fullname = filename
        keyname = os.path.splitext(filename.split(os.path.sep)[-1])[0]
        try:
            out[keyname] = read(fullname)
            continue
        except:
            pass

        try:
            out[keyname] = Network(fullname)
            continue
        except:
            pass

    if f_unit is not None:
        for keyname in out:
            try:
                out[keyname].frequency.unit = f_unit
            except:
                pass

    if obj_type is not None:
        out = dict([(k, out[k]) for k in out if
            isinstance(out[k],sys.modules[__name__].__dict__[obj_type])])

    return out
def metadata_dict_from_readme(path: Path) -> Optional[Dict[str, List[str]]]:
    
    yaml_block = yaml_block_from_readme(path=path)
    if yaml_block is None:
        return None
    metada_dict = yaml.load(yaml_block, Loader=NoDuplicateSafeLoader) or dict()
    return metada_dict
def transform(self, srs, driver=None, name=None, resampling='NearestNeighbour',
                  max_error=0.0):
        
        
        
        # Convert the resampling algorithm name into an algorithm id
        algorithm = GDAL_RESAMPLE_ALGORITHMS[resampling]

        if isinstance(srs, SpatialReference):
            target_srs = srs
        elif isinstance(srs, (int, str)):
            target_srs = SpatialReference(srs)
        else:
            raise TypeError(
                'Transform only accepts SpatialReference, string, and integer '
                'objects.'
            )
        # Create warped virtual dataset in the target reference system
        target = capi.auto_create_warped_vrt(
            self._ptr, self.srs.wkt.encode(), target_srs.wkt.encode(),
            algorithm, max_error, c_void_p()
        )
        target = GDALRaster(target)

        # Construct the target warp dictionary from the virtual raster
        data = {
            'srid': target_srs.srid,
            'width': target.width,
            'height': target.height,
            'origin': [target.origin.x, target.origin.y],
            'scale': [target.scale.x, target.scale.y],
            'skew': [target.skew.x, target.skew.y],
        }

        # Set the driver and filepath if provided
        if driver:
            data['driver'] = driver

        if name:
            data['name'] = name

        # Warp the raster into new srid
        return self.warp(data, resampling=resampling, max_error=max_error)
def get_api_from_headers(headers, method=None, path=None, data=None):
    

    # initialize result
    result = API_UNKNOWN, 0

    target = headers.get("x-amz-target", "")
    host = headers.get("host", "")
    auth_header = headers.get("authorization", "")

    if not auth_header and "." not in host:
        return result[0], result[1], path, host

    ls_target = headers.get(HEADER_LOCALSTACK_TARGET, "")
    path = path or "/"

    # https://docs.aws.amazon.com/general/latest/gr/sigv4-signed-request-examples.html
    try:
        credential_scope = auth_header.split(",")[0].split()[1]
        _, _, _, service, _ = credential_scope.split("/")
        result = service, get_service_port_for_account(service, headers)
    except Exception:
        pass

    result_before = result

    # Fallback rules and route customizations applied below
    if host.endswith("cloudfront.net"):
        path = path or "/"
        result = "cloudfront", config.PORT_CLOUDFRONT
    elif target.startswith("AWSCognitoIdentityProviderService") or "cognito-idp." in host:
        result = "cognito-idp", config.PORT_COGNITO_IDP
    elif target.startswith("AWSCognitoIdentityService") or "cognito-identity." in host:
        result = "cognito-identity", config.PORT_COGNITO_IDENTITY
    elif result[0] == "s3" or uses_host_addressing(headers):
        result = "s3", config.PORT_S3
    elif result[0] == "states" in auth_header or host.startswith("states."):
        result = "stepfunctions", config.PORT_STEPFUNCTIONS
    elif "route53." in host:
        result = "route53", config.PORT_ROUTE53
    elif result[0] == "monitoring":
        result = "cloudwatch", config.PORT_CLOUDWATCH
    elif result[0] == "email":
        result = "ses", config.PORT_SES
    elif result[0] == "execute-api" or ".execute-api." in host:
        result = "apigateway", config.PORT_APIGATEWAY
    elif target.startswith("Firehose_"):
        result = "firehose", config.PORT_FIREHOSE
    elif target.startswith("DynamoDB_"):
        result = "dynamodb", config.PORT_DYNAMODB
    elif target.startswith("DynamoDBStreams") or host.startswith("streams.dynamodb."):
        # Note: DDB streams requests use ../dynamodb/.. auth header, hence we also need to update result_before
        result = result_before = "dynamodbstreams", config.PORT_DYNAMODBSTREAMS
    elif ls_target == "web":
        result = "web", config.PORT_WEB_UI
    elif result[0] == "EventBridge" or target.startswith("AWSEvents"):
        result = "events", config.PORT_EVENTS
    elif target.startswith("ResourceGroupsTaggingAPI_"):
        result = "resourcegroupstaggingapi", config.PORT_RESOURCEGROUPSTAGGINGAPI
    elif result[0] == "resource-groups":
        result = "resource-groups", config.PORT_RESOURCE_GROUPS

    return result[0], result_before[1] or result[1], path, host
def get_auth_string(method, path, headers, data=None):
    
    
    

    if auth_header := headers.get("authorization", ""):
        return auth_header

    data_components = parse_request_data(method, path, data)
    algorithm = data_components.get("X-Amz-Algorithm")
    credential = data_components.get("X-Amz-Credential")
    signature = data_components.get("X-Amz-Signature")
    signed_headers = data_components.get("X-Amz-SignedHeaders")

    if algorithm and credential and signature and signed_headers:
        return (
            f"{algorithm} Credential={credential}, "
            + f"SignedHeaders={signed_headers}, "
            + f"Signature={signature}"
        )

    return ""
def fit(self, X, y=None, sample_weight=None):
        
        
        self._validate_params()

        X = self._validate_data(
            X,
            accept_sparse="csr",
            dtype=[np.float64, np.float32],
            order="C",
            copy=self.copy_x,
            accept_large_sparse=False,
        )

        self._check_params_vs_input(X)

        random_state = check_random_state(self.random_state)
        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
        self._n_threads = _openmp_effective_n_threads()

        # Validate init array
        init = self.init
        init_is_array_like = _is_arraylike_not_scalar(init)
        if init_is_array_like:
            init = check_array(init, dtype=X.dtype, copy=True, order="C")
            self._validate_center_shape(X, init)

        # subtract of mean of x for more accurate distance computations
        if not sp.issparse(X):
            X_mean = X.mean(axis=0)
            # The copy was already done above
            X -= X_mean

            if init_is_array_like:
                init -= X_mean

        # precompute squared norms of data points
        x_squared_norms = row_norms(X, squared=True)

        if self._algorithm == "elkan":
            kmeans_single = _kmeans_single_elkan
        else:
            kmeans_single = _kmeans_single_lloyd
            self._check_mkl_vcomp(X, X.shape[0])

        best_inertia, best_labels = None, None

        for i in range(self._n_init):
            # Initialize centers
            centers_init = self._init_centroids(
                X,
                x_squared_norms=x_squared_norms,
                init=init,
                random_state=random_state,
                sample_weight=sample_weight,
            )
            if self.verbose:
                print("Initialization complete")

            # run a k-means once
            labels, inertia, centers, n_iter_ = kmeans_single(
                X,
                sample_weight,
                centers_init,
                max_iter=self.max_iter,
                verbose=self.verbose,
                tol=self._tol,
                n_threads=self._n_threads,
            )

            # determine if these results are the best so far
            # we chose a new run if it has a better inertia and the clustering is
            # different from the best so far (it's possible that the inertia is
            # slightly better even if the clustering is the same with potentially
            # permuted labels, due to rounding errors)
            if best_inertia is None or (
                inertia < best_inertia
                and not _is_same_clustering(labels, best_labels, self.n_clusters)
            ):
                best_labels = labels
                best_centers = centers
                best_inertia = inertia
                best_n_iter = n_iter_

        if not sp.issparse(X):
            if not self.copy_x:
                X += X_mean
            best_centers += X_mean

        distinct_clusters = len(set(best_labels))
        if distinct_clusters < self.n_clusters:
            warnings.warn(
                "Number of distinct clusters ({}) found smaller than "
                "n_clusters ({}). Possibly due to duplicate points "
                "in X.".format(distinct_clusters, self.n_clusters),
                ConvergenceWarning,
                stacklevel=2,
            )

        self.cluster_centers_ = best_centers
        self._n_features_out = self.cluster_centers_.shape[0]
        self.labels_ = best_labels
        self.inertia_ = best_inertia
        self.n_iter_ = best_n_iter
        return self
def _labels_inertia(X, sample_weight, x_squared_norms, centers,
                    n_threads=None):
    
    
    n_samples = X.shape[0]
    n_clusters = centers.shape[0]

    n_threads = _openmp_effective_n_threads(n_threads)

    sample_weight = _check_normalize_sample_weight(sample_weight, X)
    labels = np.full(n_samples, -1, dtype=np.int32)
    weight_in_clusters = np.zeros(n_clusters, dtype=centers.dtype)
    center_shift = np.zeros_like(weight_in_clusters)

    if sp.issparse(X):
        _labels = lloyd_iter_chunked_sparse
        _inertia = _inertia_sparse
    else:
        _labels = lloyd_iter_chunked_dense
        _inertia = _inertia_dense

    _labels(X, sample_weight, x_squared_norms, centers, centers,
            weight_in_clusters, labels, center_shift, n_threads,
            update_centers=False)

    inertia = _inertia(X, sample_weight, centers, labels)

    return labels, inertia
def k_means(
    X,
    n_clusters,
    *,
    sample_weight=None,
    init="k-means++",
    n_init="auto",
    max_iter=300,
    verbose=False,
    tol=1e-4,
    random_state=None,
    copy_x=True,
    algorithm="lloyd",
    return_n_iter=False,
):
    
    
    est = KMeans(
        n_clusters=n_clusters,
        init=init,
        n_init=n_init,
        max_iter=max_iter,
        verbose=verbose,
        tol=tol,
        random_state=random_state,
        copy_x=copy_x,
        algorithm=algorithm,
    ).fit(X, sample_weight=sample_weight)
    if return_n_iter:
        return est.cluster_centers_, est.labels_, est.inertia_, est.n_iter_
    else:
        return est.cluster_centers_, est.labels_, est.inertia_
def _kmeans_single_elkan(X, sample_weight, centers_init, max_iter=300,
                         verbose=False, x_squared_norms=None, tol=1e-4,
                         n_threads=1):
    
    
    n_samples = X.shape[0]
    n_clusters = centers_init.shape[0]

    # Buffers to avoid new allocations at each iteration.
    centers = centers_init
    centers_new = np.zeros_like(centers)
    weight_in_clusters = np.zeros(n_clusters, dtype=X.dtype)
    labels = np.full(n_samples, -1, dtype=np.int32)
    center_half_distances = euclidean_distances(centers) / 2
    distance_next_center = np.partition(np.asarray(center_half_distances),
                                        kth=1, axis=0)[1]
    upper_bounds = np.zeros(n_samples, dtype=X.dtype)
    lower_bounds = np.zeros((n_samples, n_clusters), dtype=X.dtype)
    center_shift = np.zeros(n_clusters, dtype=X.dtype)

    if sp.issparse(X):
        init_bounds = init_bounds_sparse
        elkan_iter = elkan_iter_chunked_sparse
        _inertia = _inertia_sparse
    else:
        init_bounds = init_bounds_dense
        elkan_iter = elkan_iter_chunked_dense
        _inertia = _inertia_dense

    init_bounds(X, centers, center_half_distances,
                labels, upper_bounds, lower_bounds)

    for i in range(max_iter):
        elkan_iter(X, sample_weight, centers, centers_new,
                   weight_in_clusters, center_half_distances,
                   distance_next_center, upper_bounds, lower_bounds,
                   labels, center_shift, n_threads)

        # compute new pairwise distances between centers and closest other
        # center of each center for next iterations
        center_half_distances = euclidean_distances(centers_new) / 2
        distance_next_center = np.partition(
            np.asarray(center_half_distances), kth=1, axis=0)[1]

        if verbose:
            inertia = _inertia(X, sample_weight, centers, labels)
            print(f"Iteration {i}, inertia {inertia}")

        centers, centers_new = centers_new, centers

        center_shift_tot = (center_shift**2).sum()
        if center_shift_tot <= tol:
            if verbose:
                print(f"Converged at iteration {i}: center shift "
                      f"{center_shift_tot} within tolerance {tol}.")
            break

    if center_shift_tot > 0:
        # rerun E-step so that predicted labels match cluster centers
        elkan_iter(X, sample_weight, centers, centers, weight_in_clusters,
                   center_half_distances, distance_next_center,
                   upper_bounds, lower_bounds, labels, center_shift,
                   n_threads, update_centers=False)

    inertia = _inertia(X, sample_weight, centers, labels)

    return labels, inertia, centers, i + 1
def fit(self, X, y=None, sample_weight=None):
        
        
        X = self._validate_data(X, accept_sparse='csr',
                                dtype=[np.float64, np.float32],
                                order='C', accept_large_sparse=False)

        self._check_params(X)
        random_state = check_random_state(self.random_state)
        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
        n_samples, n_features = X.shape

        # Validate init array
        init = self.init
        if hasattr(init, '__array__'):
            init = check_array(init, dtype=X.dtype, copy=True, order='C')
            self._validate_center_shape(X, init)

        self._check_mkl_vcomp(X, self._batch_size)

        # precompute squared norms of data points
        x_squared_norms = row_norms(X, squared=True)

        # Validation set for the init
        validation_indices = random_state.randint(0, n_samples,
                                                  self._init_size)
        X_valid = X[validation_indices]
        sample_weight_valid = sample_weight[validation_indices]
        x_squared_norms_valid = x_squared_norms[validation_indices]

        # perform several inits with random subsets
        best_inertia = None
        for init_idx in range(self._n_init):
            if self.verbose:
                print(f"Init {init_idx + 1}/{self._n_init} with method {init}")

            # Initialize the centers using only a fraction of the data as we
            # expect n_samples to be very large when using MiniBatchKMeans.
            cluster_centers = self._init_centroids(
                X, x_squared_norms=x_squared_norms, init=init,
                random_state=random_state, init_size=self._init_size)

            # Compute inertia on a validation set.
            _, inertia = _labels_inertia_threadpool_limit(
                X_valid, sample_weight_valid, x_squared_norms_valid,
                cluster_centers, n_threads=self._n_threads)

            if self.verbose:
                print(f"Inertia for init {init_idx + 1}/{self._n_init}: "
                      f"{inertia}")
            if best_inertia is None or inertia < best_inertia:
                init_centers = cluster_centers
                best_inertia = inertia

        centers = init_centers
        centers_new = np.empty_like(centers)

        # Initialize counts
        self._counts = np.zeros(self.n_clusters, dtype=X.dtype)

        # Attributes to monitor the convergence
        self._ewa_inertia = None
        self._ewa_inertia_min = None
        self._no_improvement = 0

        # Initialize number of samples seen since last reassignment
        self._n_since_last_reassign = 0

        n_steps = (self.max_iter * n_samples) // self._batch_size

        with threadpool_limits(limits=1, user_api="blas"):
            # Perform the iterative optimization until convergence
            for i in range(n_steps):
                # Sample a minibatch from the full dataset
                minibatch_indices = random_state.randint(0, n_samples,
                                                         self._batch_size)

                # Perform the actual update step on the minibatch data
                batch_inertia = _mini_batch_step(
                    X=X[minibatch_indices],
                    x_squared_norms=x_squared_norms[minibatch_indices],
                    sample_weight=sample_weight[minibatch_indices],
                    centers=centers,
                    centers_new=centers_new,
                    weight_sums=self._counts,
                    random_state=random_state,
                    random_reassign=self._random_reassign(),
                    reassignment_ratio=self.reassignment_ratio,
                    verbose=self.verbose,
                    n_threads=self._n_threads)

                if self._tol > 0.0:
                    centers_squared_diff = np.sum((centers_new - centers)**2)
                else:
                    centers_squared_diff = 0

                centers, centers_new = centers_new, centers

                # Monitor convergence and do early stopping if necessary
                if self._mini_batch_convergence(
                        i, n_steps, n_samples, centers_squared_diff,
                        batch_inertia):
                    break

        self.cluster_centers_ = centers

        self.n_steps_ = i + 1
        self.n_iter_ = int(np.ceil(((i + 1) * self._batch_size) / n_samples))

        if self.compute_labels:
            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(
                X, sample_weight, x_squared_norms, self.cluster_centers_,
                n_threads=self._n_threads)
        else:
            self.inertia_ = self._ewa_inertia * n_samples

        return self
def _init_centroids(
        self,
        X,
        x_squared_norms,
        init,
        random_state,
        sample_weight,
        init_size=None,
        n_centroids=None,
    ):
        
        
        n_samples = X.shape[0]
        n_clusters = self.n_clusters if n_centroids is None else n_centroids

        if init_size is not None and init_size < n_samples:
            init_indices = random_state.randint(0, n_samples, init_size)
            X = X[init_indices]
            x_squared_norms = x_squared_norms[init_indices]
            n_samples = X.shape[0]
            sample_weight = sample_weight[init_indices]

        if isinstance(init, str) and init == "k-means++":
            centers, _ = _kmeans_plusplus(
                X,
                n_clusters,
                random_state=random_state,
                x_squared_norms=x_squared_norms,
                sample_weight=sample_weight,
            )
        elif isinstance(init, str) and init == "random":
            seeds = random_state.choice(
                n_samples,
                size=n_clusters,
                replace=False,
                p=sample_weight / sample_weight.sum(),
            )
            centers = X[seeds]
        elif _is_arraylike_not_scalar(self.init):
            centers = init
        elif callable(init):
            centers = init(X, n_clusters, random_state=random_state)
            centers = check_array(centers, dtype=X.dtype, copy=False, order="C")
            self._validate_center_shape(X, centers)

        if sp.issparse(centers):
            centers = centers.toarray()

        return centers
def predict(self, X):
        
        
        check_is_fitted(self)

        X = self._check_test_data(X)

        # sample weights are not used by predict but cython helpers expect an array
        sample_weight = np.ones(X.shape[0], dtype=X.dtype)

        labels = _labels_inertia_threadpool_limit(
            X,
            sample_weight,
            self.cluster_centers_,
            n_threads=self._n_threads,
            return_inertia=False,
        )

        return labels
def dialog_from_sentence(sentence, skill_path, lang):
    
    
    dialog_paths = join(skill_path, 'dialog', lang, '*.dialog')
    best = (None, 0)
    for path in glob(dialog_paths):
        patterns = load_dialog_file(path)
        match, _ = _match_dialog_patterns(patterns, sentence.lower())
        if match is not False:
            if len(patterns[match]) > best[1]:
                best = (path, len(patterns[match]))
    if best[0] is not None:
        return basename(best[0])
    else:
        return None
def _match_dialog_patterns(dialogs, sentence):
    
    
    # Allow custom fields to be anything
    # i.e {field} gets turned into ".*"
    regexes = []
    for dialog in dialogs:
        data = {element[1]: '.*'
                for element in Formatter().parse(dialog)}
        regexes.append(dialog.format(**data))

    # Remove double whitespaces and ensure that it matches from
    # the beginning of the line.
    regexes = ['^' + ' '.join(reg.split()) for reg in regexes]
    debug = 'MATCHING: {}\n'.format(sentence)
    for index, regex in enumerate(regexes):
        match = re.match(regex, sentence)
        debug += '---------------\n'
        debug += '{} {}\n'.format(regex, match is not None)
        if match:
            return index, debug
    else:
        return False, debug
def cleanup():
    
    
    
    compiledir = theano.config.compiledir
    for directory in os.listdir(compiledir):
        file = None
        try:
            try:
                filename = os.path.join(compiledir, directory, "key.pkl")
                file = open(filename, 'rb')
                #print file
                try:
                    keydata = cPickle.load(file)
                    for key in list(keydata.keys):
                        have_npy_abi_version = False
                        have_c_compiler = False
                        for obj in flatten(key):
                            if isinstance(obj, numpy.ndarray):
                                keydata.remove_key(key)
                                break
                            elif isinstance(obj, basestring):
                                if obj.startswith('NPY_ABI_VERSION=0x'):
                                    have_npy_abi_version = True
                                elif obj.startswith('c_compiler_str='):
                                    have_c_compiler = True

                        if not have_npy_abi_version or not have_c_compiler:
                            keydata.remove_key(key)
                    if len(keydata.keys) == 0:
                        shutil.rmtree(os.path.join(compiledir, directory))

                except EOFError:
                    print ("ERROR while reading this key file '%s'."
                           " Delete its directory" % filename)
            except IOError:
                pass
        finally:
            if file is not None:
                file.close()
def is_on(self) -> bool:
        
        if self.coordinator.data:
            data: dict = self.coordinator.data[Platform.SWITCH]
            for torrent in data.values():
                item = torrent.popitem()
                if not item[1]:
                    return True
        return False
def get(self, model_directory: str) -> str:
        
            
        
        # Expend model directory if needed.
        if not isabs(model_directory):
            model_directory = join(self.DEFAULT_MODEL_PATH, model_directory)
        # Download it if not exists.
        model_probe: str = join(model_directory, self.MODEL_PROBE_PATH)
        if not exists(model_probe):
            if not exists(model_directory):
                makedirs(model_directory)
                self.download(
                    model_directory.split(sep)[-1],
                    model_directory)
                self.writeProbe(model_directory)
        return model_directory
def make_masks(self, src, tgt=None, wav_len=None, pad_idx=0):
        
        
        src_key_padding_mask = None
        if wav_len is not None:
            abs_len = torch.round(wav_len * src.shape[1])
            src_key_padding_mask = ~length_to_mask(abs_len).bool()

        src_mask = None

        # If no decoder in the transformer...
        if tgt is not None:
            tgt_key_padding_mask = get_key_padding_mask(tgt, pad_idx=pad_idx)
            tgt_mask = get_lookahead_mask(tgt)
        else:
            tgt_key_padding_mask = None
            tgt_mask = None

        return src_key_padding_mask, tgt_key_padding_mask, src_mask, tgt_mask
def fetch_transaction_fees(self, codes=None, params={}):
        
        
        
        self.load_markets()
        result = {}
        response = self.privatePostAccountFees(params)
        #
        # {
        #     'withdraw': {
        #         'BTC': '0.0004',
        #     }
        # }
        #
        fees = self.safe_value(response, 'withdraw')
        ids = list(fees.keys())
        for i in range(0, len(ids)):
            id = ids[i]
            code = self.safe_currency_code(id)
            if codes is not None and not self.in_array(code, codes):
                continue
            result[code] = {
                'withdraw': self.safe_number(fees, id),
                'deposit': {},
                'info': self.safe_number(fees, id),
            }
        return result
def fetch_my_trades(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' fetchMyTrades() requires a symbol argument')
        self.load_markets()
        market = self.market(symbol)
        request = {
            'symbol': market['id'],
        }
        if limit is not None:
            request['limit_trades'] = limit
        if since is not None:
            request['timestamp'] = self.parse_to_int(since / 1000)
        response = self.privatePostMytrades(self.extend(request, params))
        return self.parse_trades(response, market, since, limit)
def test_permutation(self):
        
        # Check over two calls to see if the random state is correctly updated.
        random = RandomStreams(utt.fetch_seed())
        fn = function([], random.permutation((20,), 10), updates=random.updates())

        fn_val0 = fn()
        fn_val1 = fn()

        rng_seed = numpy.random.RandomState(utt.fetch_seed()).randint(2**30)
        rng = numpy.random.RandomState(int(rng_seed)) #int() is for 32bit

        # rng.permutation outputs one vector at a time, so we iterate.
        numpy_val0 = numpy.asarray([rng.permutation(10) for i in range(20)])
        numpy_val1 = numpy.asarray([rng.permutation(10) for i in range(20)])

        assert numpy.all(fn_val0 == numpy_val0)
        assert numpy.all(fn_val1 == numpy_val1)
def _get_sensor(self, monitor):
        
        return monitor
def vol_circular_cylinder(radius: float, height: float) -> float:
    
    
    if height < 0 or radius < 0:
        raise ValueError("vol_circular_cylinder() only accepts non-negative values")
    return pi * pow(radius, 2) * height
def vol_conical_frustum(height: float, radius_1: float, radius_2: float) -> float:
    
    
    # Volume is 1/3 * pi * height *
    #           (radius_1 squared + radius_2 squared + radius_1 * radius_2)
    if radius_1 < 0 or radius_2 < 0 or height < 0:
        raise ValueError("vol_conical_frustum() only accepts non-negative values")
    return (
        1
        / 3
        * pi
        * height
        * (pow(radius_1, 2) + pow(radius_2, 2) + radius_1 * radius_2)
    )
def get_group_permissions(self, obj=None):
        
        
        
        permissions = set()
        for backend in auth.get_backends():
            if hasattr(backend, "get_group_permissions"):
                if obj is not None:
                    permissions.update(backend.get_group_permissions(self,
                                                                     obj))
                else:
                    permissions.update(backend.get_group_permissions(self))
        return permissions
def email_user(self, subject, message, from_email=None, **kwargs):
        
        send_mail(subject, message, from_email, [self.email], **kwargs)
def get_new_command(command, settings):
    
    
    
    dest = command.script.split()[1].split(os.sep)
    if dest[-1] == '':
        dest = dest[:-1]
    cwd = os.getcwd()
    for directory in dest:
        if directory == ".":
            continue
        elif directory == "..":
            cwd = os.path.split(cwd)[0]
            continue
        best_matches = get_close_matches(directory, _get_sub_dirs(cwd), cutoff=MAX_ALLOWED_DIFF)
        if len(best_matches):
            cwd = os.path.join(cwd, best_matches[0])
        else:
            return cd_mkdir.get_new_command(command, settings)
    return "cd {0}".format(cwd)
def fetch_open_orders(self, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):
        
        
        
        self.check_required_symbol('fetchOpenOrders', symbol)
        self.load_markets()
        market = self.market(symbol)
        request = {
            'coin_pair': market['id'],
            'status_list': '[2]',  # open only
        }
        response = self.privatePostListOrders(self.extend(request, params))
        responseData = self.safe_value(response, 'response_data', {})
        orders = self.safe_value(responseData, 'orders', [])
        return self.parse_orders(orders, market, since, limit)
def cancel_order(self, id: str, symbol: Str = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' cancelOrder() requires a symbol argument')
        self.load_markets()
        market = self.market(symbol)
        request = {
            'coin_pair': market['id'],
            'order_id': id,
        }
        response = self.privatePostCancelOrder(self.extend(request, params))
        #
        #     {
        #         "response_data": {
        #             "order": {
        #                 "order_id": 2176769,
        #                 "coin_pair": "BRLBCH",
        #                 "order_type": 2,
        #                 "status": 3,
        #                 "has_fills": False,
        #                 "quantity": "0.10000000",
        #                 "limit_price": "1996.15999",
        #                 "executed_quantity": "0.00000000",
        #                 "executed_price_avg": "0.00000",
        #                 "fee": "0.00000000",
        #                 "created_timestamp": "1536956488",
        #                 "updated_timestamp": "1536956499",
        #                 "operations": []
        #             }
        #         },
        #         "status_code": 100,
        #         "server_unix_timestamp": "1536956499"
        #     }
        #
        responseData = self.safe_value(response, 'response_data', {})
        order = self.safe_value(responseData, 'order', {})
        return self.parse_order(order, market)
def get_trial_checkpoints_paths(self, trial, metric=TRAINING_ITERATION):
        
        
        if isinstance(trial, str):
            trial_dir = os.path.expanduser(trial)
            # Get checkpoints from logdir.
            chkpt_df = TrainableUtil.get_checkpoints_paths(trial_dir)

            # Join with trial dataframe to get metrics.
            trial_df = self.trial_dataframes[trial_dir]
            path_metric_df = chkpt_df.merge(
                trial_df, on="training_iteration", how="inner")
            return path_metric_df[["chkpt_path", metric]].values.tolist()
        elif isinstance(trial, Trial):
            checkpoints = trial.checkpoint_manager.best_checkpoints()
            return [[c.value, c.result[metric]] for c in checkpoints]
        else:
            raise ValueError("trial should be a string or a Trial instance.")
def get_best_checkpoint(self, trial, metric=TRAINING_ITERATION,
                            mode="max"):
        
        
        assert mode in ["max", "min"]
        checkpoint_paths = self.get_trial_checkpoints_paths(trial, metric)
        if mode == "max":
            return max(checkpoint_paths, key=lambda x: x[1])[0]
        else:
            return min(checkpoint_paths, key=lambda x: x[1])[0]
def get_best_trial(self, metric, mode="max", scope="all"):
        
        
        if mode not in ["max", "min"]:
            raise ValueError(
                "ExperimentAnalysis: attempting to get best trial for "
                "metric {} for mode {} not in [\"max\", \"min\"]".format(
                    metric, mode))
        if scope not in ["all", "last", "avg", "last-5-avg", "last-10-avg"]:
            raise ValueError(
                "ExperimentAnalysis: attempting to get best trial for "
                "metric {} for scope {} not in [\"all\", \"last\", \"avg\", "
                "\"last-5-avg\", \"last-10-avg\"]".format(metric, scope))
        best_trial = None
        best_metric_score = None
        for trial in self.trials:
            if metric not in trial.metric_analysis:
                continue

            if scope in ["last", "avg", "last-5-avg", "last-10-avg"]:
                metric_score = trial.metric_analysis[metric][scope]
            else:
                metric_score = trial.metric_analysis[metric][mode]

            if best_metric_score is None:
                best_metric_score = metric_score
                best_trial = trial
                continue

            if (mode == "max") and (best_metric_score < metric_score):
                best_metric_score = metric_score
                best_trial = trial
            elif (mode == "min") and (best_metric_score > metric_score):
                best_metric_score = metric_score
                best_trial = trial

        return best_trial
def get_all_configs(self, prefix: bool = False) -> Dict[str, Dict]:
        
        
        fail_count = 0
        for path in self._get_trial_paths():
            try:
                with open(os.path.join(path, EXPR_PARAM_FILE)) as f:
                    config = json.load(f)
                    if prefix:
                        for k in list(config):
                            config[CONFIG_PREFIX + k] = config.pop(k)
                    self._configs[path] = config
            except Exception:
                fail_count += 1

        if fail_count:
            logger.warning(
                "Couldn't read config from {} paths".format(fail_count))
        return self._configs
def parse(self, text):
        
        
        
        return _re_token.sub(self.__format_token, text)
def create_pandas_dataframe_agent(
    llm: LanguageModelLike,
    df: Any,
    agent_type: Union[
        AgentType, Literal["openai-tools", "tool-calling"]
    ] = AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    callback_manager: Optional[BaseCallbackManager] = None,
    prefix: Optional[str] = None,
    suffix: Optional[str] = None,
    input_variables: Optional[List[str]] = None,
    verbose: bool = False,
    return_intermediate_steps: bool = False,
    max_iterations: Optional[int] = 15,
    max_execution_time: Optional[float] = None,
    early_stopping_method: str = "force",
    agent_executor_kwargs: Optional[Dict[str, Any]] = None,
    include_df_in_prompt: Optional[bool] = True,
    number_of_head_rows: int = 5,
    extra_tools: Sequence[BaseTool] = (),
    engine: Literal["pandas", "modin"] = "pandas",
    **kwargs: Any,
) -> AgentExecutor:
    

      # noqa: E501
    try:
        if engine == "modin":
            import modin.pandas as pd
        elif engine == "pandas":
            import pandas as pd
        else:
            raise ValueError(
                f"Unsupported engine {engine}. It must be one of 'modin' or 'pandas'."
            )
    except ImportError as e:
        raise ImportError(
            f"`{engine}` package not found, please install with `pip install {engine}`"
        ) from e

    if is_interactive_env():
        pd.set_option("display.max_columns", None)

    for _df in df if isinstance(df, list) else [df]:
        if not isinstance(_df, pd.DataFrame):
            raise ValueError(f"Expected pandas DataFrame, got {type(_df)}")

    if input_variables:
        kwargs = kwargs or {}
        kwargs["input_variables"] = input_variables
    if kwargs:
        warnings.warn(
            f"Received additional kwargs {kwargs} which are no longer supported."
        )

    df_locals = {}
    if isinstance(df, list):
        for i, dataframe in enumerate(df):
            df_locals[f"df{i + 1}"] = dataframe
    else:
        df_locals["df"] = df
    tools = [PythonAstREPLTool(locals=df_locals)] + list(extra_tools)

    if agent_type == AgentType.ZERO_SHOT_REACT_DESCRIPTION:
        if include_df_in_prompt is not None and suffix is not None:
            raise ValueError(
                "If suffix is specified, include_df_in_prompt should not be."
            )
        prompt = _get_prompt(
            df,
            prefix=prefix,
            suffix=suffix,
            include_df_in_prompt=include_df_in_prompt,
            number_of_head_rows=number_of_head_rows,
        )
        agent: Union[BaseSingleActionAgent, BaseMultiActionAgent] = RunnableAgent(
            runnable=create_react_agent(llm, tools, prompt),  # type: ignore
            input_keys_arg=["input"],
            return_keys_arg=["output"],
        )
    elif agent_type in (AgentType.OPENAI_FUNCTIONS, "openai-tools", "tool-calling"):
        prompt = _get_functions_prompt(
            df,
            prefix=prefix,
            suffix=suffix,
            include_df_in_prompt=include_df_in_prompt,
            number_of_head_rows=number_of_head_rows,
        )
        if agent_type == AgentType.OPENAI_FUNCTIONS:
            runnable = create_openai_functions_agent(
                cast(BaseLanguageModel, llm), tools, prompt
            )
            agent = RunnableAgent(
                runnable=runnable,
                input_keys_arg=["input"],
                return_keys_arg=["output"],
            )
        else:
            if agent_type == "openai-tools":
                runnable = create_openai_tools_agent(
                    cast(BaseLanguageModel, llm), tools, prompt
                )
            else:
                runnable = create_tool_calling_agent(
                    cast(BaseLanguageModel, llm), tools, prompt
                )
            agent = RunnableMultiActionAgent(
                runnable=runnable,
                input_keys_arg=["input"],
                return_keys_arg=["output"],
            )
    else:
        raise ValueError(
            f"Agent type {agent_type} not supported at the moment. Must be one of "
            "'tool-calling', 'openai-tools', 'openai-functions', or "
            "'zero-shot-react-description'."
        )
    return AgentExecutor(
        agent=agent,
        tools=tools,
        callback_manager=callback_manager,
        verbose=verbose,
        return_intermediate_steps=return_intermediate_steps,
        max_iterations=max_iterations,
        max_execution_time=max_execution_time,
        early_stopping_method=early_stopping_method,
        **(agent_executor_kwargs or {}),
    )
def partial_fit(self, X, y=None):
        
        
        has_components = hasattr(self, "components_")

        if not has_components:
            self._validate_params()

        X = self._validate_data(
            X, dtype=[np.float64, np.float32], order="C", reset=not has_components
        )

        if not has_components:
            # This instance has not been fitted yet (fit or partial_fit)
            self._check_params(X)
            self._random_state = check_random_state(self.random_state)

            dictionary = self._initialize_dict(X, self._random_state)

            self.n_steps_ = 0

            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)
            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)
        else:
            dictionary = self.components_

        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)

        self.components_ = dictionary
        self.n_steps_ += 1

        return self
def dict_learning(
    X,
    n_components,
    *,
    alpha,
    max_iter=100,
    tol=1e-8,
    method="lars",
    n_jobs=None,
    dict_init=None,
    code_init=None,
    callback=None,
    verbose=False,
    random_state=None,
    return_n_iter=False,
    positive_dict=False,
    positive_code=False,
    method_max_iter=1000,
):
    
    
    estimator = DictionaryLearning(
        n_components=n_components,
        alpha=alpha,
        max_iter=max_iter,
        tol=tol,
        fit_algorithm=method,
        n_jobs=n_jobs,
        dict_init=dict_init,
        callback=callback,
        code_init=code_init,
        verbose=verbose,
        random_state=random_state,
        positive_code=positive_code,
        positive_dict=positive_dict,
        transform_max_iter=method_max_iter,
    )
    code = estimator.fit_transform(X)
    if return_n_iter:
        return (
            code,
            estimator.components_,
            estimator.error_,
            estimator.n_iter_,
        )
    return code, estimator.components_, estimator.error_
def available_flag(self) -> asyncio.Event:
        
        return self._wrap_event_flag
def assertExtractedParametersMatch(self, format_string, command, values):
        
        
        
        extracted_params = extract_parameters_for_action_alias_db(
            action_alias_db=self.action_alias_db,
            format_str=format_string,
            param_stream=command)

        if extracted_params != values:
            msg = ('Extracted parameters from command string "%s" against format string "%s"'
                   ' didn\'t match the provided values: ' % (command, format_string))

            # Note: We intercept the exception so we can can include diff for the dictionaries
            try:
                self.assertEqual(extracted_params, values)
            except AssertionError as e:
                msg += str(e)

            raise AssertionError(msg)
def conv2d(input,
           filters,
           input_shape=None,
           filter_shape=None,
           border_mode='valid',
           subsample=(1, 1),
           filter_flip=True):
    
    

    conv_op = AbstractConv2d(imshp=input_shape,
                             kshp=filter_shape,
                             border_mode=border_mode,
                             subsample=subsample,
                             filter_flip=filter_flip)
    return conv_op(input, filters)
def conv2d_grad_wrt_inputs(output_grad,
                           filters,
                           input_shape,
                           filter_shape=None,
                           border_mode='valid',
                           subsample=(1, 1),
                           filter_flip=True,
                           filter_dilation=(1, 1),
                           num_groups=1,
                           unshared=False):
    

    

    filters = as_tensor_variable(filters)
    output_grad = as_tensor_variable(output_grad)

    # checking the type of input_shape
    for dim in [0, 1]:
        assert isinstance(input_shape[dim], (theano.tensor.TensorConstant,
                                             integer_types, type(None)))
    for dim in [2, 3]:
        assert isinstance(input_shape[dim], (theano.tensor.TensorVariable,
                                             theano.tensor.TensorConstant,
                                             integer_types))

    # checking the type of filter_shape
    if filter_shape is not None:
        for dim in [0, 1, 2, 3]:
            assert isinstance(filter_shape[dim], (theano.tensor.TensorConstant,
                                                  integer_types, type(None)))
        if unshared:
            for dim in [4, 5]:
                assert isinstance(filter_shape[dim], (theano.tensor.TensorConstant,
                                                      integer_types, type(None)))

    # setting the last two dimensions of input_shape to None, if
    # the type of these dimensions is TensorVariable.
    numerical_input_shape = list(input_shape)
    for dim in [2, 3]:
        if isinstance(input_shape[dim], theano.tensor.TensorVariable):
            numerical_input_shape[dim] = None

    grad_input_op = AbstractConv2d_gradInputs(imshp=numerical_input_shape,
                                              kshp=filter_shape,
                                              border_mode=border_mode,
                                              subsample=subsample,
                                              filter_flip=filter_flip,
                                              filter_dilation=filter_dilation,
                                              num_groups=num_groups,
                                              unshared=unshared)

    return grad_input_op(filters, output_grad, input_shape[-2:])
def get_conv_gradinputs_shape_1axis(kernel_shape, top_shape, border_mode,
                                    subsample, dilation):
    
    

    
    if None in [kernel_shape, top_shape, border_mode,
                subsample, dilation]:
        return None
    if subsample != 1:
        return None

    # Implicit dilated kernel shape
    dil_kernel_shape = (kernel_shape - 1) * dilation + 1
    if border_mode == "half":
        pad_l = pad_r = dil_kernel_shape // 2
    elif border_mode == "full":
        pad_l = pad_r = dil_kernel_shape - 1
    elif border_mode == "valid":
        pad_l = pad_r = 0
    else:
        if isinstance(border_mode, tuple):
            pad_l, pad_r = border_mode
        else:
            pad_l = pad_r = border_mode
        if pad_l < 0 or pad_r < 0:
            raise ValueError("border_mode must be >= 0")

    # In case of symbolic shape, we want to build the smallest graph
    # image_shape = (top_shape - 1) * s - 2 * pad + dil_kernel_shape + a
    # where 0 <= a < subsample, but we have checked that subsample == 1
    image_shape = (top_shape + dil_kernel_shape - 1)
    if pad_l > 0:
        image_shape -= pad_l
    if pad_r > 0:
        image_shape -= pad_r

    return image_shape
def bilinear_upsampling(input,
                        ratio=None,
                        frac_ratio=None,
                        batch_size=None,
                        num_input_channels=None,
                        use_1D_kernel=True):
    
    

    if ratio and frac_ratio:
        raise ValueError("can't use ratio and frac_ratio together")
    if not (ratio or frac_ratio):
        raise ValueError("No ratio (or frac_ratio) provided")

    if frac_ratio:
        if use_1D_kernel:
            raise ValueError('For fractional ratios 1D kernel'
                             'method not implemented. You may want to pass '
                             'use_1D_kernel as False')

    if not use_1D_kernel:
        return frac_bilinear_upsampling(input,
                                        ratio=ratio,
                                        frac_ratio=frac_ratio)

    # the remaining case if integer ratio with use_1D_kernel
    T = theano.tensor
    try:
        up_bs = batch_size * num_input_channels
    except TypeError:
        up_bs = None
    row, col = input.shape[2:]
    up_input = input.reshape((-1, 1, row, col))

    # concatenating the first and last row and column
    # first and last row
    concat_mat = T.concatenate((up_input[:, :, :1, :], up_input,
                                up_input[:, :, -1:, :]), axis=2)
    # first and last col
    concat_mat = T.concatenate((concat_mat[:, :, :, :1], concat_mat,
                                concat_mat[:, :, :, -1:]), axis=3)
    concat_col = col + 2

    pad = 2 * ratio - (ratio - 1) // 2 - 1

    if use_1D_kernel:
        kern = bilinear_kernel_1D(ratio=ratio, normalize=True)
        # upsampling rows
        upsampled_row = conv2d_grad_wrt_inputs(output_grad=concat_mat,
                                               filters=kern[np.newaxis,
                                                            np.newaxis, :,
                                                            np.newaxis],
                                               input_shape=(up_bs, 1,
                                                            row * ratio,
                                                            concat_col),
                                               filter_shape=(1, 1, None, 1),
                                               border_mode=(pad, 0),
                                               subsample=(ratio, 1),
                                               filter_flip=True,
                                               filter_dilation=(1, 1))
        # upsampling cols
        upsampled_mat = conv2d_grad_wrt_inputs(output_grad=upsampled_row,
                                               filters=kern[np.newaxis,
                                                            np.newaxis,
                                                            np.newaxis, :],
                                               input_shape=(up_bs, 1,
                                                            row * ratio,
                                                            col * ratio),
                                               filter_shape=(1, 1, 1, None),
                                               border_mode=(0, pad),
                                               subsample=(1, ratio),
                                               filter_flip=True,
                                               filter_dilation=(1, 1))
    else:
        kern = bilinear_kernel_2D(ratio=ratio, normalize=True)
        upsampled_mat = conv2d_grad_wrt_inputs(output_grad=concat_mat,
                                               filters=kern[np.newaxis,
                                                            np.newaxis, :, :],
                                               input_shape=(up_bs, 1,
                                                            row * ratio,
                                                            col * ratio),
                                               filter_shape=(1, 1, None, None),
                                               border_mode=(pad, pad),
                                               subsample=(ratio, ratio),
                                               filter_flip=True,
                                               filter_dilation=(1, 1))

    return upsampled_mat.reshape((input.shape[0], input.shape[1],
                                  row * ratio, col * ratio))
def get_nodes_to_launch(
            self,
            nodes: List[NodeID],
            launching_nodes: Dict[NodeType, int],
            resource_demands: List[ResourceDict],
            unused_resources_by_ip: Dict[NodeIP, ResourceDict],
            pending_placement_groups: List[PlacementGroupTableData],
            max_resources_by_ip: Dict[NodeIP, ResourceDict],
            ensure_min_cluster_size: List[ResourceDict] = None,
    ) -> (Dict[NodeType, int], List[ResourceDict]):
        
        
        if self.is_legacy_yaml():
            # When using legacy yaml files we need to infer the head & worker
            # node resources from the static node resources from LoadMetrics.
            self._infer_legacy_node_resources_if_needed(max_resources_by_ip)

        self._update_node_resources_from_runtime(nodes, max_resources_by_ip)

        node_resources: List[ResourceDict]
        node_type_counts: Dict[NodeType, int]
        node_resources, node_type_counts = self.calculate_node_resources(
            nodes, launching_nodes, unused_resources_by_ip)

        logger.debug("Cluster resources: {}".format(node_resources))
        logger.debug("Node counts: {}".format(node_type_counts))
        # Step 2: add nodes to add to satisfy min_workers for each type
        (node_resources,
         node_type_counts,
         adjusted_min_workers) = \
            _add_min_workers_nodes(
                node_resources, node_type_counts, self.node_types,
                self.max_workers, self.head_node_type, ensure_min_cluster_size)

        # Step 3: get resource demands of placement groups and return the
        # groups that should be strictly spread.
        logger.debug(f"Placement group demands: {pending_placement_groups}")
        placement_group_demand_vector, strict_spreads = \
            placement_groups_to_resource_demands(pending_placement_groups)
        # Place placement groups demand vector at the beginning of the resource
        # demands vector to make it consistent (results in the same types of
        # nodes to add) with pg_demands_nodes_max_launch_limit calculated later
        resource_demands = placement_group_demand_vector + resource_demands

        if self.is_legacy_yaml() and \
                not self.node_types[NODE_TYPE_LEGACY_WORKER]["resources"]:
            # Need to launch worker nodes to later infer their
            # resources.
            # We add request_resources() demands here to make sure we launch
            # a single worker sometimes even if min_workers = 0 and resource
            # demands is empty.
            if ensure_min_cluster_size:
                request_resources_demands = ensure_min_cluster_size
            else:
                request_resources_demands = []
            return self._legacy_worker_node_to_launch(
                nodes, launching_nodes, node_resources,
                resource_demands + request_resources_demands), []

        spread_pg_nodes_to_add, node_resources, node_type_counts = \
            self.reserve_and_allocate_spread(
                strict_spreads, node_resources, node_type_counts)

        # Calculate the nodes to add for bypassing max launch limit for
        # placement groups and spreads.
        unfulfilled_placement_groups_demands, _ = get_bin_pack_residual(
            node_resources, placement_group_demand_vector)
        # Add 1 to account for the head node.
        max_to_add = self.max_workers + 1 - sum(node_type_counts.values())
        pg_demands_nodes_max_launch_limit, _ = get_nodes_for(
            self.node_types, node_type_counts, self.head_node_type, max_to_add,
            unfulfilled_placement_groups_demands)
        placement_groups_nodes_max_limit = {
            node_type: spread_pg_nodes_to_add.get(node_type, 0) +
            pg_demands_nodes_max_launch_limit.get(node_type, 0)
            for node_type in self.node_types
        }

        # Step 4/5: add nodes for pending tasks, actors, and non-strict spread
        # groups
        unfulfilled, _ = get_bin_pack_residual(node_resources,
                                               resource_demands)
        logger.debug("Resource demands: {}".format(resource_demands))
        logger.debug("Unfulfilled demands: {}".format(unfulfilled))
        nodes_to_add_based_on_demand, final_unfulfilled = get_nodes_for(
            self.node_types, node_type_counts, self.head_node_type, max_to_add,
            unfulfilled)
        logger.debug("Final unfulfilled: {}".format(final_unfulfilled))
        # Merge nodes to add based on demand and nodes to add based on
        # min_workers constraint. We add them because nodes to add based on
        # demand was calculated after the min_workers constraint was respected.
        total_nodes_to_add = {}

        for node_type in self.node_types:
            nodes_to_add = (adjusted_min_workers.get(
                node_type, 0) + spread_pg_nodes_to_add.get(node_type, 0) +
                            nodes_to_add_based_on_demand.get(node_type, 0))
            if nodes_to_add > 0:
                total_nodes_to_add[node_type] = nodes_to_add

        # Limit the number of concurrent launches
        total_nodes_to_add = self._get_concurrent_resource_demand_to_launch(
            total_nodes_to_add, unused_resources_by_ip.keys(), nodes,
            launching_nodes, adjusted_min_workers,
            placement_groups_nodes_max_limit)

        logger.debug("Node requests: {}".format(total_nodes_to_add))
        return total_nodes_to_add, final_unfulfilled
def _get_concurrent_resource_demand_to_launch(
            self,
            to_launch: Dict[NodeType, int],
            connected_nodes: List[NodeIP],
            non_terminated_nodes: List[NodeID],
            pending_launches_nodes: Dict[NodeType, int],
            adjusted_min_workers: Dict[NodeType, int],
    ) -> Dict[NodeType, int]:
        
        
        updated_nodes_to_launch = {}
        running_nodes, pending_nodes = \
            self._separate_running_and_pending_nodes(
                non_terminated_nodes, connected_nodes,
            )
        for node_type in to_launch:
            # Enforce here max allowed pending nodes to be frac of total
            # running nodes.
            max_allowed_pending_nodes = max(
                UPSCALING_INITIAL_NUM_NODES,
                int(self.upscaling_speed * running_nodes[node_type]))
            total_pending_nodes = pending_launches_nodes.get(
                node_type, 0) + pending_nodes[node_type]

            upper_bound = max(
                max_allowed_pending_nodes - total_pending_nodes,

                # Allow more nodes if this is to respect min_workers or
                # request_resources().
                adjusted_min_workers.get(node_type, 0))

            if upper_bound > 0:
                updated_nodes_to_launch[node_type] = min(
                    upper_bound, to_launch[node_type])

        return updated_nodes_to_launch
def deserialize(cls, info: TypeInfo, data: JsonDict, api: SemanticAnalyzerPluginInterface) -> PydanticModelField:
        
        data = data.copy()
        typ = deserialize_and_fixup_type(data.pop('type'), api)
        return cls(type=typ, info=info, **data)
def __init__(self,
               bifpn_num_iterations,
               bifpn_num_filters,
               fpn_min_level,
               fpn_max_level,
               input_max_level,
               is_training,
               conv_hyperparams,
               freeze_batchnorm,
               bifpn_node_params=None,
               use_native_resize_op=False,
               name=None):
    
    
    super(KerasBiFpnFeatureMaps, self).__init__(name=name)
    bifpn_node_config = _create_bifpn_node_config(
        bifpn_num_iterations,
        bifpn_num_filters,
        fpn_min_level,
        fpn_max_level,
        input_max_level,
        bifpn_node_params,
        use_native_resize_op=use_native_resize_op)
    bifpn_input_config = _create_bifpn_input_config(fpn_min_level,
                                                    fpn_max_level,
                                                    input_max_level)
    bifpn_output_node_names = _get_bifpn_output_node_names(
        fpn_min_level, fpn_max_level, bifpn_node_config)

    self.bifpn_node_config = bifpn_node_config
    self.bifpn_output_node_names = bifpn_output_node_names
    self.node_input_blocks = []
    self.node_combine_op = []
    self.node_post_combine_block = []

    all_node_params = bifpn_input_config
    all_node_names = [node['name'] for node in all_node_params]
    for node_config in bifpn_node_config:
      # Maybe transform and/or resample input feature maps.
      input_blocks = []
      for input_name in node_config['inputs']:
        if input_name not in all_node_names:
          raise ValueError(
              'Input feature map ({}) does not exist:'.format(input_name))
        input_index = all_node_names.index(input_name)
        input_params = all_node_params[input_index]
        input_block = node_config['input_op'](
            name='{}/input_{}/'.format(node_config['name'], input_name),
            input_scale=input_params['scale'],
            input_num_channels=input_params.get('num_channels', None),
            output_scale=node_config['scale'],
            output_num_channels=node_config['num_channels'],
            conv_hyperparams=conv_hyperparams,
            is_training=is_training,
            freeze_batchnorm=freeze_batchnorm)
        input_blocks.append((input_index, input_block))

      # Combine input feature maps.
      combine_op = _create_bifpn_combine_op(
          num_inputs=len(input_blocks),
          name=(node_config['name'] + '/combine'),
          combine_method=node_config['combine_method'])

      # Post-combine layers.
      post_combine_block = []
      if node_config['post_combine_op']:
        post_combine_block.extend(node_config['post_combine_op'](
            name=node_config['name'] + '/post_combine/',
            conv_hyperparams=conv_hyperparams,
            is_training=is_training,
            freeze_batchnorm=freeze_batchnorm))

      self.node_input_blocks.append(input_blocks)
      self.node_combine_op.append(combine_op)
      self.node_post_combine_block.append(post_combine_block)
      all_node_params.append(node_config)
      all_node_names.append(node_config['name'])
def publish_sqs_metric(
    account_id: str,
    region: str,
    queue_name: str,
    metric: str,
    value: float = 1,
    unit: str = "Count",
):
    
    
    
    cw_client = connect_to(region_name=region, aws_access_key_id=account_id).cloudwatch
    try:
        cw_client.put_metric_data(
            Namespace="AWS/SQS",
            MetricData=[
                {
                    "MetricName": metric,
                    "Dimensions": [{"Name": "QueueName", "Value": queue_name}],
                    "Unit": unit,
                    "Timestamp": datetime.utcnow().replace(tzinfo=timezone.utc),
                    "Value": value,
                }
            ],
        )
    except Exception as e:
        LOG.info(f'Unable to put metric data for metric "{metric}" to CloudWatch: {e}')
def _convert_lambda_function_resource(name, resource_properties, layers):  # pylint: disable=invalid-name
        
        
        

        # CodeUri is set to "." in order to get code locally from current directory. AWS::Lambda::Function's ``Code``
        # property does not support specifying a local path
        codeuri = SamFunctionProvider._extract_lambda_function_code(resource_properties, "Code")

        LOG.debug("Found Lambda function with name='%s' and CodeUri='%s'", name, codeuri)

        return SamFunctionProvider._build_function_configuration(name, codeuri, resource_properties, layers)
def _extract_functions(
        stacks: List[Stack],
        use_raw_codeuri: bool = False,
        ignore_code_extraction_warnings: bool = False,
        locate_layer_nested: bool = False,
    ) -> Dict[str, Function]:
        
        
        

        result: Dict[str, Function] = {}  # a dict with full_path as key and extracted function as value
        for stack in stacks:
            for name, resource in stack.resources.items():

                resource_type = resource.get("Type")
                resource_properties = resource.get("Properties", {})
                resource_metadata = resource.get("Metadata", None)
                # Add extra metadata information to properties under a separate field.
                if resource_metadata:
                    resource_properties["Metadata"] = resource_metadata

                if resource_type in [AWS_SERVERLESS_FUNCTION, AWS_LAMBDA_FUNCTION]:
                    resource_package_type = resource_properties.get("PackageType", ZIP)

                    code_property_key = SamBaseProvider.CODE_PROPERTY_KEYS[resource_type]
                    image_property_key = SamBaseProvider.IMAGE_PROPERTY_KEYS[resource_type]

                    if resource_package_type == ZIP and SamBaseProvider._is_s3_location(
                        resource_properties.get(code_property_key)
                    ):

                        # CodeUri can be a dictionary of S3 Bucket/Key or a S3 URI, neither of which are supported
                        if not ignore_code_extraction_warnings:
                            SamFunctionProvider._warn_code_extraction(resource_type, name, code_property_key)
                        continue

                    if (
                        resource_package_type == IMAGE
                        and SamBaseProvider._is_ecr_uri(resource_properties.get(image_property_key))
                        and not SamFunctionProvider._metadata_has_necessary_entries_for_image_function_to_be_built(
                            resource_metadata
                        )
                    ):
                        # ImageUri can be an ECR uri, which is not supported
                        if not ignore_code_extraction_warnings:
                            SamFunctionProvider._warn_imageuri_extraction(resource_type, name, image_property_key)
                        continue

                if resource_type == AWS_SERVERLESS_FUNCTION:
                    layers = SamFunctionProvider._parse_layer_info(
                        stack,
                        resource_properties.get("Layers", []),
                        use_raw_codeuri,
                        ignore_code_extraction_warnings=ignore_code_extraction_warnings,
                        locate_layer_nested=locate_layer_nested,
                        stacks=stacks if locate_layer_nested else None,
                        function_id=resource_metadata.get("SamResourceId", "") if locate_layer_nested else None,
                    )
                    function = SamFunctionProvider._convert_sam_function_resource(
                        stack,
                        name,
                        resource_properties,
                        layers,
                        use_raw_codeuri,
                    )
                    result[function.full_path] = function

                elif resource_type == AWS_LAMBDA_FUNCTION:
                    layers = SamFunctionProvider._parse_layer_info(
                        stack,
                        resource_properties.get("Layers", []),
                        use_raw_codeuri,
                        ignore_code_extraction_warnings=ignore_code_extraction_warnings,
                        locate_layer_nested=locate_layer_nested,
                        stacks=stacks if locate_layer_nested else None,
                        function_id=resource_metadata.get("SamResourceId", "") if locate_layer_nested else None,
                    )
                    function = SamFunctionProvider._convert_lambda_function_resource(
                        stack, name, resource_properties, layers, use_raw_codeuri
                    )
                    result[function.full_path] = function

                # We don't care about other resource types. Just ignore them

        return result
def _build_function_configuration(
        stack: Stack,
        name: str,
        codeuri: Optional[str],
        resource_properties: Dict,
        layers: List,
        inlinecode: Optional[str],
        imageuri: Optional[str],
        use_raw_codeuri: bool = False,
    ) -> Function:
        
        
        
        metadata = resource_properties.get("Metadata", None)
        if metadata and "DockerContext" in metadata and not use_raw_codeuri:
            LOG.debug(
                "--base-dir is presented not, adjusting uri %s relative to %s",
                metadata["DockerContext"],
                stack.location,
            )
            metadata["DockerContext"] = SamLocalStackProvider.normalize_resource_path(
                stack.location, metadata["DockerContext"]
            )

        if codeuri and not use_raw_codeuri:
            LOG.debug("--base-dir is presented not, adjusting uri %s relative to %s", codeuri, stack.location)
            codeuri = SamLocalStackProvider.normalize_resource_path(stack.location, codeuri)

        return Function(
            stack_path=stack.stack_path,
            name=name,
            functionname=resource_properties.get("FunctionName", name),
            packagetype=resource_properties.get("PackageType", ZIP),
            runtime=resource_properties.get("Runtime"),
            memory=resource_properties.get("MemorySize"),
            timeout=resource_properties.get("Timeout"),
            handler=resource_properties.get("Handler"),
            codeuri=codeuri,
            imageuri=imageuri if imageuri else resource_properties.get("ImageUri"),
            imageconfig=resource_properties.get("ImageConfig"),
            environment=resource_properties.get("Environment"),
            rolearn=resource_properties.get("Role"),
            events=resource_properties.get("Events"),
            layers=layers,
            metadata=metadata,
            inlinecode=inlinecode,
            codesign_config_arn=resource_properties.get("CodeSigningConfigArn", None),
        )
def set_mixed_precision_policy(dtype, loss_scale=None):
  
  # TODO(b/191894773): Remove loss_scale argument
  assert loss_scale is None, (
      'The loss_scale argument must be None. The argument exists for '
      'historical reasons and will be removed soon.')
  if dtype == tf.float16:
    tf_keras.mixed_precision.set_global_policy('mixed_float16')
  elif dtype == tf.bfloat16:
    tf_keras.mixed_precision.set_global_policy('mixed_bfloat16')
  elif dtype == tf.float32:
    tf_keras.mixed_precision.set_global_policy('float32')
  else:
    raise ValueError('Unexpected dtype: %s' % dtype)
def condense(self, llm: LLM):
        
        
        

        try:
            prompt = prompts.get_summarize_monologue_prompt(self.thoughts)
            messages = [{'content': prompt, 'role': 'user'}]
            resp = llm.completion(messages=messages)
            summary_resp = resp['choices'][0]['message']['content']
            self.thoughts = prompts.parse_summary_response(summary_resp)
        except Exception as e:
            logger.error('Error condensing thoughts: %s', str(e), exc_info=False)

            # TODO If the llm fails with ContextWindowExceededError, we can try to condense the monologue chunk by chunk
            raise
def add_event(self, t: dict):
        
        
        
        if not isinstance(t, dict):
            raise AgentEventTypeError()
        self.thoughts.append(t)
def update(self):
        
        self.smartplug.update()
        self.smartplug.update_energy()
def create_backward_schedule_func(self, out_vars):
        
        
        assert isinstance(out_vars, tuple)
        if self._backward_schedule_func is None:
            self._backward_schedule_func = StaticScheduleFunction(self._schedule_manager, out_vars, is_forward=False)
        else:
            raise RuntimeError("Cannot create. A backward schedule already exists.")
        # Now follow the backward references for each variable in out_vars to find out which
        # function created it and also find the corresponding index of the variable in the
        # output tuple of that function. This information will then be used during the first
        # backward pass. Specifically, each gradients variable that corresponds to an input
        # argument of static_chain.backward() will need to have its `data` attribute first
        # replaced by a corresponding static array before it is used by the backward-pass
        # functions. To accomplish this, we will mark each creator function of a variable in
        # `out_vars` with an attribute that contains the needed information.
        self._creator_funcs_to_backward_static_arrays = dict()
        for chain_arg_index in range(len(out_vars)):
            var = out_vars[chain_arg_index]
            creator_func = var.creator
            #print('Creator function of current variable: ', creator_func)
            if creator_func is None:
                print("Warning: Trying to create backward schedule but creator function of output variable of static chain is None!")
            else:
                # map each positional index of the variable in the function's output tuple
                # to its corresponding static array.

                # Now find the positional index of var in the creator function's output tuple.
                var_id = id(var.node)
                for func_arg_index in range(len(creator_func.outputs)):
                    creator_output_id = id(creator_func.outputs[func_arg_index]())
                    if var_id == creator_output_id:
                        # Found the variable.
                        # During the backward pass, we will need to copy the
                        # `data` array from the func_arg_index position in the
                        # tuple of input gradients variables to the
                        # chain_arg_index'th static input array of the
                        # backward schedule.
                        # (func_arg_index, chain_arg_index)
                        backward_static_arrays_info = getattr(creator_func, '_backward_static_arrays_info', None)
                        if backward_static_arrays_info is None:
                            backward_static_arrays_info = list()
                            creator_func._backward_static_arrays_info = backward_static_arrays_info
                        backward_static_arrays_info.append((func_arg_index, chain_arg_index))

        return self._backward_schedule_func
def end_forward(self):
        

        
        if not self._end_forward:
            for key in self.in_use_count:
                self.in_use_count[key] = 0
            self._end_forward = True

            if self._train_count > self._max_in_use_train:
                self._max_in_use_train = self._train_count
                if self._verbosity_level >= 2:
                    print("Maximum in-use schedules per training iteration: ",
                          self._max_in_use_train)
            self._train_count = 0
def get_schedule(self, in_vars, enable_double_backprop=False):
        
        
        if self._end_forward:
            self._end_forward = False
        if self._minimize_cache_size:
            if chainer.config.train != self._prev_train_config:
                # Training config changed, so clear caches.
                self._prev_train_config = chainer.config.train
                if self._verbosity_level >= 2:
                    print("Clearing schedule cache...")
                self.schedules.clear()
                self.in_use_count.clear()
            # todo (vogel): Also check if minibatch size has changed and clear schedules.

        if chainer.config.train is False or chainer.config.enable_backprop is False:
            key_str = 'test:' + ''.join(str(x.shape) for x in in_vars)
            # If the maximum number of in-use schedules in any iteration
            # during training mode was exactly 1, assume it should also
            # be 1 for test mode.

            if key_str in self.schedules:
                sched_list = self.schedules[key_str]
                sched = sched_list[0]
            else:
                sched = StaticScheduleFunction(self,
                                               verbosity_level=self._verbosity_level,
                                               enable_double_backprop=enable_double_backprop)
                self.schedules[key_str] = [sched]
            return sched

        else:
            key_str = 'train:' + ''.join(str(x.shape) for x in in_vars)
            #print("key: \n", key_str)
            self._train_count += 1

            if key_str in self.schedules:
                sched_list = self.schedules[key_str]
                available_index = self.in_use_count[key_str]
                if available_index >= len(sched_list):
                    sched = StaticScheduleFunction(self,
                                                   verbosity_level=self._verbosity_level,
                                                   enable_double_backprop=enable_double_backprop)
                    sched_list.append(sched)

                sched = sched_list[available_index]
                self.in_use_count[key_str] = available_index + 1
            else:
                sched = StaticScheduleFunction(self,
                                               enable_double_backprop=enable_double_backprop)
                self.schedules[key_str] = [sched]
                self.in_use_count[key_str] = 1

        return sched
def create_out_arrays(self, out_vars, use_grad=False):
        
        
        self._out_vars = out_vars # fixme: not used? remove it?
        if use_grad:
            self._out_arrays = [y.grad for y in out_vars]
        else:
            self._out_arrays = [y.data for y in out_vars]
def _run_param_hooks(self):
        

        
        for hook in self.param_hooks:
            #print('running parameter hook: ', hook)
            (unique_array_index, param_attribute_location) = hook
            (params_list_index, attribute_location) = param_attribute_location
            param = self.params_list[params_list_index]
            schedule_grad_var = self.grad_var_list[params_list_index]
            if schedule_grad_var is not None:
                if param.grad_var is None:
                    if self.verbosity_level >= 2:
                        print('Somebody removed grad_var.')
                    if schedule_grad_var.data is not None:
                        if param.data.dtype != schedule_grad_var.data.dtype:
                            raise RuntimeError('It is not allowed to change the parameter dtype in a static chain!')
                        param.grad_var = schedule_grad_var

            if attribute_location == 'data':
                # This is the corresponding parameter array, which might
                # have had its reference changed to a different array or set
                # to None.
                possibly_modified_parameter_array = \
                    self.params_list[params_list_index].data
                if self.unique_arrays[unique_array_index] is not possibly_modified_parameter_array:
                    if self.verbosity_level >= 2:
                        print('The parameter data attribute has changed: ', self.params_list[params_list_index])
                    if self.unique_arrays[unique_array_index] is not None and possibly_modified_parameter_array is not None:
                        # Set the schedule's array to refer to the 'data'
                        # attribute of the parameter.
                        if self.verbosity_level >= 2:
                            print('Setting schedule array equal to data attribute reference.')
                        if self.unique_arrays[unique_array_index].dtype != possibly_modified_parameter_array.dtype:
                            raise RuntimeError('It is not allowed to change the parameter dtype in a static chain!')
                        self.unique_arrays[unique_array_index] = possibly_modified_parameter_array
                    elif self.unique_arrays[unique_array_index] is not None and possibly_modified_parameter_array is None:
                        # The data attribute was set to None by outside code.
                        if self.verbosity_level >= 2:
                            print('Zero-ing and updating parameter data attribute reference.')
                        self.params_list[params_list_index].data = self.unique_arrays[unique_array_index]
            elif attribute_location == 'grad':
                # This is the corresponding parameter array, which might
                # have had its reference changed to a different array or set
                # to None.
                possibly_modified_parameter_array = \
                    self.params_list[params_list_index].grad
                if self.unique_arrays[unique_array_index] is not possibly_modified_parameter_array:
                    if self.verbosity_level >= 2:
                        print('The parameter grad attribute has changed: ', self.params_list[params_list_index])
                    if self.unique_arrays[unique_array_index] is not None and possibly_modified_parameter_array is not None:
                        # Set the schedule's array to refer to the 'grad'
                        # attribute of the parameter.
                        if self.verbosity_level >= 2:
                            print('Setting schedule array equal to grad attribute reference.')
                        if self.unique_arrays[unique_array_index].dtype != possibly_modified_parameter_array:
                            raise RuntimeError('It is not allowed to change the parameter dtype in a static chain!')
                        self.unique_arrays[unique_array_index] = possibly_modified_parameter_array
                    elif self.unique_arrays[unique_array_index] is not None and possibly_modified_parameter_array is None:
                        # The grad attribute was set to None by outside code.
                        if self.verbosity_level >= 2:
                            print('Zero-ing and updating parameter grad attribute reference.')
                        self.params_list[params_list_index].grad = self.unique_arrays[unique_array_index]
def __init__(self, func, args, kwargs, hooks, unique_arrays,
                 func_name=None):
        
        
        self.func = func
        self.args = args
        self.kwargs = kwargs
        self.hooks = hooks
        self.unique_arrays = unique_arrays
        self.func_name = func_name
def append_function(self, func, args, kwargs, func_name=None,
                        return_arrays=None):
        

        

        inputs_hooks = []
        if 'inputs' in kwargs:
            in_list = kwargs['inputs']
            assert isinstance(in_list, list)
            for ind, x in enumerate(in_list):
                if _is_xp(x):
                    if id(x) not in self.array_id_to_unique_index:
                        self.unique_arrays.append(x)
                        unique_ind = len(self.unique_arrays) - 1
                        self.array_id_to_unique_index[id(x)] = unique_ind
                    else:
                        unique_ind = self.array_id_to_unique_index[id(x)]
                    inputs_hooks.append((ind, unique_ind))
                    # Now that the hook has been added, we can delete
                    # array reference from 'args'. This is safe because
                    # unique_arrays contians the master reference.
                    in_list[ind] = None

        outputs_hooks = []
        if 'outputs' in kwargs:
            out_list = kwargs['outputs']
            assert isinstance(out_list, list)
            for ind, x in enumerate(out_list):
                if _is_xp(x):
                    if id(x) not in self.array_id_to_unique_index:
                        self.unique_arrays.append(x)
                        unique_ind = len(self.unique_arrays) - 1
                        self.array_id_to_unique_index[id(x)] = unique_ind
                    else:
                        unique_ind = self.array_id_to_unique_index[id(x)]
                    outputs_hooks.append((ind, unique_ind))
                    # Now that the hook has been added, we can delete
                    # array reference from 'args'. This is safe because
                    # unique_arrays contians the master reference.
                    out_list[ind] = None

        # A list of hooks (each is a tuple) that will be used to set
        # correct array references in 'unique_arrays' after executing
        # the static schedule function 'func'. These hooks update
        # the references in 'unique_arrays' to refer to the arrays
        # that were dynamically allocated in the return value of
        # 'func'.
        return_hooks = []
        if return_arrays is not None:
            assert (isinstance(return_arrays, list) or
                    isinstance(return_arrays, tuple))
            for ret_index, item in enumerate(return_arrays):
                if _is_xp(item):
                    if id(item) not in self.array_id_to_unique_index:
                        # todo: consider only appending a weak reference so
                        # that
                        # we can know when it is no longer needed and add a
                        # delete hook at that time.
                        self.unique_arrays.append(item)
                        unique_index = len(self.unique_arrays) - 1
                        self.array_id_to_unique_index[id(item)] = \
                            unique_index
                    else:
                        # Since all of the return arrays are suppoed to
                        # have been dynamically allocated inside 'func',
                        # they had better not already be in unique_arrays.
                        # If so, it is an error.
                        raise RuntimeError('Found result array from schedule '
                                           'function already in '
                                           'unique_arrays!')
                    return_hooks.append((ret_index, unique_index))
                    self.dynamically_allocated_unique_index.add(unique_index)

        if self.verbosity_level >= 2:
            print('Adding function to static schedule: ', func)

        self.schedule_info_list.append(ScheduleInfo(func, args, kwargs,
                                                    inputs_hooks,
                                                    outputs_hooks,
                                                    return_hooks,
                                                    self.unique_arrays,
                                                    func_name=func_name))
def repo_list(recipe_folder="tests/recipes", field="HF_repo"):
    
    
    HF_repos = []

    # Loop over all recipe CSVs
    for recipe_csvfile in os.listdir(recipe_folder):
        with open(
            os.path.join(recipe_folder, recipe_csvfile), newline=""
        ) as csvf:
            reader = csv.DictReader(csvf, delimiter=",", skipinitialspace=True)
            for row in reader:
                if len(row[field]) > 0:
                    repos = row[field].split(" ")
                    for repo in repos:
                        HF_repos.append(repo)
    HF_repos = set(HF_repos)
    return HF_repos
def sample(
        self, mask: Optional[Tuple[Optional[int], Optional[np.ndarray]]] = None
    ) -> str:
        
        
        if mask is not None:
            assert isinstance(
                mask, tuple
            ), f"Expects the mask type to be a tuple, actual type: {type(mask)}"
            assert (
                len(mask) == 2
            ), f"Expects the mask length to be two, actual length: {len(mask)}"
            length, charlist_mask = mask

            if length is not None:
                assert np.issubdtype(
                    type(length), np.integer
                ), f"Expects the Text sample length to be an integer, actual type: {type(length)}"
                assert (
                    self.min_length <= length <= self.max_length
                ), f"Expects the Text sample length be between {self.min_length} and {self.max_length}, actual length: {length}"

            if charlist_mask is not None:
                assert isinstance(
                    charlist_mask, np.ndarray
                ), f"Expects the Text sample mask to be an np.ndarray, actual type: {type(charlist_mask)}"
                assert (
                    charlist_mask.dtype == np.int8
                ), f"Expects the Text sample mask to be an np.ndarray, actual dtype: {charlist_mask.dtype}"
                assert charlist_mask.shape == (
                    len(self.character_set),
                ), f"expects the Text sample mask to be {(len(self.character_set),)}, actual shape: {charlist_mask.shape}"
                assert np.all(
                    np.logical_or(charlist_mask == 0, charlist_mask == 1)
                ), f"Expects all masks values to 0 or 1, actual values: {charlist_mask}"
        else:
            length, charlist_mask = None, None

        if length is None:
            length = self.np_random.integers(self.min_length, self.max_length + 1)

        if charlist_mask is None:
            string = self.np_random.choice(self.character_list, size=length)
        else:
            valid_mask = charlist_mask == 1
            valid_indexes = np.where(valid_mask)[0]
            if len(valid_indexes) == 0:
                if self.min_length == 0:
                    string = ""
                else:
                    # Otherwise the string will not be contained in the space
                    raise ValueError(
                        f"Trying to sample with a minimum length > 0 ({self.min_length}) but the character mask is all zero meaning that no character could be sampled."
                    )
            else:
                string = "".join(
                    self.character_list[index]
                    for index in self.np_random.choice(valid_indexes, size=length)
                )

        return "".join(string)
def _parse_source_state(self, state):
        
        
        self._state = state.state
        self._source = state.entity_id
        self._latitude = state.attributes.get(ATTR_LATITUDE)
        self._longitude = state.attributes.get(ATTR_LONGITUDE)
def from_yaml(cls, config: ConfigType):
        
        person = cls(config)
        person.editable = False
        return person
def metrics_as_dict(metric):
  
  
  if isinstance(metric, tf_keras.metrics.Metric):
    metrics = {metric.name: metric}
  elif isinstance(metric, list):
    metrics = {m.name: m for m in metric}
  elif isinstance(metric, dict):
    metrics = metric
  elif not metric:
    return {}
  else:
    metrics = {'metric': metric}
  return metrics
def _ncut_relabel(rag, thresh, num_cuts, random_generator):
    
    
    d, w = _ncut.DW_matrices(rag)
    m = w.shape[0]

    if m > 2:
        d2 = d.copy()
        # Since d is diagonal, we can directly operate on its data
        # the inverse of the square root
        d2.data = np.reciprocal(np.sqrt(d2.data, out=d2.data), out=d2.data)

        # Refer Shi & Malik 2001, Equation 7, Page 891
        A = d2 * (d - w) * d2
        # Initialize the vector to ensure reproducibility.
        v0 = random_generator.random(A.shape[0])
        vals, vectors = linalg.eigsh(A, which='SM', v0=v0,
                                     k=min(100, m - 2))

        # Pick second smallest eigenvector.
        # Refer Shi & Malik 2001, Section 3.2.3, Page 893
        vals, vectors = np.real(vals), np.real(vectors)
        index2 = _ncut_cy.argmin2(vals)
        ev = vectors[:, index2]

        cut_mask, mcut = get_min_ncut(ev, d, w, num_cuts)
        if (mcut < thresh):
            # Sub divide and perform N-cut again
            # Refer Shi & Malik 2001, Section 3.2.5, Page 893
            sub1, sub2 = partition_by_cut(cut_mask, rag)

            _ncut_relabel(sub1, thresh, num_cuts, random_generator)
            _ncut_relabel(sub2, thresh, num_cuts, random_generator)
            return

    # The N-cut wasn't small enough, or could not be computed.
    # The remaining graph is a region.
    # Assign `ncut label` by picking any label from the existing nodes, since
    # `labels` are unique, `new_label` is also unique.
    _label_all(rag, 'ncut label')
def __init__(self, channel):
        
        super().__init__(channel)
        self._attr_name = self._channel.get_name()
def __setattr__(self, key, val):
        
        if key == MESSAGE_ROUTE_CAUSE_BY:
            new_val = any_to_str(val)
        elif key == MESSAGE_ROUTE_FROM:
            new_val = any_to_str(val)
        elif key == MESSAGE_ROUTE_TO:
            new_val = any_to_str_set(val)
        else:
            new_val = val
        super().__setattr__(key, new_val)
def add_tasks(self, tasks: list[Task]):
        
        
        
        if not tasks:
            return

        # Topologically sort the new tasks to ensure correct dependency order
        new_tasks = self._topological_sort(tasks)

        if not self.tasks:
            # If there are no existing tasks, set the new tasks as the current tasks
            self.tasks = new_tasks

        else:
            # Find the length of the common prefix between existing and new tasks
            prefix_length = 0
            for old_task, new_task in zip(self.tasks, new_tasks):
                if old_task.task_id != new_task.task_id or old_task.instruction != new_task.instruction:
                    break
                prefix_length += 1

            # Combine the common prefix with the remainder of the new tasks
            final_tasks = self.tasks[:prefix_length] + new_tasks[prefix_length:]
            self.tasks = final_tasks

        # Update current_task_id to the first unfinished task in the merged list
        self._update_current_task()

        # Update the task map for quick access to tasks by ID
        self.task_map = {task.task_id: task for task in self.tasks}
def put(self, id, requester_user=None):
        
        

        
        return "Received data for inquiry %s" % id
def swap_row_csr(X, m, n):
    
    
    
    if m < 0:
        m += X.shape[0]
    if n < 0:
        n += X.shape[0]
    if m > n:
        m, n = n, m

    indptr = X.indptr
    m_ptr1 = indptr[m]
    m_ptr2 = indptr[m + 1]
    n_ptr1 = indptr[n]
    n_ptr2 = indptr[n + 1]
    nz_m = m_ptr2 - m_ptr1
    nz_n = n_ptr2 - n_ptr1


    if nz_m != nz_n:
        # Modify indptr first
        X.indptr[m + 2:n] += nz_n - nz_m
        X.indptr[m + 1] = X.indptr[m] + nz_n
        X.indptr[n] = X.indptr[n + 1] - nz_m

    X.indices = np.concatenate([X.indices[:m_ptr1], X.indices[n_ptr1:n_ptr2],
                                X.indices[m_ptr2:n_ptr1],
                                X.indices[m_ptr1:m_ptr2],
                                X.indices[n_ptr2:]])
    X.data = np.concatenate([X.data[:m_ptr1], X.data[n_ptr1:n_ptr2],
                             X.data[m_ptr2:n_ptr1], X.data[m_ptr1:m_ptr2],
                             X.data[n_ptr2:]])
def min_max_axis(X, axis, ignore_nan=False):
    
    
    if isinstance(X, sp.csr_matrix) or isinstance(X, sp.csc_matrix):
        if ignore_nan:
            return _sparse_nan_min_max(X, axis=axis)
        else:
            return _sparse_min_max(X, axis=axis)
    else:
        _raise_typeerror(X)
def csc_row_median(csc):
    
     

    
    from ..preprocessing.imputation import _get_median

    if not isinstance(csc, sp.csc_matrix):
        raise TypeError("Expected matrix of CSC format, got %s" % csc.format)

    indptr = csc.indptr
    n_samples, n_features = csc.shape
    median = np.zeros(n_features)

    for f_ind, ptr in enumerate(indptr[:-1]):

        # Prevent modifying csc in place
        data = np.copy(csc.data[ptr: indptr[f_ind + 1]])
        nz = n_samples - data.size
        median[f_ind] = _get_median(data, nz)

    return median
def csc_row_median(X):
     

    
    if not isinstance(X, sp.csc_matrix):
        raise TypeError("Expected matrix of CSC format, got %s" % X.format)

    indptr = X.indptr
    n_samples, n_features = X.shape
    median = np.zeros(n_features)

    for f_ind, ptr in enumerate(indptr[:-1]):

        # Prevent modifying X in place
        data = np.copy(X.data[ptr: indptr[f_ind + 1]])
        nz = n_samples - data.size
        median[f_ind] = _get_median(data, nz)

    return median
def mean_variance_axis(X, axis, weights=None, return_sum_weights=False):
    
    
    _raise_error_wrong_axis(axis)

    if isinstance(X, sp.csr_matrix):
        if axis == 0:
            return _csr_mean_var_axis0(
                X, weights=weights, return_sum_weights=return_sum_weights)
        else:
            return _csc_mean_var_axis0(
                X.T, weights=weights, return_sum_weights=return_sum_weights)
    elif isinstance(X, sp.csc_matrix):
        if axis == 0:
            return _csc_mean_var_axis0(
                X, weights=weights, return_sum_weights=return_sum_weights)
        else:
            return _csr_mean_var_axis0(
                X.T, weights=weights, return_sum_weights=return_sum_weights)
    else:
        _raise_typeerror(X)
def incr_mean_variance_axis(X, *, axis, last_mean, last_var, last_n):
    

    
    _raise_error_wrong_axis(axis)

    if not isinstance(X, (sp.csr_matrix, sp.csc_matrix)):
        _raise_typeerror(X)

    if not (np.size(last_mean) == np.size(last_var) == np.size(last_n)):
        raise ValueError(
            "last_mean, last_var, last_n do not have the same shapes."
        )

    if axis == 1:
        if np.size(last_mean) != X.shape[0]:
            raise ValueError(
                f"If axis=1, then last_mean, last_n, last_var should be of "
                f"size n_samples {X.shape[0]} (Got {np.size(last_mean)})."
            )
    else:  # axis == 0
        if np.size(last_mean) != X.shape[1]:
            raise ValueError(
                f"If axis=0, then last_mean, last_n, last_var should be of "
                f"size n_features {X.shape[1]} (Got {np.size(last_mean)})."
            )

    if isinstance(X, sp.csr_matrix):
        if axis == 0:
            return _incr_mean_var_axis0(X, last_mean=last_mean,
                                        last_var=last_var, last_n=last_n)
        else:
            return _incr_mean_var_axis0(X.T, last_mean=last_mean,
                                        last_var=last_var, last_n=last_n)
    elif isinstance(X, sp.csc_matrix):
        if axis == 0:
            return _incr_mean_var_axis0(X, last_mean=last_mean,
                                        last_var=last_var, last_n=last_n)
        else:
            return _incr_mean_var_axis0(X.T, last_mean=last_mean,
                                        last_var=last_var, last_n=last_n)
def evaluate_word_pairs(self, pairs, delimiter='\t', restrict_vocab=300000, case_insensitive=True,
                            dummy4unknown=False):
        
        
        
        ok_vocab = [(w, self.vocab[w]) for w in self.index2word[:restrict_vocab]]
        ok_vocab = dict((w.upper(), v) for w, v in reversed(ok_vocab)) if case_insensitive else dict(ok_vocab)

        similarity_gold = []
        similarity_model = []
        oov = 0

        original_vocab = self.vocab
        self.vocab = ok_vocab

        for line_no, line in enumerate(utils.smart_open(pairs)):
            line = utils.to_unicode(line)
            if line.startswith('#'):
                # May be a comment
                continue
            else:
                try:
                    if case_insensitive:
                        a, b, sim = [word.upper() for word in line.split(delimiter)]
                    else:
                        a, b, sim = [word for word in line.split(delimiter)]
                    sim = float(sim)
                except:
                    logger.info('skipping invalid line #%d in %s', line_no, pairs)
                    continue
                if a not in ok_vocab or b not in ok_vocab:
                    oov += 1
                    if dummy4unknown:
                        similarity_model.append(0.0)
                        similarity_gold.append(sim)
                        continue
                    else:
                        logger.debug('skipping line #%d with OOV words: %s', line_no, line.strip())
                        continue
                similarity_gold.append(sim)  # Similarity from the dataset
                similarity_model.append(self.similarity(a, b))  # Similarity from the model
        self.vocab = original_vocab
        spearman = stats.spearmanr(similarity_gold, similarity_model)
        pearson = stats.pearsonr(similarity_gold, similarity_model)
        oov_ratio = float(oov) / (len(similarity_gold) + oov) * 100

        logger.debug(
            'Pearson correlation coefficient against %s: %f with p-value %f',
            pairs, pearson[0], pearson[1]
        )
        logger.debug(
            'Spearman rank-order correlation coefficient against %s: %f with p-value %f',
            pairs, spearman[0], spearman[1]
        )
        logger.debug('Pairs with unknown words: %d' % oov)
        self.log_evaluate_word_pairs(pearson, spearman, oov_ratio, pairs)
        return pearson, spearman, oov_ratio
def __contains__(self, word):
        

        
        return True
def evaluate_word_analogies(
            self, analogies, restrict_vocab=300000, case_insensitive=True,
            dummy4unknown=False, similarity_function='most_similar'):
        

        
        ok_keys = self.index_to_key[:restrict_vocab]
        if case_insensitive:
            ok_vocab = {k.upper(): self.get_index(k) for k in reversed(ok_keys)}
        else:
            ok_vocab = {k: self.get_index(k) for k in reversed(ok_keys)}
        oov = 0
        logger.info("Evaluating word analogies for top %i words in the model on %s", restrict_vocab, analogies)
        sections, section = [], None
        quadruplets_no = 0
        with utils.open(analogies, 'rb') as fin:
            for line_no, line in enumerate(fin):
                line = utils.to_unicode(line)
                if line.startswith(': '):
                    # a new section starts => store the old section
                    if section:
                        sections.append(section)
                        self._log_evaluate_word_analogies(section)
                    section = {'section': line.lstrip(': ').strip(), 'correct': [], 'incorrect': []}
                else:
                    if not section:
                        raise ValueError("Missing section header before line #%i in %s" % (line_no, analogies))
                    try:
                        if case_insensitive:
                            a, b, c, expected = [word.upper() for word in line.split()]
                        else:
                            a, b, c, expected = [word for word in line.split()]
                    except ValueError:
                        logger.info("Skipping invalid line #%i in %s", line_no, analogies)
                        continue
                    quadruplets_no += 1
                    if a not in ok_vocab or b not in ok_vocab or c not in ok_vocab or expected not in ok_vocab:
                        oov += 1
                        if dummy4unknown:
                            logger.debug('Zero accuracy for line #%d with OOV words: %s', line_no, line.strip())
                            section['incorrect'].append((a, b, c, expected))
                        else:
                            logger.debug("Skipping line #%i with OOV words: %s", line_no, line.strip())
                        continue
                    original_key_to_index = self.key_to_index
                    self.key_to_index = ok_vocab
                    ignore = {a, b, c}  # input words to be ignored
                    predicted = None
                    # find the most likely prediction using 3CosAdd (vector offset) method
                    # TODO: implement 3CosMul and set-based methods for solving analogies

                    sims = self.most_similar(positive=[b, c], negative=[a], topn=5, restrict_vocab=restrict_vocab)
                    self.key_to_index = original_key_to_index
                    for element in sims:
                        predicted = element[0].upper() if case_insensitive else element[0]
                        if predicted in ok_vocab and predicted not in ignore:
                            if predicted != expected:
                                logger.debug("%s: expected %s, predicted %s", line.strip(), expected, predicted)
                            break
                    if predicted == expected:
                        section['correct'].append((a, b, c, expected))
                    else:
                        section['incorrect'].append((a, b, c, expected))
        if section:
            # store the last section, too
            sections.append(section)
            self._log_evaluate_word_analogies(section)

        total = {
            'section': 'Total accuracy',
            'correct': list(itertools.chain.from_iterable(s['correct'] for s in sections)),
            'incorrect': list(itertools.chain.from_iterable(s['incorrect'] for s in sections)),
        }

        oov_ratio = float(oov) / quadruplets_no * 100
        logger.info('Quadruplets with out-of-vocabulary words: %.1f%%', oov_ratio)
        if not dummy4unknown:
            logger.info(
                'NB: analogies containing OOV words were skipped from evaluation! '
                'To change this behavior, use "dummy4unknown=True"'
            )
        analogies_score = self._log_evaluate_word_analogies(total)
        sections.append(total)
        # Return the overall score and the full lists of correct and incorrect analogies
        return analogies_score, sections
def similarity_matrix(self, dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100, dtype=REAL):
        

        
        index = WordEmbeddingSimilarityIndex(self, threshold=threshold, exponent=exponent)
        similarity_matrix = SparseTermSimilarityMatrix(
            index, dictionary, tfidf=tfidf, nonzero_limit=nonzero_limit, dtype=dtype)
        return similarity_matrix.matrix
def get_vector(self, key, norm=False):
        

        
        index = self.get_index(key)
        if norm:
            self.fill_norms()
            result = self.vectors[index] / self.norms[index]
        else:
            result = self.vectors[index]

        result.setflags(write=False)  # disallow direct tampering that would invalidate `norms` etc
        return result
def _log_evaluate_word_analogies(section):
        

        
        correct, incorrect = len(section['correct']), len(section['incorrect'])

        if correct + incorrect == 0:
            return 0.0

        score = correct / (correct + incorrect)
        logger.info("%s: %.1f%% (%i/%i)", section['section'], 100.0 * score, correct, correct + incorrect)
        return score
def resize_vectors(self, seed=0):
        

        target_shape = (len(self.index_to_key), self.vector_size)
        self.vectors = prep_vectors(target_shape, prior_vectors=self.vectors, seed=seed)
        # TODO: support memmap?
#        if hasattr(self, 'mapfile_path') and self.mapfile_path:
#            self.vectors = np.memmap(self.mapfile_path, shape=(target_count, self.vector_size), mode='w+', dtype=REAL)

        self.allocate_vecattrs()
        self.norms = None
def transform_data(
        query: IntrinioKeyMetricsQueryParams,
        data: List[Dict],
        **kwargs: Any,
    ) -> List[IntrinioKeyMetricsData]:
        

        # Sort the results by the order of the symbols in the query.
        symbols = query.symbol.split(",")
        data = sorted(
            data,
            key=(
                lambda item: (
                    symbols.index(item["symbol"])
                    if item["symbol"] in symbols
                    else len(symbols)
                )
            ),
        )

        results: List[IntrinioKeyMetricsData] = []
        for item in data:

            if item.get("marketcap") is None or isinstance(item.get("marketcap"), dict):
                warn(f"Symbol Error: No data found for {item.get('symbol')}")
                continue

            for key, value in item.copy().items():
                # A bad response in a field will return a dict here, so we remove it.
                if isinstance(value, dict):
                    _ = item.pop(key)

            results.append(IntrinioKeyMetricsData.model_validate(item))

        return results
def main(input_dir, no_input, checkout, verbose):
    
    if verbose:
        logging.basicConfig(
            format='%(levelname)s %(filename)s: %(message)s',
            level=logging.DEBUG
        )
    else:
        # Log info and above to console
        logging.basicConfig(
            format='%(levelname)s: %(message)s',
            level=logging.INFO
        )

    cookiecutter(input_dir, checkout, no_input)
def main(template, no_input, checkout, verbose):
    
    if verbose:
        logging.basicConfig(
            format='%(levelname)s %(filename)s: %(message)s',
            level=logging.DEBUG
        )
    else:
        # Log info and above to console
        logging.basicConfig(
            format='%(levelname)s: %(message)s',
            level=logging.INFO
        )

    cookiecutter(template, checkout, no_input)
def check_arrays(*arrays, **options):
    
    
    sparse_format = options.pop('sparse_format', None)
    if sparse_format not in (None, 'csr', 'csc', 'dense'):
        raise ValueError('Unexpected sparse format: %r' % sparse_format)
    copy = options.pop('copy', False)
    check_ccontiguous = options.pop('check_ccontiguous', False)
    dtype = options.pop('dtype', None)
    force_arrays = options.pop('force_arrays', True)
    allow_nans = options.pop('allow_nans', False)
    allow_nd = options.pop('allow_nd', False)

    if options:
        raise TypeError("Unexpected keyword arguments: %r" % options.keys())

    if len(arrays) == 0:
        return None

    n_samples = _num_samples(arrays[0])

    checked_arrays = []
    for array in arrays:
        array_orig = array
        if array is None:
            # special case: ignore optional y=None kwarg pattern
            checked_arrays.append(array)
            continue
        size = _num_samples(array)

        if size != n_samples:
            raise ValueError("Found array with dim %d. Expected %d"
                             % (size, n_samples))

        if (force_arrays or hasattr(array, "__array__")
                or hasattr(array, "shape")):
            if sp.issparse(array):
                if sparse_format == 'csr':
                    array = array.tocsr()
                elif sparse_format == 'csc':
                    array = array.tocsc()
                elif sparse_format == 'dense':
                    raise TypeError('A sparse matrix was passed, but dense '
                                    'data is required. Use X.toarray() to '
                                    'convert to a dense numpy array.')
                if check_ccontiguous:
                    array.data = np.ascontiguousarray(array.data, dtype=dtype)
                elif hasattr(array, 'data'):
                    array.data = np.asarray(array.data, dtype=dtype)
                elif array.dtype != dtype:
                    # Cast on the required dtype
                    array = array.astype(dtype)
                if not allow_nans:
                    if hasattr(array, 'data'):
                        _assert_all_finite(array.data)
                    else:
                        # DOK sparse matrices
                        _assert_all_finite(array.values())
            else:
                if check_ccontiguous:
                    array = np.ascontiguousarray(array, dtype=dtype)
                elif dtype is not None or force_arrays:
                    array = np.asarray(array, dtype=dtype)
                if not allow_nans:
                    _assert_all_finite(array)

            if force_arrays and not allow_nd and array.ndim >= 3:
                raise ValueError("Found array with dim %d. Expected <= 2" %
                                 array.ndim)

        if copy and array is array_orig:
            array = array.copy()
        checked_arrays.append(array)

    return checked_arrays
def as_float_array(X, copy=True):
    
    
    if isinstance(X, np.matrix) or (not isinstance(X, np.ndarray)
                                    and not sp.issparse(X)):
        return safe_asarray(X, dtype=np.float64)
    elif X.dtype in [np.float32, np.float64]:
        return X.copy() if copy else X
    else:
        return X.astype(np.float32 if X.dtype == np.int32 else np.float64)
def safe_asarray(X, dtype=None, order=None, copy=False, force_all_finite=True):
    
    
    if sp.issparse(X):
        if not isinstance(X, (sp.coo_matrix, sp.csc_matrix, sp.csr_matrix)):
            X = X.tocsr()
        elif copy:
            X = X.copy()
        if force_all_finite:
            _assert_all_finite(X.data)
        # enforces dtype on data array (order should be kept the same).
        X.data = np.asarray(X.data, dtype=dtype)
    else:
        X = np.array(X, dtype=dtype, order=order, copy=copy)
        if force_all_finite:
            _assert_all_finite(X)
    return X
def _ensure_sparse_format(
    spmatrix,
    accept_sparse,
    dtype,
    copy,
    force_all_finite,
    accept_large_sparse,
    estimator_name=None,
    input_name="",
):
    
    
    if dtype is None:
        dtype = spmatrix.dtype

    changed_format = False

    if isinstance(accept_sparse, str):
        accept_sparse = [accept_sparse]

    # Indices dtype validation
    _check_large_sparse(spmatrix, accept_large_sparse)

    if accept_sparse is False:
        raise TypeError(
            "A sparse matrix was passed, but dense "
            "data is required. Use X.toarray() to "
            "convert to a dense numpy array."
        )
    elif isinstance(accept_sparse, (list, tuple)):
        if len(accept_sparse) == 0:
            raise ValueError(
                "When providing 'accept_sparse' "
                "as a tuple or list, it must contain at "
                "least one string value."
            )
        # ensure correct sparse format
        if spmatrix.format not in accept_sparse:
            # create new with correct sparse
            spmatrix = spmatrix.asformat(accept_sparse[0])
            changed_format = True
    elif accept_sparse is not True:
        # any other type
        raise ValueError(
            "Parameter 'accept_sparse' should be a string, "
            "boolean or list of strings. You provided "
            "'accept_sparse={}'.".format(accept_sparse)
        )

    if dtype != spmatrix.dtype:
        # convert dtype
        spmatrix = spmatrix.astype(dtype)
    elif copy and not changed_format:
        # force copy
        spmatrix = spmatrix.copy()

    if force_all_finite:
        if not hasattr(spmatrix, "data"):
            warnings.warn(
                "Can't check %s sparse matrix for nan or inf." % spmatrix.format,
                stacklevel=2,
            )
        else:
            _assert_all_finite(
                spmatrix.data,
                allow_nan=force_all_finite == "allow-nan",
                estimator_name=estimator_name,
                input_name=input_name,
            )

    return spmatrix
def check_X_y(X, y, accept_sparse=False, accept_large_sparse=True,
              dtype="numeric", order=None, copy=False, force_all_finite=True,
              ensure_2d=True, allow_nd=False, multi_output=False,
              ensure_min_samples=1, ensure_min_features=1, y_numeric=False,
              estimator=None):
    
    
    if y is None:
        raise ValueError("y cannot be None")

    X = check_array(X, accept_sparse=accept_sparse,
                    accept_large_sparse=accept_large_sparse,
                    dtype=dtype, order=order, copy=copy,
                    force_all_finite=force_all_finite,
                    ensure_2d=ensure_2d, allow_nd=allow_nd,
                    ensure_min_samples=ensure_min_samples,
                    ensure_min_features=ensure_min_features,
                    estimator=estimator)
    if multi_output:
        y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,
                        dtype=None)
    else:
        y = column_or_1d(y, warn=True)
        _assert_all_finite(y)
    if y_numeric and y.dtype.kind == 'O':
        y = y.astype(np.float64)

    check_consistent_length(X, y)

    return X, y
def has_fit_parameter(estimator, parameter):
     
    
    return parameter in getargspec(estimator.fit)[0]
def check_symmetric(array, tol=1E-10, raise_warning=True,
                    raise_exception=False):
    
    
    if (array.ndim != 2) or (array.shape[0] != array.shape[1]):
        raise ValueError("array must be 2-dimensional and square. "
                         "shape = {0}".format(array.shape))

    if sp.issparse(array):
        diff = (array - array.T).data
        symmetric = np.all(abs(diff) < tol)
    else:
        symmetric = np.allclose(array, array.T, atol=tol)

    if not symmetric:
        if raise_exception:
            raise ValueError("Array must be symmetric")
        if raise_warning:
            warnings.warn("Array is not symmetric, and will be converted "
                          "to symmetric by average with its transpose.")
        if sp.issparse(array):
            conversion = 'to' + array.format
            array = getattr(0.5 * (array + array.T), conversion)()
        else:
            array = 0.5 * (array + array.T)

    return array
def check_memory(memory):
    
    

    if memory is None or isinstance(memory, six.string_types):
        if LooseVersion(joblib_version) < '0.12':
            memory = Memory(cachedir=memory, verbose=0)
        else:
            memory = Memory(location=memory, verbose=0)
    elif not hasattr(memory, 'cache'):
        raise ValueError("'memory' should be None, a string or have the same"
                         " interface as joblib.Memory."
                         " Got memory='{}' instead.".format(memory))
    return memory
def _check_psd_eigenvalues(lambdas, enable_warnings=False):
    

    

    lambdas = np.array(lambdas)
    is_double_precision = lambdas.dtype == np.float64

    # note: the minimum value available is
    #  - single-precision: np.finfo('float32').eps = 1.2e-07
    #  - double-precision: np.finfo('float64').eps = 2.2e-16

    # the various thresholds used for validation
    # we may wish to change the value according to precision.
    significant_imag_ratio = 1e-5
    significant_neg_ratio = 1e-5 if is_double_precision else 5e-3
    significant_neg_value = 1e-10 if is_double_precision else 1e-6
    small_pos_ratio = 1e-12 if is_double_precision else 2e-7

    # Check that there are no significant imaginary parts
    if not np.isreal(lambdas).all():
        max_imag_abs = np.abs(np.imag(lambdas)).max()
        max_real_abs = np.abs(np.real(lambdas)).max()
        if max_imag_abs > significant_imag_ratio * max_real_abs:
            raise ValueError(
                "There are significant imaginary parts in eigenvalues (%g "
                "of the maximum real part). Either the matrix is not PSD, or "
                "there was an issue while computing the eigendecomposition "
                "of the matrix."
                % (max_imag_abs / max_real_abs))

        # warn about imaginary parts being removed
        if enable_warnings:
            warnings.warn("There are imaginary parts in eigenvalues (%g "
                          "of the maximum real part). Either the matrix is not"
                          " PSD, or there was an issue while computing the "
                          "eigendecomposition of the matrix. Only the real "
                          "parts will be kept."
                          % (max_imag_abs / max_real_abs),
                          PositiveSpectrumWarning)

    # Remove all imaginary parts (even if zero)
    lambdas = np.real(lambdas)

    # Check that there are no significant negative eigenvalues
    max_eig = lambdas.max()
    if max_eig < 0:
        raise ValueError("All eigenvalues are negative (maximum is %g). "
                         "Either the matrix is not PSD, or there was an "
                         "issue while computing the eigendecomposition of "
                         "the matrix." % max_eig)

    else:
        min_eig = lambdas.min()
        if (min_eig < -significant_neg_ratio * max_eig
                and min_eig < -significant_neg_value):
            raise ValueError("There are significant negative eigenvalues (%g"
                             " of the maximum positive). Either the matrix is "
                             "not PSD, or there was an issue while computing "
                             "the eigendecomposition of the matrix."
                             % (-min_eig / max_eig))
        elif min_eig < 0:
            # Remove all negative values and warn about it
            if enable_warnings:
                warnings.warn("There are negative eigenvalues (%g of the "
                              "maximum positive). Either the matrix is not "
                              "PSD, or there was an issue while computing the"
                              " eigendecomposition of the matrix. Negative "
                              "eigenvalues will be replaced with 0."
                              % (-min_eig / max_eig),
                              PositiveSpectrumWarning)
            lambdas[lambdas < 0] = 0

    # Check for conditioning (small positive non-zeros)
    too_small_lambdas = (0 < lambdas) & (lambdas < small_pos_ratio * max_eig)
    if too_small_lambdas.any():
        if enable_warnings:
            warnings.warn("Badly conditioned PSD matrix spectrum: the largest "
                          "eigenvalue is more than %g times the smallest. "
                          "Small eigenvalues will be replaced with 0."
                          "" % (1 / small_pos_ratio),
                          PositiveSpectrumWarning)
        lambdas[too_small_lambdas] = 0

    return lambdas
def _check_sample_weight(sample_weight, X, dtype=None, copy=False):
    
    
    n_samples = _num_samples(X)

    if dtype is not None and dtype not in [np.float32, np.float64]:
        dtype = np.float64

    if sample_weight is None:
        sample_weight = np.ones(n_samples, dtype=dtype)
    elif isinstance(sample_weight, numbers.Number):
        sample_weight = np.full(n_samples, sample_weight, dtype=dtype)
    else:
        if dtype is None:
            dtype = [np.float64, np.float32]
        sample_weight = check_array(
            sample_weight, accept_sparse=False, ensure_2d=False, dtype=dtype,
            order="C", copy=copy
        )
        if sample_weight.ndim != 1:
            raise ValueError("Sample weights must be 1D array or scalar")

        if sample_weight.shape != (n_samples,):
            raise ValueError("sample_weight.shape == {}, expected {}!"
                             .format(sample_weight.shape, (n_samples,)))
    return sample_weight
def column_or_1d(y, *, dtype=None, warn=False):
    
    
    xp, _ = get_namespace(y)
    y = xp.asarray(y, dtype=dtype)
    shape = y.shape
    if len(shape) == 1:
        return _asarray_with_order(xp.reshape(y, -1), order="C", xp=xp)
    if len(shape) == 2 and shape[1] == 1:
        if warn:
            warnings.warn(
                "A column-vector y was passed when a 1d array was"
                " expected. Please change the shape of y to "
                "(n_samples, ), for example using ravel().",
                DataConversionWarning,
                stacklevel=2,
            )
        return _asarray_with_order(xp.reshape(y, -1), order="C", xp=xp)

    raise ValueError(
        "y should be a 1d array, got an array of shape {} instead.".format(shape)
    )
def check_scalar(
    x,
    name,
    target_type,
    *,
    min_val=None,
    max_val=None,
    include_boundaries="both",
):
    
    

    if not isinstance(x, target_type):
        raise TypeError(f"{name} must be an instance of {target_type}, not {type(x)}.")

    expected_include_boundaries = ("left", "right", "both", "neither")
    if include_boundaries not in expected_include_boundaries:
        raise ValueError(
            f"Unknown value for `include_boundaries`: {repr(include_boundaries)}. "
            f"Possible values are: {expected_include_boundaries}."
        )

    if max_val is None and include_boundaries == "right":
        raise ValueError(
            "`include_boundaries`='right' without specifying explicitly `max_val` "
            "is inconsistent."
        )

    if min_val is None and include_boundaries == "left":
        raise ValueError(
            "`include_boundaries`='left' without specifying explicitly `min_val` "
            "is inconsistent."
        )

    comparison_operator = (
        operator.lt if include_boundaries in ("left", "both") else operator.le
    )
    if min_val is not None and comparison_operator(x, min_val):
        raise ValueError(
            f"{name} == {x}, must be"
            f" {'>=' if include_boundaries in ('left', 'both') else '>'} {min_val}."
        )

    comparison_operator = (
        operator.gt if include_boundaries in ("right", "both") else operator.ge
    )
    if max_val is not None and comparison_operator(x, max_val):
        raise ValueError(
            f"{name} == {x}, must be"
            f" {'<=' if include_boundaries in ('right', 'both') else '<'} {max_val}."
        )

    return x
def log_epoch(
        self, epoch_stats, train_stats, valid_stats=None, verbose=False,
    ):
        
        
        raise NotImplementedError
def log_stats(
        self,
        stats_meta,
        train_stats=None,
        valid_stats=None,
        test_stats=None,
        verbose=True,
    ):
        
        string_summary = self._stats_to_string(stats_meta)
        for dataset, stats in [
            ("train", train_stats),
            ("valid", valid_stats),
            ("test", test_stats),
        ]:
            if stats is None:
                continue
            summary = {}
            for stat, value_list in stats.items():
                summary[stat] = self.summary_fns[stat](value_list)
            string_summary += " - " + self._stats_to_string(summary, dataset)

        with open(self.save_file, "a") as fout:
            print(string_summary, file=fout)
        if verbose:
            logger.info(string_summary)
def hyperopt_space(self, space: Optional[str] = None) -> List[Dimension]:
        
        
        
        spaces: List[Dimension] = []
        if space == 'buy' or (space is None and self.has_space('buy')):
            logger.debug("Hyperopt has 'buy' space")
            spaces += self.custom_hyperopt.indicator_space()
        if space == 'sell' or (space is None and self.has_space('sell')):
            logger.debug("Hyperopt has 'sell' space")
            spaces += self.custom_hyperopt.sell_indicator_space()
        if space == 'roi' or (space is None and self.has_space('roi')):
            logger.debug("Hyperopt has 'roi' space")
            spaces += self.custom_hyperopt.roi_space()
        if space == 'stoploss' or (space is None and self.has_space('stoploss')):
            logger.debug("Hyperopt has 'stoploss' space")
            spaces += self.custom_hyperopt.stoploss_space()
        return spaces
def stoploss_space() -> Dict[str, Any]:
        
        
        
        return {
            'stoploss': hp.quniform('stoploss', -0.5, -0.02, 0.02),
        }
def stoploss_space() -> List[Dimension]:
        
        
        
        return [
            Real(-0.5, -0.02, name='stoploss'),
        ]
def load_previous_results(trials_file) -> List:
        
        
        
        trials: List = []
        if trials_file.is_file() and trials_file.stat().st_size > 0:
            trials = Hyperopt._read_trials(trials_file)
            if trials[0].get('is_best') is None:
                raise OperationalException(
                        "The file with Hyperopt results is incompatible with this version "
                        "of Freqtrade and cannot be loaded.")
            logger.info(f"Loaded {len(trials)} previous evaluations from disk.")
        return trials
def print_results(self, results) -> None:
        
        
        
        is_best = results['is_best']

        if self.print_all or is_best:
            print(
                HyperoptTools.get_result_table(
                    self.config, results, self.total_epochs,
                    self.print_all, self.print_colorized,
                    self.hyperopt_table_header
                )
            )
            self.hyperopt_table_header = 2
def load_inference_source(source=None, batch=1, vid_stride=1, buffer=False):
    
    
    
    source, stream, screenshot, from_img, in_memory, tensor = check_source(source)
    source_type = source.source_type if in_memory else SourceTypes(stream, screenshot, from_img, tensor)

    # Dataloader
    if tensor:
        dataset = LoadTensor(source)
    elif in_memory:
        dataset = source
    elif stream:
        dataset = LoadStreams(source, vid_stride=vid_stride, buffer=buffer)
    elif screenshot:
        dataset = LoadScreenshots(source)
    elif from_img:
        dataset = LoadPilAndNumpy(source)
    else:
        dataset = LoadImagesAndVideos(source, batch=batch, vid_stride=vid_stride)

    # Attach source types to the dataset
    setattr(dataset, "source_type", source_type)

    return dataset
def __init__(self, max_workers=None, mp_context=None,
                 initializer=None, initargs=()):
        
        
        _check_system_limits()

        if max_workers is None:
            self._max_workers = os.cpu_count() or 1
        else:
            if max_workers <= 0:
                raise ValueError("max_workers must be greater than 0")

            self._max_workers = max_workers
        if mp_context is None:
            mp_context = mp.get_context()
        self._mp_context = mp_context

        if initializer is not None and not callable(initializer):
            raise TypeError("initializer must be a callable")
        self._initializer = initializer
        self._initargs = initargs

        # Make the call queue slightly larger than the number of processes to
        # prevent the worker processes from idling. But don't make it too big
        # because futures in the call queue cannot be cancelled.
        queue_size = self._max_workers + EXTRA_QUEUED_CALLS
        self._call_queue = mp_context.Queue(queue_size)
        # Killed worker processes can produce spurious "broken pipe"
        # tracebacks in the queue's own worker thread. But we detect killed
        # processes anyway, so silence the tracebacks.
        self._call_queue._ignore_epipe = True
        self._result_queue = mp_context.SimpleQueue()
        self._work_ids = queue.Queue()
        self._queue_management_thread = None
        # Map of pids to processes
        self._processes = {}

        # Shutdown is a two-step process.
        self._shutdown_thread = False
        self._shutdown_lock = threading.Lock()
        self._broken = False
        self._queue_count = 0
        self._pending_work_items = {}
def __init__(self, max_workers=None, mp_context=None,
                 initializer=None, initargs=(), *, max_tasks_per_child=None):
        
        
        _check_system_limits()

        if max_workers is None:
            self._max_workers = os.cpu_count() or 1
            if sys.platform == 'win32':
                self._max_workers = min(_MAX_WINDOWS_WORKERS,
                                        self._max_workers)
        else:
            if max_workers <= 0:
                raise ValueError("max_workers must be greater than 0")
            elif (sys.platform == 'win32' and
                max_workers > _MAX_WINDOWS_WORKERS):
                raise ValueError(
                    f"max_workers must be <= {_MAX_WINDOWS_WORKERS}")

            self._max_workers = max_workers

        if mp_context is None:
            mp_context = mp.get_context()
        self._mp_context = mp_context

        if initializer is not None and not callable(initializer):
            raise TypeError("initializer must be a callable")
        self._initializer = initializer
        self._initargs = initargs

        if max_tasks_per_child is not None:
            if not isinstance(max_tasks_per_child, int):
                raise TypeError("max_tasks_per_child must be an integer")
            elif max_tasks_per_child <= 0:
                raise ValueError("max_tasks_per_child must be >= 1")
        self._max_tasks_per_child = max_tasks_per_child

        # Management thread
        self._executor_manager_thread = None

        # Map of pids to processes
        self._processes = {}

        # Shutdown is a two-step process.
        self._shutdown_thread = False
        self._shutdown_lock = threading.Lock()
        self._idle_worker_semaphore = threading.Semaphore(0)
        self._broken = False
        self._queue_count = 0
        self._pending_work_items = {}
        self._cancel_pending_futures = False

        # _ThreadWakeup is a communication channel used to interrupt the wait
        # of the main loop of executor_manager_thread from another thread (e.g.
        # when calling executor.submit or executor.shutdown). We do not use the
        # _result_queue to send wakeup signals to the executor_manager_thread
        # as it could result in a deadlock if a worker process dies with the
        # _result_queue write lock still acquired.
        #
        # _shutdown_lock must be locked to access _ThreadWakeup.
        self._executor_manager_thread_wakeup = _ThreadWakeup()

        # Create communication channels for the executor
        # Make the call queue slightly larger than the number of processes to
        # prevent the worker processes from idling. But don't make it too big
        # because futures in the call queue cannot be cancelled.
        queue_size = self._max_workers + EXTRA_QUEUED_CALLS
        self._call_queue = _SafeQueue(
            max_size=queue_size, ctx=self._mp_context,
            pending_work_items=self._pending_work_items,
            shutdown_lock=self._shutdown_lock,
            thread_wakeup=self._executor_manager_thread_wakeup)
        # Killed worker processes can produce spurious "broken pipe"
        # tracebacks in the queue's own worker thread. But we detect killed
        # processes anyway, so silence the tracebacks.
        self._call_queue._ignore_epipe = True
        self._result_queue = mp_context.SimpleQueue()
        self._work_ids = queue.Queue()
def uniform(*args, **kwargs):
    
    
    device = kwargs.pop('device', None)

    a = numpy.random.uniform(*args, **kwargs)
    return chainerx.array(a, device=device, copy=False)
def can_edit_address(user, address):
    
    
    has_perm = user.has_perm("account.manage_users")
    belongs_to_user = address in user.addresses.all()
    return has_perm or belongs_to_user
def can_edit_address(user, address):
    
    
    return (
        user.has_perm("account.manage_users")
        or user.addresses.filter(pk=address.pk).exists()
    )
def writelines(self, list_of_data):
        
        
        if not PY34:
            # In Python 3.3, bytes.join() doesn't handle memoryview.
            list_of_data = (
                bytes(data) if isinstance(data, memoryview) else data
                for data in list_of_data)
        self.write(b''.join(list_of_data))
def _convert_dataset(dataset_split):
  
  
  image_files = _get_files('image', dataset_split)
  label_files = _get_files('label', dataset_split)

  num_images = len(image_files)
  num_labels = len(label_files)
  num_per_shard = int(math.ceil(num_images / _NUM_SHARDS))

  if num_images != num_labels:
    raise RuntimeError("The number of images and labels doesn't match: {} {}".format(num_images, num_labels))

  image_reader = build_data.ImageReader('png', channels=3)
  label_reader = build_data.ImageReader('png', channels=1)

  for shard_id in range(_NUM_SHARDS):
    shard_filename = '%s-%05d-of-%05d.tfrecord' % (
        dataset_split, shard_id, _NUM_SHARDS)
    output_filename = os.path.join(FLAGS.output_dir, shard_filename)
    with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:
      start_idx = shard_id * num_per_shard
      end_idx = min((shard_id + 1) * num_per_shard, num_images)
      for i in range(start_idx, end_idx):
        sys.stdout.write('\r>> Converting image %d/%d shard %d' % (
            i + 1, num_images, shard_id))
        sys.stdout.flush()
        # Read the image.
        image_data = tf.gfile.FastGFile(image_files[i], 'rb').read()
        height, width = image_reader.read_image_dims(image_data)
        # Read the semantic segmentation annotation.
        seg_data = tf.gfile.FastGFile(label_files[i], 'rb').read()
        seg_height, seg_width = label_reader.read_image_dims(seg_data)
        if height != seg_height or width != seg_width:
          raise RuntimeError('Shape mismatched between image and label.')
        # Convert to tf example.
        re_match = _IMAGE_FILENAME_RE.search(image_files[i])
        if re_match is None:
          raise RuntimeError('Invalid image filename: ' + image_files[i])
        filename = os.path.basename(re_match.group(1))
        example = build_data.image_seg_to_tfexample(
            image_data, filename, height, width, seg_data)
        tfrecord_writer.write(example.SerializeToString())
    sys.stdout.write('\n')
    sys.stdout.flush()
def unpack_params(params, attr_name, buffer,
                  transfer_dtype, stream=None):
    
    if len(params) == 0:
        return
    xp = chainer.backend.get_array_module(getattr(params[0], attr_name))
    offset = 0
    for param in params:
        v = getattr(param, attr_name)
        size = v.size * np.dtype(transfer_dtype).itemsize
        grad_dtype = v.dtype
        if grad_dtype != transfer_dtype:
            v = xp.array(v, copy=False, dtype=transfer_dtype)
        buffer.to_device(v, size, offset, stream)
        offset += size
        if grad_dtype != transfer_dtype:
            setattr(param, attr_name, v.astype(grad_dtype))
def __init__(
        self,
        vocab: Vocab,
        model: Optional[Model],
        name: str = "lemmatizer",
        *,
        mode: str = "lookup",
        overwrite: bool = False,
    ) -> None:
        
        
        self.vocab = vocab
        self.model = model
        self.name = name
        self._mode = mode
        self.lookups = Lookups()
        self.overwrite = overwrite
        self._validated = False
        if self.mode == "lookup":
            self.lemmatize = self.lookup_lemmatize
        elif self.mode == "rule":
            self.lemmatize = self.rule_lemmatize
        else:
            mode_attr = f"{self.mode}_lemmatize"
            if not hasattr(self, mode_attr):
                raise ValueError(Errors.E1003.format(mode=mode))
            self.lemmatize = getattr(self, mode_attr)
        self.cache = {}
def get_variables_available_in_checkpoint(variables,
                                          checkpoint_path,
                                          include_global_step=True):
  
  
  if isinstance(variables, list):
    variable_names_map = {variable.op.name: variable for variable in variables}
  elif isinstance(variables, dict):
    variable_names_map = variables
  else:
    raise ValueError('`variables` is expected to be a list or dict.')
  ckpt_reader = tf.train.NewCheckpointReader(checkpoint_path)
  ckpt_vars_to_shape_map = ckpt_reader.get_variable_to_shape_map()
  if not include_global_step:
    ckpt_vars_to_shape_map.pop(tf.GraphKeys.GLOBAL_STEP, None)
  vars_in_ckpt = {}
  for variable_name, variable in sorted(variable_names_map.items()):
    if variable_name in ckpt_vars_to_shape_map:
      if ckpt_vars_to_shape_map[variable_name] == variable.shape.as_list():
        vars_in_ckpt[variable_name] = variable
      else:
        logging.warning('Variable [%s] is available in checkpoint, but has an '
                        'incompatible shape with model variable.',
                        variable_name)
    else:
      logging.warning('Variable [%s] is not available in checkpoint',
                      variable_name)
  if isinstance(variables, list):
    return vars_in_ckpt.values()
  return vars_in_ckpt
def display_on_jupyter(self):
        
        
        
        data_source = get_max_limited_datas(self.origin_data_source, JUPYTER_BYTE_LIMIT)
        props = self._get_props(data_source)
        iframe_html = self._get_render_iframe(props)

        display_html(iframe_html)
def display_on_jupyter(self):
        
        
        
        data_source = get_max_limited_datas(self.origin_data_source, JUPYTER_BYTE_LIMIT)
        props = self._get_props(
            "jupyter",
            data_source,
            len(self.origin_data_source) > len(data_source)
        )
        iframe_html = self._get_render_iframe(props)

        if len(self.origin_data_source) > len(data_source):
            upload_tool = BatchUploadDatasToolOnJupyter()
            display_html(iframe_html)
            upload_tool.run(
                records=self.origin_data_source,
                sample_data_count=0,
                data_source_id=self.data_source_id,
                gid=self.gid,
                tunnel_id=self.tunnel_id,
            )
        else:
            display_html(iframe_html)
def current_cover_position(self):
        
        
        pos = self._module.get_position(self._channel)
        return 100 - pos
def walk(top, topdown=True, onerror=None, followlinks=False):
    

    

    dirs = []
    nondirs = []

    # We may not have read permission for top, in which case we can't
    # get a list of the files the directory contains.  os.walk
    # always suppressed the exception then, rather than blow up for a
    # minor reason when (say) a thousand readable directories are still
    # left to visit.  That logic is copied here.
    try:
        # Note that scandir is global in this module due
        # to earlier import-*.
        scandir_it = scandir(top)
    except OSError as error:
        if onerror is not None:
            onerror(error)
        return

    while True:
        try:
            try:
                entry = next(scandir_it)
            except StopIteration:
                break
        except OSError as error:
            if onerror is not None:
                onerror(error)
            return

        try:
            is_dir = entry.is_dir()
        except OSError:
            # If is_dir() raises an OSError, consider that the entry is not
            # a directory, same behaviour than os.path.isdir().
            is_dir = False

        if is_dir:
            dirs.append(entry.name)
        else:
            nondirs.append(entry.name)

        if not topdown and is_dir:
            # Bottom-up: recurse into sub-directory, but exclude symlinks to
            # directories if followlinks is False
            if followlinks:
                walk_into = True
            else:
                try:
                    is_symlink = entry.is_symlink()
                except OSError:
                    # If is_symlink() raises an OSError, consider that the
                    # entry is not a symbolic link, same behaviour than
                    # os.path.islink().
                    is_symlink = False
                walk_into = not is_symlink

            if walk_into:
                yield from walk(entry.path, topdown, onerror, followlinks)

    # Yield before recursion if going top down
    if topdown:
        yield top, dirs, nondirs

        # Recurse into sub-directories
        islink, join = path.islink, path.join
        for name in dirs:
            new_path = join(top, name)
            # Issue #23605: os.path.islink() is used instead of caching
            # entry.is_symlink() result during the loop on os.scandir() because
            # the caller can replace the directory entry during the "yield"
            # above.
            if followlinks or not islink(new_path):
                yield from walk(new_path, topdown, onerror, followlinks)
    else:
        # Yield after recursion if going bottom up
        yield top, dirs, nondirs
def makedirs(name, mode=0o777, exist_ok=False):
    

    
    head, tail = path.split(name)
    if not tail:
        head, tail = path.split(head)
    if head and tail and not path.exists(head):
        try:
            makedirs(head, mode, exist_ok)
        except FileExistsError:
            # be happy if someone already created the path
            pass
        cdir = curdir
        if isinstance(tail, bytes):
            cdir = bytes(curdir, 'ASCII')
        if tail == cdir:           # xxx/newdir/. exists if xxx/newdir exists
            return
    try:
        mkdir(name, mode)
    except OSError as e:
        if not exist_ok or e.errno != errno.EEXIST or not path.isdir(name):
            raise
def fsencode(filename):
    
    
    
    if isinstance(filename, bytes):
        return filename
    elif isinstance(filename, str):
        encoding = sys.getfilesystemencoding()
        if encoding == 'mbcs':
            return filename.encode(encoding)
        else:
            return filename.encode(encoding, 'surrogateescape')
    else:
        raise TypeError("expect bytes or str, not %s" % type(filename).__name__)
def __init__(self, device: Device) -> None:
        
        capabilities = device.capabilities
        super().__init__(device, capabilities)

        self._rooms: list[Room] = []

        self._attr_fan_speed_list = [
            level.display_name for level in capabilities.fan_speed.types
        ]
def loss(labels, predictions, weights=None, from_logits=False):
  
  
  # When using these functions with the Keras core API, we will need to squeeze
  # the labels tensor - Keras adds a spurious inner dimension.
  labels, predictions = _adjust_labels(labels, predictions)
  _validate_rank(labels, predictions, weights)

  example_losses = tf.keras.losses.sparse_categorical_crossentropy(
      labels, predictions, from_logits=from_logits)

  if weights is None:
    return tf.reduce_mean(example_losses)
  weights = tf.cast(weights, predictions.dtype)
  return tf.math.divide_no_nan(
      tf.reduce_sum(example_losses * weights), tf.reduce_sum(weights))
def get_order(row, col):
    
    
    if _is_c_order(row, col):
        return 'C'
    if _is_c_order(col, row):
        return 'F'
    return 'other'
def solution(nth: int = 10001) -> int:
    
    
    

    try:
        nth = int(nth)
    except (TypeError, ValueError):
        raise TypeError("Parameter nth must be int or castable to int.") from None
    if nth <= 0:
        raise ValueError("Parameter nth must be greater than or equal to one.")
    primes = []
    num = 2
    while len(primes) < nth:
        if isprime(num):
            primes.append(num)
            num += 1
        else:
            num += 1
    return primes[len(primes) - 1]
def __init__(self,
               filter_size,
               output_size,
               num_units,
               is_training,
               forget_bias=1.0,
               activation=tf.tanh,
               use_batch_norm=False,
               flatten_state=False,
               groups=4,
               clip_state=False,
               scale_state=False,
               output_bottleneck=False,
               pre_bottleneck=False,
               is_quantized=False,
               visualize_gates=False,
               conv_op_overrides=None):
    
    
    if clip_state and scale_state:
      raise ValueError('clip_state and scale_state cannot both be enabled.')

    self._filter_size = list(filter_size)
    self._output_size = list(output_size)
    self._num_units = num_units
    self._is_training = is_training
    self._forget_bias = forget_bias
    self._activation = activation
    self._use_batch_norm = use_batch_norm
    self._viz_gates = visualize_gates
    self._flatten_state = flatten_state
    self._param_count = self._num_units
    self._groups = groups
    self._scale_state = scale_state
    self._clip_state = clip_state
    self._output_bottleneck = output_bottleneck
    self._pre_bottleneck = pre_bottleneck
    self._is_quantized = is_quantized
    for dim in self._output_size:
      self._param_count *= dim
    self._conv_op_overrides = conv_op_overrides
    if self._conv_op_overrides and len(self._conv_op_overrides) != 2:
      raise ValueError('Bottleneck and Convolutional layer should be overriden'
                       'together')
def __init__(self, parent, title, text, modal=True,
                 _htest=False, _utest=False):
        
        
        Toplevel.__init__(self, parent)
        self.configure(borderwidth=5)
        # Place dialog below parent if running htest.
        self.geometry("=%dx%d+%d+%d" % (750, 500,
                           parent.winfo_rootx() + 10,
                           parent.winfo_rooty() + (10 if not _htest else 100)))
        # TODO: get fg/bg from theme.
        self.bg = '#ffffff'
        self.fg = '#000000'

        self.create_widgets()
        self.title(title)
        self.protocol("WM_DELETE_WINDOW", self.ok)
        self.parent = parent
        self.text.focus_set()
        # Bind keys for closing this dialog.
        self.bind('<Return>', self.ok)
        self.bind('<Escape>', self.ok)
        self.text.insert(0.0, text)
        self.text.config(state=DISABLED)

        if modal:
            self.transient(parent)
            self.grab_set()
            if not _utest:
                self.wait_window()
def view_file(parent, title, filename, encoding=None, modal=True, _utest=False):
    
    
    try:
        with open(filename, 'r', encoding=encoding) as file:
            contents = file.read()
    except OSError:
        showerror(title='File Load Error',
                  message=f'Unable to load file {filename!r} .',
                  parent=parent)
    except UnicodeDecodeError as err:
        showerror(title='Unicode Decode Error',
                  message=str(err),
                  parent=parent)
    else:
        return view_text(parent, title, contents, modal, _utest=_utest)
    return None
def __init__(self, parent, title, text, modal=True, wrap='word',
                 *, _htest=False, _utest=False):
        
        
        super().__init__(parent)
        self['borderwidth'] = 5
        # Place dialog below parent if running htest.
        x = parent.winfo_rootx() + 10
        y = parent.winfo_rooty() + (10 if not _htest else 100)
        self.geometry(f'=750x500+{x}+{y}')

        self.title(title)
        self.viewframe = ViewFrame(self, text, wrap=wrap)
        self.protocol("WM_DELETE_WINDOW", self.ok)
        self.button_ok = button_ok = Button(self, text='Close',
                                            command=self.ok, takefocus=False)
        self.viewframe.pack(side='top', expand=True, fill='both')

        self.is_modal = modal
        if self.is_modal:
            self.transient(parent)
            self.grab_set()
            if not _utest:
                self.wait_window()
def requires_local(self):
        
        
        
        return []
def _onDevicesDiscovered(self, clusters: List[CloudClusterResponse]) -> None:
        
        
        new_devices = []
        remote_clusters_added = False
        host_guid_map = {machine.getMetaDataEntry(self.META_HOST_GUID): device_cluster_id
                         for device_cluster_id, machine in self._um_cloud_printers.items()
                         if machine.getMetaDataEntry(self.META_HOST_GUID)}
        
        machine_manager = CuraApplication.getInstance().getMachineManager()

        for cluster_data in clusters:
            device = CloudOutputDevice(self._api, cluster_data)
            # If the machine already existed before, it will be present in the host_guid_map
            if cluster_data.host_guid in host_guid_map:
                machine = machine_manager.getMachine(device.printerType, {self.META_HOST_GUID: cluster_data.host_guid})
                if machine and machine.getMetaDataEntry(self.META_CLUSTER_ID) != device.key:
                    # If the retrieved device has a different cluster_id than the existing machine, bring the existing
                    # machine up-to-date.
                    self._updateOutdatedMachine(outdated_machine = machine, new_cloud_output_device = device)

            # Create a machine if we don't already have it. Do not make it the active machine.
            # We only need to add it if it wasn't already added by "local" network or by cloud.
            if machine_manager.getMachine(device.printerType, {self.META_CLUSTER_ID: device.key}) is None \
                    and machine_manager.getMachine(device.printerType, {self.META_NETWORK_KEY: cluster_data.host_name + "*"}) is None:  # The host name is part of the network key.
                new_devices.append(device)
            elif device.getId() not in self._remote_clusters:
                self._remote_clusters[device.getId()] = device
                remote_clusters_added = True
            # If a printer that was removed from the account is re-added, change its metadata to mark it not removed
            # from the account
            elif not parseBool(self._um_cloud_printers[device.key].getMetaDataEntry(META_UM_LINKED_TO_ACCOUNT, "true")):
                self._um_cloud_printers[device.key].setMetaDataEntry(META_UM_LINKED_TO_ACCOUNT, True)
            # As adding a lot of machines might take some time, ensure that the GUI (and progress message) is updated
            CuraApplication.getInstance().processEvents()

        # Inform the Cloud printers model about new devices.
        new_devices_list_of_dicts = [{
                "key": d.getId(),
                "name": d.name,
                "machine_type": d.printerTypeName,
                "firmware_version": d.firmwareVersion} for d in new_devices]
        discovered_cloud_printers_model = CuraApplication.getInstance().getDiscoveredCloudPrintersModel()
        discovered_cloud_printers_model.addDiscoveredCloudPrinters(new_devices_list_of_dicts)

        if not new_devices:
            if remote_clusters_added:
                self._connectToActiveMachine()
            return

        # Sort new_devices on online status first, alphabetical second.
        # Since the first device might be activated in case there is no active printer yet,
        # it would be nice to prioritize online devices
        online_cluster_names = {c.friendly_name.lower() for c in clusters if c.is_online and not c.friendly_name is None}
        new_devices.sort(key = lambda x: ("a{}" if x.name.lower() in online_cluster_names else "b{}").format(x.name.lower()))

        message = Message(
            title = self.i18n_catalog.i18ncp(
                "info:status",
                "New printer detected from your Ultimaker account",
                "New printers detected from your Ultimaker account",
                len(new_devices)
            ),
            progress = 0,
            lifetime = 0,
            message_type = Message.MessageType.POSITIVE
        )
        message.show()

        new_devices_added = []

        for idx, device in enumerate(new_devices):
            message_text = self.i18n_catalog.i18nc("info:status Filled in with printer name and printer model.", "Adding printer {name} ({model}) from your account").format(name = device.name, model = device.printerTypeName)
            message.setText(message_text)
            if len(new_devices) > 1:
                message.setProgress((idx / len(new_devices)) * 100)
            CuraApplication.getInstance().processEvents()
            self._remote_clusters[device.getId()] = device

            # If there is no active machine, activate the first available cloud printer
            activate = not CuraApplication.getInstance().getMachineManager().activeMachine

            if self._createMachineFromDiscoveredDevice(device.getId(), activate = activate):
                new_devices_added.append(device)

        message.setProgress(None)

        max_disp_devices = 3
        if len(new_devices_added) > max_disp_devices:
            num_hidden = len(new_devices_added) - max_disp_devices
            device_name_list = ["<li>{} ({})</li>".format(device.name, device.printerTypeName) for device in new_devices[0: max_disp_devices]]
            device_name_list.append("<li>" + self.i18n_catalog.i18ncp("info:{0} gets replaced by a number of printers", "... and {0} other", "... and {0} others", num_hidden) + "</li>")
            device_names = "".join(device_name_list)
        else:
            device_names = "".join(["<li>{} ({})</li>".format(device.name, device.printerTypeName) for device in new_devices_added])
        if new_devices_added:
            message_text = self.i18n_catalog.i18nc("info:status", "Printers added from Digital Factory:") + f"<ul>{device_names}</ul>"
            message.setText(message_text)
        else:
            message.hide()
def _onGetRemoteClustersFinished(self, clusters: List[CloudClusterResponse]) -> None:
        

        self._um_cloud_printers = {m.getMetaDataEntry(self.META_CLUSTER_ID): m for m in
                                   CuraApplication.getInstance().getContainerRegistry().findContainerStacks(
                                       type = "machine") if m.getMetaDataEntry(self.META_CLUSTER_ID, None)}
        new_clusters = []
        all_clusters = {c.cluster_id: c for c in clusters}  # type: Dict[str, CloudClusterResponse]
        online_clusters = {c.cluster_id: c for c in clusters if c.is_online}  # type: Dict[str, CloudClusterResponse]

        # Add the new printers in Cura. If a printer was previously added and is rediscovered, set its metadata to
        # reflect that and mark the printer not removed from the account
        for device_id, cluster_data in all_clusters.items():
            if device_id not in self._remote_clusters:
                new_clusters.append(cluster_data)
            if device_id in self._um_cloud_printers and not parseBool(self._um_cloud_printers[device_id].getMetaDataEntry(META_UM_LINKED_TO_ACCOUNT, "true")):
                self._um_cloud_printers[device_id].setMetaDataEntry(META_UM_LINKED_TO_ACCOUNT, True)
        self._onDevicesDiscovered(new_clusters)

        # Hide the current removed_printers_message, if there is any
        if self._removed_printers_message:
            self._removed_printers_message.actionTriggered.disconnect(self._onRemovedPrintersMessageActionTriggered)
            self._removed_printers_message.hide()

        # Remove the CloudOutput device for offline printers
        offline_device_keys = set(self._remote_clusters.keys()) - set(online_clusters.keys())
        for device_id in offline_device_keys:
            self._onDiscoveredDeviceRemoved(device_id)

        # Handle devices that were previously added in Cura but do not exist in the account anymore (i.e. they were
        # removed from the account)
        removed_device_keys = set(self._um_cloud_printers.keys()) - set(all_clusters.keys())
        if removed_device_keys:
            self._devicesRemovedFromAccount(removed_device_keys)

        if new_clusters or offline_device_keys or removed_device_keys:
            self.discoveredDevicesChanged.emit()
        if offline_device_keys:
            # If the removed device was active we should connect to the new active device
            self._connectToActiveMachine()

        self._syncing = False
        self._account.setSyncState(self.SYNC_SERVICE_NAME, SyncState.SUCCESS)
def __call__(
        self,
        source: Union[str, Path, int, list, tuple, PIL.Image.Image, np.ndarray, torch.Tensor] = None,
        stream: bool = False,
        **kwargs,
    ) -> list:
        
        
        
        return self.predict(source, stream, **kwargs)
def save(self, filename: Union[str, Path] = "saved_model.pt", use_dill=True) -> None:
        
        
        
        self._check_is_pytorch_model()
        from ultralytics import __version__
        from datetime import datetime

        updates = {
            "date": datetime.now().isoformat(),
            "version": __version__,
            "license": "AGPL-3.0 License (https://ultralytics.com/license)",
            "docs": "https://docs.ultralytics.com",
        }
        torch.save({**self.ckpt, **updates}, filename, use_dill=use_dill)
def process_payment(
    payment_information: PaymentData, config: ApiConfig
) -> GatewayResponse:
    
    
    result = api.register_transaction(config, payment_information)

    return GatewayResponse(
        is_success=result.status == PaymentStatus.SUCCESS,
        action_required=False,
        kind=TransactionKind.CAPTURE,
        amount=payment_information.amount,
        currency=payment_information.currency,
        transaction_id=result.psp_reference,
        error=parse_errors(result.errors),
        raw_response=result.raw_response,
        psp_reference=result.psp_reference,
    )
def test_last_dependency(self):
        
        
        
        # Load graph
        loader = MigrationLoader(connection)
        # Make state
        before = self.make_project_state([])
        after = self.make_project_state([self.book_migrations_fk])
        after.real_apps = ["migrations"]
        autodetector = MigrationAutodetector(before, after)
        changes = autodetector._detect_changes(graph=loader.graph)
        # Right number of migrations?
        self.assertNumberMigrations(changes, 'otherapp', 1)
        self.assertOperationTypes(changes, 'otherapp', 0, ["CreateModel"])
        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name="Book")
        # Right dependencies?
        self.assertEqual(changes['otherapp'][0].dependencies, [("migrations", "0002_second")])
def test_add_alter_order_with_respect_to(self):
        
        
        
        # Make state
        before = self.make_project_state([self.author_name])
        after = self.make_project_state([self.book, self.author_with_book_order_wrt])
        autodetector = MigrationAutodetector(before, after)
        changes = autodetector._detect_changes()
        # Right number of migrations?
        self.assertNumberMigrations(changes, 'testapp', 1)
        self.assertOperationTypes(changes, 'testapp', 0, ["AddField", "AlterOrderWithRespectTo"])
        self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name="author", name="book")
        self.assertOperationAttributes(changes, 'testapp', 0, 1, name="author", order_with_respect_to="book")
def test_add_alter_order_with_respect_to(self):
        
        
        
        # Make state
        before = self.make_project_state([self.author_name])
        after = self.make_project_state([self.book, self.author_with_book_order_wrt])
        autodetector = MigrationAutodetector(before, after)
        changes = autodetector._detect_changes()
        # Right number/type of migrations?
        self.assertNumberMigrations(changes, 'testapp', 1)
        self.assertOperationTypes(changes, 'testapp', 0, ["AddField", "AlterOrderWithRespectTo"])
        self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name="author", name="book")
        self.assertOperationAttributes(changes, 'testapp', 0, 1, name="author", order_with_respect_to="book")
def test_multiple_bases(self):
        
        A = ModelState("app", "A", [("a_id", models.AutoField(primary_key=True))])
        B = ModelState("app", "B", [("b_id", models.AutoField(primary_key=True))])
        C = ModelState("app", "C", [], bases=("app.A", "app.B"))
        D = ModelState("app", "D", [], bases=("app.A", "app.B"))
        E = ModelState("app", "E", [], bases=("app.A", "app.B"))
        changes = self.get_changes([], [A, B, C, D, E])
        # Right number/type of migrations?
        self.assertNumberMigrations(changes, "app", 1)
        self.assertOperationTypes(
            changes,
            "app",
            0,
            ["CreateModel", "CreateModel", "CreateModel", "CreateModel", "CreateModel"],
        )
        self.assertOperationAttributes(changes, "app", 0, 0, name="A")
        self.assertOperationAttributes(changes, "app", 0, 1, name="B")
        self.assertOperationAttributes(changes, "app", 0, 2, name="C")
        self.assertOperationAttributes(changes, "app", 0, 3, name="D")
        self.assertOperationAttributes(changes, "app", 0, 4, name="E")
def test_multiple_bases(self):
        
        
        
        A = ModelState("app", "A", [("a_id", models.AutoField(primary_key=True))])
        B = ModelState("app", "B", [("b_id", models.AutoField(primary_key=True))])
        C = ModelState("app", "C", [], bases=("app.A", "app.B"))
        D = ModelState("app", "D", [], bases=("app.A", "app.B"))
        E = ModelState("app", "E", [], bases=("app.A", "app.B"))
        changes = self.get_changes([], [A, B, C, D, E])
        # Right number/type of migrations?
        self.assertNumberMigrations(changes, "app", 1)
        self.assertOperationTypes(
            changes,
            "app",
            0,
            ["CreateModel", "CreateModel", "CreateModel", "CreateModel", "CreateModel"],
        )
        self.assertOperationAttributes(changes, "app", 0, 0, name="A")
        self.assertOperationAttributes(changes, "app", 0, 1, name="B")
        self.assertOperationAttributes(changes, "app", 0, 2, name="C")
        self.assertOperationAttributes(changes, "app", 0, 3, name="D")
        self.assertOperationAttributes(changes, "app", 0, 4, name="E")
def infer_reuse_pattern(fgraph, outputs_to_disown):
    
    
    
    rval = set()
    for o in outputs_to_disown:
        view_tree_set(alias_root(o), rval)
    # remove from rval all of the inputs, constants, values.
    rval = set(r for r in rval if r.owner is not None)

    return rval
def convert_function_input(input):
    
    
    
    if isinstance(input, (SymbolicInput, SymbolicInputKit)):
        return input
    elif isinstance(input, gof.Constant):
        raise TypeError('A Constant instance is not a legal function input',
                        input)
    elif isinstance(input, gof.Variable):
        return In(input)
    elif isinstance(input, (list, tuple)):
        orig = input
        if not input:
            raise TypeError("Nonsensical input specification: %s" % input)
        if isinstance(input[0], basestring):
            name = input[0]
            input = input[1:]
        else:
            name = None
        if isinstance(input[0], (list, tuple)):
            if len(input[0]) != 2 or len(input) != 2:
                raise TypeError("Invalid input syntax: %s (check "
                                "documentation or use an In instance)" % orig)
            (variable, update), value = input
        elif isinstance(input[0], gof.Variable):
            if len(input) == 1:
                variable, update, value = input[0], None, None
            elif len(input) == 2:
                (variable, value), update = input, None
            else:
                raise TypeError("Invalid input syntax: %s (check "
                                "documentation or use an In instance)" % orig)
        elif isinstance(input[0], (SymbolicInput, SymbolicInputKit)):
            if len(input) == 1:
                return input[0]
            elif len(input) == 2:
                input, value = input
                if name is not None:
                    input.name = name
                input.value = value
                return input
        else:
            raise TypeError("The input specification is not valid: %s" % input)

        if not isinstance(variable, gof.Variable):
            raise TypeError("Unknown input type: %s, expected Variable "
                            "instance" % type(variable), variable)
        if update is not None and not isinstance(update, gof.Variable):
            raise TypeError("Unknown update type: %s, expected Variable "
                            "instance" % type(update), update)
        if (value is not None and
            isinstance(value, (gof.Variable, SymbolicInput))):
            raise TypeError("The value for input %s should not be a Variable "
                            "or SymbolicInput instance (got: %s)" %
                            (variable, value))

        return In(variable, name=name, value=value, update=update)
    else:
        raise TypeError("Unknown input type: %s, expected Variable instance" %
                        type(input), input)
def health(self):
        
        return get_cluster_health_status(self.url)
def _transform_weaviate_filter_operator(operator: str) -> str:
    
    if operator == "!=":
        return "not_equal"
    elif operator == "==":
        return "equal"
    elif operator == ">":
        return "greater_than"
    elif operator == "<":
        return "less_than"
    elif operator == ">=":
        return "greater_or_equal"
    elif operator == "<=":
        return "less_or_equal"
    else:
        raise ValueError(f"Filter operator {operator} not supported")
def convert(cls, value: float, from_unit: str, to_unit: str) -> float:
        
        
        # We cannot use the implementation from BaseUnitConverter here because the temperature
        # units do not use the same floor: 0°C, 0°F and 0K do not align
        if from_unit == to_unit:
            return value

        if from_unit == TEMP_CELSIUS:
            if to_unit == TEMP_FAHRENHEIT:
                return cls._celsius_to_fahrenheit(value)
            if to_unit == TEMP_KELVIN:
                return cls._celsius_to_kelvin(value)
            raise HomeAssistantError(
                UNIT_NOT_RECOGNIZED_TEMPLATE.format(to_unit, cls.UNIT_CLASS)
            )

        if from_unit == TEMP_FAHRENHEIT:
            if to_unit == TEMP_CELSIUS:
                return cls._fahrenheit_to_celsius(value)
            if to_unit == TEMP_KELVIN:
                return cls._celsius_to_kelvin(cls._fahrenheit_to_celsius(value))
            raise HomeAssistantError(
                UNIT_NOT_RECOGNIZED_TEMPLATE.format(to_unit, cls.UNIT_CLASS)
            )

        if from_unit == TEMP_KELVIN:
            if to_unit == TEMP_CELSIUS:
                return cls._kelvin_to_celsius(value)
            if to_unit == TEMP_FAHRENHEIT:
                return cls._celsius_to_fahrenheit(cls._kelvin_to_celsius(value))
            raise HomeAssistantError(
                UNIT_NOT_RECOGNIZED_TEMPLATE.format(to_unit, cls.UNIT_CLASS)
            )
        raise HomeAssistantError(
            UNIT_NOT_RECOGNIZED_TEMPLATE.format(from_unit, cls.UNIT_CLASS)
        )
def main():
    
    try:
        nums = input("Enter two integers separated by comma (,): ").split(",")
        num_1 = int(nums[0])
        num_2 = int(nums[1])
        print(f"greatest_common_divisor({num_1}, {num_2}) = {greatest_common_divisor(num_1, num_2)}")
        print(f"By iterative gcd({num_1}, {num_2}) = {gcd_by_iterative(num_1, num_2)}")
    except (IndexError, UnboundLocalError, ValueError):
        print("Wrong input")
def gcd_by_iterative(x: int, y: int) -> int:
    
    
    
    while y:  # --> when y=0 then loop will terminate and return x as final GCD.
        x, y = y, x % y
    return abs(x)
def native_value(self) -> str:
        
        return self.coordinator.data[DATA_KEY_ADDONS][self._addon_slug][
            self.entity_description.key
        ]
def is_config(windows=None, linux=None, osx=None, **kwargs):
    
    
    return (
        windows in (None, is_windows)
        and linux in (None, is_linux)
        and osx in (None, is_osx)
    )
def run(
        self,
        context: Dict,
    ) -> Dict:
        
        
        
        context = context.copy()
        question = self.advance_to_next_question()
        while question:
            answer = question.ask(context=context)
            context[question.key] = answer
            question = self.advance_to_next_question(answer)
        return context
def rename(self, *args, **kwargs):
        
        
        
        self.move(*args, **kwargs)
def get_password_path(self):
        
        
        if LINUX or BSD:
            app_path = os.environ.get('XDG_CONFIG_HOME') or os.path.expanduser('~/.config')
        elif MACOS:
            app_path = os.path.join(os.environ.get('HOME'), 'Library')
        elif WINDOWS:
            app_path = os.environ.get('APPDATA')
        else:
            app_path = '.'

        # Append the Glances folder
        app_path = os.path.join(app_path, 'glances')

        return app_path
def get_hash(self, plain_password, salt=''):
        
        return to_hex(hashlib.pbkdf2_hmac('sha256', plain_password.encode(), salt.encode(), 100000, dklen=128))
def DelfFeaturePostProcessing(boxes, descriptors, use_pca, pca_parameters=None):
  
  

  # Get center of descriptor boxes, corresponding to feature locations.
  locations = CalculateKeypointCenters(boxes)
  final_descriptors = PostProcessDescriptors(descriptors, use_pca,
                                             pca_parameters)

  return locations, final_descriptors
def assertExtractedParametersMatch(self, format_string, command, values):
        
        
        
        extracted_params = extract_parameters(action_alias_db=self.action_alias_db,
                                              format_str=format_string,
                                              param_stream=command)

        if extracted_params != values:
            msg = ('Extracted parameters from command string "%s" against format string "%s"'
                   ' didn\'t match the provided values: ' % (command, format_string))

            # Note: We intercept the exception so we can can include diff for the dictionaries
            try:
                self.assertEqual(extracted_params, values)
            except AssertionError as e:
                msg += str(e)

            raise AssertionError(msg)
def smacof(
    dissimilarities,
    *,
    metric=True,
    n_components=2,
    init=None,
    n_init=8,
    n_jobs=None,
    max_iter=300,
    verbose=0,
    eps=1e-3,
    random_state=None,
    return_n_iter=False,
    normalized_stress=False,
):
    
    

    dissimilarities = check_array(dissimilarities)
    random_state = check_random_state(random_state)
    if normalized_stress and metric:
        raise ValueError(
            "Normalized stress is not supported for metric MDS. Either set"
            " `normalized_stress=False` or use `metric=False`."
        )
    if hasattr(init, "__array__"):
        init = np.asarray(init).copy()
        if not n_init == 1:
            warnings.warn(
                "Explicit initial positions passed: "
                "performing only one init of the MDS instead of %d" % n_init
            )
            n_init = 1

    best_pos, best_stress = None, None

    if effective_n_jobs(n_jobs) == 1:
        for it in range(n_init):
            pos, stress, n_iter_ = _smacof_single(
                dissimilarities,
                metric=metric,
                n_components=n_components,
                init=init,
                max_iter=max_iter,
                verbose=verbose,
                eps=eps,
                random_state=random_state,
                normalized_stress=normalized_stress,
            )
            if best_stress is None or stress < best_stress:
                best_stress = stress
                best_pos = pos.copy()
                best_iter = n_iter_
    else:
        seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)
        results = Parallel(n_jobs=n_jobs, verbose=max(verbose - 1, 0))(
            delayed(_smacof_single)(
                dissimilarities,
                metric=metric,
                n_components=n_components,
                init=init,
                max_iter=max_iter,
                verbose=verbose,
                eps=eps,
                random_state=seed,
                normalized_stress=normalized_stress,
            )
            for seed in seeds
        )
        positions, stress, n_iters = zip(*results)
        best = np.argmin(stress)
        best_stress = stress[best]
        best_pos = positions[best]
        best_iter = n_iters[best]

    if return_n_iter:
        return best_pos, best_stress, best_iter
    else:
        return best_pos, best_stress
def __init__(self,
               box_specs_list,
               base_anchor_size=None,
               anchor_strides=None,
               anchor_offsets=None,
               clip_window=None):
    
    
    if isinstance(box_specs_list, list) and all(
        [isinstance(list_item, list) for list_item in box_specs_list]):
      self._box_specs = box_specs_list
    else:
      raise ValueError('box_specs_list is expected to be a '
                       'list of lists of pairs')
    if base_anchor_size is None:
      base_anchor_size = tf.constant([256, 256], dtype=tf.float32)
    self._base_anchor_size = base_anchor_size
    self._anchor_strides = anchor_strides
    self._anchor_offsets = anchor_offsets
    if clip_window is not None and clip_window.get_shape().as_list() != [4]:
      raise ValueError('clip_window must either be None or a shape [4] tensor')
    self._clip_window = clip_window
    self._scales = []
    self._aspect_ratios = []
    for box_spec in self._box_specs:
      if not all([isinstance(entry, tuple) and len(entry) == 2
                  for entry in box_spec]):
        raise ValueError('box_specs_list is expected to be a '
                         'list of lists of pairs')
      scales, aspect_ratios = zip(*box_spec)
      self._scales.append(scales)
      self._aspect_ratios.append(aspect_ratios)

    for arg, arg_name in zip([self._anchor_strides, self._anchor_offsets],
                             ['anchor_strides', 'anchor_offsets']):
      if arg and not (isinstance(arg, list) and
                      len(arg) == len(self._box_specs)):
        raise ValueError('%s must be a list with the same length '
                         'as self._box_specs' % arg_name)
      if arg and not all([
          isinstance(list_item, tuple) and len(list_item) == 2
          for list_item in arg
      ]):
        raise ValueError('%s must be a list of pairs.' % arg_name)
def _generate(self, feature_map_shape_list, im_height=1, im_width=1):
    
    
    if not (isinstance(feature_map_shape_list, list)
            and len(feature_map_shape_list) == len(self._box_specs)):
      raise ValueError('feature_map_shape_list must be a list with the same '
                       'length as self._box_specs')
    if not all([isinstance(list_item, tuple) and len(list_item) == 2
                for list_item in feature_map_shape_list]):
      raise ValueError('feature_map_shape_list must be a list of pairs.')

    im_height = tf.to_float(im_height)
    im_width = tf.to_float(im_width)

    if not self._anchor_strides:
      anchor_strides = [(1.0 / tf.to_float(pair[0]), 1.0 / tf.to_float(pair[1]))
                        for pair in feature_map_shape_list]
    else:
      anchor_strides = [(tf.to_float(stride[0]) / im_height,
                         tf.to_float(stride[1]) / im_width)
                        for stride in self._anchor_strides]
    if not self._anchor_offsets:
      anchor_offsets = [(0.5 * stride[0], 0.5 * stride[1])
                        for stride in anchor_strides]
    else:
      anchor_offsets = [(tf.to_float(offset[0]) / im_height,
                         tf.to_float(offset[1]) / im_width)
                        for offset in self._anchor_offsets]

    for arg, arg_name in zip([anchor_strides, anchor_offsets],
                             ['anchor_strides', 'anchor_offsets']):
      if not (isinstance(arg, list) and len(arg) == len(self._box_specs)):
        raise ValueError('%s must be a list with the same length '
                         'as self._box_specs' % arg_name)
      if not all([isinstance(list_item, tuple) and len(list_item) == 2
                  for list_item in arg]):
        raise ValueError('%s must be a list of pairs.' % arg_name)

    anchor_grid_list = []
    min_im_shape = tf.minimum(im_height, im_width)
    scale_height = min_im_shape / im_height
    scale_width = min_im_shape / im_width
    base_anchor_size = [
        scale_height * self._base_anchor_size[0],
        scale_width * self._base_anchor_size[1]
    ]
    for feature_map_index, (grid_size, scales, aspect_ratios, stride,
                            offset) in enumerate(
                                zip(feature_map_shape_list, self._scales,
                                    self._aspect_ratios, anchor_strides,
                                    anchor_offsets)):
      tiled_anchors = grid_anchor_generator.tile_anchors(
          grid_height=grid_size[0],
          grid_width=grid_size[1],
          scales=scales,
          aspect_ratios=aspect_ratios,
          base_anchor_size=base_anchor_size,
          anchor_stride=stride,
          anchor_offset=offset)
      if self._clip_window is not None:
        tiled_anchors = box_list_ops.clip_to_window(
            tiled_anchors, self._clip_window, filter_nonoverlapping=False)
      num_anchors_in_layer = tiled_anchors.num_boxes_static()
      if num_anchors_in_layer is None:
        num_anchors_in_layer = tiled_anchors.num_boxes()
      anchor_indices = feature_map_index * tf.ones([num_anchors_in_layer])
      tiled_anchors.add_field('feature_map_index', anchor_indices)
      anchor_grid_list.append(tiled_anchors)

    return anchor_grid_list
def __init__(self,
               is_training,
               depth_multiplier,
               min_depth,
               pad_to_multiple,
               conv_hyperparams,
               freeze_batchnorm,
               inplace_batchnorm_update,
               bifpn_min_level=3,
               bifpn_max_level=7,
               bifpn_num_iterations=3,
               bifpn_num_filters=64,
               bifpn_combine_method='fast_attention',
               use_explicit_padding=None,
               use_depthwise=None,
               use_native_resize_op=False,
               override_base_feature_extractor_hyperparams=None,
               name='EfficientDet-D0'):
    
    
    super(SSDEfficientNetB0BiFPNKerasFeatureExtractor, self).__init__(
        is_training=is_training,
        depth_multiplier=depth_multiplier,
        min_depth=min_depth,
        pad_to_multiple=pad_to_multiple,
        conv_hyperparams=conv_hyperparams,
        freeze_batchnorm=freeze_batchnorm,
        inplace_batchnorm_update=inplace_batchnorm_update,
        bifpn_min_level=bifpn_min_level,
        bifpn_max_level=bifpn_max_level,
        bifpn_num_iterations=bifpn_num_iterations,
        bifpn_num_filters=bifpn_num_filters,
        bifpn_combine_method=bifpn_combine_method,
        efficientnet_version='efficientnet-b0',
        use_explicit_padding=use_explicit_padding,
        use_depthwise=use_depthwise,
        use_native_resize_op=use_native_resize_op,
        override_base_feature_extractor_hyperparams=
        override_base_feature_extractor_hyperparams,
        name=name)
def get_ordering_field(self, field_name):
        
        
        
        try:
            field = self.lookup_opts.get_field(field_name)
            return field.name
        except FieldDoesNotExist:
            # See whether field_name is a name of a non-field
            # that allows sorting.
            if callable(field_name):
                attr = field_name
            elif hasattr(self.model_admin, field_name):
                attr = getattr(self.model_admin, field_name)
            else:
                try:
                    attr = getattr(self.model, field_name)
                except AttributeError:
                    if LOOKUP_SEP in field_name:
                        return field_name
                    raise
            if isinstance(attr, property) and hasattr(attr, "fget"):
                attr = attr.fget
            return getattr(attr, "admin_order_field", None)
def for_user(cls, user):
        
        try:
            accounts = SocialAccount.objects.filter(
                user=user,
                provider=cls.adapter.provider_id
            )
            return [cls(user=user, account=account) for account in accounts]
        except SocialAccount.DoesNotExist:
            return None
def send_build_status(self, build, commit, state, link_to_build=False):
        
        
        
        raise NotImplementedError
def setup_webhook(self, project, integration=None):
        
        
        
        raise NotImplementedError
def sync(self):
        
        
        
        remote_repositories = self.sync_repositories()
        remote_organizations, remote_repositories_organizations = self.sync_organizations()

        # Delete RemoteRepository where the user doesn't have access anymore
        # (skip RemoteRepository tied to a Project on this user)
        all_remote_repositories = remote_repositories + remote_repositories_organizations
        repository_full_names = [r.full_name for r in all_remote_repositories if r is not None]
        self.user.oauth_repositories.exclude(
            Q(full_name__in=repository_full_names) | Q(project__isnull=False)
        ).delete()

        # Delete RemoteOrganization where the user doesn't have access anymore
        organization_slugs = [o.slug for o in remote_organizations if o is not None]
        self.user.oauth_organizations.exclude(slug__in=organization_slugs).delete()
def from_examples(
        cls,
        examples: List[dict],
        embeddings: Embeddings,
        vectorstore_cls: Type[VectorStore],
        k: int = 4,
        input_keys: Optional[List[str]] = None,
        fetch_k: int = 20,
        example_keys: Optional[List[str]] = None,
        vectorstore_kwargs: Optional[dict] = None,
        **vectorstore_cls_kwargs: Any,
    ) -> MaxMarginalRelevanceExampleSelector:
        
        
        string_examples = [cls._example_to_text(eg, input_keys) for eg in examples]
        vectorstore = vectorstore_cls.from_texts(
            string_examples, embeddings, metadatas=examples, **vectorstore_cls_kwargs
        )
        return cls(
            vectorstore=vectorstore,
            k=k,
            fetch_k=fetch_k,
            input_keys=input_keys,
            example_keys=example_keys,
            vectorstore_kwargs=vectorstore_kwargs,
        )
def process(self):
         
        
        logger.info('Starting, this may take a while...')
        # from lib.queue_manager import queue_manager
        # queue_manager.debug_monitor(3)
        self._threaded_redirector("load")
        self._run_extraction()
        for thread in self._threads:
            thread.join()
        self._alignments.save()
        Utils.finalize(self._images.process_count + self._existing_count,
                       self._alignments.faces_count,
                       self._verify_output)
def align_face(self, faces, align_eyes, size, filename, padding=48):
          
        final_faces = list()
        image = faces["image"]
        landmarks = faces["landmarks"]
        detected_faces = faces["detected_faces"]
        for idx, face in enumerate(detected_faces):
            detected_face = DetectedFace()
            detected_face.from_dlib_rect(face, image)
            detected_face.landmarksXY = landmarks[idx]
            detected_face.frame_dims = image.shape[:2]
            detected_face.load_aligned(image,
                                       size=size,
                                       padding=padding,
                                       align_eyes=align_eyes)
            final_faces.append({"file_location": self.output_dir / Path(filename).stem,
                                "face": detected_face})
        faces["detected_faces"] = final_faces
def _output_faces(self, saver: Optional[ImagesSaver], extract_media: ExtractMedia) -> None:
         
        
        logger.trace("Outputting faces for %s", extract_media.filename)  # type: ignore
        final_faces = []
        filename = os.path.splitext(os.path.basename(extract_media.filename))[0]
        extension = ".png"

        for idx, face in enumerate(extract_media.detected_faces):
            output_filename = f"{filename}_{idx}{extension}"
            meta = dict(alignments=face.to_png_meta(),
                        source=dict(alignments_version=self._alignments.version,
                                    original_filename=output_filename,
                                    face_index=idx,
                                    source_filename=os.path.basename(extract_media.filename),
                                    source_is_video=self._images.is_video,
                                    source_frame_dims=extract_media.image_size))
            image = encode_image(face.aligned.face, extension, metadata=meta)

            if saver is not None:
                saver.save(output_filename, image)
            final_faces.append(face.to_alignment())
        self._alignments.data[os.path.basename(extract_media.filename)] = dict(faces=final_faces)
        del extract_media
def _output_processing(self, extract_media, size):
         
        
        for face in extract_media.detected_faces:
            face.load_aligned(extract_media.image, size=size)
            face.thumbnail = generate_thumbnail(face.aligned_face, size=80, quality=60)
        self._post_process.do_actions(extract_media)
        extract_media.remove_image()

        faces_count = len(extract_media.detected_faces)
        if faces_count == 0:
            logger.verbose("No faces were detected in image: %s",
                           os.path.basename(extract_media.filename))

        if not self._verify_output and faces_count > 1:
            self._verify_output = True
def _validate_inputs(cls,
                         filter_files: Optional[List[str]],
                         nfilter_files: Optional[List[str]]) -> Tuple[List[str], List[str]]:
         
        
        error = False
        retval: List[List[str]] = []

        for files in (filter_files, nfilter_files):

            if isinstance(files, list) and len(files) == 1 and os.path.isdir(files[0]):
                # Get images from folder, if folder passed in
                dirname = files[0]
                files = [os.path.join(dirname, fname)
                         for fname in os.listdir(dirname)
                         if os.path.splitext(fname)[-1].lower() in _image_extensions]
                logger.debug("Collected files from folder '%s': %s", dirname,
                             [os.path.basename(f) for f in files])

            filt_files = [] if files is None else files
            for file in filt_files:
                if (not os.path.isfile(file) or
                        os.path.splitext(file)[-1].lower() not in _image_extensions):
                    logger.warning("Filter file '%s' does not exist or is not an image file", file)
                    error = True
            retval.append(filt_files)

        filters = retval[0]
        nfilters = retval[1]
        f_fnames = set(os.path.basename(fname) for fname in filters)
        n_fnames = set(os.path.basename(fname) for fname in nfilters)
        if f_fnames.intersection(n_fnames):
            error = True
            logger.warning("filter and nfilter filenames should be unique. The following "
                           "filenames exist in both folders: %s", f_fnames.intersection(n_fnames))

        if error:
            logger.error("There was a problem processing filter files. See the above warnings for "
                         "details")
            sys.exit(1)
        logger.debug("filter_files: %s, nfilter_files: %s", retval[0], retval[1])

        return filters, nfilters
def get_instances(
        self,
        instance_ids: List[str] = None,
        status_filter: Set[int] = None,
    ) -> Tuple[Dict[str, Instance], int]:
        
        
        instance_ids = instance_ids or []
        status_filter = status_filter or set()
        pairs, version = self._storage.get(self._table_name, instance_ids)
        instances = {}
        for instance_id, (instance_data, entry_version) in pairs.items():
            instance = Instance()
            instance.ParseFromString(instance_data)
            instance.version = entry_version
            if status_filter and instance.status not in status_filter:
                continue
            instances[instance_id] = instance
        return instances, version
def isValid(text, parsed=None, immersiveMode=None):
    
        
    
    return any(word in text for word in ["拍照", "拍张照"])
def async_alarm_arm_away(self, code=None):
        
        
        return self.hass.loop.run_in_executor(
            None, self.alarm_arm_away, code)
def _non_max_suppression_as_is(boxes: tf.Tensor,
                               scores: tf.Tensor,
                               output_size: int,
                               iou_threshold: float = 0.5,
                               refinements: int = 0) -> tf.Tensor:
  
  
  batch_shape = boxes.shape[:-2]
  batch_size = np.prod(batch_shape, dtype=np.int32)
  boxes_size = boxes.shape[-2]
  if boxes.shape[-1] != 4:
    raise ValueError(f'Boxes shape ({boxes.shape}) last dimension must be 4 '
                     'to represent [y1, x1, y2, x2] boxes coordinates')
  if scores.shape != boxes.shape[:-1]:
    raise ValueError(f'Boxes shape ({boxes.shape}) and scores shape '
                     f'({scores.shape}) do not match.')
  order = tf.range(boxes_size, dtype=tf.float32)
  relative_order = _tensor_sum_vectors(order, -order)
  relative_scores = _tensor_sum_vectors(scores, -scores)
  similar = _greater(_tensor_product_iou(boxes) - iou_threshold)
  worse = _greater(relative_scores)
  same_later = _and(_same(relative_scores), _greater(relative_order))
  similar_worse_or_same_later = _and(similar, _or(worse, same_later))
  for _ in range(refinements):
    similar_worse_or_same_later = _refine_nms_graph_to_original_algorithm(
        similar_worse_or_same_later)
  prunable = _reduce_or(similar_worse_or_same_later, axis=-1)
  remaining = tf.constant(1.) - prunable
  scores = tf.reshape(tf.exp(scores), [1, 1, batch_size, boxes_size])
  remaining = tf.reshape(remaining, [1, 1, batch_size, boxes_size])
  # top_k runs on TPU cores, let it happen, TPU tiles implementation is slower.
  top_k = tf.math.top_k(scores * remaining, output_size)
  indices = (
      tf.cast(top_k.indices, top_k.values.dtype) * _greater(top_k.values) -
      _same(top_k.values))
  return tf.reshape(indices, batch_shape + [output_size])
def logout(request):
    
    
    
    request.session.flush()
    if hasattr(request, 'user'):
        from django.contrib.auth.models import AnonymousUser
        request.user = AnonymousUser()
def extract_patches(arr, patch_shape=8, extraction_step=1):
    

    arr_ndim = arr.ndim

    if isinstance(patch_shape, numbers.Number):
        patch_shape = tuple([patch_shape] * arr_ndim)
    if isinstance(extraction_step, numbers.Number):
        extraction_step = tuple([extraction_step] * arr_ndim)

    patch_strides = arr.strides

    slices = [slice(None, None, st) for st in extraction_step]
    indexing_strides = arr[slices].strides

    patch_indices_shape = (np.array(arr.shape) - np.array(patch_shape)) /\
        np.array(extraction_step) + 1

    shape = tuple(list(patch_indices_shape) + list(patch_shape))
    strides = tuple(list(indexing_strides) + list(patch_strides))

    patches = as_strided(arr, shape=shape, strides=strides)
    return patches
def extract_patches(arr, patch_shape=8, extraction_step=1):
    
    

    arr_ndim = arr.ndim

    if isinstance(patch_shape, numbers.Number):
        patch_shape = tuple([patch_shape] * arr_ndim)
    if isinstance(extraction_step, numbers.Number):
        extraction_step = tuple([extraction_step] * arr_ndim)

    patch_strides = arr.strides

    slices = [slice(None, None, st) for st in extraction_step]
    indexing_strides = arr[slices].strides

    patch_indices_shape = ((np.array(arr.shape) - np.array(patch_shape)) /
        np.array(extraction_step)) + 1

    shape = tuple(list(patch_indices_shape) + list(patch_shape))
    strides = tuple(list(indexing_strides) + list(patch_strides))

    patches = as_strided(arr, shape=shape, strides=strides)
    return patches
def transform(self, X):
        
        
        X = self._validate_data(
            X=X,
            ensure_2d=False,
            allow_nd=True,
            ensure_min_samples=1,
            ensure_min_features=1,
            reset=False,
        )
        random_state = check_random_state(self.random_state)
        n_imgs, img_height, img_width = X.shape[:3]
        if self.patch_size is None:
            patch_size = img_height // 10, img_width // 10
        else:
            if len(self.patch_size) != 2:
                raise ValueError(
                    f"patch_size must be a tuple of two integers. Got {self.patch_size}"
                    " instead."
                )
            patch_size = self.patch_size

        n_imgs, img_height, img_width = X.shape[:3]
        X = np.reshape(X, (n_imgs, img_height, img_width, -1))
        n_channels = X.shape[-1]

        # compute the dimensions of the patches array
        patch_height, patch_width = patch_size
        n_patches = _compute_n_patches(
            img_height, img_width, patch_height, patch_width, self.max_patches
        )
        patches_shape = (n_imgs * n_patches,) + patch_size
        if n_channels > 1:
            patches_shape += (n_channels,)

        # extract the patches
        patches = np.empty(patches_shape)
        for ii, image in enumerate(X):
            patches[ii * n_patches : (ii + 1) * n_patches] = extract_patches_2d(
                image,
                patch_size,
                max_patches=self.max_patches,
                random_state=random_state,
            )
        return patches
def _check_attn_shift(self, attn, prev_attn_peak):
        
        
        # Block the candidates that exceed the max shift
        _, attn_peak = torch.max(attn, dim=1)
        lt_cond = attn_peak <= (prev_attn_peak + self.max_attn_shift)
        mt_cond = attn_peak > (prev_attn_peak - self.max_attn_shift)

        # True if not exceed limit
        # Multiplication equals to element-wise and for tensor
        cond = (lt_cond * mt_cond).unsqueeze(1)
        return cond, attn_peak
def _get_top_score_prediction(self, hyps_and_scores, topk):
        
        
        top_hyps, top_log_probs, top_scores, top_lengths = [], [], [], []
        batch_size = len(hyps_and_scores)

        # Collect hypotheses
        for i in range(len(hyps_and_scores)):
            hyps, log_probs, scores = zip(*hyps_and_scores[i])
            top_hyps += hyps
            top_scores += scores
            top_log_probs += log_probs
            top_lengths += [len(hyp) for hyp in hyps]
        top_hyps = torch.nn.utils.rnn.pad_sequence(
            top_hyps, batch_first=True, padding_value=0
        )
        top_scores = torch.stack((top_scores), dim=0).view(batch_size, -1)
        top_lengths = torch.tensor(
            top_lengths, dtype=torch.int, device=top_scores.device
        )
        # Get topk indices
        topk_scores, indices = top_scores.topk(self.topk, dim=-1)
        indices = (indices + self.beam_offset.unsqueeze(1)).view(
            batch_size * self.topk
        )
        # Select topk hypotheses
        topk_hyps = torch.index_select(top_hyps, dim=0, index=indices,)
        topk_hyps = topk_hyps.view(batch_size, self.topk, -1)
        topk_lengths = torch.index_select(top_lengths, dim=0, index=indices,)
        topk_lengths = topk_lengths.view(batch_size, self.topk)
        topk_log_probs = [top_lengths[index.item()] for index in indices]

        return topk_hyps, topk_scores, topk_lengths, topk_log_probs
def _check_eos_threshold(self, log_probs):
        
        
        max_probs, _ = torch.max(log_probs, dim=-1)
        eos_probs = log_probs[:, self.eos_index]
        cond = eos_probs > (self.eos_threshold * max_probs)
        return cond
def forward(self, enc_states, wav_len):
        
        
        raise NotImplementedError
        return
def _model_decode(
    model, softmax, fc, inp_tokens, memory, enc_states, temperature
):
    
    
    memory = _update_mem(inp_tokens, memory)
    pred = model.decode(memory, enc_states)
    prob_dist = softmax(fc(pred) / temperature)
    return prob_dist, memory, pred[:, -1, :]
def reset_mem(self, batch_size, device):
        
        return torch.tensor([self.decoder_input_tokens] * batch_size).to(device)
def _get_top_prediction(self, hyps, scores, log_probs):
        
        
        batch_size = hyps.size(0)
        max_length = hyps.size(1)
        top_lengths = [max_length] * batch_size

        # Collect lengths of top hyps
        for pred_index in range(batch_size):
            pred = hyps[pred_index]
            pred_length = (pred == self.eos_index).nonzero(as_tuple=False)
            if len(pred_length) > 0:
                top_lengths[pred_index] = pred_length[0].item()
        # Convert lists to tensors
        top_lengths = torch.tensor(
            top_lengths, dtype=torch.float, device=hyps.device
        )

        # Pick top log probabilities
        top_log_probs = log_probs

        # Use SpeechBrain style lengths
        top_lengths = top_lengths / max_length

        return (
            hyps.unsqueeze(1),
            top_lengths.unsqueeze(1),
            scores.unsqueeze(1),
            top_log_probs.unsqueeze(1),
        )
def null_point_align(self, mobject):
        
        
        
        for m1, m2 in (self, mobject), (mobject, self):
            if m1.has_no_points() and m2.has_points():
                m2.push_self_into_submobjects()
        return self
def forwards_move_repos(apps, schema_editor):
    
    db = schema_editor.connection.alias

    # Organizations
    GithubOrganization = apps.get_model('oauth', 'GithubOrganization')
    BitbucketTeam = apps.get_model('oauth', 'BitbucketTeam')
    RemoteOrganization = apps.get_model('oauth', 'RemoteOrganization')
    for org in chunks(GithubOrganization.objects.all()):
        new_org = RemoteOrganization.objects.using(db).create(
            pub_date=org.pub_date,
            modified_date=org.modified_date,
            active=org.active,
            slug=org.login,
            name=org.name,
            email=org.email,
            url=org.html_url,
            source='github',
        )
        for user in org.users.iterator():
            new_org.users.add(user)
        try:
            data = eval(org.json)
            new_org.avatar_url = data['avatar_url']
            new_org.json = json.dumps(data)
        except:
            pass
        new_org.save()
        log.info('Migrated organization: %s', org.name)

    for org in chunks(BitbucketTeam.objects.all()):
        new_org = RemoteOrganization.objects.using(db).create(
            pub_date=org.pub_date,
            modified_date=org.modified_date,
            active=org.active,
            slug=org.login,
            name=org.name,
            email=org.email,
            url=org.html_url,
            source='bitbucket',
        )
        for user in org.users.iterator():
            new_org.users.add(user)
        try:
            new_org.json = json.dumps(eval(org.json))
        except:
            pass
        new_org.save()
        log.info('Migrated organization: %s', org.name)

    # Now repositories
    GithubProject = apps.get_model('oauth', 'GithubProject')
    BitbucketProject = apps.get_model('oauth', 'BitbucketProject')
    RemoteRepository = apps.get_model('oauth', 'RemoteRepository')

    for project in chunks(GithubProject.objects.all()):
        new_repo = RemoteRepository.objects.using(db).create(
            pub_date=project.pub_date,
            modified_date=project.modified_date,
            active=project.active,
            name=project.name,
            full_name=project.full_name,
            description=project.description,
            ssh_url=project.ssh_url,
            html_url=project.html_url,
            vcs='git',
            source='github',
        )
        for user in project.users.iterator():
            new_repo.users.add(user)
        if project.organization is not None:
            new_repo.organization = (RemoteOrganization
                                    .objects
                                    .using(db)
                                    .get(slug=project.organization.login))
        try:
            data = eval(project.json)
            new_repo.avatar_url = data.get('owner', {}).get('avatar_url', None)
            new_repo.admin = data.get('permissions', {}).get('admin', False)
            new_repo.private = data.get('private', False)
            if new_repo.private:
                new_repo.clone_url = data.get('ssh_url')
            else:
                new_repo.clone_url = data.get('clone_url')
            new_repo.json = json.dumps(data)
        except (SyntaxError, ValueError):
            pass
        new_repo.save()
        log.info('Migrated project: %s', project.name)

    for project in chunks(BitbucketProject.objects.all()):
        new_repo = RemoteRepository.objects.using(db).create(
            pub_date=project.pub_date,
            modified_date=project.modified_date,
            active=project.active,
            name=project.name,
            full_name=project.full_name,
            description=project.description,
            ssh_url=project.ssh_url,
            html_url=project.html_url,
            admin=False,
            vcs=project.vcs,
            source='bitbucket',
        )
        for user in project.users.iterator():
            new_repo.users.add(user)
        if project.organization is not None:
            new_repo.organization = (RemoteOrganization
                                    .objects
                                    .using(db)
                                    .get(slug=project.organization.login))
        try:
            data = eval(project.json)
            new_repo.avatar_url = (data.get('links', {})
                                   .get('avatar', {})
                                   .get('href', None))
            new_repo.private = data.get('is_private', False)
            new_repo.json = json.dumps(data)

            clone_urls = {location['name']: location['href']
                              for location
                              in data.get('links', {}).get('clone', {})}
            if new_repo.private:
                new_repo.clone_url = clone_urls.get('ssh', project.git_url)
            else:
                new_repo.clone_url = clone_urls.get('https', project.html_url)
        except (SyntaxError, ValueError):
            pass
        new_repo.save()
        log.info('Migrated project: %s', project.name)
def from_run_and_data_type(
        cls,
        evaluator: StringEvaluator,
        run_type: str,
        data_type: DataType,
        input_key: Optional[str] = None,
        prediction_key: Optional[str] = None,
        reference_key: Optional[str] = None,
        tags: Optional[List[str]] = None,
    ) -> StringRunEvaluatorChain:
        
        

          # noqa: E501

        # Configure how run inputs/predictions are passed to the evaluator
        if run_type == "llm":
            run_mapper: StringRunMapper = LLMStringRunMapper()
        elif run_type == "chain":
            run_mapper = ChainStringRunMapper(
                input_key=input_key, prediction_key=prediction_key
            )
        else:
            raise ValueError(
                f"Unsupported run type {run_type}. Expected one of 'llm' or 'chain'."
            )

        # Configure how example rows are fed as a reference string to the evaluator
        if reference_key is not None or data_type in (DataType.llm, DataType.chat):
            example_mapper = StringExampleMapper(reference_key=reference_key)
        elif evaluator.requires_reference:
            raise ValueError(
                f"Evaluator {evaluator.evaluation_name} requires a reference"
                " example from the dataset. Please specify the reference key from"
                " amongst the dataset outputs keys."
            )
        else:
            example_mapper = None
        return cls(
            name=evaluator.evaluation_name,
            run_mapper=run_mapper,
            example_mapper=example_mapper,
            string_evaluator=evaluator,
            tags=tags,
        )
def _print_routes(routes, host, port, ssl_enabled=False):
        
        
        

        print_lines = []
        protocol = "https" if ssl_enabled else "http"
        for route in routes:
            methods_str = "[{}]".format(", ".join(route.methods))
            output = "Mounting {} at {}://{}:{}{} {}".format(
                route.function_name, protocol, host, port, route.path, methods_str
            )
            print_lines.append(output)

            LOG.info(output)

        return print_lines
def fetch_deposits(self, code: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):
        
        
        
        self.load_markets()
        paginate = False
        paginate, params = self.handle_option_and_params(params, 'fetchDeposits', 'paginate')
        if paginate:
            return self.fetch_paginated_call_dynamic('fetchDeposits', code, since, limit, params)
        request = {}
        currency = None
        if code is not None:
            currency = self.currency(code)
            request['currency'] = currency['id']
        if limit is not None:
            request['limit'] = limit
        if since is not None:
            start = self.parse_to_int(since / 1000)
            request['from'] = start
            request['to'] = self.sum(start, 30 * 24 * 60 * 60)
        request, params = self.handle_until_option('to', request, params)
        response = self.privateWalletGetDeposits(self.extend(request, params))
        return self.parse_transactions(response, currency)
def fetch_ohlcv(self, symbol, timeframe='1m', since=None, limit=None, params={}):
        
        
        
        self.load_markets()
        market = self.market(symbol)
        price = self.safe_string(params, 'price')
        request = {}
        request, params = self.prepare_request(market, None, params)
        request['interval'] = self.timeframes[timeframe]
        method = 'publicSpotGetCandlesticks'
        maxLimit = 1000
        if market['contract']:
            maxLimit = 1999
            limit = maxLimit if (limit is None) else min(limit, maxLimit)
            if market['future']:
                method = 'publicDeliveryGetSettleCandlesticks'
            elif market['swap']:
                method = 'publicFuturesGetSettleCandlesticks'
            isMark = (price == 'mark')
            isIndex = (price == 'index')
            if isMark or isIndex:
                request['contract'] = price + '_' + market['id']
                params = self.omit(params, 'price')
        limit = maxLimit if (limit is None) else min(limit, maxLimit)
        until = self.safe_integer(params, 'until')
        if until is not None:
            until = int(until / 1000)
            params = self.omit(params, 'until')
        if since is not None:
            duration = self.parse_timeframe(timeframe)
            request['from'] = int(since / 1000)
            toTimestamp = self.sum(request['from'], limit * duration - 1)
            currentTimestamp = self.seconds()
            to = min(toTimestamp, currentTimestamp)
            if until is not None:
                request['to'] = min(to, until)
            else:
                request['to'] = to
        else:
            if until is not None:
                request['to'] = until
            request['limit'] = limit
        response = getattr(self, method)(self.extend(request, params))
        return self.parse_ohlcvs(response, market, timeframe, since, limit)
def fetch_transaction_fees(self, codes=None, params={}):
        
        
        
        self.load_markets()
        response = self.privateWalletGetWithdrawStatus(params)
        #
        #    {
        #        "currency": "MTN",
        #        "name": "Medicalchain",
        #        "name_cn": "Medicalchain",
        #        "deposit": "0",
        #        "withdraw_percent": "0%",
        #        "withdraw_fix": "900",
        #        "withdraw_day_limit": "500000",
        #        "withdraw_day_limit_remain": "500000",
        #        "withdraw_amount_mini": "900.1",
        #        "withdraw_eachtime_limit": "90000000000",
        #        "withdraw_fix_on_chains": {
        #            "ETH": "900"
        #        }
        #    }
        #
        result = {}
        withdrawFees = {}
        for i in range(0, len(response)):
            withdrawFees = {}
            entry = response[i]
            currencyId = self.safe_string(entry, 'currency')
            code = self.safe_currency_code(currencyId)
            if (codes is not None) and not self.in_array(code, codes):
                continue
            withdrawFixOnChains = self.safe_value(entry, 'withdraw_fix_on_chains')
            if withdrawFixOnChains is None:
                withdrawFees = self.safe_number(entry, 'withdraw_fix')
            else:
                chainKeys = list(withdrawFixOnChains.keys())
                for i in range(0, len(chainKeys)):
                    chainKey = chainKeys[i]
                    withdrawFees[chainKey] = self.parse_number(withdrawFixOnChains[chainKey])
            result[code] = {
                'withdraw': withdrawFees,
                'deposit': None,
                'info': entry,
            }
        return result
def fetch_tickers(self, symbols=None, params={}):
        
        
        
        self.load_markets()
        symbols = self.market_symbols(symbols)
        first = self.safe_string(symbols, 0)
        market = None
        if first is not None:
            market = self.market(first)
        type, query = self.handle_market_type_and_params('fetchTickers', market, params)
        request, requestParams = self.prepare_request(None, type, query)
        method = self.get_supported_mapping(type, {
            'spot': 'publicSpotGetTickers',
            'margin': 'publicSpotGetTickers',
            'swap': 'publicFuturesGetSettleTickers',
            'future': 'publicDeliveryGetSettleTickers',
        })
        response = getattr(self, method)(self.extend(request, requestParams))
        return self.parse_tickers(response, symbols)
def fetch_ticker(self, symbol: str, params={}):
        
        
        
        self.load_markets()
        market = self.market(symbol)
        request, query = self.prepare_request(market, None, params)
        method = self.get_supported_mapping(market['type'], {
            'spot': 'publicSpotGetTickers',
            'margin': 'publicSpotGetTickers',
            'swap': 'publicFuturesGetSettleTickers',
            'future': 'publicDeliveryGetSettleTickers',
            'option': 'publicOptionsGetTickers',
        })
        if market['option']:
            marketId = market['id']
            optionParts = marketId.split('-')
            request['underlying'] = self.safe_string(optionParts, 0)
        response = getattr(self, method)(self.extend(request, query))
        ticker = None
        if market['option']:
            for i in range(0, len(response)):
                entry = response[i]
                if entry['name'] == market['id']:
                    ticker = entry
                    break
        else:
            ticker = self.safe_value(response, 0)
        return self.parse_ticker(ticker, market)
def fetch_deposit_address(self, code: str, params={}):
        
        
        
        self.load_markets()
        currency = self.currency(code)
        rawNetwork = self.safe_string_upper(params, 'network')
        params = self.omit(params, 'network')
        request = {
            'currency': currency['id'],
        }
        response = self.privateWalletGetDepositAddress(self.extend(request, params))
        #
        #    {
        #        "currency": "XRP",
        #        "address": "rHcFoo6a9qT5NHiVn1THQRhsEGcxtYCV4d 391331007",
        #        "multichain_addresses": [
        #            {
        #                "chain": "XRP",
        #                "address": "rHcFoo6a9qT5NHiVn1THQRhsEGcxtYCV4d",
        #                "payment_id": "391331007",
        #                "payment_name": "Tag",
        #                "obtain_failed": 0
        #            }
        #        ]
        #    }
        #
        currencyId = self.safe_string(response, 'currency')
        code = self.safe_currency_code(currencyId)
        networkId = self.network_code_to_id(rawNetwork, code)
        network = None
        tag = None
        address = None
        if networkId is not None:
            addresses = self.safe_value(response, 'multichain_addresses')
            for i in range(0, len(addresses)):
                entry = addresses[i]
                entryNetwork = self.safe_string(entry, 'chain')
                if networkId == entryNetwork:
                    obtainFailed = self.safe_integer(entry, 'obtain_failed')
                    if obtainFailed:
                        break
                    address = self.safe_string(entry, 'address')
                    tag = self.safe_string(entry, 'payment_id')
                    network = self.network_id_to_code(networkId, code)
                    break
        else:
            addressField = self.safe_string(response, 'address')
            if addressField is not None:
                if addressField.find('New address is being generated for you, please wait') >= 0:
                    raise BadResponse(self.id + ' ' + 'New address is being generated for you, please wait a few seconds and try again to get the address.')
                if addressField.find(' ') >= 0:
                    splitted = addressField.split(' ')
                    address = splitted[0]
                    tag = splitted[1]
                else:
                    address = addressField
        self.check_address(address)
        return {
            'info': response,
            'code': code,  # kept here for backward-compatibility, but will be removed soon
            'currency': code,
            'address': address,
            'tag': tag,
            'network': network,
        }
def fetch_order(self, id: str, symbol: Optional[str] = None, params={}):
        
        
        
        self.load_markets()
        stop = self.safe_value_2(params, 'is_stop_order', 'stop', False)
        params = self.omit(params, ['is_stop_order', 'stop'])
        clientOrderId = self.safe_string_2(params, 'text', 'clientOrderId')
        orderId = id
        if clientOrderId is not None:
            params = self.omit(params, ['text', 'clientOrderId'])
            if clientOrderId[0] != 't':
                clientOrderId = 't-' + clientOrderId
            orderId = clientOrderId
        market = None if (symbol is None) else self.market(symbol)
        type, query = self.handle_market_type_and_params('fetchOrder', market, params)
        contract = (type == 'swap') or (type == 'future') or (type == 'option')
        request, requestParams = self.prepare_request(market, type, query) if contract else self.spot_order_prepare_request(market, stop, query)
        request['order_id'] = orderId
        methodMiddle = 'PriceOrders' if stop else 'Orders'
        method = self.get_supported_mapping(type, {
            'spot': 'privateSpotGet' + methodMiddle + 'OrderId',
            'margin': 'privateSpotGet' + methodMiddle + 'OrderId',
            'swap': 'privateFuturesGetSettle' + methodMiddle + 'OrderId',
            'future': 'privateDeliveryGetSettle' + methodMiddle + 'OrderId',
            'option': 'privateOptionsGetOrdersOrderId',
        })
        response = getattr(self, method)(self.extend(request, requestParams))
        return self.parse_order(response, market)
def edit_order(self, id: str, symbol, type, side, amount=None, price=None, params={}):
        
        
        
        self.load_markets()
        market = self.market(symbol)
        marketType, query = self.handle_market_type_and_params('editOrder', market, params)
        account = self.convert_type_to_account(marketType)
        isLimitOrder = (type == 'limit')
        if account == 'spot':
            if not isLimitOrder:
                # exchange doesn't have market orders for spot
                raise InvalidOrder(self.id + ' editOrder() does not support ' + type + ' orders for ' + marketType + ' markets')
        request = {
            'order_id': id,
            'currency_pair': market['id'],
            'account': account,
        }
        if amount is not None:
            request['amount'] = self.amount_to_precision(symbol, amount)
        if price is not None:
            request['price'] = self.price_to_precision(symbol, price)
        response = None
        if market['spot']:
            response = self.privateSpotPatchOrdersOrderId(self.extend(request, query))
        else:
            request['settle'] = market['settleId']
            response = self.privateFuturesPutSettleOrdersOrderId(self.extend(request, query))
        #
        #     {
        #         "id": "243233276443",
        #         "text": "apiv4",
        #         "create_time": "1670908873",
        #         "update_time": "1670914102",
        #         "create_time_ms": 1670908873077,
        #         "update_time_ms": 1670914102241,
        #         "status": "open",
        #         "currency_pair": "ADA_USDT",
        #         "type": "limit",
        #         "account": "spot",
        #         "side": "sell",
        #         "amount": "10",
        #         "price": "0.6",
        #         "time_in_force": "gtc",
        #         "iceberg": "0",
        #         "left": "10",
        #         "fill_price": "0",
        #         "filled_total": "0",
        #         "fee": "0",
        #         "fee_currency": "USDT",
        #         "point_fee": "0",
        #         "gt_fee": "0",
        #         "gt_maker_fee": "0",
        #         "gt_taker_fee": "0",
        #         "gt_discount": False,
        #         "rebated_fee": "0",
        #         "rebated_fee_currency": "ADA"
        #     }
        #
        return self.parse_order(response, market)
def fetch_funding_rate_history(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' fetchFundingRateHistory() requires a symbol argument')
        self.load_markets()
        market = self.market(symbol)
        if not market['swap']:
            raise BadSymbol(self.id + ' fetchFundingRateHistory() supports swap contracts only')
        request, query = self.prepare_request(market, None, params)
        if limit is not None:
            request['limit'] = limit
        method = 'publicFuturesGetSettleFundingRate'
        response = getattr(self, method)(self.extend(request, query))
        #
        #     {
        #         "r": "0.00063521",
        #         "t": "1621267200000",
        #     }
        #
        rates = []
        for i in range(0, len(response)):
            entry = response[i]
            timestamp = self.safe_timestamp(entry, 't')
            rates.append({
                'info': entry,
                'symbol': symbol,
                'fundingRate': self.safe_number(entry, 'r'),
                'timestamp': timestamp,
                'datetime': self.iso8601(timestamp),
            })
        sorted = self.sort_by(rates, 'timestamp')
        return self.filter_by_symbol_since_limit(sorted, market['symbol'], since, limit)
def lc_attributes(self) -> Dict:
        
        
        return {}
def fetch_ohlcv(self, symbol, timeframe='1m', since=None, limit=None, params={}):
        
        
        
        self.load_markets()
        market = self.market(symbol)
        marketId = market['id']
        parsedTimeframe = self.safe_integer(self.timeframes, timeframe)
        request = {
            'symbol': marketId,
        }
        if parsedTimeframe is not None:
            request['granularity'] = parsedTimeframe
        else:
            request['granularity'] = timeframe
        duration = self.parse_timeframe(timeframe) * 1000
        endAt = self.milliseconds()
        if since is not None:
            request['from'] = since
            if limit is None:
                limit = self.safe_integer(self.options, 'fetchOHLCVLimit', 200)
            endAt = self.sum(since, limit * duration)
            request['to'] = endAt
        elif limit is not None:
            since = endAt - limit * duration
            request['from'] = since
        response = self.publicGetKlineQuery(self.extend(request, params))
        #
        #    {
        #        "code": "200000",
        #        "data": [
        #            [1636459200000, 4779.3, 4792.1, 4768.7, 4770.3, 78051],
        #            [1636460100000, 4770.25, 4778.55, 4757.55, 4777.25, 80164],
        #            [1636461000000, 4777.25, 4791.45, 4774.5, 4791.3, 51555]
        #        ]
        #    }
        #
        data = self.safe_value(response, 'data', [])
        return self.parse_ohlcvs(data, market, timeframe, since, limit)
def set_margin_mode(self, marginMode: str, symbol: str = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' setMarginMode() requires a symbol argument')
        if (marginMode != '0') and (marginMode != '1') and (marginMode != 'isolated') and (marginMode != 'cross'):
            raise ArgumentsRequired(self.id + ' setMarginMode() marginMode must be 0/isolated or 1/cross')
        self.load_markets()
        if marginMode == 'isolated':
            marginMode = '0'
        if marginMode == 'cross':
            marginMode = '1'
        market = self.market(symbol)
        request = {
            'symbol': market['id'],
            'marginType': self.parse_to_int(marginMode),
        }
        return self.privatePostMarginTypeChange(request)
def tb_lineno(tb):
    
    
    return tb.tb_lineno
def print_exception(etype, value, tb, limit=None, file=None):
	
	if not file:
		file = sys.stderr
	if tb:
		_print(file, 'Traceback (most recent call last):')
		print_tb(tb, limit, file)
	lines = format_exception_only(etype, value)
	for line in lines[:-1]:
		_print(file, line, ' ')
	_print(file, lines[-1], '')
def print_exc(limit=None, file=None, chain=True):
    
    print_exception(sys.exception(), limit=limit, file=file, chain=chain)
def print_last(limit=None, file=None, chain=True):
    
    if not hasattr(sys, "last_exc") and not hasattr(sys, "last_type"):
        raise ValueError("no last exception")

    if hasattr(sys, "last_exc"):
        print_exception(sys.last_exc, limit, file, chain)
    else:
        print_exception(sys.last_type, sys.last_value, sys.last_traceback,
                        limit, file, chain)
def format_tb(tb, limit=None):
    
    return format_list(extract_tb(tb, limit))
def __init__(self, filename, lineno, name, *, lookup_line=True,
            locals=None, line=None):
        
        
        self.filename = filename
        self.lineno = lineno
        self.name = name
        self._line = line
        if lookup_line:
            self.line
        self.locals = \
            dict((k, repr(v)) for k, v in locals.items()) if locals else None
def add_column(
        self,
        header: "RenderableType" = "",
        footer: "RenderableType" = "",
        *,
        header_style: StyleType = None,
        footer_style: StyleType = None,
        style: StyleType = None,
        justify: "JustifyMethod" = "left",
        overflow: "OverflowMethod" = "ellipsis",
        width: int = None,
        min_width: int = None,
        max_width: int = None,
        ratio: int = None,
        no_wrap: bool = False,
    ) -> None:
        
        

        column = Column(
            _index=len(self.columns),
            header=header,
            footer=footer,
            header_style=header_style or "",
            footer_style=footer_style or "",
            style=style or "",
            justify=justify,
            overflow=overflow,
            width=width,
            min_width=min_width,
            max_width=max_width,
            ratio=ratio,
            no_wrap=no_wrap,
        )
        self.columns.append(column)
def grid(
        cls,
        *headers: Union[Column, str],
        padding: PaddingDimensions = 0,
        collapse_padding: bool = True,
        pad_edge: bool = False,
        expand: bool = False,
    ) -> "Table":
        
        
        return cls(
            *headers,
            box=None,
            padding=padding,
            collapse_padding=collapse_padding,
            show_header=False,
            show_footer=False,
            show_edge=False,
            pad_edge=pad_edge,
            expand=expand,
        )
def _calculate_column_widths(
        self, console: "Console", max_width: int, minimums: bool = False
    ) -> List[int]:
        
        columns = self.columns
        width_ranges = [
            self._measure_column(console, column, max_width)
            for column_index, column in enumerate(columns)
        ]
        widths = [_range.maximum or 1 for _range in width_ranges]

        get_padding_width = self._get_padding_width

        if self.expand:
            ratios = [col.ratio or 0 for col in columns if col.flexible]
            if any(ratios):
                fixed_widths = [
                    0 if column.flexible else _range.maximum
                    for _range, column in zip(width_ranges, columns)
                ]
                flex_minimum = [
                    (column.width or 1) + get_padding_width(column.index)
                    for column in columns
                    if column.flexible
                ]
                flexible_width = max_width - sum(fixed_widths)
                flex_widths = ratio_divide(flexible_width, ratios, flex_minimum)
                iter_flex_widths = iter(flex_widths)
                for index, column in enumerate(columns):
                    if column.flexible:
                        widths[index] = fixed_widths[index] + next(iter_flex_widths)
        table_width = sum(widths)

        if table_width > max_width:
            excess_width = table_width - max_width
            widths = ratio_reduce(
                excess_width,
                [0 if column.no_wrap else 1 for column in columns],
                [width_range.span for width_range in width_ranges],
                widths,
            )
            table_width = sum(widths)

        if table_width > max_width:
            excess_width = table_width - max_width
            widths = ratio_reduce(
                excess_width,
                [1 if column.no_wrap else 0 for column in columns],
                [width_range.span for width_range in width_ranges],
                widths,
            )
            table_width = sum(widths)

        if table_width > max_width:
            excess_width = table_width - max_width
            widths = ratio_reduce(
                excess_width, [1 for column in columns], widths, widths,
            )
            table_width = sum(widths)

            # flex_widths = [_range.span for _range in width_ranges]
            # excess_width = table_width - max_width

            # widths = [
            #     max(width_range.minimum + 2, width - excess_width)
            #     for width_range, width, excess_width in zip(
            #         width_ranges, widths, ratio_divide(excess_width, flex_widths)
            #     )
            # ]

            # flex_widths = [
            #     0 if column.no_wrap else 1
            #     for width_range, column in zip(width_ranges, columns)
            # ]
            # if any(flex_widths):
            #     widths = [
            #         max(width_range.minimum, width - excess_width)
            #         for width_range, width, excess_width in zip(
            #             width_ranges,
            #             widths,
            #             ratio_divide(excess_width, flex_widths),
            #         )
            #     ]
            table_width = sum(widths)
        # if table_width > max_width:
        #     excess_width = table_width - max_width
        #     flex_widths = [0 if column.no_wrap else 1 for column in columns]

        #     if any(flex_widths):
        #         widths = [
        #             max(width_range.minimum, width - excess_width)
        #             for width_range, width, excess_width in zip(
        #                 width_ranges,
        #                 widths,
        #                 ratio_divide(excess_width, flex_widths),
        #             )
        #         ]
        #     table_width = sum(widths)
        # if table_width > max_width:
        #     flex_widths = [1 for column in columns]
        #     excess_width = table_width - max_width
        #     widths = [
        #         width - excess_width
        #         for width_range, width, excess_width in zip(
        #             width_ranges, widths, ratio_divide(excess_width, flex_widths),
        #         )
        #     ]

        elif table_width < max_width and self.expand:
            pad_widths = ratio_divide(max_width - table_width, widths)
            widths = [_width + pad for _width, pad in zip(widths, pad_widths)]

        return widths
def match(self, path_pattern, *, case_sensitive=None):
        
        
        
        if not isinstance(path_pattern, PurePathBase):
            path_pattern = self.with_segments(path_pattern)
        if case_sensitive is None:
            case_sensitive = _is_case_sensitive(self.pathmod)
        sep = path_pattern.pathmod.sep
        path_parts = self.parts[::-1]
        pattern_parts = path_pattern.parts[::-1]
        if not pattern_parts:
            raise ValueError("empty pattern")
        if len(path_parts) < len(pattern_parts):
            return False
        if len(path_parts) > len(pattern_parts) and path_pattern.anchor:
            return False
        for path_part, pattern_part in zip(path_parts, pattern_parts):
            match = _compile_pattern(pattern_part, sep, case_sensitive, recursive=False)
            if match(path_part) is None:
                return False
        return True
def _select_recursive(parent_paths, dir_only, follow_symlinks, match):
    
    
    if follow_symlinks is None:
        follow_symlinks = False
    for parent_path in parent_paths:
        if match is not None:
            # If we're filtering paths through a regex, record the length of
            # the parent path. We'll pass it to match(path, pos=...) later.
            parent_len = len(str(parent_path._make_child_relpath('_'))) - 1
        paths = [parent_path._make_child_relpath('')]
        while paths:
            path = paths.pop()
            if match is None or match(str(path), parent_len):
                # Yield *directory* path that matches pattern (if any).
                yield path
            try:
                # We must close the scandir() object before proceeding to
                # avoid exhausting file descriptors when globbing deep trees.
                with path._scandir() as scandir_it:
                    entries = list(scandir_it)
            except OSError:
                pass
            else:
                for entry in entries:
                    # Handle directory entry.
                    try:
                        if entry.is_dir(follow_symlinks=follow_symlinks):
                            # Recurse into this directory.
                            paths.append(path._make_child_direntry(entry))
                            continue
                    except OSError:
                        pass

                    # Handle file entry.
                    if not dir_only:
                        # Avoid cost of making a path object for non-matching
                        # files by matching against the os.DirEntry object.
                        if match is None or match(path._direntry_str(entry), parent_len):
                            # Yield *file* path that matches pattern (if any).
                            yield path._make_child_direntry(entry)
def evenly_distribute_cpus_gpus_distributed(
    trial_runner: "trial_runner.TrialRunner",
    trial: Trial,
    result: Dict[str, Any],
    scheduler: "ResourceChangingScheduler",
) -> Union[None, PlacementGroupFactory]:
    
    

    if log_once("evenly_distribute_cpus_gpus_deprecated"):
        warnings.warn(
            "DeprecationWarning: `evenly_distribute_cpus_gpus` "
            "and `evenly_distribute_cpus_gpus_distributed` are "
            "being deprecated. Use `DistributeResources()` and "
            "`DistributeResources(add_bundles=False)` instead "
            "for equivalent functionality."
        )

    return _DistributeResourcesDistributedDefault(
        trial_runner, trial, result, scheduler
    )
def test_cache_multiple_builds_same_prev_clean():
    
    
    
    c = TestClient()
    c.save({"conanfile.py": GenConanfile("pkg", "0.1")})
    c.run("create .")
    create_out = c.out
    c.run("cache path pkg/0.1:da39a3ee5e6b4b0d3255bfef95601890afd80709")
    path1 = str(c.stdout)
    assert path1 in create_out
    c.run("create .")
    create_out = c.out
    c.run("cache path pkg/0.1:da39a3ee5e6b4b0d3255bfef95601890afd80709")
    path2 = str(c.stdout)
    assert path2 in create_out
    assert path1 == path2

    builds_folder = os.path.join(c.cache_folder, "p", "b")
    assert len(os.listdir(builds_folder)) == 1  # only one build
    c.run('cache clean')
    assert len(os.listdir(builds_folder)) == 1  # one build not cleaned
    c.run('remove * -c')
    assert len(os.listdir(builds_folder)) == 0
def add_config_arguments(parser):
    
    
    parser = _add_core_arguments(parser)

    return parser
def init_inference(model,
                   triangular_masking=True,
                   mp_size=1,
                   training_mp_size=1,
                   mpu=None,
                   ep_group=None,
                   expert_mp_group=None,
                   checkpoint=None,
                   dtype=None,
                   injection_policy=None,
                   replace_method='auto',
                   quantization_setting=None,
                   replace_with_kernel_inject=False,
                   return_tuple=True,
                   ep_size=1,
                   moe=False,
                   moe_experts=1,
                   moe_type='standard',
                   args=None,
                   enable_cuda_graph=False,
                   save_mp_checkpoint_path=None,
                   base_dir="",
                   max_tokens=1024):
    
    
    log_dist("DeepSpeed info: version={}, git-hash={}, git-branch={}".format(
        __version__,
        __git_hash__,
        __git_branch__),
             ranks=[0])

    engine = InferenceEngine(model,
                             triangular_masking,
                             mp_size,
                             training_mp_size,
                             ep_size,
                             mpu,
                             ep_group,
                             expert_mp_group,
                             checkpoint,
                             dtype,
                             injection_policy,
                             return_tuple,
                             replace_method,
                             quantization_setting,
                             replace_with_kernel_inject,
                             moe,
                             moe_experts,
                             moe_type,
                             args,
                             enable_cuda_graph,
                             save_mp_checkpoint_path,
                             base_dir,
                             max_tokens)

    return engine
def initialize(args=None,
               model: torch.nn.Module = None,
               optimizer: Optional[Union[Optimizer,
                                         DeepSpeedOptimizerCallable]] = None,
               model_parameters: Optional[torch.nn.Module] = None,
               training_data: Optional[torch.utils.data.Dataset] = None,
               lr_scheduler: Optional[Union[_LRScheduler,
                                            DeepSpeedSchedulerCallable]] = None,
               mpu=None,
               dist_init_required: Optional[bool] = None,
               collate_fn=None,
               config=None,
               config_params=None):
    
    
    log_dist("DeepSpeed info: version={}, git-hash={}, git-branch={}".format(
        __version__,
        __git_hash__,
        __git_branch__),
             ranks=[0])

    assert model is not None, "deepspeed.initialize requires a model"

    if not isinstance(model, PipelineModule):
        engine = DeepSpeedEngine(args=args,
                                 model=model,
                                 optimizer=optimizer,
                                 model_parameters=model_parameters,
                                 training_data=training_data,
                                 lr_scheduler=lr_scheduler,
                                 mpu=mpu,
                                 dist_init_required=dist_init_required,
                                 collate_fn=collate_fn,
                                 config=config,
                                 config_params=config_params)
    else:
        assert mpu is None, "mpu must be None with pipeline parallelism"
        engine = PipelineEngine(args=args,
                                model=model,
                                optimizer=optimizer,
                                model_parameters=model_parameters,
                                training_data=training_data,
                                lr_scheduler=lr_scheduler,
                                mpu=model.mpu(),
                                dist_init_required=dist_init_required,
                                collate_fn=collate_fn,
                                config=config,
                                config_params=config_params)

    return_items = [
        engine,
        engine.optimizer,
        engine.training_dataloader,
        engine.lr_scheduler
    ]
    return tuple(return_items)
def __init__(
      self,
      params: cfg.DataConfig,
      dataset_fn=tf.data.TFRecordDataset,
      decoder_fn: Optional[Callable[..., Any]] = None,
      combine_fn: Optional[Callable[..., Any]] = None,
      sample_fn: Optional[Callable[..., Any]] = None,
      parser_fn: Optional[Callable[..., Any]] = None,
      filter_fn: Optional[Callable[..., tf.Tensor]] = None,
      transform_and_batch_fn: Optional[
          Callable[
              [tf.data.Dataset, Optional[tf.distribute.InputContext]],
              tf.data.Dataset,
          ]
      ] = None,
      postprocess_fn: Optional[Callable[..., Any]] = None,
  ):
    
    
    if params.input_path and params.tfds_name:
      raise ValueError('At most one of `input_path` and `tfds_name` can be '
                       'specified, but got %s and %s.' %
                       (params.input_path, params.tfds_name))

    if (isinstance(params.input_path, cfg.base_config.Config) or
        isinstance(params.tfds_name, cfg.base_config.Config)
        ) and combine_fn is None:
      raise ValueError(
          'A combine_fn is required if `input_path` or `tfds_name` is a dict.')

    self._tfds_name = params.tfds_name
    self._tfds_data_dir = params.tfds_data_dir
    self._matched_files = None
    if not params.input_path:
      # Read dataset from TFDS.
      if not params.tfds_split:
        raise ValueError(
            '`tfds_name` is %s, but `tfds_split` is not specified.' %
            params.tfds_name)
    else:
      self._matched_files = self.get_files(params.input_path)

    self._global_batch_size = params.global_batch_size
    self._is_training = params.is_training
    self._drop_remainder = params.drop_remainder
    self._shuffle_buffer_size = params.shuffle_buffer_size
    self._cache = params.cache
    self._cycle_length = params.cycle_length
    self._block_length = params.block_length
    self._deterministic = params.deterministic
    self._sharding = params.sharding
    self._tfds_split = params.tfds_split
    self._tfds_as_supervised = params.tfds_as_supervised
    self._tfds_skip_decoding_feature = params.tfds_skip_decoding_feature

    self._dataset_fn = dataset_fn
    self._decoder_fn = decoder_fn
    self._combine_fn = combine_fn
    self._sample_fn = sample_fn
    self._parser_fn = parser_fn
    self._transform_and_batch_fn = transform_and_batch_fn
    self._postprocess_fn = postprocess_fn
    self._filter_fn = filter_fn
    self._seed = params.seed
    self._prefetch_buffer_size = (
        params.prefetch_buffer_size or tf.data.experimental.AUTOTUNE)
    self._autotune_algorithm = params.autotune_algorithm

    # When tf.data service is enabled, each data service worker should get
    # different random seeds. Thus, we set `seed` to None.
    # Sharding should also be disabled because tf data service handles how
    # each worker shard data with `processing_mode` in distribute method.
    if params.enable_tf_data_service:
      self._seed = None
      self._sharding = False

    self._enable_tf_data_service = (
        params.enable_tf_data_service and params.tf_data_service_address)
    self._tf_data_service_address = params.tf_data_service_address
    self._enable_shared_tf_data_service_between_parallel_trainers = (
        params.enable_shared_tf_data_service_between_parallel_trainers)
    self._apply_tf_data_service_before_batching = (
        params.apply_tf_data_service_before_batching)
    self._trainer_id = params.trainer_id
    if self._enable_tf_data_service:
      # Add a random seed as the tf.data service job name suffix, so tf.data
      # service doesn't reuse the previous state if TPU worker gets preempted.
      # It's necessary to add global batch size into the tf data service job
      # name because when tuning batch size with vizier and tf data service is
      # also enable, the tf data servce job name should be different for
      # different vizier trials since once batch size is changed, from the
      # tf.data perspective, the dataset is a different instance, and a
      # different job name should be used for tf data service. Otherwise, the
      # model would read tensors from the incorrect tf data service job, which
      # would causes dimension mismatch on the batch size dimension.
      self._tf_data_service_job_name = (
          f'{params.tf_data_service_job_name}_bs{params.global_batch_size}_'
          f'{self.static_randnum}')
      self._enable_round_robin_tf_data_service = params.get(
          'enable_round_robin_tf_data_service', False)
      if self._enable_shared_tf_data_service_between_parallel_trainers:
        # When shared tf.data service is enabled, only a single tf.data service
        # instance should be created and shared between parallel trainers. If
        # the global batch size is different across trainers,
        # params.apply_tf_data_service_before_batching should be set to true
        # because tf.data service with different batch sizes will be considered
        # separate tf.data service instances.
        self._tf_data_service_job_name = (
            f'{params.tf_data_service_job_name}_{self.static_randnum}')
def market(
        self,
        symbol: Annotated[
            Union[str, List[str]],
            OpenBBCustomParameter(
                description="Symbol to get data for. Multiple items allowed for provider(s): yfinance."
            ),
        ],
        start_date: Annotated[
            Union[datetime.date, None, str],
            OpenBBCustomParameter(
                description="Start date of the data, in YYYY-MM-DD format."
            ),
        ] = None,
        end_date: Annotated[
            Union[datetime.date, None, str],
            OpenBBCustomParameter(
                description="End date of the data, in YYYY-MM-DD format."
            ),
        ] = None,
        provider: Optional[Literal["fmp", "intrinio", "polygon", "yfinance"]] = None,
        **kwargs
    ) -> OBBject:
        
          # noqa: E501

        simplefilter("always", DeprecationWarning)
        warn(
            "This endpoint is deprecated; use `/index/price/historical` instead. Deprecated in OpenBB Platform V4.1 to be removed in V4.3.",
            category=DeprecationWarning,
            stacklevel=2,
        )

        return self._run(
            "/index/market",
            **filter_inputs(
                provider_choices={
                    "provider": self._get_provider(
                        provider,
                        "/index/market",
                        ("fmp", "intrinio", "polygon", "yfinance"),
                    )
                },
                standard_params={
                    "symbol": symbol,
                    "start_date": start_date,
                    "end_date": end_date,
                },
                extra_params=kwargs,
                extra_info={"symbol": {"multiple_items_allowed": ["yfinance"]}},
            )
        )
def available(
        self, provider: Optional[Literal["fmp", "yfinance"]] = None, **kwargs
    ) -> OBBject:
        
          # noqa: E501

        return self._run(
            "/index/available",
            **filter_inputs(
                provider_choices={
                    "provider": provider,
                },
                standard_params={},
                extra_params=kwargs,
            )
        )
def __init__(self, config=None, resources=None, task_history_impl=None, **kwargs):
        
        
        
        self._config = config or scheduler(**kwargs)
        self._state = SimpleTaskState(self._config.state_path)

        if task_history_impl:
            self._task_history = task_history_impl
        elif self._config.record_task_history:
            import db_task_history  # Needs sqlalchemy, thus imported here
            self._task_history = db_task_history.DbTaskHistory()
        else:
            self._task_history = history.NopHistory()
        self._resources = resources or configuration.get_config().getintdict('resources')  # TODO: Can we make this a Parameter?
        self._make_task = functools.partial(
            Task, disable_failures=self._config.disable_failures,
            disable_window=self._config.disable_window)
def device(self):
        
        if self._device is None:
            visitor = _GetSoleDeviceVisitor()
            self.device_resident_accept(visitor)
            sole_device = visitor.sole_device
            if sole_device is None:
                self._device = self._MIXED_DEVICE
            else:
                self._device = sole_device

        if self._device is self._MIXED_DEVICE:
            return None
        return self._device
def device(self):
        
        return self._device
def to_gpu(
            self,
            device=None,  # type: tp.Optional[types.CudaDeviceSpec]
    ):
        # type: (...) -> 'DeviceResident'
        

        
        cuda.check_cuda_available()
        cuda_device = cuda._get_device_or_current(device)
        device = chainer.backends.cuda.GpuDevice(cuda_device)
        visitor = _ToDeviceVisitor(
            device,
            entry_method_info=('to_gpu', {'device': device.device}),
            skip_between_cupy_devices=True)
        self.__to_device(visitor)
        return self
def device_resident_accept(self, visitor):
        
        visitor.visit_device_resident(self)
def to_intel64(self):
        # type: () -> 'DeviceResident'
        

        
        intel64.check_ideep_available()
        visitor = _ToDeviceVisitor(
            chainer.get_device(intel64.Intel64Device()),
            entry_method_info=('to_intel64', {}),
            starting_device_resident=self)
        self.__to_device(visitor)
        return self
def _get_tf_exploration_action_op(self, q_values, explore, timestep):
        
        
        epsilon = self.epsilon_schedule(timestep if timestep is not None else
                                        self.last_timestep)

        # Get the exploit action as the one with the highest logit value.
        exploit_action = tf.argmax(q_values, axis=1)

        batch_size = tf.shape(q_values)[0]
        # Mask out actions with q-value=-inf so that we don't even consider
        # them for exploration.
        random_valid_action_logits = tf.where(
            tf.equal(q_values, tf.float32.min),
            tf.ones_like(q_values) * tf.float32.min, tf.ones_like(q_values))
        random_actions = tf.squeeze(
            tf.multinomial(random_valid_action_logits, 1), axis=1)

        chose_random = tf.random_uniform(
            tf.stack([batch_size]),
            minval=0, maxval=1, dtype=epsilon.dtype) \
            < epsilon

        action = tf.cond(
            pred=tf.constant(explore, dtype=tf.bool)
            if isinstance(explore, bool) else explore,
            true_fn=(
                lambda: tf.where(chose_random, random_actions, exploit_action)
            ),
            false_fn=lambda: exploit_action)

        assign_op = tf.assign(self.last_timestep, timestep)
        with tf.control_dependencies([assign_op]):
            return action, tf.zeros_like(action, dtype=tf.float32)
def _get_main_node(self, html):
        
        
        
        body = html.body
        main_node = body.css_first('[role=main]')
        if main_node:
            return main_node

        main_node = body.css_first('main')
        if main_node:
            return main_node

        # TODO: this could be done in smarter way,
        # checking for common parents between all h nodes.
        first_header = body.css_first('h1')
        if first_header:
            return first_header.parent

        return None
def _is_section(self, tag):
        
        
        
        is_h_tag = re.match(r"h\d$", tag.tag)
        return is_h_tag or tag.tag == "header"
def _parse_section_content(self, tag, *, depth=0):
        
        
        
        contents = []
        section_found = False

        next_tag = tag
        while next_tag:
            if section_found or self._is_section(next_tag):
                section_found = True
                break

            if self._is_code_section(next_tag):
                content = self._parse_code_section(next_tag)
            elif depth <= 0 or not next_tag.child:
                content = self._parse_content(next_tag.text())
            else:
                content, section_found = self._parse_section_content(
                    tag=next_tag.child,
                    depth=depth - 1
                )

            if content:
                contents.append(content)
            next_tag = next_tag.next

        return ' '.join(contents), section_found
def _generate_domains_data(self, body):
        
        
        

        domain_data = {}
        dl_tags = body.css('dl')
        number_of_domains = 0

        for dl_tag in dl_tags:

            dt = dl_tag.css('dt')
            dd = dl_tag.css('dd')

            # len(dt) should be equal to len(dd)
            # because these tags go together.
            for title, desc in zip(dt, dd):
                try:
                    id_ = title.attributes.get('id', '')
                    if id_:
                        docstrings = self._parse_domain_tag(desc)
                        domain_data[id_] = docstrings
                        number_of_domains += 1
                    if number_of_domains >= self.max_inner_documents:
                        log.warning(
                            'Limit of inner domains exceeded. project=%s version=%s limit=%i',
                            self.project.slug, self.version.slug, self.max_inner_documents,
                        )
                        break
                except Exception:
                    log.exception('Error parsing docstring for domains')

        return domain_data
def _clean_body(self, body):
        
        
        
        nodes_to_be_removed = itertools.chain(
            # Navigation nodes
            body.css('nav'),
            body.css('[role=navigation]'),
            body.css('[role=search]'),
            # Permalinks
            body.css('.headerlink'),
            # Line numbers from code blocks, they are very noisy in contents.
            # This convention is popular in Sphinx.
            body.css(".linenos"),
            body.css(".lineno"),
        )
        for node in nodes_to_be_removed:
            node.decompose()

        return body
def recent(self):
		
		
		name = 'RECENT'
		typ, dat = self._untagged_response('OK', name)
		if dat[-1]:
			return typ, dat
		self.untagged_responses = {}
		typ, dat = self._simple_command('NOOP')
		return self._untagged_response(typ, name)
def select(self, mailbox='INBOX', readonly=False):
        
        
        self.untagged_responses = {}    # Flush old responses.
        self.is_readonly = readonly
        if readonly:
            name = 'EXAMINE'
        else:
            name = 'SELECT'
        typ, dat = self._simple_command(name, mailbox)
        if typ != 'OK':
            self.state = 'AUTH'     # Might have been 'SELECTED'
            return typ, dat
        self.state = 'SELECTED'
        if 'READ-ONLY' in self.untagged_responses \
                and not readonly:
            if __debug__:
                if self.debug >= 1:
                    self._dump_ur(self.untagged_responses)
            raise self.readonly('%s is not writable' % mailbox)
        return typ, self.untagged_responses.get('EXISTS', [None])
def open(self, host = '', port = IMAP4_PORT):
        
        
        self.host = host
        self.port = port
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.sock.connect((host, port))
        self.file = self.sock.makefile('rb')
def authenticate(self, mechanism, authobject):
		
		
		mech = string.upper(mechanism)
		cap = 'AUTH=%s' % mech
		if not cap in self.capabilities:
			raise self.error("Server doesn't allow %s authentication." % mech)
		self.literal = _Authenticator(authobject).process
		typ, dat = self._simple_command('AUTHENTICATE', mech)
		if typ != 'OK':
			raise self.error(dat)
		self.state = 'AUTH'
		return typ, dat
def append(self, mailbox, flags, date_time, message):
		
		
		name = 'APPEND'
		if not mailbox:
			mailbox = 'INBOX'
		if flags:
			if (flags[0],flags[-1]) != ('(',')'):
				flags = '(%s)' % flags
		else:
			flags = None
		if date_time:
			date_time = Time2Internaldate(date_time)
		else:
			date_time = None
		self.literal = message
		return self._simple_command(name, mailbox, flags, date_time)
def search(self, charset, *criteria):
        
        
        name = 'SEARCH'
        if charset:
            if self.utf8_enabled:
                raise IMAP4.error("Non-None charset not valid in UTF8 mode")
            typ, dat = self._simple_command(name, 'CHARSET', charset, *criteria)
        else:
            typ, dat = self._simple_command(name, *criteria)
        return self._untagged_response(typ, dat, name)
def store(self, message_set, command, flags):
		
		
		if (flags[0],flags[-1]) != ('(',')'):
			flags = '(%s)' % flags	# Avoid quoting the flags
		typ, dat = self._simple_command('STORE', message_set, command, flags)
		return self._untagged_response(typ, dat, 'FETCH')
def Time2Internaldate(date_time):

    
    

    if isinstance(date_time, (int, float)):
        tt = time.localtime(date_time)
    elif isinstance(date_time, (tuple, time.struct_time)):
        tt = date_time
    elif isinstance(date_time, str) and (date_time[0],date_time[-1]) == ('"','"'):
        return date_time        # Assume in correct format
    else:
        raise ValueError("date_time not of a known type")

    dt = time.strftime("%d-%b-%Y %H:%M:%S", tt)
    if dt[0] == '0':
        dt = ' ' + dt[1:]
    if time.daylight and tt[-1]:
        zone = -time.altzone
    else:
        zone = -time.timezone
    return '"' + dt + " %+03d%02d" % divmod(zone//60, 60) + '"'
def async_load_platform(hass, component, platform, discovered=None,
                        hass_config=None):
    
    
    did_lock = False
    if component not in hass.config.components:
        setup_lock = hass.data.get('setup_lock')
        if setup_lock and setup_lock.locked():
            did_lock = True
            yield from setup_lock.acquire()

    setup_success = True

    try:
        # Could have been loaded while waiting for lock.
        if component not in hass.config.components:
            setup_success = yield from bootstrap.async_setup_component(
                hass, component, hass_config)
    finally:
        if did_lock:
            setup_lock.release()

    # No need to fire event if we could not setup component
    if not setup_success:
        return

    data = {
        ATTR_SERVICE: EVENT_LOAD_PLATFORM.format(component),
        ATTR_PLATFORM: platform,
    }

    if discovered is not None:
        data[ATTR_DISCOVERED] = discovered

    hass.bus.async_fire(EVENT_PLATFORM_DISCOVERED, data)
def reset_session(self):
          
        logger.debug("Reset current training session")
        self.clear_session()
        session = get_config().session
        if not session.initialized:
            logger.debug("Training not running")
            print("Training not running")
            return
        msg = "Currently running training session"
        self.session = session
        self.set_session_summary(msg)
def set_help(btntype):
          
        logger.debug("Setting help")
        hlp = ""
        if btntype == "reload":
            hlp = "Load/Refresh stats for the currently training session"
        elif btntype == "clear":
            hlp = "Clear currently displayed session stats"
        elif btntype == "save":
            hlp = "Save session stats to csv"
        elif btntype == "load":
            hlp = "Load saved session stats"
        return hlp
def set_vars(self):
         
        
        return dict(selected_id=tk.StringVar(),
                    refresh_graph=get_config().tk_vars["refreshgraph"],
                    is_training=get_config().tk_vars["istraining"],
                    analysis_folder=get_config().tk_vars["analysis_folder"])
def _set_callbacks(self):
         
        
        self.vars["refresh_graph"].trace("w", self._update_current_session)
        self.vars["analysis_folder"].trace("w", self._populate_from_folder)
def attach_cluster(config_file: str,
                   start: bool,
                   use_screen: bool,
                   use_tmux: bool,
                   override_cluster_name: Optional[str],
                   no_config_cache: bool = False,
                   new: bool = False,
                   port_forward: Optional[Port_forward] = None) -> None:
    
    

    if use_tmux:
        if new:
            cmd = "tmux new"
        else:
            cmd = "tmux attach || tmux new"
    elif use_screen:
        if new:
            cmd = "screen -L"
        else:
            cmd = "screen -L -xRR"
    else:
        if new:
            raise ValueError(
                "--new only makes sense if passing --screen or --tmux")
        cmd = "$SHELL"

    exec_cluster(
        config_file,
        cmd=cmd,
        run_env="auto",
        screen=False,
        tmux=False,
        stop=False,
        start=start,
        override_cluster_name=override_cluster_name,
        no_config_cache=no_config_cache,
        port_forward=port_forward,
    )
def _get_running_head_node(
        config: Dict[str, Any],
        printable_config_file: str,
        override_cluster_name: Optional[str],
        create_if_needed: bool = False,
        _provider: Optional[NodeProvider] = None,
        _allow_uninitialized_state: bool = False,
) -> str:
    

    
    provider = _provider or _get_node_provider(config["provider"],
                                               config["cluster_name"])
    head_node_tags = {
        TAG_RAY_NODE_KIND: NODE_KIND_HEAD,
    }
    nodes = provider.non_terminated_nodes(head_node_tags)
    head_node = None
    _backup_head_node = None
    for node in nodes:
        node_state = provider.node_tags(node).get(TAG_RAY_NODE_STATUS)
        if node_state == STATUS_UP_TO_DATE:
            head_node = node
        else:
            _backup_head_node = node
            cli_logger.warning(f"Head node ({node}) is in state {node_state}.")

    if head_node is not None:
        return head_node
    elif create_if_needed:
        get_or_create_head_node(
            config,
            printable_config_file=printable_config_file,
            restart_only=False,
            no_restart=False,
            yes=True,
            override_cluster_name=override_cluster_name)
        # NOTE: `_allow_uninitialized_state` is forced to False if
        # `create_if_needed` is set to True. This is to ensure that the
        # commands executed after creation occur on an actually running
        # cluster.
        return _get_running_head_node(
            config,
            printable_config_file,
            override_cluster_name,
            create_if_needed=False,
            _allow_uninitialized_state=False)
    else:
        if _allow_uninitialized_state and _backup_head_node is not None:
            cli_logger.warning(
                f"The head node being returned: {_backup_head_node} is not "
                "`up-to-date`. If you are not debugging a startup issue "
                "it is recommended to restart this head node with: {}",
                cf.bold(f"  ray down  {printable_config_file}"))

            return _backup_head_node
        raise RuntimeError("Head node of cluster ({}) not found!".format(
            config["cluster_name"]))
def from_estimator(
        cls,
        estimator,
        X,
        y,
        *,
        groups=None,
        train_sizes=np.linspace(0.1, 1.0, 5),
        cv=None,
        scoring=None,
        exploit_incremental_learning=False,
        n_jobs=None,
        pre_dispatch="all",
        verbose=0,
        shuffle=False,
        random_state=None,
        error_score=np.nan,
        fit_params=None,
        ax=None,
        negate_score=False,
        score_name=None,
        score_type="both",
        std_display_style="fill_between",
        line_kw=None,
        fill_between_kw=None,
        errorbar_kw=None,
    ):
        
        
        check_matplotlib_support(f"{cls.__name__}.from_estimator")

        score_name = _validate_score_name(score_name, scoring, negate_score)

        train_sizes, train_scores, test_scores = learning_curve(
            estimator,
            X,
            y,
            groups=groups,
            train_sizes=train_sizes,
            cv=cv,
            scoring=scoring,
            exploit_incremental_learning=exploit_incremental_learning,
            n_jobs=n_jobs,
            pre_dispatch=pre_dispatch,
            verbose=verbose,
            shuffle=shuffle,
            random_state=random_state,
            error_score=error_score,
            return_times=False,
            fit_params=fit_params,
        )

        viz = cls(
            train_sizes=train_sizes,
            train_scores=train_scores,
            test_scores=test_scores,
            score_name=score_name,
        )
        return viz.plot(
            ax=ax,
            negate_score=negate_score,
            score_type=score_type,
            std_display_style=std_display_style,
            line_kw=line_kw,
            fill_between_kw=fill_between_kw,
            errorbar_kw=errorbar_kw,
        )
def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:
        
        
        if not self._launched:
            return tensor
        if not isinstance(tensor, Tensor):
            raise NotImplementedError(
                f"`{type(self).__name__}.all_gather` is only implemented for tensors. Given {tensor}"
            )
        if tensor.dim() == 0:
            tensor = tensor.unsqueeze(0)
        if tensor.device.type != "xla":
            tensor = tensor.to(self.root_device)

        import torch_xla.core.functions as xf
        import torch_xla.core.xla_model as xm

        return xf.all_gather(tensor) if sync_grads else xm.all_gather(tensor)
def header_encode(header_bytes, charset='iso-8859-1'):
    
    
    # Return empty headers unchanged
    if not header_bytes:
        return str(header_bytes)
    # Iterate over every byte, encoding if necessary.
    encoded = []
    for octet in header_bytes:
        encoded.append(_QUOPRI_HEADER_MAP[octet])
    # Now add the RFC chrome to each encoded chunk and glue the chunks
    # together.
    return '=?%s?q?%s?=' % (charset, EMPTYSTRING.join(encoded))
def header_decode(s):
    
    
    s = s.replace('_', ' ')
    return re.sub(r'=[a-fA-F0-9]{2}', _unquote_match, s, flags=re.ASCII)
def body_encode(body, maxlinelen=76, eol=NL):
    

    

    if maxlinelen < 4:
        raise ValueError("maxlinelen must be at least 4")
    if not body:
        return body

    # The last line may or may not end in eol, but all other lines do.
    last_has_eol = (body[-1] in '\r\n')

    # This accumulator will make it easier to build the encoded body.
    encoded_body = _body_accumulator(maxlinelen, eol)

    lines = body.splitlines()
    last_line_no = len(lines) - 1
    for line_no, line in enumerate(lines):
        last_char_index = len(line) - 1
        for i, c in enumerate(line):
            if body_check(ord(c)):
                c = quote(c)
            encoded_body.write_char(c, i==last_char_index)
        # Add an eol if input line had eol.  All input lines have eol except
        # possibly the last one.
        if line_no < last_line_no or last_has_eol:
            encoded_body.newline()

    return encoded_body.getvalue()
def pair_candles(self, pair, timeframe, limit=None, columns=None):
        
        
        params = {
            "pair": pair,
            "timeframe": timeframe,
        }
        if limit:
            params['limit'] = limit

        if columns is not None:
            params['columns'] = columns
            return self._post(
                "pair_candles",
                data=params
            )

        return self._get("pair_candles", params=params)
def create_order(self, symbol: str, type: OrderType, side: OrderSide, amount, price=None, params={}):
        
        
        
        self.load_markets()
        market = self.market(symbol)
        request = {
            'pair': market['id'],
            'type': side,
        }
        # for market buy it requires the amount of quote currency to spend
        if (type == 'market') and (side == 'buy'):
            quoteAmount = None
            createMarketBuyOrderRequiresPrice = True
            createMarketBuyOrderRequiresPrice, params = self.handle_option_and_params(params, 'createOrder', 'createMarketBuyOrderRequiresPrice', True)
            cost = self.safe_string(params, 'cost')
            params = self.omit(params, 'cost')
            if cost is not None:
                quoteAmount = self.cost_to_precision(symbol, cost)
            elif createMarketBuyOrderRequiresPrice:
                if price is None:
                    raise InvalidOrder(self.id + ' createOrder() requires the price argument for market buy orders to calculate the total cost to spend(amount * price), alternatively set the createMarketBuyOrderRequiresPrice option or param to False and pass the cost to spend in the amount argument')
                else:
                    amountString = self.number_to_string(amount)
                    priceString = self.number_to_string(price)
                    costRequest = Precise.string_mul(amountString, priceString)
                    quoteAmount = self.cost_to_precision(symbol, costRequest)
            else:
                quoteAmount = self.cost_to_precision(symbol, amount)
            request['amount'] = quoteAmount
        else:
            request['amount'] = self.amount_to_precision(symbol, amount)
        if type == 'limit':
            request['price'] = self.number_to_string(price)
        else:
            request['order_type'] = type
        response = self.privatePostPlaceOrderPair(self.extend(request, params))
        #
        #     {
        #         "id": "12978363524",
        #         "time": 1586610022259,
        #         "type": "buy",
        #         "price": "0.033934",
        #         "amount": "0.10722802",
        #         "pending": "0.10722802",
        #         "complete": False
        #     }
        #
        placedAmount = self.safe_string(response, 'amount')
        remaining = self.safe_string(response, 'pending')
        timestamp = self.safe_value(response, 'time')
        complete = self.safe_value(response, 'complete')
        status = 'closed' if complete else 'open'
        filled = None
        if (placedAmount is not None) and (remaining is not None):
            filled = Precise.string_max(Precise.string_sub(placedAmount, remaining), '0')
        return self.safe_order({
            'id': self.safe_string(response, 'id'),
            'info': response,
            'clientOrderId': None,
            'timestamp': timestamp,
            'datetime': self.iso8601(timestamp),
            'lastTradeTimestamp': None,
            'type': type,
            'side': self.safe_string(response, 'type'),
            'symbol': market['symbol'],
            'status': status,
            'price': self.safe_string(response, 'price'),
            'amount': placedAmount,
            'cost': None,
            'average': None,
            'remaining': remaining,
            'filled': filled,
            'fee': None,
            'trades': None,
        })
def fetch_closed_orders(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}) -> List[Order]:
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' fetchClosedOrders() requires a symbol argument')
        self.load_markets()
        method = 'privatePostArchivedOrdersPair'
        market = self.market(symbol)
        request = {'pair': market['id']}
        response = getattr(self, method)(self.extend(request, params))
        return self.parse_orders(response, market, since, limit)
def __init__(
        self,
        in_blocks: BlockList,
        stats: DatasetStats,
        dataset_uuid=None,
        *,
        run_by_consumer: bool,
    ):
        
        
        self._in_blocks = in_blocks
        self._in_stats = stats
        # A computed snapshot of some prefix of stages.
        self._snapshot_blocks = None
        self._snapshot_stats = None
        # Chains of stages.
        self._stages_before_snapshot = []
        self._stages_after_snapshot = []
        # Cache of optimized stages.
        self._last_optimized_stages = None

        self._dataset_uuid = dataset_uuid or uuid.uuid4().hex
        if not stats.dataset_uuid:
            stats.dataset_uuid = self._dataset_uuid

        self._run_by_consumer = run_by_consumer
def stats(self) -> DatasetStats:
        
        
        if not self._snapshot_stats:
            return DatasetStats(stages={}, parent=None)
        return self._snapshot_stats
def is_read_only(self, root_op: Optional[LogicalOperator] = None) -> bool:
        
        if root_op is None:
            root_op = self._logical_plan.dag
        return isinstance(root_op, Read) and len(root_op.input_dependencies) == 0
def execute(
        self,
        allow_clear_input_blocks: bool = True,
        force_read: bool = False,
        preserve_order: bool = False,
    ) -> BlockList:
        
        
        context = DatasetContext.get_current()
        if not ray.available_resources().get("CPU"):
            if log_once("cpu_warning"):
                logger.get_logger().warning(
                    "Warning: The Ray cluster currently does not have "
                    "any available CPUs. The Dataset job will hang unless more CPUs "
                    "are freed up. A common reason is that cluster resources are "
                    "used by Actors or Tune trials; see the following link "
                    "for more details: "
                    "https://docs.ray.io/en/master/data/dataset-internals.html#datasets-and-tune"  # noqa: E501
                )
        if not self.has_computed_output():
            if self._run_with_new_execution_backend():
                from ray.data._internal.execution.bulk_executor import BulkExecutor
                from ray.data._internal.execution.streaming_executor import (
                    StreamingExecutor,
                )
                from ray.data._internal.execution.legacy_compat import (
                    execute_to_legacy_block_list,
                )

                if context.use_streaming_executor:
                    executor = StreamingExecutor(
                        copy.deepcopy(context.execution_options)
                    )
                else:
                    executor = BulkExecutor(copy.deepcopy(context.execution_options))
                blocks = execute_to_legacy_block_list(
                    executor,
                    self,
                    allow_clear_input_blocks=allow_clear_input_blocks,
                    dataset_uuid=self._dataset_uuid,
                    preserve_order=preserve_order,
                )
                # TODO(ekl) we shouldn't need to set this in the future once we move
                # to a fully lazy execution model, unless .cache() is used. The reason
                # we need it right now is since the user may iterate over a Dataset
                # multiple times after fully executing it once.
                if not self._run_by_consumer:
                    blocks._owned_by_consumer = False
                stats = executor.get_stats()
                stats_summary_string = stats.to_summary().to_string(
                    include_parent=False
                )
                logger.get_logger(log_to_stdout=context.enable_auto_log_stats).info(
                    stats_summary_string,
                )

            else:
                blocks, stats, stages = self._optimize()

                for stage_idx, stage in enumerate(stages):
                    if allow_clear_input_blocks:
                        clear_input_blocks = self._should_clear_input_blocks(
                            blocks, stage_idx
                        )
                    else:
                        clear_input_blocks = False
                    stats_builder = stats.child_builder(stage.name)
                    blocks, stage_info = stage(
                        blocks, clear_input_blocks, self._run_by_consumer
                    )
                    if stage_info:
                        stats = stats_builder.build_multistage(stage_info)
                    else:
                        stats = stats_builder.build(blocks)
                    stats.dataset_uuid = self._dataset_uuid
                    stats_summary_string = stats.to_summary().to_string(
                        include_parent=False,
                    )
                    logger.get_logger(log_to_stdout=context.enable_auto_log_stats).info(
                        stats_summary_string,
                    )

            # Set the snapshot to the output of the final stage.
            self._snapshot_blocks = blocks
            self._snapshot_stats = stats
            self._snapshot_stats.dataset_uuid = self._dataset_uuid
            self._stages_before_snapshot += self._stages_after_snapshot
            self._stages_after_snapshot = []
        if _is_lazy(self._snapshot_blocks) and force_read:
            self._snapshot_blocks = self._snapshot_blocks.compute_to_blocklist()
        return self._snapshot_blocks
def _run_with_new_execution_backend(self) -> bool:
        
        return self._context.new_execution_backend
def deep_copy(self) -> "ExecutionPlan":
        
        
        in_blocks = self._in_blocks
        if isinstance(in_blocks, BlockList):
            in_blocks = in_blocks.copy()
        plan_copy = ExecutionPlan(
            in_blocks,
            copy.copy(self._in_stats),
            run_by_consumer=self._run_by_consumer,
        )
        if self._snapshot_blocks:
            # Copy over the existing snapshot.
            plan_copy._snapshot_blocks = self._snapshot_blocks.copy()
            plan_copy._snapshot_stats = copy.copy(self._snapshot_stats)
        plan_copy._stages_before_snapshot = self._stages_before_snapshot.copy()
        plan_copy._stages_after_snapshot = self._stages_after_snapshot.copy()
        return plan_copy
def __str__(self):
        
        frac, whole = modf(self.size * self.percent / 100.0)
        ret = curses_bars[8] * int(whole)
        if frac > 0:
            ret += curses_bars[int(frac * 8)]
            whole += 1
        ret += self.__empty_char * int(self.size - whole)
        if self.__with_text:
            ret = '{0}{1:>5}%'.format(ret, self.percent)
        return self.__pre_char + ret + self.__post_char
def to_docker_compose_yaml(
        self,
        output_path: Optional[str] = None,
        network_name: Optional[str] = None,
        include_gateway: bool = True,
    ):
        
        
        
        import yaml

        if self._build_level.value < FlowBuildLevel.GRAPH.value:
            self.build(copy_flow=False)

        output_path = output_path or 'docker-compose.yml'
        network_name = network_name or 'jina-network'

        from jina.orchestrate.deployments.config.docker_compose import (
            DockerComposeConfig,
        )

        docker_compose_dict = {
            'version': '3.3',
            'networks': {network_name: {'driver': 'bridge'}},
        }

        services = {}

        for node, v in self._deployment_nodes.items():
            if v.external or (node == 'gateway' and not include_gateway):
                continue

            docker_compose_deployment = DockerComposeConfig(
                args=v.args,
                deployments_addresses=self._get_docker_compose_deployments_addresses(),
            )
            service_configs = docker_compose_deployment.to_docker_compose_config()
            for service_name, service in service_configs:
                service['networks'] = [network_name]
                services[service_name] = service

        docker_compose_dict['services'] = services
        with open(output_path, 'w+') as fp:
            yaml.dump(docker_compose_dict, fp, sort_keys=False)

        command = (
            'docker-compose up'
            if output_path is None
            else f'docker-compose -f {output_path} up'
        )

        self.logger.info(
            f'Docker compose file has been created under {output_path}. You can use it by running `{command}`'
        )
def build(self, copy_flow: bool = False, **kwargs) -> 'Flow':
        
        
        
        if multiprocessing.get_start_method().lower() == 'fork':
            os.environ['GRPC_ENABLE_FORK_SUPPORT'] = '1'

        op_flow = copy.deepcopy(self) if copy_flow else self

        if op_flow.args.inspect == FlowInspectType.COLLECT:
            op_flow.gather_inspect(copy_flow=False)

        if GATEWAY_NAME not in op_flow._deployment_nodes:
            op_flow._add_gateway(
                needs={op_flow._last_deployment},
                graph_description=op_flow._get_graph_representation(),
                deployments_addresses=op_flow._get_deployments_addresses(),
                deployments_metadata=op_flow._get_deployments_metadata(),
                graph_conditions=op_flow._get_graph_conditions(),
                deployments_no_reduce=op_flow._get_disabled_reduce_deployments(),
                uses=op_flow.gateway_args.uses,
            )

        removed_deployments = []

        # if set no_inspect then all inspect related nodes are removed
        if op_flow.args.inspect == FlowInspectType.REMOVE:
            filtered_deployment_nodes = OrderedDict()
            for k, v in op_flow._deployment_nodes.items():
                if not v.role.is_inspect:
                    filtered_deployment_nodes[k] = v
                else:
                    removed_deployments.append(v.name)

            op_flow._deployment_nodes = filtered_deployment_nodes
            reverse_inspect_map = {
                v: k for k, v in op_flow._inspect_deployments.items()
            }
            while (
                len(op_flow._last_changed_deployment) > 0
                and len(removed_deployments) > 0
                and op_flow._last_deployment in removed_deployments
            ):
                op_flow._last_changed_deployment.pop()

        for end, deployment in op_flow._deployment_nodes.items():
            # if an endpoint is being inspected, then replace it with inspected Deployment
            # but not those inspect related node
            if op_flow.args.inspect.is_keep:
                deployment.needs = set(
                    ep
                    if deployment.role.is_inspect
                    else op_flow._inspect_deployments.get(ep, ep)
                    for ep in deployment.needs
                )
            else:
                deployment.needs = set(
                    reverse_inspect_map.get(ep, ep) for ep in deployment.needs
                )

        hanging_deployments = _hanging_deployments(op_flow)
        if hanging_deployments:
            op_flow.logger.warning(
                f'{hanging_deployments} are "floating" in this flow with no deployment receiving from them, '
                f'you may want to double check if it is intentional or some mistake'
            )
        op_flow._build_level = FlowBuildLevel.GRAPH
        if len(removed_deployments) > 0:
            # very dirty
            op_flow._deployment_nodes[GATEWAY_NAME].args.graph_description = json.dumps(
                op_flow._get_graph_representation()
            )
            op_flow._deployment_nodes[
                GATEWAY_NAME
            ].args.deployments_addresses = json.dumps(
                op_flow._get_deployments_addresses()
            )

            op_flow._deployment_nodes[GATEWAY_NAME].update_pod_args()
        return op_flow
def add(
        self,
        **kwargs,
    ) -> Union['Flow', 'AsyncFlow']:
        # implementation_stub_inject_start_add

        
        
        # implementation_stub_inject_end_add

        needs = kwargs.pop('needs', None)
        copy_flow = kwargs.pop('copy_flow', True)
        deployment_role = kwargs.get('deployment_role', DeploymentRoleType.DEPLOYMENT)

        op_flow = copy.deepcopy(self) if copy_flow else self

        # deployment naming logic
        deployment_name = kwargs.get('name', None)

        if deployment_name in op_flow._deployment_nodes:
            new_name = f'{deployment_name}{len(op_flow._deployment_nodes)}'
            self.logger.debug(
                f'"{deployment_name}" is used in this Flow already! renamed it to "{new_name}"'
            )
            deployment_name = new_name

        if not deployment_name:
            deployment_name = f'executor{len(op_flow._deployment_nodes)}'

        if not deployment_name.isidentifier():
            # hyphen - can not be used in the name
            raise ValueError(
                f'name: {deployment_name} is invalid, please follow the python variable name conventions'
            )

        # needs logic
        needs = op_flow._parse_endpoints(
            op_flow, deployment_name, needs, connect_to_last_deployment=True
        )

        # set the kwargs inherit from `Flow(kwargs1=..., kwargs2=)`
        for key, value in op_flow._common_kwargs.items():

            # do not inherit from all the argument from the flow and respect EXECUTOR_ARGS_BLACKLIST
            if key not in kwargs and key not in EXECUTOR_ARGS_BLACKLIST:
                kwargs[key] = value

        # update kwargs of this Deployment
        kwargs.update(
            dict(
                name=deployment_name,
                deployment_role=deployment_role,
                log_config=kwargs.get('log_config')
                if 'log_config' in kwargs
                else self.args.log_config,
            )
        )
        parser = set_deployment_parser()
        if deployment_role == DeploymentRoleType.GATEWAY:
            parser = set_gateway_parser()

        args = ArgNamespace.kwargs2namespace(
            kwargs, parser, True, fallback_parsers=FALLBACK_PARSERS
        )

        # deployment workspace if not set then derive from flow workspace
        if args.workspace:
            args.workspace = os.path.abspath(args.workspace)
        else:
            args.workspace = self.workspace

        args.noblock_on_start = True

        if len(needs) > 1 and args.external and args.no_reduce:
            raise ValueError(
                'External Executors with multiple needs have to do auto reduce.'
            )

        op_flow._deployment_nodes[deployment_name] = Deployment(
            args, needs, include_gateway=False
        )

        if not args.floating:
            op_flow._last_deployment = deployment_name

        return op_flow
def compute_hessian_eigenvalues(image, sigma, sorting='none',
                                mode='constant', cval=0):
    
    
    

    # Convert image to float
    image = img_as_float(image)

    # Make nD hessian
    hessian_elements = hessian_matrix(image, sigma=sigma, order='rc',
                                      mode=mode, cval=cval)

    # Correct for scale
    hessian_elements = [(sigma ** 2) * e for e in hessian_elements]

    # Compute Hessian eigenvalues
    hessian_eigenvalues = hessian_matrix_eigvals(hessian_elements)

    if sorting == 'abs':

        # Sort eigenvalues by absolute values in ascending order
        hessian_eigenvalues = _sortbyabs(hessian_eigenvalues, axis=0)

    elif sorting == 'val':

        # Sort eigenvalues by values in ascending order
        hessian_eigenvalues = np.sort(hessian_eigenvalues, axis=0)

    # Return Hessian eigenvalues
    return hessian_eigenvalues
def hessian(image, sigmas=range(1, 10, 2), scale_range=None, scale_step=None,
            beta1=None, beta2=None, alpha=0.5, beta=0.5, gamma=15,
            black_ridges=True, mode=None, cval=0):
    
    

    if mode is None:
        warn("Previously, hessian implicitly used 'constant' as the "
             "border mode when dealing with the edge of the array. The new "
             "behavior is 'reflect'. To recover the old behavior, use "
             "mode='constant'. To avoid this warning, please explicitly "
             "set the mode.", category=FutureWarning, stacklevel=2)
        mode = 'reflect'

    filtered = frangi(image, sigmas=sigmas, scale_range=scale_range,
                      scale_step=scale_step, beta1=beta1, beta2=beta2,
                      alpha=alpha, beta=beta, gamma=gamma,
                      black_ridges=black_ridges, mode=mode, cval=cval)

    filtered[filtered <= 0] = 1
    return filtered
def frangi(image, sigmas=range(1, 10, 2), scale_range=None,
           scale_step=None, alpha=0.5, beta=0.5, gamma=15,
           black_ridges=True, mode='reflect', cval=0):
    
    
    
    if scale_range is not None and scale_step is not None:
        warn('Use keyword parameter `sigmas` instead of `scale_range` and '
             '`scale_range` which will be removed in version 0.17.',
             stacklevel=2)
        sigmas = np.arange(scale_range[0], scale_range[1], scale_step)

    # Check image dimensions
    check_nD(image, [2, 3])

    # Check (sigma) scales
    sigmas = _check_sigmas(sigmas)

    # Rescale filter parameters
    alpha_sq = 2 * alpha ** 2
    beta_sq = 2 * beta ** 2
    gamma_sq = 2 * gamma ** 2

    # Get image dimensions
    ndim = image.ndim

    # Invert image to detect dark ridges on light background
    if black_ridges:
        image = invert(image)

    # Generate empty (n+1)D arrays for storing auxiliary images filtered
    # at different (sigma) scales
    filtered_array = np.zeros(sigmas.shape + image.shape)
    lambdas_array = np.zeros_like(filtered_array)

    # Filtering for all (sigma) scales
    for i, sigma in enumerate(sigmas):

        # Calculate (abs sorted) eigenvalues
        lambda1, *lambdas = compute_hessian_eigenvalues(image, sigma,
                                                        sorting='abs',
                                                        mode=mode, cval=cval)

        # Compute sensitivity to deviation from a plate-like
        # structure see equations (11) and (15) in reference [1]_
        r_a = np.inf if ndim == 2 else _divide_nonzero(*lambdas) ** 2

        # Compute sensitivity to deviation from a blob-like structure,
        # see equations (10) and (15) in reference [1]_,
        # np.abs(lambda2) in 2D, np.sqrt(np.abs(lambda2 * lambda3)) in 3D
        filtered_raw = np.abs(np.multiply.reduce(lambdas)) ** (1/len(lambdas))
        r_b = _divide_nonzero(lambda1, filtered_raw) ** 2

        # Compute sensitivity to areas of high variance/texture/structure,
        # see equation (12)in reference [1]_
        r_g = sum([lambda1 ** 2] + [lambdai ** 2 for lambdai in lambdas])

        # Compute output image for given (sigma) scale and store results in
        # (n+1)D matrices, see equations (13) and (15) in reference [1]_
        filtered_array[i] = ((1 - np.exp(-r_a / alpha_sq))
                             * np.exp(-r_b / beta_sq)
                             * (1 - np.exp(-r_g / gamma_sq)))

        lambdas_array[i] = np.max(lambdas, axis=0)

    # Remove background
    filtered_array[lambdas_array > 0] = 0

    # Return for every pixel the maximum value over all (sigma) scales
    return np.max(filtered_array, axis=0)
def convex_hull_object(image, *, connectivity=2):
    
    
    if image.ndim > 2:
        raise ValueError("Input must be a 2D image")

    if connectivity not in (1, 2):
        raise ValueError('`connectivity` must be either 1 or 2.')

    labeled_im = label(image, connectivity=connectivity, background=0)
    convex_obj = np.zeros(image.shape, dtype=bool)
    convex_img = np.zeros(image.shape, dtype=bool)

    for i in range(1, labeled_im.max() + 1):
        convex_obj = convex_hull_image(labeled_im == i)
        convex_img = np.logical_or(convex_img, convex_obj)

    return convex_img
def convex_hull_image(image, offset_coordinates=True, tolerance=1e-10,
                      include_borders=True):
    

    
    ndim = image.ndim
    if np.count_nonzero(image) == 0:
        warn("Input image is entirely zero, no valid convex hull. "
             "Returning empty image", UserWarning)
        return np.zeros(image.shape, dtype=bool)
    # In 2D, we do an optimisation by choosing only pixels that are
    # the starting or ending pixel of a row or column.  This vastly
    # limits the number of coordinates to examine for the virtual hull.
    if ndim == 2:
        coords = possible_hull(np.ascontiguousarray(image, dtype=np.uint8))
    else:
        coords = np.transpose(np.nonzero(image))
        if offset_coordinates:
            # when offsetting, we multiply number of vertices by 2 * ndim.
            # therefore, we reduce the number of coordinates by using a
            # convex hull on the original set, before offsetting.
            try:
                hull0 = ConvexHull(coords)
            except QhullError as err:
                warn(f"Failed to get convex hull image. "
                     f"Returning empty image, see error message below:\n"
                     f"{err}")
                return np.zeros(image.shape, dtype=bool)
            coords = hull0.points[hull0.vertices]

    # Add a vertex for the middle of each pixel edge
    if offset_coordinates:
        offsets = _offsets_diamond(image.ndim)
        coords = (coords[:, np.newaxis, :] + offsets).reshape(-1, ndim)

    # repeated coordinates can *sometimes* cause problems in
    # scipy.spatial.ConvexHull, so we remove them.
    coords = unique_rows(coords)

    # Find the convex hull
    try:
        hull = ConvexHull(coords)
    except QhullError as err:
        warn(f"Failed to get convex hull image. "
             f"Returning empty image, see error message below:\n"
             f"{err}")
        return np.zeros(image.shape, dtype=bool)
    vertices = hull.points[hull.vertices]

    # If 2D, use fast Cython function to locate convex hull pixels
    if ndim == 2:
        labels = grid_points_in_poly(image.shape, vertices, binarize=False)
        # If include_borders is True, we include vertices (2) and edge
        # points (3) in the mask, otherwise only the inside of the hull (1)
        mask = labels >= 1 if include_borders else labels == 1
    else:
        gridcoords = np.reshape(np.mgrid[tuple(map(slice, image.shape))],
                                (ndim, -1))

        coords_in_hull = _check_coords_in_hull(gridcoords,
                                               hull.equations, tolerance)
        mask = np.reshape(coords_in_hull, image.shape)

    return mask
def _predict_recursive(self, X, sample_weight, cluster_node):
        
        
        if cluster_node.left is None:
            # This cluster has no subcluster. Labels are just the label of the cluster.
            return np.full(X.shape[0], cluster_node.label, dtype=np.int32)

        # Determine if data points belong to the left or right subcluster
        centers = np.vstack((cluster_node.left.center, cluster_node.right.center))
        if hasattr(self, "_X_mean"):
            centers += self._X_mean

        cluster_labels = _labels_inertia_threadpool_limit(
            X,
            sample_weight,
            centers,
            self._n_threads,
            return_inertia=False,
        )
        mask = cluster_labels == 0

        # Compute the labels for each subset of the data points.
        labels = np.full(X.shape[0], -1, dtype=np.int32)

        labels[mask] = self._predict_recursive(
            X[mask], sample_weight[mask], cluster_node.left
        )

        labels[~mask] = self._predict_recursive(
            X[~mask], sample_weight[~mask], cluster_node.right
        )

        return labels
def _inertia_per_cluster(self, X, centers, labels, sample_weight):
        
        
        n_clusters = centers.shape[0]  # = 2 since centers comes from a bisection
        _inertia = _inertia_sparse if sp.issparse(X) else _inertia_dense

        inertia_per_cluster = np.empty(n_clusters)
        for label in range(n_clusters):
            inertia_per_cluster[label] = _inertia(
                X, sample_weight, centers, labels, self._n_threads, single_label=label
            )

        return inertia_per_cluster
def process_exception_by_middleware(self, exception, request):
        
        
        
        for middleware_method in self._exception_middleware:
            response = middleware_method(request, exception)
            if response:
                return response
        return None
def color_mode(self):
        
        
        color_mode_tmp = COLOR_MODE_MAP.get(self._feature.color_mode, ColorMode.ONOFF)
        if color_mode_tmp == ColorMode.COLOR_TEMP:
            self._attr_min_mireds = 1
            self._attr_max_mireds = 255

        return color_mode_tmp
def Planning(self, animation=True):
        
        
        

        self.nodeList = [self.start]
        while True:
            # Random Sampling
            if random.randint(0, 100) > self.goalSampleRate:
                rnd = [random.uniform(self.minrand, self.maxrand), random.uniform(
                    self.minrand, self.maxrand)]
            else:
                rnd = [self.end.x, self.end.y]

            # Find nearest node
            nind = self.GetNearestListIndex(self.nodeList, rnd)
            # print(nind)

            # expand tree
            nearestNode = self.nodeList[nind]
            theta = math.atan2(rnd[1] - nearestNode.y, rnd[0] - nearestNode.x)

            newNode = copy.deepcopy(nearestNode)
            newNode.x += self.expandDis * math.cos(theta)
            newNode.y += self.expandDis * math.sin(theta)
            newNode.parent = nind

            if not self.__CollisionCheck(newNode, self.obstacleList):
                continue

            self.nodeList.append(newNode)

            # check goal
            dx = newNode.x - self.end.x
            dy = newNode.y - self.end.y
            d = math.sqrt(dx * dx + dy * dy)
            if d <= self.expandDis:
                print("Goal!!")
                break

            if animation:
                self.DrawGraph(rnd)

        path = [[self.end.x, self.end.y]]
        lastIndex = len(self.nodeList) - 1
        while self.nodeList[lastIndex].parent is not None:
            node = self.nodeList[lastIndex]
            path.append([node.x, node.y])
            lastIndex = node.parent
        path.append([self.start.x, self.start.y])

        return path
def __call__(self, doc: Doc) -> Doc:
        
        
        matches = sorted(self.matcher(doc))

        for match_id, start, end in matches:
            span = Span(doc, start, end, label=match_id)
            attrs = self.attrs[span.label]
            index = self.indices[span.label]
            try:
                token = span[index]
            except IndexError:
                raise ValueError(
                    Errors.E1001.format(
                        patterns=self.matcher.get(span.label),
                        span=[t.text for t in span],
                        index=index,
                    )
                ) from None
            set_token_attrs(token, attrs)
        return doc
def __init__(
        self, vocab: Vocab, name: str = "attribute_ruler", *, validate: bool = False
    ) -> None:
        
        
        self.name = name
        self.vocab = vocab
        self.matcher = Matcher(self.vocab, validate=validate)
        self.attrs = []
        self._attrs_unnormed = []  # store for reference
        self.indices = []
def push(
    repo_full_name: str,
    object: Any,
    *,
    api_url: Optional[str] = None,
    api_key: Optional[str] = None,
    parent_commit_hash: Optional[str] = "latest",
    new_repo_is_public: bool = False,
    new_repo_description: str = "",
) -> str:
    
    
    
    client = _get_client(api_url=api_url, api_key=api_key)
    manifest_json = dumps(object)
    resp = client.push(
        repo_full_name,
        manifest_json,
        parent_commit_hash=parent_commit_hash,
        new_repo_is_public=new_repo_is_public,
        new_repo_description=new_repo_description,
    )
    commit_hash: str = resp["commit"]["commit_hash"]
    return commit_hash
def say(
        self,
        msg,
        cache=False,
        plugin="",
        onCompleted=None,
        append_history=True,
    ):
        
        
        
        append_history and self.appendHistory(1, msg, plugin=plugin)
        is_too_long = False
        pattern = r"http[s]?://.+"
        if re.match(pattern, msg):
            logger.info("内容包含URL，屏蔽后续内容")
            msg = re.sub(pattern, "", msg)
        msg = utils.stripPunctuation(msg)
        msg = msg.strip()
        if not msg:
            return
        logger.info(f"即将朗读语音：{msg}")
        if config.get("trim_too_long_text", True) and len(msg) > int(
            config.get("max_text_length", 128)
        ):
            # 文本太长，TTS 会报错
            logger.info("文本超长，需进行截断")
            # 采用截断的方案
            lines = re.split("。|！|？|\.|\!|\?|\n", msg)
            shorter_msg = ""
            if "\n" in msg:
                idx = 0
                while True:
                    shorter_msg += lines[idx]
                    idx += 1
                    if len(shorter_msg) >= config.get("max_text_length", 128):
                        break
                msg = shorter_msg
            else:
                msg = msg[0 : config.get("max_text_length", 128)]
            logger.info(f"截断后的文本：{msg}")
            is_too_long = True
        voice = ""
        cache_path = ""
        if utils.getCache(msg):
            logger.info("命中缓存，播放缓存语音")
            voice = utils.getCache(msg)
            cache_path = utils.getCache(msg)
        else:
            try:
                voice = self.tts.get_speech(msg)
                cache_path = utils.saveCache(voice, msg)
            except Exception as e:
                logger.error(f"语音合成失败：{e}", stack_info=True)
        if self.onSay:
            logger.info(cache)
            audio = "http://{}:{}/audio/{}".format(
                config.get("/server/host"),
                config.get("/server/port"),
                os.path.basename(cache_path),
            )
            logger.info(f"onSay: {msg}, {audio}")
            self.onSay(msg, audio, plugin=plugin)
            self.onSay = None
        if onCompleted is None:
            onCompleted = lambda: self._onCompleted(msg)
        self.player = Player.SoxPlayer()
        if config.get("trim_too_long_text", True) and is_too_long:
            self.player.preappendCompleted(
                lambda: self.say("后面的内容太长了，我就不念了", append_history=False)
            )
        self.player.play(voice, not cache, onCompleted)
        utils.lruCache()
def doResponse(self, query, UUID="", onSay=None, onStream=None):
        
        
        
        statistic.report(1)
        self.interrupt()
        self.appendHistory(0, query, UUID)

        if onSay:
            self.onSay = onSay

        if onStream:
            self.onStream = onStream

        if query.strip() == "":
            self.pardon()
            return

        lastImmersiveMode = self.immersiveMode

        parsed = self.doParse(query)
        if self._InGossip(query) or not self.brain.query(query, parsed):
            # 进入闲聊
            if self.nlu.hasIntent(parsed, "PAUSE") or "闭嘴" in query:
                # 停止说话
                self.player.stop()
            else:
                # 没命中技能，使用机器人回复
                if self.ai.SLUG == "openai" and self.immersiveMode not in ["geek"]:
                    stream = self.ai.stream_chat(query)
                    self.stream_say(stream, True, onCompleted=self.checkRestore)
                else:
                    msg = self.ai.chat(query, parsed)
                    self.say(msg, True, onCompleted=self.checkRestore)
        else:
            # 命中技能
            if lastImmersiveMode and lastImmersiveMode != self.matchPlugin:
                if self.player:
                    if self.player.is_playing():
                        logger.debug("等说完再checkRestore")
                        self.player.appendOnCompleted(lambda: self.checkRestore())
                else:
                    logger.debug("checkRestore")
                    self.checkRestore()
def activeListen(self, silent=False):
        
        
        
        if self.immersiveMode:
            self.player.stop()
        elif self.player.is_playing():
            self.player.join()  # 确保所有音频都播完
        logger.info("进入主动聆听...")
        try:
            if not silent:
                self.lifeCycleHandler.onWakeup()
            listener = snowboydecoder.ActiveListener(
                [constants.getHotwordModel(config.get("hotword", "wukong.pmdl"))]
            )
            voice = listener.listen(
                silent_count_threshold=config.get("silent_threshold", 15),
                recording_timeout=config.get("recording_timeout", 5) * 4,
            )
            if not silent:
                self.lifeCycleHandler.onThink()
            if voice:
                query = self.asr.transcribe(voice)
                utils.check_and_delete(voice)
                return query
            return ""
        except Exception as e:
            logger.error(f"主动聆听失败：{e}", stack_info=True)
            traceback.print_exc()
            return ""
def vat_id(self) -> str:

        vat_id_random_section = (
            '#######'
        )

        vat_id_possible_initial_numbers = (
            '0',
            '1'
        )
        
        
        
        generated_initial_number = self.random_element(vat_id_possible_initial_numbers)
        vat_without_check = self.bothify(generated_initial_number + vat_id_random_section)
        vat_as_int = int(vat_without_check)
        vat_check = 97 - (vat_as_int % 97)
        vat_check_str = f"{vat_check:0>2}"

        return "BE" + vat_without_check + vat_check_str
def _device_count_nvml() -> int:
    
    
    visible_devices = _parse_visible_devices()
    if not visible_devices:
        return 0
    try:
        if type(visible_devices[0]) is str:
            # Skip MIG parsing
            if visible_devices[0].startswith("MIG-"):
                return -1
            uuids = _raw_device_uuid_nvml()
            if uuids is None:
                return -1
            visible_devices = _transform_uuid_to_ordinals(cast(List[str], visible_devices), uuids)
        else:
            raw_cnt = _raw_device_count_nvml()
            if raw_cnt <= 0:
                return raw_cnt
            # Trim the list up to a maximum available device
            for idx, val in enumerate(visible_devices):
                if cast(int, val) >= raw_cnt:
                    return idx
    except OSError:
        return -1
    except AttributeError:
        return -1
    return len(visible_devices)
def is_cuda_available() -> bool:
    
    # We set `PYTORCH_NVML_BASED_CUDA_CHECK=1` in lightning.fabric.__init__.py
    return torch.cuda.is_available()
def prepare_filter(cursor, sorting_fields, sorting_direction):
    
    
    
    filter_kwargs = Q()
    for field_id in range(len(sorting_fields)):
        field_expression = {}
        for val_id in range(len(cursor[:field_id])):
            field_expression[sorting_fields[val_id]] = cursor[val_id]
        field_expression[f"{sorting_fields[field_id]}__{sorting_direction}"] = cursor[
            field_id
        ]
        filter_kwargs |= Q(**field_expression)
    return filter_kwargs
def prepare_filter(cursor, sorting_fields, sorting_direction):
    
    
    filter_kwargs = Q()
    for index, field_name in enumerate(sorting_fields):
        field_expression = {}
        for cursor_id, cursor_value in enumerate(cursor[:index]):
            field_expression[sorting_fields[cursor_id]] = cursor_value
        field_expression[f"{field_name}__{sorting_direction}"] = cursor[index]
        filter_kwargs |= Q(**field_expression)
    return filter_kwargs
def __init__(self, measured, ideals, sloppy_input=False,
        is_reciprocal=True,name=None, self_calibration=False,*args, **kwargs):
        


        

        # allow them to pass di
        # lets make an ideal flush thru for them :
        if hasattr(measured, 'keys'):
            measured = measured.values()
            if sloppy_input == False:
                warn('dictionary passed, sloppy_input automatically activated')
                sloppy_input = True

        if hasattr(ideals, 'keys'):
            ideals = ideals.values()
            if sloppy_input == False:
                warn('dictionary passed, sloppy_input automatically activated')
                sloppy_input = True

        # fill measured and ideals with copied lists of input
        self.measured = [ntwk.copy() for ntwk in measured]
        self.ideals = [ntwk.copy() for ntwk in ideals]

        self.sloppy_input=sloppy_input
        if sloppy_input:
            self.measured, self.ideals = \
                align_measured_ideals(self.measured, self.ideals)

        self.self_calibration = self_calibration
        if self_calibration == False and len(self.measured) != len(self.ideals):
            raise(IndexError('The length of measured and ideals lists are different. Number of ideals must equal the number of measured. If you are using `sloppy_input` ensure the names are uniquely alignable.'))


        # ensure all the measured Networks' frequency's are the same
        for measure in self.measured:
            if self.measured[0].frequency != measure.frequency:
                raise(ValueError('measured Networks dont have matching frequencies.'))
        # ensure that all ideals have same frequency of the measured
        # if not, then attempt to interpolate
        for k in list(range(len(self.ideals))):
            if self.ideals[k].frequency != self.measured[0].frequency:
                print('Warning: Frequency information doesn\'t match on ideals[{}], attempting to interpolate the ideal[{}] Network ..'.format(k,k))
                try:
                    # try to resample our ideals network to match
                    # the measurement frequency
                    self.ideals[k].interpolate_self(\
                        self.measured[0].frequency)
                    print('Success')

                except:
                    raise(IndexError('Failed to interpolate. Check frequency of ideals[{}].'.format(k)))


        # passed to calibration algorithm in run()
        self.kwargs = kwargs
        self.name = name
        self.is_reciprocal = is_reciprocal

        # initialized internal properties to None
        self._residual_ntwks = None
        self._caled_ntwks =None
        self._caled_ntwk_sets = None
def coefs_8term(self):
        
        


        

        d = self.coefs
        if all([k in d.keys() for k in coefs_list_3term]):
            raise ValueError("Can't convert one port error terms to two port error terms")

        # Check if we have all 12-term keys and convert to 8-term if we do.
        if all([k in d.keys() for k in coefs_list_12term]):
            return convert_12term_2_8term(d)

        return d
def convert_12term_2_8term(coefs_12term, redundant_k = False):
    
    

    

    # Nomenclature taken from Roger Marks
    Edf = coefs_12term['forward directivity']
    Esf = coefs_12term['forward source match']
    Erf = coefs_12term['forward reflection tracking']
    Etf = coefs_12term['forward transmission tracking']
    Elf = coefs_12term['forward load match']
    Eif = coefs_12term.get('forward isolation',0)

    Edr = coefs_12term['reverse directivity']
    Esr = coefs_12term['reverse source match']
    Err = coefs_12term['reverse reflection tracking']
    Elr = coefs_12term['reverse load match']
    Etr = coefs_12term['reverse transmission tracking']
    Eir = coefs_12term.get('reverse isolation',0)

    # these are given in eq (30) - (33) in Roger Mark's paper listed in
    # the docstring
    # NOTE: k = e10/e23 = alpha/beta
    #   the 'k' nomenclature is from Soares Speciale
    gamma_f = (Elf - Esr)/(Err + Edr*(Elf - Esr))
    gamma_r = (Elr - Esf)/(Erf + Edf*(Elr - Esf))

    k_first  =   Etf/(Err + Edr*(Elf  - Esr) )
    k_second =1/(Etr/(Erf + Edf *(Elr - Esf)))
    k = (k_first + k_second)/2.
    coefs_8term = {}
    for l in ['forward directivity','forward source match',
        'forward reflection tracking','reverse directivity',
        'reverse reflection tracking','reverse source match',
        'forward isolation', 'reverse isolation']:
        coefs_8term[l] = coefs_12term[l].copy()

    coefs_8term['forward switch term'] = gamma_f
    coefs_8term['reverse switch term'] = gamma_r
    coefs_8term['k'] = k
    if redundant_k:
        coefs_8term['k first'] = k_first
        coefs_8term['k second'] = k_second
    return coefs_8term
def apply_cal_to_list(self, ntwk_list):
        
        
        
        if hasattr(ntwk_list, 'keys'):
            return {k: self.apply_cal(nw) for k, nw in ntwk_list.items()}
        else:
            return [self.apply_cal(k) for k in ntwk_list]
def remove_and_cal(self, std):
        
        

        
        measured, ideals = copy(self.measured), copy(self.ideals)
        i,m  = self.pop(std)
        self.run()
        c = self.apply_cal(m)
        self.measured = measured
        self.ideals = ideals
        self.run()
        return c,i
def __init__(self, measured, ideals, switch_terms=None, isolation=None,
            z0=50, match_fit='l', *args, **kwargs):
        
        
        

        self.z0 = z0
        # TODO: Second port not implemented.
        self.match_port = 0
        # Maximum frequency to assume that open behaves like ideal capacitor when
        # using match_fit == 'lc'.
        self.lc_fit_c_freq = kwargs.get('lc_fit_c_freq', float('inf'))

        self.match_fit = match_fit
        if self.match_port not in [0, 1]:
            raise ValueError('match_port must be either 0 or 1.')

        super().__init__(
            measured = measured,
            ideals = ideals,
            switch_terms = switch_terms,
            isolation = isolation,
            *args, **kwargs)
def score(self, examples: Iterable[Example]) -> Dict[str, Any]:
        
        
        scores = {}
        if hasattr(self.nlp.tokenizer, "score"):
            scores.update(self.nlp.tokenizer.score(examples, **self.cfg))
        for name, component in self.nlp.pipeline:
            if hasattr(component, "score"):
                scores.update(component.score(examples, **self.cfg))
        return scores
def score_deps(
        examples: Iterable[Example],
        attr: str,
        *,
        getter: Callable[[Token, str], Any] = getattr,
        head_attr: str = "head",
        head_getter: Callable[[Token, str], Token] = getattr,
        ignore_labels: Tuple[str] = tuple(),
        **cfg,
    ) -> Dict[str, Any]:
        
        
        unlabelled = PRFScore()
        labelled = PRFScore()
        labelled_per_dep = dict()
        for example in examples:
            gold_doc = example.reference
            pred_doc = example.predicted
            align = example.alignment
            gold_deps = set()
            gold_deps_per_dep = {}
            for gold_i, token in enumerate(gold_doc):
                dep = getter(token, attr)
                head = head_getter(token, head_attr)
                if dep not in ignore_labels:
                    gold_deps.add((gold_i, head.i, dep))
                    if dep not in labelled_per_dep:
                        labelled_per_dep[dep] = PRFScore()
                    if dep not in gold_deps_per_dep:
                        gold_deps_per_dep[dep] = set()
                    gold_deps_per_dep[dep].add((gold_i, head.i, dep))
            pred_deps = set()
            pred_deps_per_dep = {}
            for token in pred_doc:
                if token.orth_.isspace():
                    continue
                if align.x2y.lengths[token.i] != 1:
                    gold_i = None
                else:
                    gold_i = align.x2y[token.i].dataXd[0, 0]
                dep = getter(token, attr)
                head = head_getter(token, head_attr)
                if dep not in ignore_labels and token.orth_.strip():
                    if align.x2y.lengths[head.i] == 1:
                        gold_head = align.x2y[head.i].dataXd[0, 0]
                    else:
                        gold_head = None
                    # None is indistinct, so we can't just add it to the set
                    # Multiple (None, None) deps are possible
                    if gold_i is None or gold_head is None:
                        unlabelled.fp += 1
                        labelled.fp += 1
                    else:
                        pred_deps.add((gold_i, gold_head, dep))
                        if dep not in labelled_per_dep:
                            labelled_per_dep[dep] = PRFScore()
                        if dep not in pred_deps_per_dep:
                            pred_deps_per_dep[dep] = set()
                        pred_deps_per_dep[dep].add((gold_i, gold_head, dep))
            labelled.score_set(pred_deps, gold_deps)
            for dep in labelled_per_dep:
                labelled_per_dep[dep].score_set(
                    pred_deps_per_dep.get(dep, set()), gold_deps_per_dep.get(dep, set())
                )
            unlabelled.score_set(
                set(item[:2] for item in pred_deps), set(item[:2] for item in gold_deps)
            )
        return {
            f"{attr}_uas": unlabelled.fscore,
            f"{attr}_las": labelled.fscore,
            f"{attr}_las_per_type": {
                k: v.to_dict() for k, v in labelled_per_dep.items()
            },
        }
def score_token_attr(
        examples: Iterable[Example],
        attr: str,
        *,
        getter: Callable[[Token, str], Any] = getattr,
        missing_values: Set[Any] = MISSING_VALUES,
        **cfg,
    ) -> Dict[str, Any]:
        
        
        tag_score = PRFScore()
        for example in examples:
            gold_doc = example.reference
            pred_doc = example.predicted
            align = example.alignment
            gold_tags = set()
            missing_indices = set()
            for gold_i, token in enumerate(gold_doc):
                value = getter(token, attr)
                if value not in missing_values:
                    gold_tags.add((gold_i, getter(token, attr)))
                else:
                    missing_indices.add(gold_i)
            pred_tags = set()
            for token in pred_doc:
                if token.orth_.isspace():
                    continue
                if align.x2y.lengths[token.i] == 1:
                    gold_i = align.x2y[token.i].dataXd[0, 0]
                    if gold_i not in missing_indices:
                        pred_tags.add((gold_i, getter(token, attr)))
            tag_score.score_set(pred_tags, gold_tags)
        score_key = f"{attr}_acc"
        if len(tag_score) == 0:
            return {score_key: None}
        else:
            return {score_key: tag_score.fscore}
def scores(self):
        
        
        return {
            "uas": self.uas,
            "las": self.las,
            "las_per_type": self.las_per_type,
            "ents_p": self.ents_p,
            "ents_r": self.ents_r,
            "ents_f": self.ents_f,
            "ents_per_type": self.ents_per_type,
            "tags_acc": self.tags_acc,
            "pos_acc": self.pos_acc,
            "morphs_acc": self.morphs_acc,
            "morphs_per_type": self.morphs_per_type,
            "sent_p": self.sent_p,
            "sent_r": self.sent_r,
            "sent_f": self.sent_f,
            "token_acc": self.token_acc,
            "textcat_f": self.textcat_f,
            "textcat_auc": self.textcat_auc,
            "textcats_f_per_cat": self.textcats_f_per_cat,
            "textcats_auc_per_cat": self.textcats_auc_per_cat,
        }
def call(self, x, axis=None):
    
    
    if axis is None:
      axis = [1, 2]
    return gem(x, power=self.power, eps=self.eps, axis=axis)
def main(argv=None, save_main_session=True):
  
  

  args, pipeline_args = parse_args(argv)

  pipeline_options = beam.options.pipeline_options.PipelineOptions(
            pipeline_args)
  pipeline_options.view_as(
      beam.options.pipeline_options.SetupOptions).save_main_session = (
          save_main_session)

  dirname = os.path.dirname(args.detection_output_tfrecord)
  tf.io.gfile.makedirs(dirname)

  p = beam.Pipeline(options=pipeline_options)

  construct_pipeline(
      p,
      args.detection_input_tfrecord,
      args.detection_output_tfrecord,
      args.detection_model_dir,
      args.confidence_threshold,
      args.num_shards)

  p.run()
def extract_data(
        query: SecCompanyFilingsQueryParams,  # pylint: disable=unused-argument
        credentials: Optional[Dict[str, str]],
        **kwargs: Any,
    ) -> List[Dict]:
        
        filings = pd.DataFrame()

        if query.symbol and not query.cik:
            query.cik = symbol_map(query.symbol.lower(), use_cache=query.use_cache)
            if not query.cik:
                return []
        if query.cik is None:
            return []

        # The leading 0s need to be inserted but are typically removed from the data to store as an integer.
        if len(query.cik) != 10:  # type: ignore
            cik_: str = ""
            temp = 10 - len(query.cik)  # type: ignore
            for i in range(temp):
                cik_ = cik_ + "0"
            query.cik = cik_ + str(query.cik)  # type: ignore

        url = f"https://data.sec.gov/submissions/CIK{query.cik}.json"
        r = (
            requests.get(url, headers=HEADERS, timeout=5)
            if query.use_cache is False
            else sec_session_company_filings.get(url, headers=HEADERS, timeout=5)
        )
        if r.status_code == 200:
            data = r.json()
            filings = pd.DataFrame.from_records(data["filings"]["recent"])
            if len(filings) >= 1000:
                new_urls = pd.DataFrame(data["filings"]["files"])
                for i in new_urls.index:
                    new_cik: str = data["filings"]["files"][i]["name"]
                    new_url: str = "https://data.sec.gov/submissions/" + new_cik
                    r_ = (
                        requests.get(new_url, headers=HEADERS, timeout=5)
                        if query.use_cache is False
                        else sec_session_company_filings.get(
                            new_url, headers=HEADERS, timeout=5
                        )
                    )
                    if r_.status_code == 200:
                        data_ = r_.json()
                        additional_data = pd.DataFrame.from_records(data_)
                        filings = pd.concat([filings, additional_data], axis=0)
        cols = [
            "reportDate",
            "filingDate",
            "acceptanceDateTime",
            "act",
            "form",
            "items",
            "primaryDocDescription",
            "primaryDocument",
            "accessionNumber",
            "fileNumber",
            "filmNumber",
            "isInlineXBRL",
            "isXBRL",
            "size",
        ]
        filings = (
            pd.DataFrame(filings, columns=cols)
            .fillna(value="N/A")
            .replace("N/A", None)
            .astype(str)
        )
        filings = filings.sort_values(by=["reportDate", "filingDate"], ascending=False)
        base_url = f"https://www.sec.gov/Archives/edgar/data/{query.cik}/"
        filings["primaryDocumentUrl"] = (
            base_url
            + filings["accessionNumber"].str.replace("-", "")
            + "/"
            + filings["primaryDocument"]
        )
        filings["completeSubmissionUrl"] = (
            base_url + filings["accessionNumber"] + ".txt"
        )
        filings["filingDetailUrl"] = (
            base_url + filings["accessionNumber"] + "-index.htm"
        )
        if "type" in query.model_dump() and query.type is not None:
            filings = filings[filings["form"] == query.type]

        if "limit" in query.model_dump():
            filings = filings.head(query.limit) if query.limit != 0 else filings

        return filings.to_dict("records")
def __init__(
        self,
        build_context: BuildContext,
        package_context: PackageContext,
        deploy_context: DeployContext,
        sync_context: SyncContext,
    ):
        
        
        self._build_context = build_context
        self._package_context = package_context
        self._deploy_context = deploy_context
        self._sync_context = sync_context

        self._code_sync_resources = set()

        session = Session(profile_name=self._deploy_context.profile, region_name=self._deploy_context.region)
        self._cfn_client = self._boto_client("cloudformation", session)
        self._s3_client = self._boto_client("s3", session)
def _remove_resource_field(
        self,
        resource_logical_id: str,
        resource_type: str,
        resource_dict: Dict,
        linked_resources: Optional[Set[str]] = None,
        built_resource_dict: Optional[Dict] = None,
    ) -> Optional[str]:
        
        
        
        linked_resources = linked_resources or set()
        processed_logical_id = None

        if resource_type == AWS_LAMBDA_FUNCTION:
            for field in LAMBDA_FUNCTION_REMOVAL_MAP.get(resource_type, {}).get("Code", []):
                # We sanitize only if the provided resource is local
                # Lambda function's Code property accepts dictionary values
                if (
                    built_resource_dict
                    and isinstance(built_resource_dict.get("Properties", {}).get("Code"), dict)
                    and is_local_path(built_resource_dict.get("Properties", {}).get("Code", {}).get(field, None))
                ) or resource_logical_id in linked_resources:
                    resource_dict.get("Properties", {}).get("Code", {}).pop(field, None)
                    processed_logical_id = resource_logical_id
                # SAM templates also accepts local paths for AWS::Lambda::Function's Code property
                # Which will be transformed into a dict containing S3Bucket and S3Key after packaging
                if (
                    built_resource_dict
                    and isinstance(built_resource_dict.get("Properties", {}).get("Code"), str)
                    and is_local_path(built_resource_dict.get("Properties", {}).get("Code"))
                ):
                    resource_dict.get("Properties", {}).get("Code", {}).pop("S3Bucket", None)
                    resource_dict.get("Properties", {}).get("Code", {}).pop("S3Key", None)
                    processed_logical_id = resource_logical_id
        else:
            for field in GENERAL_REMOVAL_MAP.get(resource_type, []):
                if resource_type in SYNCABLE_STACK_RESOURCES:
                    if not isinstance(resource_dict.get("Properties", {}).get(field, None), dict):
                        resource_dict.get("Properties", {}).pop(field, None)
                        processed_logical_id = resource_logical_id
                elif (
                    built_resource_dict and is_local_path(built_resource_dict.get("Properties", {}).get(field, None))
                ) or resource_logical_id in linked_resources:
                    resource_dict.get("Properties", {}).pop(field, None)
                    processed_logical_id = resource_logical_id

        return processed_logical_id
def distributed_circuit_2_propagation_impedance(distributed_admittance,
        distributed_impedance):
    
    
    propagation_constant = \
            sqrt(distributed_impedance*distributed_admittance)
    characteristic_impedance = \
            sqrt(distributed_impedance/distributed_admittance)
    return (propagation_constant, characteristic_impedance)
def Gamma0_2_swr(Gamma0: NumberLike):
    
    

    
    return (1 + npy.abs(Gamma0)) / (1 - npy.abs(Gamma0))
def voltage_current_propagation(v1, i1, z0, theta):
    
    
    
    # outer product by broadcasting of the electrical length
    #theta = gamma[:, npy.newaxis] * d  # (nbfreqs x nbd)
    # ABCD parameters of a transmission line (gamma, z0)
    A = npy.cosh(theta)
    B = z0*npy.sinh(theta)
    C = npy.sinh(theta)/z0
    D = npy.cosh(theta)
    # transpose and de-transpose operations are necessary 
    # for linalg.inv to inverse square matrices
    ABCD = npy.array([[A, B],[C, D]]).transpose()   
    inv_ABCD = npy.linalg.inv(ABCD).transpose()
    
    v2 = inv_ABCD[0,0] * v1 + inv_ABCD[0,1] * i1
    i2 = inv_ABCD[1,0] * v1 + inv_ABCD[1,1] * i1
    return v2, i2
def __init__(self,
               is_training,
               first_stage_features_stride,
               batch_norm_trainable=False,
               reuse_weights=None,
               weight_decay=0.0,
               activation_fn=tf.nn.relu):
    
    
    super(FasterRCNNResnet101FeatureExtractor,
          self).__init__('resnet_v1_101', resnet_v1.resnet_v1_101, is_training,
                         first_stage_features_stride, batch_norm_trainable,
                         reuse_weights, weight_decay, activation_fn)
def preprocess(self, resized_inputs):
    

    
    if resized_inputs.shape.as_list()[3] == 3:
      channel_means = [123.68, 116.779, 103.939]
      return resized_inputs - [[channel_means]]
    else:
      return resized_inputs
def __hash_new(name, data=b''):
    
    
    try:
        return _hashlib.new(name, data)
    except ValueError:
        # If the _hashlib module (OpenSSL) doesn't support the named
        # hash, try using our builtin implementations.
        # This allows for SHA224/256 and SHA384/512 support even though
        # the OpenSSL library prior to 0.9.8 doesn't provide them.
        return __get_builtin_constructor(name)(data)
def __py_new(name, data=b'', **kwargs):
    
    
    return __get_builtin_constructor(name)(data, **kwargs)
def benchmark() -> None:
    
    from timeit import timeit

    print("Running performance benchmarks...")
    setup = "from string import printable ; from __main__ import atbash, atbash_slow"
    print(f"> atbash_slow(): {timeit('atbash_slow(printable)', setup=setup)} seconds")
    print(f">      atbash(): {timeit('atbash(printable)', setup=setup)} seconds")
def execute_flow(
    flow_file: Path,
    working_dir: Path,
    output_dir: Path,
    connections: dict,
    inputs: Mapping[str, Any],
    *,
    run_id: str = None,
    run_aggregation: bool = True,
    enable_stream_output: bool = False,
    allow_generator_output: bool = False,  # TODO: remove this
    init_kwargs: Optional[dict] = None,
    **kwargs,
) -> LineResult:
    
    
    flow_executor = FlowExecutor.create(
        flow_file, connections, working_dir, raise_ex=False, init_kwargs=init_kwargs, **kwargs
    )
    flow_executor.enable_streaming_for_llm_flow(lambda: enable_stream_output)
    with _change_working_dir(working_dir), _force_flush_tracer_provider():
        # Execute nodes in the flow except the aggregation nodes
        # TODO: remove index=0 after UX no longer requires a run id similar to batch runs
        # (run_id_index, eg. xxx_0) for displaying the interface
        line_result = flow_executor.exec_line(
            inputs, index=0, allow_generator_output=allow_generator_output, run_id=run_id
        )
        # persist the output to the output directory
        line_result.output = flow_executor._multimedia_processor.persist_multimedia_data(
            line_result.output, base_dir=working_dir, sub_dir=output_dir
        )
        if run_aggregation and line_result.aggregation_inputs:
            # Convert inputs of aggregation to list type
            flow_inputs = {k: [v] for k, v in inputs.items()}
            aggregation_inputs = {k: [v] for k, v in line_result.aggregation_inputs.items()}
            aggregation_results = flow_executor.exec_aggregation(
                flow_inputs, aggregation_inputs=aggregation_inputs, run_id=run_id
            )
            line_result.node_run_infos = {**line_result.node_run_infos, **aggregation_results.node_run_infos}
            line_result.run_info.metrics = aggregation_results.metrics
            # The aggregation inputs of line results is not utilized in the flow test. So we set it into None.
            line_result.aggregation_inputs = None
        if isinstance(line_result.output, dict):
            # remove line_number from output
            line_result.output.pop(LINE_NUMBER_KEY, None)
        return line_result
def __init__(self,
               is_training,
               first_stage_features_stride=16,
               batch_norm_trainable=False,
               conv_hyperparams=None,
               weight_decay=0.0,
               fpn_min_level=2,
               fpn_max_level=6,
               additional_layer_depth=256,
               override_base_feature_extractor_hyperparams=False):
    
    
    super(FasterRCNNResnet101FpnKerasFeatureExtractor, self).__init__(
        is_training=is_training,
        first_stage_features_stride=first_stage_features_stride,
        conv_hyperparams=conv_hyperparams,
        resnet_v1_base_model=resnet_v1.resnet_v1_101,
        resnet_v1_base_model_name='resnet_v1_101',
        batch_norm_trainable=batch_norm_trainable,
        weight_decay=weight_decay,
        fpn_min_level=fpn_min_level,
        fpn_max_level=fpn_max_level,
        additional_layer_depth=additional_layer_depth,
        override_base_feature_extractor_hyperparams=override_base_feature_extractor_hyperparams)
def test_should_bypass_proxies(url, expected, monkeypatch):
    
    
    monkeypatch.setenv('no_proxy', '192.168.0.0/24,127.0.0.1,localhost.localdomain,172.16.1.1')
    monkeypatch.setenv('NO_PROXY', '192.168.0.0/24,127.0.0.1,localhost.localdomain,172.16.1.1')
    assert should_bypass_proxies(url) == expected
def map_to_legacy_structure(parameter_name: str, new_parameter: StackParameter) -> LegacyParameter:
    
    
    
    return LegacyParameter(
        LogicalResourceId=new_parameter["ParameterKey"],
        Type="Parameter",
        Properties=LegacyParameterProperties(
            ParameterType=new_parameter.get("ParameterType"),
            ParameterValue=new_parameter.get("ParameterValue"),
            ResolvedValue=new_parameter.get("ResolvedValue"),
            Value=new_parameter.get("ResolvedValue", new_parameter.get("ParameterValue")),
        ),
    )
def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic'):
    
    
    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR
    image = _decode_and_center_crop(image_bytes, image_size, resize_method)
    image = tf.reshape(image, [image_size, image_size, 3])
    image = tf.image.convert_image_dtype(
        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)
    return image
def _format_msg(
    msg: str,
    *args: Any,
    no_format: bool = None,
    _tags: Dict[str, Any] = None,
    _numbered: Tuple[str, int, int] = None,
    **kwargs: Any
):
    
    

    if isinstance(msg, str) or isinstance(msg, ColorfulString):
        tags_str = ""
        if _tags is not None:
            tags_list = []
            for k, v in _tags.items():
                if v is True:
                    tags_list += [k]
                    continue
                if v is False:
                    continue

                tags_list += [k + "=" + v]
            if tags_list:
                tags_str = cf.reset(cf.dimmed(" [{}]".format(", ".join(tags_list))))

        numbering_str = ""
        if _numbered is not None:
            chars, i, n = _numbered
            numbering_str = cf.dimmed(chars[0] + str(i) + "/" + str(n) + chars[1]) + " "

        if no_format:
            # todo: throw if given args/kwargs?
            return numbering_str + msg + tags_str
        return numbering_str + msg.format(*args, **kwargs) + tags_str

    if kwargs:
        raise ValueError("We do not support printing kwargs yet.")

    res = [msg, *args]
    res = [str(x) for x in res]
    return ", ".join(res)
def delete_value(self, name, local=True, scope=SYSTEM_SCOPE):
        
        
        
        name = self._get_full_key_name(name=name, local=local)

        client = self._get_api_client()

        instance = KeyValuePair()
        instance.id = name
        instance.name = name

        self._logger.audit('Deleting value from the datastore (name=%s)', name)

        try:
            params = {'scope': scope}
            client.keys.delete(instance=instance, params=params)
        except Exception:
            return False

        return True
def list_values(self, local=True, prefix=None, limit=None, offset=0):
        
        
        
        client = self.get_api_client()
        self._logger.debug("Retrieving all the values from the datastore")

        limit = limit or cfg.CONF.api.max_page_size
        key_prefix = self._get_full_key_prefix(local=local, prefix=prefix)
        kvps = client.keys.get_all(prefix=key_prefix, limit=limit, offset=offset)
        return kvps
def set_color(img, coords, color, alpha=None):
    

    
    rr, cc = coords

    if alpha is None:
        rr, cc = _coords_inside_image(rr, cc, img.shape)
    else:
        rr, cc, alpha = _coords_inside_image(rr, cc, img.shape, val=alpha)

    if not np.isscalar(color):
        color = np.array(color)
        if color.shape[0] != img.shape[-1]:
            raise ValueError('Color shape ({}) must match last '
                             'image dimension ({}).'.format(color.shape[0],
                                                            img.shape[-1]))

    if alpha is None:
        img[rr, cc] = color
    else:
        if np.isscalar(alpha) or np.isscalar(color):
            color = color * alpha
        else:
            color = color * alpha[:, None]


        # Strategy: try operation directly.  If scalar / correctly shaped
        # vector, this will work.  If not (e.g. vector alpha but color image),
        # try broadcasting.
        try:
            vals = img[rr, cc] * (1 - alpha)
        except ValueError:
            vals = img[rr, cc] * (1 - alpha[:, None])

        try:
            img[rr, cc] = vals + color
        except ValueError:
            img[rr, cc] = vals + color[:, None]
def ellipse(r, c, yradius, xradius, shape=None, orientation=0.):
    

    

    center = np.array([r, c])
    radiuses = np.array([yradius, xradius])
    # allow just rotation with in range 90 degree
    orientation = orientation % (np.pi / 2.)

    # The upper_left and lower_right corners of the smallest rectangle
    # containing the ellipse.
    radiuses_yrot = max([yradius * np.sin(orientation), xradius * np.cos(orientation)])
    radiuses_xrot = max([yradius * np.cos(orientation), xradius * np.sin(orientation)])
    radiuses_rot = np.array([radiuses_yrot, radiuses_xrot])
    upper_left = np.ceil(center - radiuses_rot).astype(int)
    lower_right = np.floor(center + radiuses_rot).astype(int)

    if shape is not None:
        # Constrain upper_left and lower_right by shape boundary.
        upper_left = np.maximum(upper_left, np.array([0, 0]))
        lower_right = np.minimum(lower_right, np.array(shape[:2]) - 1)

    shifted_center = center - upper_left
    bounding_shape = lower_right - upper_left + 1

    rr, cc = _ellipse_in_shape(bounding_shape, shifted_center, radiuses,
                               orientation)
    rr.flags.writeable = True
    cc.flags.writeable = True
    rr += upper_left[0]
    cc += upper_left[1]
    return rr, cc
def circle(r, c, radius, shape=None):
    
    
    warnings.warn("circle is deprecated in favor of "
                  "disk."
                  "circle will be removed in version 0.19",
                  FutureWarning, stacklevel=2)
    return disk((r, c), radius, shape=shape)
def bezier_curve(r0, c0, r1, c1, r2, c2, weight, shape=None):
    
    
    return _bezier_curve(r0, c0, r1, c1, r2, c2, weight, shape)
def polygon_perimeter(cr, cc, shape=None, clip=False):
    

    
    if clip:
        if shape is None:
            raise ValueError("Must specify clipping shape")
        clip_box = np.array([0, 0, shape[0] - 1, shape[1] - 1])
    else:
        clip_box = np.array([np.min(cr), np.min(cc),
                             np.max(cr), np.max(cc)])

    # Do the clipping irrespective of whether clip is set.  This
    # ensures that the returned polygon is closed and is an array.
    cr, cc = polygon_clip(cr, cc, *clip_box)

    cr = np.round(cr).astype(int)
    cc = np.round(cc).astype(int)

    # Construct line segments
    pr, pc = [], []
    for i in range(len(cr) - 1):
        line_r, line_c = line(cr[i], cc[i], cr[i + 1], cc[i + 1])
        pr.extend(line_r)
        pc.extend(line_c)

    pr = np.asarray(pr)
    pc = np.asarray(pc)

    if shape is None:
        return pr, pc
    else:
        return _coords_inside_image(pr, pc, shape)
def line(y0, x0, y1, x1):
    

    
    return _line(y0, x0, y1, x1)
def rectangle(start, end=None, extent=None, shape=None):
    

    
    tl, br = _rectangle_slice(start=start, end=end, extent=extent)

    if shape is not None:
        br = np.minimum(shape, br)
        tl = np.maximum(np.zeros_like(shape), tl)
    coords = np.meshgrid(*[np.arange(st, en) for st, en in zip(tuple(tl),
                                                               tuple(br))])
    return coords
def _validate_unlimited_amount(conf: Dict[str, Any]) -> None:
    
    
    
    if (not conf.get('edge', {}).get('enabled')
        and conf.get('max_open_trades') == float('inf')
            and conf.get('stake_amount') == constants.UNLIMITED_STAKE_AMOUNT):
        raise ConfigurationError("`max_open_trades` and `stake_amount` cannot both be unlimited.")
def decode_asr(self, tgt, encoder_out):
        
        
        tgt_mask = get_lookahead_mask(tgt)
        tgt = self.custom_tgt_module(tgt)
        if self.attention_type == "RelPosMHAXL":
            # we use fixed positional encodings in the decoder
            tgt = tgt + self.positional_encoding_decoder(tgt)
            encoder_out = encoder_out + self.positional_encoding_decoder(
                encoder_out
            )
        elif self.positional_encoding_type == "fixed_abs_sine":
            tgt = tgt + self.positional_encoding(tgt)  # add the encodings here

        prediction, _, multihead_attns = self.asr_decoder(
            tgt, encoder_out, tgt_mask=tgt_mask
        )

        return prediction, multihead_attns[-1]
def _get_test_conanfile_path(tf, conanfile_path):
    
    

    if tf is False:
        # Look up for testing conanfile can be disabled if tf (test folder) is False
        return None

    base_folder = os.path.dirname(conanfile_path)
    test_conanfile_path = os.path.join(base_folder, tf or "test_package", "conanfile.py")
    if os.path.exists(test_conanfile_path):
        return test_conanfile_path
    if tf:
        raise ConanException("test folder '{tf}' not available, or it doesn't have a conanfile.py")
def test_inconsistent_input():
    
    X_ = np.random.random((5, 10))
    y_ = np.ones(X_.shape[0])

    clf = logistic.LogisticRegression()

    # Wrong dimensions for training data
    y_wrong = y_[:-1]
    assert_raises(ValueError, clf.fit, X, y_wrong)

    # Wrong dimensions for test data
    assert_raises(ValueError,
                  clf.fit(X_, y_).predict,
                  np.random.random((3, 12)))
def convert_zero_checkpoint_to_fp32_state_dict(checkpoint_dir, output_file, tag=None, exclude_frozen_parameters=False):
    
    
    

    state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir, tag, exclude_frozen_parameters)
    print(f"Saving fp32 state dict to {output_file}")
    torch.save(state_dict, output_file)
def open(self, text, searchphrase=None, *, insert_tags=None):
        
        
        SearchDialogBase.open(self, text, searchphrase)
        self.ok = True
        self.insert_tags = insert_tags
def __init__(self, vectors_u, vectors_v, indices_u, indices_v, regularization_coeff=1.0):
        
        

        
        self.vectors_u = vectors_u.T[np.newaxis, :, :]  # (1, dim, batch_size)
        self.vectors_v = vectors_v  # (1 + neg_size, dim, batch_size)
        self.indices_u = indices_u
        self.indices_v = indices_v
        self.regularization_coeff = regularization_coeff

        self.poincare_dists = None
        self.euclidean_dists = None

        self.norms_u = None
        self.norms_v = None
        self.alpha = None
        self.beta = None
        self.gamma = None

        self.gradients_u = None
        self.distance_gradients_u = None
        self.gradients_v = None
        self.distance_gradients_v = None

        self.loss = None

        self._distances_computed = False
        self._gradients_computed = False
        self._distance_gradients_computed = False
        self._loss_computed = False
def evaluate_spearman(self, embedding):
        

        
        predicted_scores = []
        expected_scores = []
        skipped = 0
        count = 0
        vocab_trie = self.create_vocab_trie(embedding)
        for (word_1, word_2), expected_score in self.scores.items():
            try:
                predicted_score = self.score_function(embedding, vocab_trie, word_1, word_2)
            except ValueError:
                skipped += 1
                continue
            count += 1
            predicted_scores.append(predicted_score)
            expected_scores.append(expected_score)
        logger.info('skipped pairs: %d out of %d' % (skipped, len(self.scores)))
        spearman = spearmanr(expected_scores, predicted_scores)
        return spearman.correlation
def most_similar(self, node_or_vector, topn=10, restrict_vocab=None):
        

        
        if isinstance(topn, Integral) and topn < 1:
            return []

        if not restrict_vocab:
            all_distances = self.distances(node_or_vector)
        else:
            nodes_to_use = self.index2word[:restrict_vocab]
            all_distances = self.distances(node_or_vector, nodes_to_use)

        if isinstance(node_or_vector, string_types + (int,)):
            node_index = self.vocab[node_or_vector].index
        else:
            node_index = None
        if not topn:
            closest_indices = matutils.argsort(all_distances)
        else:
            closest_indices = matutils.argsort(all_distances, topn=1 + topn)
        result = [
            (self.index2word[index], float(all_distances[index]))
            for index in closest_indices if (not node_index or index != node_index)  # ignore the input node
        ]
        if topn:
            result = result[:topn]
        return result
def _wrap(text, wrap_max=80, indent=4):
    
    
    indent = indent * ' '
    wrap_width = wrap_max - len(indent)
    if isinstance(text, Path):
        text = path2str(text)
    return textwrap.fill(text, width=wrap_width, initial_indent=indent,
                         subsequent_indent=indent, break_long_words=False,
                         break_on_hyphens=False)
def compile_infix_regex(entries: Iterable[Union[str, Pattern]]) -> Pattern:
    
    
    expression = "|".join([piece for piece in entries if piece.strip()])
    return re.compile(expression)
def load_model(name, **overrides):
    
    
    data_path = get_data_path()
    if not data_path or not data_path.exists():
        raise IOError(Errors.E049.format(path=path2str(data_path)))
    if isinstance(name, basestring_):  # in data dir / shortcut
        if name in set([d.name for d in data_path.iterdir()]):
            return load_model_from_link(name, **overrides)
        if is_package(name):  # installed as package
            return load_model_from_package(name, **overrides)
        if Path(name).exists():  # path to model data directory
            return load_model_from_path(Path(name), **overrides)
    elif hasattr(name, "exists"):  # Path or Path-like to model data
        return load_model_from_path(name, **overrides)
    raise IOError(Errors.E050.format(name=name))
def minibatch_by_words(examples, size, count_words=len, tolerance=0.2, discard_oversize=False):
    
    if isinstance(size, int):
        size_ = itertools.repeat(size)
    elif isinstance(size, List):
        size_ = iter(size)
    else:
        size_ = size

    target_size = next(size_)
    tol_size = target_size * tolerance
    batch = []
    current_size = 0

    for example in examples:
        n_words = count_words(example.doc)
        # add the example to the current batch if it still fits
        if (current_size + n_words) < (target_size + tol_size):
            batch.append(example)
            current_size += n_words
        else:
            # if the current example exceeds the batch size, it is returned separately
            # but only if discard_oversize=False.
            if current_size > target_size:
                if not discard_oversize:
                    yield [example]
            # yield the previous batch and start a new one
            else:
                yield batch
                target_size = next(size_)
                tol_size = target_size * tolerance
                # In theory it may happen that the current example now exceeds the new target_size,
                # but that seems like an unimportant edge case if batch sizes are variable anyway?
                batch = [example]
                current_size = n_words

    # yield the final batch
    if batch:
        yield batch
def validate_json(data, validator):
    
    
    errors = []
    for err in sorted(validator.iter_errors(data), key=lambda e: e.path):
        if err.path:
            err_path = "[{}]".format(" -> ".join([str(p) for p in err.path]))
        else:
            err_path = ""
        msg = err.message + " " + err_path
        if err.context:  # Error has suberrors, e.g. if schema uses anyOf
            suberrs = ["  - {}".format(suberr.message) for suberr in err.context]
            msg += ":\n{}".format("".join(suberrs))
        errors.append(msg)
    return errors
def load_language_data(path):
    
    
    path = ensure_path(path)
    if path.exists():
        return srsly.read_json(path)
    path = path.with_suffix(path.suffix + ".gz")
    if path.exists():
        return srsly.read_gzip_json(path)
    # TODO: move to spacy.errors
    raise ValueError("Can't find language data file: {}".format(path2str(path)))
def run_command(
    command: Union[str, List[str]],
    *,
    stdin: Optional[Any] = None,
    capture: bool=False,
) -> Optional[subprocess.CompletedProcess]:
    
    
    if isinstance(command, str):
        cmd_list = split_command(command)
        cmd_str = command
    else:
        cmd_list = command
        cmd_str = " ".join(command)
    try:
        ret = subprocess.run(
            cmd_list,
            env=os.environ.copy(),
            input=stdin,
            encoding="utf8",
            check=False,
            stdout=subprocess.PIPE if capture else None,
            stderr=subprocess.STDOUT if capture else None,
        )
    except FileNotFoundError:
        # Indicates the *command* wasn't found, it's an error before the command
        # is run.
        raise FileNotFoundError(
            Errors.E970.format(str_command=cmd_str, tool=cmd_list[0])
        ) from None
    if ret.returncode != 0 and capture:
        message = f"Error running command:\n\n{cmd_str}\n\n"
        message += f"Subprocess exited with status {ret.returncode}"
        if ret.stdout is not None:
            message += f"\n\nProcess log (stdout and stderr):\n\n"
            message += ret.stdout
        error = subprocess.SubprocessError(message)
        error.ret = ret
        error.command = cmd_str
        raise error
    elif ret.returncode != 0:
        sys.exit(ret.returncode)
    return ret
def working_dir(path: Union[str, Path]) -> None:
    
    
    prev_cwd = Path.cwd()
    os.chdir(str(path))
    try:
        yield Path(path).resolve()
    finally:
        os.chdir(prev_cwd)
def load_config(path, create_objects=False):
    
    
    config = thinc.config.Config().from_disk(path)
    if create_objects:
        return registry.make_from_config(config, validate=True)
    else:
        return config
def resolve_dot_names(
    config: Config, dot_names: List[Optional[str]]
) -> List[Optional[Callable]]:
    
    
    resolved = {}
    output = []
    for name in dot_names:
        if name is None:
            output.append(name)
        else:
            section = name.split(".")[0]
            # We want to avoid resolving the same thing twice.
            if section not in resolved:
                resolved[section] = registry.resolve(config[section])
            output.append(dot_to_object(resolved, name))
    return output
def load_model_from_path(
    model_path: Path,
    *,
    meta: Optional[Dict[str, Any]] = None,
    vocab: Union["Vocab", bool] = True,
    disable: Iterable[str] = SimpleFrozenList(),
    exclude: Iterable[str] = SimpleFrozenList(),
    config: Union[Dict[str, Any], Config] = SimpleFrozenDict(),
) -> "Language":
    
    
    if not model_path.exists():
        raise IOError(Errors.E052.format(path=model_path))
    if not meta:
        meta = get_model_meta(model_path)
    config_path = model_path / "config.cfg"
    overrides = dict_to_dot(config)
    config = load_config(config_path, overrides=overrides)
    nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude)
    return nlp.from_disk(model_path, exclude=exclude, overrides=overrides)
def combine_score_weights(
    weights: List[Dict[str, float]],
    overrides: Dict[str, Optional[Union[float, int]]] = SimpleFrozenDict(),
) -> Dict[str, float]:
    
    
    # We first need to extract all None/null values for score weights that
    # shouldn't be shown in the table *or* be weighted
    result = {}
    all_weights = []
    for w_dict in weights:
        filtered_weights = {}
        for key, value in w_dict.items():
            value = overrides.get(key, value)
            if value is None:
                result[key] = None
            else:
                filtered_weights[key] = value
        all_weights.append(filtered_weights)
    for w_dict in all_weights:
        # We need to account for weights that don't sum to 1.0 and normalize
        # the score weights accordingly, then divide score by the number of
        # components.
        total = sum(w_dict.values())
        for key, value in w_dict.items():
            weight = round(value / total / len(all_weights), 2)
            result[key] = result.get(key, 0.0) + weight
    return result
def dict_to_dot(obj: Dict[str, dict], *, for_overrides: bool = False) -> Dict[str, Any]:
    
    
    return {
        ".".join(key): value
        for key, value in walk_dict(obj, for_overrides=for_overrides)
    }
def forward(self, prediction, target, lengths):
        
        

        # Check on input and label shapes
        if "ctc" not in self.cost_type:

            # Shapes cannot be too different (max 3 time steps)
            diff = abs(prediction.shape[1] - target.shape[1])
            if diff > self.allow_lab_diff:
                err_msg = (
                    "The length of labels differs from the length of the "
                    "output probabilities. (Got %i vs %i)"
                    % (target.shape[1], prediction.shape[1])
                )

                logger.error(err_msg, exc_info=True)
            prediction = prediction[:, 0 : target.shape[1], :]

        else:

            if not isinstance(lengths, list):
                err_msg = (
                    "The third input to the compute_cost function must "
                    "be a list [wav_len, lab_len] when ctc is the cost. "
                )

                logger.error(err_msg, exc_info=True)

        target = target.to(prediction.device)

        # Regression case
        reshape = True

        if len(prediction.shape) == len(target.shape):
            reshape = False

        out_costs = []

        for i, cost in enumerate(self.costs):

            # Managing avoid_pad to avoid adding costs of padded time steps
            if self.avoid_pad[i]:

                # Getting the number of sentences in the minibatch
                N_snt = prediction.shape[0]

                # Loss initialization
                loss = 0

                # Loop over all the sentences of the minibatch
                for j in range(N_snt):

                    # Selecting sentence
                    prob_curr = prediction[j]
                    lab_curr = target[j]

                    # Avoiding padded time steps
                    actual_size = int(
                        torch.round(lengths[j] * lab_curr.shape[0])
                    )

                    prob_curr = prob_curr.narrow(0, 0, actual_size)
                    lab_curr = lab_curr.narrow(0, 0, actual_size)

                    if reshape:
                        lab_curr = lab_curr.long()

                    # Loss accumulation
                    loss = loss + cost(prob_curr, lab_curr)

                # Loss averaging
                loss = loss / N_snt

                # Appending current loss
                out_costs.append(loss)

            # Managing case in which we also include the cost of padded steps.
            # This can be use when the number of padding elements is small
            # (e.g, when we sort the sentences before creating the batches)
            else:

                # Reshaping
                prob_curr = prediction
                lab_curr = target

                # Managing ctc cost for sequence-to-sequence learning
                if self.cost_type[i] == "ctc":

                    # Cast lab_curr to int32 for using Cudnn computation
                    # In the case of using CPU training, int type is mondatory.
                    lab_curr = lab_curr.int()

                    # Permuting output probs
                    prob_curr = prob_curr.transpose(0, 1)

                    # Getting the actual lengths
                    input_lengths = torch.round(
                        lengths[0] * prob_curr.shape[0]
                    ).int()
                    lab_lengths = lengths[1] * target.shape[1]
                    lab_lengths = torch.round(lab_lengths).int()

                    # Compute CTC loss
                    ctc_cost = cost(
                        prob_curr, lab_curr, input_lengths, lab_lengths
                    )

                    out_costs.append(ctc_cost)

                else:

                    # Reshaping tensors when needed
                    if reshape:
                        lab_curr = target.reshape(
                            target.shape[0] * target.shape[1]
                        ).long()

                        prob_curr = prob_curr.reshape(
                            prob_curr.shape[0] * prob_curr.shape[1],
                            prob_curr.shape[2],
                        )

                    # Cost computation
                    out_costs.append(cost(prob_curr, lab_curr))

        if len(out_costs) == 1:
            out_costs = out_costs[0]

        return out_costs
def compute_masked_loss(
    loss_fn,
    predictions,
    targets,
    length=None,
    label_smoothing=0.0,
    reduction="mean",
):
    
        
    
    mask = torch.ones_like(targets)
    if length is not None:
        mask = length_to_mask(
            length * targets.shape[1], max_len=targets.shape[1],
        )
        if len(targets.shape) == 3:
            mask = mask.unsqueeze(2).repeat(1, 1, targets.shape[2])

    loss = torch.sum(loss_fn(predictions, targets) * mask)
    if reduction == "mean":
        loss = loss / torch.sum(mask)

    if label_smoothing == 0:
        return loss
    else:
        loss_reg = -torch.sum(torch.mean(predictions, dim=1) * mask)
        if reduction == "mean":
            loss_reg = loss_reg / torch.sum(mask)

        return label_smoothing * loss_reg + (1 - label_smoothing) * loss
def forward(self, preds, targets):
        
        
        
        losses = []
        perms = []
        for pred, label in zip(preds, targets):
            loss, p = self._opt_perm_loss(pred, label)
            perms.append(p)
            losses.append(loss)
        loss = torch.stack(losses)
        return loss, perms
def forward(self, predictions, length=None, reduction=None):
        
        
        laplacian = self.laplacian(predictions)
        laplacian = laplacian.moveaxis(self.len_dim, 1)
        mask = compute_length_mask(laplacian, length).bool()
        if reduction == "batch":
            # TODO: Vectorize
            loss = torch.stack(
                [
                    item.masked_select(item_mask).var()
                    for item, item_mask in zip(laplacian, mask)
                ]
            )
        else:
            loss = laplacian.masked_select(mask).var()
        return -loss
def details(self, predictions, targets, length=None, reduction="batchmean"):
        

        
        if length is None:
            length = torch.ones(targets.size(0))
        rec_loss, dist_loss = self._compute_components(predictions, targets)
        rec_loss = _reduce_autoencoder_loss(rec_loss, length, reduction)
        dist_loss = _reduce_autoencoder_loss(dist_loss, length, reduction)
        weighted_dist_loss = self.dist_loss_weight * dist_loss
        loss = rec_loss + weighted_dist_loss

        return VariationalAutoencoderLossDetails(
            loss, rec_loss, dist_loss, weighted_dist_loss
        )
def forward(self, predictions, targets, length=None, reduction="batchmean"):
        
        
        rec_loss = self._align_length_axis(
            self.rec_loss(targets, predictions.rec, reduction=None)
        )
        return _reduce_autoencoder_loss(rec_loss, length, reduction)
def analyze_all_pipes(nlp: "Language", warn: bool = True) -> Dict[str, List[str]]:
    
    
    problems = {}
    for i, name in enumerate(nlp.pipe_names):
        problems[name] = analyze_pipes(nlp, name, i, warn=warn)
    return problems
def print_summary(
    nlp: "Language",
    *,
    keys: List[str] = ["requires", "assigns", "scores", "retokenizes"],
    pretty: bool = True,
    no_print: bool = False,
) -> Optional[Dict[str, Union[List[str], Dict[str, List[str]]]]]:
    
    
    msg = Printer(pretty=pretty, no_print=no_print)
    overview = {}
    problems = {}
    for i, name in enumerate(nlp.pipe_names):
        meta = nlp.get_pipe_meta(name)
        overview[name] = {"i": i, "name": name}
        for key in keys:
            overview[name][key] = getattr(meta, key, None)
        problems[name] = analyze_pipes(nlp, name, i, warn=False)
    msg.divider("Pipeline Overview")
    header = ["#", "Component", *[key.capitalize() for key in keys]]
    body = [[info for info in entry.values()] for entry in overview.values()]
    msg.table(body, header=header, divider=True, multiline=True)
    n_problems = sum(len(p) for p in problems.values())
    if any(p for p in problems.values()):
        msg.divider(f"Problems ({n_problems})")
        for name, problem in problems.items():
            if problem:
                msg.warn(f"'{name}' requirements not met: {', '.join(problem)}")
    else:
        msg.good("No problems found.")
    if no_print:
        return {"overview": overview, "problems": problems}
def readable_timedelta(delta):
    
    
    
    seconds, milliseconds = divmod(delta, 1000)
    minutes, seconds = divmod(seconds, 60)

    return f"{int(minutes)}:{int(seconds)}.{int(milliseconds)}"
def run(self, tasks, pattern, play_name='Ansible Ad-hoc', gather_facts='no'):
        
        
        
        self.check_pattern(pattern)
        self.results_callback = self.get_result_callback()
        cleaned_tasks = self.clean_tasks(tasks)

        play_source = dict(
            name=play_name,
            hosts=pattern,
            gather_facts=gather_facts,
            tasks=cleaned_tasks
        )

        play = Play().load(
            play_source,
            variable_manager=self.variable_manager,
            loader=self.loader,
        )

        tqm = TaskQueueManager(
            inventory=self.inventory,
            variable_manager=self.variable_manager,
            loader=self.loader,
            options=self.options,
            stdout_callback=self.results_callback,
            passwords=self.options.passwords,
        )
        try:
            tqm.run(play)
            return self.results_callback
        except Exception as e:
            raise AnsibleError(e)
        finally:
            tqm.cleanup()
            self.loader.cleanup_all_tmp_files()
def _async_current_entries(self, include_ignore: bool = False) -> List[ConfigEntry]:
        
        
        assert self.hass is not None
        config_entries = self.hass.config_entries.async_entries(self.handler)

        if include_ignore or self.source != SOURCE_USER:
            return config_entries

        return [entry for entry in config_entries if entry.source != SOURCE_IGNORE]
def execute(self, remote_invoke_input: RemoteInvokeExecutionInfo) -> RemoteInvokeExecutionInfo:
        
        
        
        remote_invoke_input = self._map_input(remote_invoke_input)
        self._boto_action_executor.validate_action_parameters(remote_invoke_input.parameters)
        remote_invoke_output = self._boto_action_executor.execute(remote_invoke_input)

        # call output mappers if the action is succeeded
        if remote_invoke_output.is_succeeded():
            return self._map_output(remote_invoke_output)

        return remote_invoke_output
def execute(self, remote_invoke_input: RemoteInvokeExecutionInfo) -> RemoteInvokeIterableResponseType:
        
        
        
        action_executor: Callable[[Any], Iterable[Union[RemoteInvokeResponse, RemoteInvokeLogOutput]]]
        payload: Union[str, Path]

        # if a file pointed is provided for payload, use specific payload and its function here
        if remote_invoke_input.is_file_provided():
            action_executor = self._execute_action_file
            payload = cast(Path, remote_invoke_input.payload_file_path)
        else:
            action_executor = self._execute_action
            payload = cast(str, remote_invoke_input.payload)

        # execute boto3 API, and update result if it is successful, update exception otherwise
        return action_executor(payload)
def mean_squared_error(image0, image1):
    
    

    
    check_shape_equality(image0, image1)
    image0, image1 = _as_floats(image0, image1)
    return np.mean((image0 - image1) ** 2, dtype=np.float64)
def peak_signal_noise_ratio(im_true, im_test, data_range=None):
    
    

    
    check_shape_equality(im_true, im_test)

    if data_range is None:
        if im_true.dtype != im_test.dtype:
            warn("Inputs have mismatched dtype.  Setting data_range based on "
                 "im_true.", stacklevel=2)
        dmin, dmax = dtype_range[im_true.dtype.type]
        true_min, true_max = np.min(im_true), np.max(im_true)
        if true_max > dmax or true_min < dmin:
            raise ValueError(
                "im_true has intensity values outside the range expected for "
                "its data type.  Please manually specify the data_range")
        if true_min >= 0:
            # most common case (255 for uint8, 1 for float)
            data_range = dmax
        else:
            data_range = dmax - dmin

    im_true, im_test = _as_floats(im_true, im_test)

    err = mean_squared_error(im_true, im_test)
    return 10 * np.log10((data_range ** 2) / err)
def pesel(self, date_of_birth=None, sex=None):
        
        
        
        if date_of_birth is None:
            date_of_birth = self.generator.date_of_birth()

        if 1800 <= date_of_birth.year <= 1899:
            month = date_of_birth.month + 80
        elif 1900 <= date_of_birth.year <= 1999:
            month = date_of_birth.month
        elif 2000 <= date_of_birth.year <= 2099:
            month = date_of_birth.month + 20
        elif 2100 <= date_of_birth.year <= 2199:
            month = date_of_birth.month + 40
        elif 2200 <= date_of_birth.year <= 2299:
            month = date_of_birth.month + 60
        else:
            raise ValueError("Date of birth is out of supported range 1800-2299")

        year = date_of_birth.year % 100

        pesel_date = f'{year:02d}{month:02d}{date_of_birth.day:02d}'
        pesel_core = ''.join(map(str, (self.random_digit() for _ in range(3))))
        pesel_sex = self.random_digit()

        if (sex == 'M' and pesel_sex % 2 == 0) or (sex == 'F' and pesel_sex % 2 == 1):
            pesel_sex = (pesel_sex + 1) % 10

        pesel = f'{pesel_date}{pesel_core}{pesel_sex}'
        pesel += str(self.pesel_compute_check_digit(pesel))

        return pesel
def call_moto_with_request(
    context: RequestContext, service_request: ServiceRequest
) -> ServiceResponse:
    
    
    
    local_context = create_aws_request_context(
        service_name=context.service.service_name,
        action=context.operation.name,
        parameters=service_request,
        region=context.region,
    )

    local_context.request.headers.update(context.request.headers)

    return call_moto(local_context)
def proxy_moto(context: RequestContext, service_request: ServiceRequest = None) -> HttpResponse:
    
    
    
    status, headers, content = dispatch_to_moto(context)

    return HttpResponse(response=content, status=status, headers=headers)
def _diff(ext_map: Dict[str, List[str]]) -> Tuple[Set[str], Set[str]]:
        
        
        add: Set[str] = set()
        remove: Set[str] = set()
        groups = OpenBBGroups.groups()

        for g in groups:
            built = set(ext_map.get(g, {}))
            installed = set(
                f"{e.name}@{getattr(e.dist, 'version', '')}"
                for e in entry_points(group=g)
            )
            add = add.union(installed - built)
            remove = remove.union(built - installed)

        return add, remove
def analyze_ticker(self, dataframe: DataFrame, metadata: dict) -> DataFrame:
        
        
        
        logger.debug("TA Analysis Launched")
        dataframe = self.advise_indicators(dataframe, metadata)
        dataframe = self.advise_entry(dataframe, metadata)
        dataframe = self.advise_exit(dataframe, metadata)
        return dataframe
def advise_all_indicators(self, data: Dict[str, DataFrame]) -> Dict[str, DataFrame]:
        
        
        
        return {pair: self.advise_indicators(pair_data.copy(), {'pair': pair}).copy()
                for pair, pair_data in data.items()}
def advise_buy(self, dataframe: DataFrame, metadata: dict) -> DataFrame:
        
        
        

        logger.debug(f"Populating enter signals for pair {metadata.get('pair')}.")

        if self._buy_fun_len == 2:
            warnings.warn("deprecated - check out the Sample strategy to see "
                          "the current function headers!", DeprecationWarning)
            return self.populate_buy_trend(dataframe)  # type: ignore
        else:
            return self.populate_buy_trend(dataframe, metadata)
def check_buy_timeout(self, pair: str, trade: Trade, order: dict,
                          current_time: datetime, **kwargs) -> bool:
        
        
        
        return False
def leverage(self, pair: str, current_time: datetime, current_rate: float,
                 proposed_leverage: float, max_leverage: float, side: str,
                 **kwargs) -> float:
        
        
        
        return 1.0
def lock_pair(self, pair: str, until: datetime) -> None:
        
        
        
        if pair not in self._pair_locked_until or self._pair_locked_until[pair] < until:
            self._pair_locked_until[pair] = until
def is_pair_locked(self, pair: str, *, candle_date: datetime = None, side: str = '*') -> bool:
        
        
        

        if not candle_date:
            # Simple call ...
            return PairLocks.is_pair_locked(pair, side=side)
        else:
            lock_time = timeframe_to_next_date(self.timeframe, candle_date)
            return PairLocks.is_pair_locked(pair, lock_time, side=side)
def get_signal(self, pair: str, interval: str, dataframe: DataFrame) -> Tuple[bool, bool]:
        
        
        
        if not isinstance(dataframe, DataFrame) or dataframe.empty:
            logger.warning('Empty candle (OHLCV) data for pair %s', pair)
            return False, False

        try:
            df_len, df_close, df_date = self.preserve_df(dataframe)
            dataframe = strategy_safe_wrapper(
                self._analyze_ticker_internal, message=""
                )(dataframe, {'pair': pair})
            self.assert_df(dataframe, df_len, df_close, df_date)
        except StrategyError as error:
            logger.warning(f"Unable to analyze candle (OHLCV) data for pair {pair}: {error}")

            return False, False

        if dataframe.empty:
            logger.warning('Empty dataframe for pair %s', pair)
            return False, False

        latest_date = dataframe['date'].max()
        latest = dataframe.loc[dataframe['date'] == latest_date].iloc[-1]
        # Explicitly convert to arrow object to ensure the below comparison does not fail
        latest_date = arrow.get(latest_date)

        # Check if dataframe is out of date
        interval_minutes = timeframe_to_minutes(interval)
        offset = self.config.get('exchange', {}).get('outdated_offset', 5)
        if latest_date < (arrow.utcnow().shift(minutes=-(interval_minutes * 2 + offset))):
            logger.warning(
                'Outdated history for pair %s. Last tick is %s minutes old',
                pair,
                (arrow.utcnow() - latest_date).seconds // 60
            )
            return False, False

        (buy, sell) = latest[SignalType.BUY.value] == 1, latest[SignalType.SELL.value] == 1
        logger.debug('trigger: %s (pair=%s) buy=%s sell=%s',
                     latest['date'], pair, str(buy), str(sell))
        return buy, sell
def get_signal(self, pair: str, timeframe: str) -> Tuple[bool, bool]:
        
        
        

        dataframe, _ = self.dp.get_analyzed_dataframe(pair, timeframe)

        if not isinstance(dataframe, DataFrame) or dataframe.empty:
            logger.warning('Empty candle (OHLCV) data for pair %s', pair)
            return False, False

        if dataframe.empty:
            logger.warning('Empty dataframe for pair %s', pair)
            return False, False

        latest_date = dataframe['date'].max()
        latest = dataframe.loc[dataframe['date'] == latest_date].iloc[-1]
        # Explicitly convert to arrow object to ensure the below comparison does not fail
        latest_date = arrow.get(latest_date)

        # Check if dataframe is out of date
        timeframe_minutes = timeframe_to_minutes(timeframe)
        offset = self.config.get('exchange', {}).get('outdated_offset', 5)
        if latest_date < (arrow.utcnow().shift(minutes=-(timeframe_minutes * 2 + offset))):
            logger.warning(
                'Outdated history for pair %s. Last tick is %s minutes old',
                pair,
                (arrow.utcnow() - latest_date).seconds // 60
            )
            return False, False

        (buy, sell) = latest[SignalType.BUY.value] == 1, latest[SignalType.SELL.value] == 1
        logger.debug('trigger: %s (pair=%s) buy=%s sell=%s',
                     latest['date'], pair, str(buy), str(sell))
        return buy, sell
def assert_df(self, dataframe: DataFrame, df_len: int, df_close: float, df_date: datetime):
        
        
        
        message = ""
        if df_len != len(dataframe):
            message = "length"
        elif df_close != dataframe["close"].iloc[-1]:
            message = "last close price"
        elif df_date != dataframe["date"].iloc[-1]:
            message = "last date"
        if message:
            if self.disable_dataframe_checks:
                logger.warning(f"Dataframe returned from strategy has mismatching {message}.")
            else:
                raise StrategyError(f"Dataframe returned from strategy has mismatching {message}.")
def stoploss_value(self, pair: str, trade: Trade, rate: float, **kwargs) -> float:
        
        
        
        return self.stoploss
def stoploss_value(self, pair: str, trade: Trade, current_rate: float, current_profit: float,
                       **kwargs) -> float:
        
        
        
        return self.stoploss
def custom_sell(self, pair: str, trade: Trade, current_time: datetime, current_rate: float,
                    current_profit: float, **kwargs) -> bool:
        
        
        
        return False
def custom_sell(self, pair: str, trade: Trade, current_time: datetime, current_rate: float,
                    current_profit: float, **kwargs) -> Optional[Union[str, bool]]:
        
        
        
        return None
def custom_stake_amount(self, pair: str, current_time: datetime, current_rate: float,
                            proposed_stake: float, min_stake: Optional[float], max_stake: float,
                            leverage: float, entry_tag: Optional[str], side: str,
                            **kwargs) -> float:
        
        
        
        return proposed_stake
def order_filled(self, pair: str, trade: Trade, order: Order,
                     current_time: datetime, **kwargs) -> None:
        
        
        
        pass
def adjust_trade_position(self, trade: Trade, current_time: datetime,
                              current_rate: float, current_profit: float,
                              min_stake: Optional[float], max_stake: float,
                              **kwargs) -> Optional[float]:
        
        
        
        return None
def feature_engineering_expand_all(self, dataframe: DataFrame, period: int,
                                       metadata: Dict, **kwargs):
        
        
        
        return dataframe
def window_reverse(windows, window_size: Tuple[int, int], img_size: Tuple[int, int]):
    
    
    
    H, W = img_size
    B = int(windows.shape[0] / (H * W / window_size[0] / window_size[1]))
    x = windows.view(B, H // window_size[0], W // window_size[1], window_size[0], window_size[1], -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x
def update(self):
        
        self._update_properties()
        dispatcher_send(self._hass, DATA_UPDATED.format(self._ipaddr))
def model(self):
        
        return self._bulb_device.model
def is_nightlight_supported(self) -> bool:
        
        
        

        return self._nightlight_brightness is not None
def get_activation(identifier, use_keras_layer=False, **kwargs):
  
  
  if isinstance(identifier, six.string_types):
    identifier = str(identifier).lower()
    if use_keras_layer:
      keras_layer_allowlist = {
          "relu": "relu",
          "linear": "linear",
          "identity": "linear",
          "swish": "swish",
          "sigmoid": "sigmoid",
          "relu6": tf.nn.relu6,
          "leaky_relu": functools.partial(tf.nn.leaky_relu, **kwargs),
          "hard_swish": activations.hard_swish,
          "hard_sigmoid": activations.hard_sigmoid,
          "mish": activations.mish,
          "gelu": functools.partial(tf.nn.gelu, **kwargs),
      }
      if identifier in keras_layer_allowlist:
        return tf_keras.layers.Activation(keras_layer_allowlist[identifier])
    name_to_fn = {
        "gelu": activations.gelu,
        "simple_swish": activations.simple_swish,
        "hard_swish": activations.hard_swish,
        "relu6": activations.relu6,
        "hard_sigmoid": activations.hard_sigmoid,
        "identity": activations.identity,
        "mish": activations.mish,
    }
    if identifier in name_to_fn:
      return tf_keras.activations.get(name_to_fn[identifier])
  return tf_keras.activations.get(identifier)
def peak_local_max(image, min_distance=1, threshold_abs=None,
                   threshold_rel=None, exclude_border=True, indices=True,
                   num_peaks=np.inf, footprint=None, labels=None,
                   num_peaks_per_label=np.inf):
    

    
    out = np.zeros_like(image, dtype=np.bool)

    threshold_abs = threshold_abs if threshold_abs is not None else image.min()

    if type(exclude_border) == bool:
        exclude_border = min_distance if exclude_border else 0

    # no peak for a trivial image
    if np.all(image == image.flat[0]):
        if indices is True:
            return np.empty((0, 2), np.int)
        else:
            return out

    # In the case of labels, call ndi on each label
    if labels is not None:
        label_values = np.unique(labels)
        # Reorder label values to have consecutive integers (no gaps)
        if np.any(np.diff(label_values) != 1):
            mask = labels >= 1
            labels[mask] = 1 + rank_order(labels[mask])[0].astype(labels.dtype)
        labels = labels.astype(np.int32)

        if exclude_border:
            # create a mask for the non-exclude region
            inner_mask = _exclude_border(np.ones_like(labels, dtype=bool),
                                         footprint, exclude_border)

        # For each label, extract a smaller image enclosing the object of
        # interest, identify num_peaks_per_label peaks and mark them in
        # variable out.
        for label_idx, obj in enumerate(ndi.find_objects(labels)):
            img_object = image[obj] * (labels[obj] == label_idx + 1)
            mask = _get_peak_mask(img_object, min_distance, footprint,
                                  threshold_abs, threshold_rel)
            if exclude_border:
                # remove peaks fall in the exclude region
                mask &= inner_mask[obj]
            coordinates = _get_high_intensity_peaks(img_object, mask,
                                                    num_peaks_per_label)
            nd_indices = tuple(coordinates.T)
            mask.fill(False)
            mask[nd_indices] = True
            out[obj] += mask

        if not indices and np.isinf(num_peaks):
            return out

        coordinates = _get_high_intensity_peaks(image, out, num_peaks)
        if indices:
            return coordinates
        else:
            out.fill(False)
            nd_indices = tuple(coordinates.T)
            out[nd_indices] = True
            return out

    # Non maximum filter
    mask = _get_peak_mask(image, min_distance, footprint, threshold_abs,
                          threshold_rel)

    if exclude_border:
        mask = _exclude_border(mask, footprint, exclude_border)

    # Select highest intensities (num_peaks)
    coordinates = _get_high_intensity_peaks(image, mask, num_peaks)

    if indices is True:
        return coordinates
    else:
        nd_indices = tuple(coordinates.T)
        out[nd_indices] = True
        return out
def _exclude_border(label, border_width):
    

    
    # zero out label borders
    for i, width in enumerate(border_width):
        if width == 0:
            continue
        label[(slice(None),) * i + (slice(None, width),)] = 0
        label[(slice(None),) * i + (slice(-width, None),)] = 0
    return label
def _write(
        self,
        file_obj: BinaryIO,
        batch_size: int,
        encoding: str = "utf-8",
        orient="records",
        lines=True,
        **to_json_kwargs,
    ) -> int:
        
        
        written = 0
        _ = to_json_kwargs.pop("path_or_buf", None)

        for offset in tqdm(range(0, len(self.dataset), batch_size)):
            batch = query_table(
                table=self.dataset.data,
                key=slice(offset, offset + batch_size),
                indices=self.dataset._indices if self.dataset._indices is not None else None,
            )
            json_str = batch.to_pandas().to_json(path_or_buf=None, orient=orient, lines=lines, **to_json_kwargs)
            written += file_obj.write(json_str.encode(encoding))
        return written
def emit(self, record):
        
        
        
        conf.database_cursor.execute("INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)",
                                     (conf.taskid, time.strftime("%X"), record.levelname,
                                     record.msg % record.args if record.args else record.msg))
def task_flush(taskid):
    
    
    
    global tasks

    if is_admin(taskid):
        for task in tasks:
            tasks[task].clean_filesystem()

        tasks = dict()
        logger.debug("Flushed task poll")
        return jsonize({"success": True})
    else:
        abort(401)
def task_list(taskid):
    
    
    
    if is_admin(taskid):
        logger.debug("Listed task pull")
        return jsonize({"tasks": tasks, "tasks_num": len(tasks)})
    else:
        abort(401)
def task_delete(taskid):
    
    
    
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug("[%s] Deleted task" % taskid)
        return jsonize({"success": True})
    else:
        response.status = 404
        logger.warning("[%s] Non-existing task ID provided to task_delete()" % taskid)
        return jsonize({"success": False, "message": "Non-existing task ID"})
def option_get(taskid):
    
    
    
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to option_get()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    options = request.json or []
    results = {}

    for option in options:
        if option in DataStore.tasks[taskid].options:
            results[option] = DataStore.tasks[taskid].options[option]
        else:
            logger.debug("[%s] Requested value for unknown option '%s'" % (taskid, option))
            return jsonize({"success": False, "message": "Unknown option '%s'" % option})

    logger.debug("[%s] Retrieved values for option(s) '%s'" % (taskid, ",".join(options)))

    return jsonize({"success": True, "options": results})
def load_npz(file, obj, path='', strict=True, ignore_names=None):
    

    
    with numpy.load(file) as f:
        d = NpzDeserializer(
            f, path=path, strict=strict, ignore_names=ignore_names)
        d.load(obj)
def create(cls, folder):
         
        
        filterfiles = (PACKAGE_TGZ_NAME, EXPORT_TGZ_NAME, CONAN_MANIFEST, CONANFILE + "c",
                       ".DS_Store")
        file_dict = {}
        for root, dirs, files in os.walk(folder):
            dirs[:] = [d for d in dirs if d != "__pycache__"]  # Avoid recursing pycache
            relative_path = os.path.relpath(root, folder)
            files = [f for f in files if f not in filterfiles]  # Avoid md5 of big TGZ files
            for f in files:
                abs_path = os.path.join(root, f)
                rel_path = os.path.normpath(os.path.join(relative_path, f))
                rel_path = rel_path.replace("\\", "/")
                if os.path.exists(abs_path):
                    file_dict[rel_path] = md5sum(abs_path)
                else:
                    raise ConanException("The file is a broken symlink, verify that "
                                         "you are packaging the needed destination files: '%s'"
                                         % abs_path)
        date = calendar.timegm(time.gmtime())

        return cls(date, file_dict)
def loads(text):
         
        
        tokens = text.split("\n")
        the_time = int(tokens[0])
        file_sums = {}
        keep_python = get_env("CONAN_KEEP_PYTHON_FILES", False)
        for md5line in tokens[1:]:
            if md5line:
                filename, file_md5 = md5line.rsplit(": ", 1)
                # FIXME: This is weird, it should never happen, maybe remove?
                if not discarded_file(filename, keep_python):
                    file_sums[filename] = file_md5
        return FileTreeManifest(the_time, file_sums)
def get_relations(self, cursor, table_name):
        
        
        
        constraints = self.get_key_columns(cursor, table_name)
        relations = {}
        for my_fieldname, other_table, other_field in constraints:
            relations[my_fieldname] = (other_field, other_table)
        return relations
def get_storage_engine(self, cursor, table_name):
        
        
        
        cursor.execute(
            "SELECT engine "
            "FROM information_schema.tables "
            "WHERE table_name = %s", [table_name])
        result = cursor.fetchone()
        if not result:
            return self.connection.features._mysql_storage_engine
        return result[0]
def _get_ray_cr_config(
        self, min_replicas=0, max_replicas=300, replicas=0
    ) -> Dict[str, Any]:
        
        
        with open(EXAMPLE_CLUSTER_PATH) as ray_cr_config_file:
            ray_cr_config_str = ray_cr_config_file.read()
        config = yaml.safe_load(ray_cr_config_str)
        cpu_group = config["spec"]["workerGroupSpecs"][0]
        cpu_group["replicas"] = replicas
        cpu_group["minReplicas"] = min_replicas
        cpu_group["maxReplicas"] = max_replicas

        # Add a GPU-annotated group.
        # (We're not using real GPUs, just adding a GPU annotation for the autoscaler
        # and Ray scheduler.)
        gpu_group = copy.deepcopy(cpu_group)
        gpu_group["rayStartParams"]["num-gpus"] = "1"
        gpu_group["replicas"] = 0
        gpu_group["minReplicas"] = 0
        gpu_group["maxReplicas"] = 1
        gpu_group["groupName"] = "fake-gpu-group"
        config["spec"]["workerGroupSpecs"].append(gpu_group)

        # Substitute images.
        for group_spec in config["spec"]["workerGroupSpecs"] + [
            config["spec"]["headGroupSpec"]
        ]:
            containers = group_spec["template"]["spec"]["containers"]

            ray_container = containers[0]
            # Confirm the first container in the example config is the Ray container.
            assert ray_container["name"] in ["ray-head", "ray-worker"]

            ray_container["image"] = RAY_IMAGE

            for container in containers:
                container["imagePullPolicy"] = PULL_POLICY

        head_containers = config["spec"]["headGroupSpec"]["template"]["spec"][
            "containers"
        ]
        autoscaler_container = [
            container
            for container in head_containers
            if container["name"] == "autoscaler"
        ].pop()
        autoscaler_container["image"] = AUTOSCALER_IMAGE

        return config
def _apply_ray_cr(
        self,
        min_replicas=0,
        cpu_replicas=0,
        gpu_replicas=0,
        validate_replicas: bool = False,
    ) -> None:
        
        
        with tempfile.NamedTemporaryFile("w") as config_file:
            if validate_replicas:
                raycluster = get_raycluster(
                    RAY_CLUSTER_NAME, namespace=RAY_CLUSTER_NAMESPACE
                )
                assert (
                    raycluster["spec"]["workerGroupSpecs"][0]["replicas"]
                    == cpu_replicas
                )
                assert (
                    raycluster["spec"]["workerGroupSpecs"][1]["replicas"]
                    == gpu_replicas
                )
                logger.info(
                    f"Validated that cpu and gpu worker replicas for "
                    f"{RAY_CLUSTER_NAME} are currently {cpu_replicas} and"
                    f" {gpu_replicas}, respectively."
                )
            cr_config = self._get_ray_cr_config(
                min_replicas=min_replicas,
                cpu_replicas=cpu_replicas,
                gpu_replicas=gpu_replicas,
            )
            yaml.dump(cr_config, config_file)
            config_file.flush()
            subprocess.check_call(["kubectl", "apply", "-f", config_file.name])
def check_field_type(self, field, field_type):
        
        
        
        errors = []
        if (field_type.startswith('varchar') and field.unique and
                (field.max_length is None or int(field.max_length) > 255)):
            errors.append(
                checks.Error(
                    'MySQL does not allow unique CharFields to have a max_length > 255.',
                    obj=field,
                    id='mysql.E001',
                )
            )

        if field.db_index and field_type.lower() in self.connection._limited_data_types:
            errors.append(
                checks.Warning(
                    'MySQL does not support a database index on %s columns.'
                    % field_type,
                    hint=(
                        "An index won't be created. Silence this warning if "
                        "you don't care about it."
                    ),
                    obj=field,
                    id='fields.W162',
                )
            )
        return errors
def fetch_balance(self, params={}):
        
        
        
        self.load_markets()
        response = None
        standard = None
        standard, params = self.handle_option_and_params(params, 'fetchBalance', 'standard', False)
        marketType, marketTypeQuery = self.handle_market_type_and_params('fetchBalance', None, params)
        if standard:
            response = self.contractV1PrivateGetBalance(marketTypeQuery)
        elif marketType == 'spot':
            response = self.spotV1PrivateGetAccountBalance(marketTypeQuery)
        else:
            response = self.swapV2PrivateGetUserBalance(marketTypeQuery)
        #
        # spot
        #
        #    {
        #        "code": 0,
        #        "msg": "",
        #        "ttl": 1,
        #        "data": {
        #            "balances": [
        #                {
        #                    "asset": "USDT",
        #                    "free": "16.73971130673954",
        #                    "locked": "0"
        #                }
        #            ]
        #        }
        #    }
        #
        # swap
        #
        #    {
        #        "code": 0,
        #        "msg": "",
        #        "data": {
        #          "balance": {
        #            "asset": "USDT",
        #            "balance": "15.6128",
        #            "equity": "15.6128",
        #            "unrealizedProfit": "0.0000",
        #            "realisedProfit": "0.0000",
        #            "availableMargin": "15.6128",
        #            "usedMargin": "0.0000",
        #            "freezedMargin": "0.0000"
        #          }
        #        }
        #    }
        # standard futures
        #    {
        #        "code":"0",
        #        "timestamp":"1691148990942",
        #        "data":[
        #           {
        #              "asset":"VST",
        #              "balance":"100000.00000000000000000000",
        #              "crossWalletBalance":"100000.00000000000000000000",
        #              "crossUnPnl":"0",
        #              "availableBalance":"100000.00000000000000000000",
        #              "maxWithdrawAmount":"100000.00000000000000000000",
        #              "marginAvailable":false,
        #              "updateTime":"1691148990902"
        #           },
        #           {
        #              "asset":"USDT",
        #              "balance":"0",
        #              "crossWalletBalance":"0",
        #              "crossUnPnl":"0",
        #              "availableBalance":"0",
        #              "maxWithdrawAmount":"0",
        #              "marginAvailable":false,
        #              "updateTime":"1691148990902"
        #           },
        #        ]
        #     }
        #
        return self.parse_balance(response)
def cancel_all_orders(self, symbol: Str = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' cancelAllOrders() requires a symbol argument')
        self.load_markets()
        market = self.market(symbol)
        if market['type'] != 'swap':
            raise BadRequest(self.id + ' cancelAllOrders is only supported for swap markets.')
        request = {
            'symbol': market['id'],
        }
        response = self.swapV2PrivateDeleteTradeAllOpenOrders(self.extend(request, params))
        #
        #    {
        #        "code": 0,
        #        "msg": "",
        #        "data": {
        #          "success": [
        #            {
        #              "symbol": "LINK-USDT",
        #              "orderId": 1597783835095859200,
        #              "side": "BUY",
        #              "positionSide": "LONG",
        #              "type": "TRIGGER_LIMIT",
        #              "origQty": "5.0",
        #              "price": "9.0000",
        #              "executedQty": "0.0",
        #              "avgPrice": "0.0000",
        #              "cumQuote": "0",
        #              "stopPrice": "9.5000",
        #              "profit": "",
        #              "commission": "",
        #              "status": "NEW",
        #              "time": 1669776326000,
        #              "updateTime": 1669776326000
        #            }
        #          ],
        #          "failed": null
        #        }
        #    }
        #
        return response
def fetch_ohlcv(self, symbol: str, timeframe='1m', since: Int = None, limit: Int = None, params={}) -> List[list]:
        
        
        
        self.load_markets()
        paginate = False
        paginate, params = self.handle_option_and_params(params, 'fetchOHLCV', 'paginate', False)
        if paginate:
            return self.fetch_paginated_call_deterministic('fetchOHLCV', symbol, since, limit, timeframe, params, 1440)
        market = self.market(symbol)
        request = {
            'symbol': market['id'],
        }
        request['interval'] = self.safe_string(self.timeframes, timeframe, timeframe)
        if since is not None:
            request['startTime'] = since
        if limit is not None:
            request['limit'] = limit
        until = self.safe_integer_2(params, 'until', 'endTime')
        if until is not None:
            params = self.omit(params, ['until'])
            request['endTime'] = until
        response = None
        if market['spot']:
            response = self.spotV1PublicGetMarketKline(self.extend(request, params))
        else:
            price = self.safe_string(params, 'price')
            params = self.omit(params, 'price')
            if price == 'mark':
                response = self.swapV1PrivateGetMarketMarkPriceKlines(self.extend(request, params))
            else:
                response = self.swapV3PublicGetQuoteKlines(self.extend(request, params))
        #
        #    {
        #        "code": 0,
        #        "msg": "",
        #        "data": [
        #          {
        #            "open": "19396.8",
        #            "close": "19394.4",
        #            "high": "19397.5",
        #            "low": "19385.7",
        #            "volume": "110.05",
        #            "time": 1666583700000
        #          },
        #          ...
        #        ]
        #    }
        #
        # fetchMarkOHLCV
        #
        #    {
        #        "code": 0,
        #        "msg": "",
        #        "data": [
        #            {
        #                "open": "42191.7",
        #                "close": "42189.5",
        #                "high": "42196.5",
        #                "low": "42189.5",
        #                "volume": "0.00",
        #                "openTime": 1706508840000,
        #                "closeTime": 1706508840000
        #            }
        #        ]
        #    }
        #
        ohlcvs = self.safe_value(response, 'data', [])
        if not isinstance(ohlcvs, list):
            ohlcvs = [ohlcvs]
        return self.parse_ohlcvs(ohlcvs, market, timeframe, since, limit)
def fetch_funding_rate_history(self, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):
        
        
        
        self.check_required_symbol('fetchFundingRateHistory', symbol)
        self.load_markets()
        paginate = False
        paginate, params = self.handle_option_and_params(params, 'fetchFundingRateHistory', 'paginate')
        if paginate:
            return self.fetch_paginated_call_deterministic('fetchFundingRateHistory', symbol, since, limit, '8h', params)
        market = self.market(symbol)
        request = {
            'symbol': market['id'],
        }
        if since is not None:
            request['startTime'] = since
        if limit is not None:
            request['limit'] = limit
        until = self.safe_integer_2(params, 'until', 'startTime')
        if until is not None:
            params = self.omit(params, ['until'])
            request['startTime'] = until
        response = self.swapV2PublicGetQuoteFundingRate(self.extend(request, params))
        #
        #    {
        #        "code":0,
        #        "msg":"",
        #        "data":[
        #          {
        #            "symbol": "BTC-USDT",
        #            "fundingRate": "0.0001",
        #            "fundingTime": 1585684800000
        #          },
        #          ...
        #        ]
        #    }
        #
        data = self.safe_value(response, 'data', [])
        rates = []
        for i in range(0, len(data)):
            entry = data[i]
            marketId = self.safe_string(entry, 'symbol')
            symbolInner = self.safe_symbol(marketId, market, '-', 'swap')
            timestamp = self.safe_integer(entry, 'fundingTime')
            rates.append({
                'info': entry,
                'symbol': symbolInner,
                'fundingRate': self.safe_number(entry, 'fundingRate'),
                'timestamp': timestamp,
                'datetime': self.iso8601(timestamp),
            })
        sorted = self.sort_by(rates, 'timestamp')
        return self.filter_by_symbol_since_limit(sorted, market['symbol'], since, limit)
def cancel_orders(self, ids: List[Int], symbol: Str = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' cancelOrders() requires a symbol argument')
        self.load_markets()
        market = self.market(symbol)
        request = {
            'symbol': market['id'],
        }
        clientOrderIds = self.safe_value(params, 'clientOrderIds')
        idsToParse = ids
        areClientOrderIds = (clientOrderIds is not None)
        if areClientOrderIds:
            idsToParse = clientOrderIds
        parsedIds = []
        for i in range(0, len(idsToParse)):
            id = idsToParse[i]
            stringId = str(id)
            parsedIds.append(stringId)
        response = None
        if market['spot']:
            spotReqKey = 'clientOrderIds' if areClientOrderIds else 'orderIds'
            request[spotReqKey] = ','.join(parsedIds)
            response = self.spotV1PrivatePostTradeCancelOrders(self.extend(request, params))
        else:
            swapReqKey = 'ClientOrderIDList' if areClientOrderIds else 'orderIdList'
            request[swapReqKey] = parsedIds
            response = self.swapV2PrivateDeleteTradeBatchOrders(self.extend(request, params))
        #
        #    {
        #        "code": 0,
        #        "msg": "",
        #        "data": {
        #          "success": [
        #            {
        #              "symbol": "LINK-USDT",
        #              "orderId": 1597783850786750464,
        #              "side": "BUY",
        #              "positionSide": "LONG",
        #              "type": "TRIGGER_MARKET",
        #              "origQty": "5.0",
        #              "price": "5.5710",
        #              "executedQty": "0.0",
        #              "avgPrice": "0.0000",
        #              "cumQuote": "0",
        #              "stopPrice": "5.0000",
        #              "profit": "0.0000",
        #              "commission": "0.000000",
        #              "status": "CANCELLED",
        #              "time": 1669776330000,
        #              "updateTime": 1672370837000
        #            }
        #          ],
        #          "failed": null
        #        }
        #    }
        #
        return response
def create_order(self, symbol: str, type: OrderType, side: OrderSide, amount: float, price: float = None, params={}):
        
        
        
        self.load_markets()
        market = self.market(symbol)
        test = self.safe_bool(params, 'test', False)
        params = self.omit(params, 'test')
        request = self.create_order_request(symbol, type, side, amount, price, params)
        response = None
        if market['swap']:
            if test:
                response = self.swapV2PrivatePostTradeOrderTest(request)
            else:
                response = self.swapV2PrivatePostTradeOrder(request)
        else:
            response = self.spotV1PrivatePostTradeOrder(request)
        #
        # spot
        #
        #    {
        #        "code": 0,
        #        "msg": "",
        #        "data": {
        #            "symbol": "XRP-USDT",
        #            "orderId": 1514090846268424192,
        #            "transactTime": 1649822362855,
        #            "price": "0.5",
        #            "origQty": "10",
        #            "executedQty": "0",
        #            "cummulativeQuoteQty": "0",
        #            "status": "PENDING",
        #            "type": "LIMIT",
        #            "side": "BUY"
        #        }
        #    }
        #
        # swap
        #
        #     {
        #         "code": 0,
        #         "msg": "",
        #         "data": {
        #             "order": {
        #                 "symbol": "BTC-USDT",
        #                 "orderId": 1709036527545438208,
        #                 "side": "BUY",
        #                 "positionSide": "LONG",
        #                 "type": "TRIGGER_LIMIT",
        #                 "clientOrderID": "",
        #                 "workingType": ""
        #             }
        #         }
        #     }
        #
        if isinstance(response, str):
            # broken api engine : order-ids are too long numbers(i.e. 1742930526912864656)
            # and json.loadscan not handle them in JS, so we have to use .parse_json            # however, when order has an attached SL/TP, their value types need extra parsing
            response = self.fix_stringified_json_members(response)
            response = self.parse_json(response)
        data = self.safe_value(response, 'data', {})
        order = self.safe_value(data, 'order', data)
        return self.parse_order(order, market)
def cancel_order(self, id: str, symbol: Str = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' cancelOrder() requires a symbol argument')
        self.load_markets()
        market = self.market(symbol)
        request = {
            'symbol': market['id'],
        }
        clientOrderId = self.safe_string_2(params, 'clientOrderId', 'clientOrderID')
        params = self.omit(params, ['clientOrderId'])
        if clientOrderId is not None:
            request['clientOrderID'] = clientOrderId
        else:
            request['orderId'] = id
        response = None
        marketType, query = self.handle_market_type_and_params('cancelOrder', market, params)
        if marketType == 'spot':
            response = self.spotV1PrivatePostTradeCancel(self.extend(request, query))
        else:
            response = self.swapV2PrivateDeleteTradeOrder(self.extend(request, query))
        #
        # spot
        #
        #   {
        #       "code": 0,
        #       "msg": "",
        #       "data": {
        #           "symbol": "XRP-USDT",
        #           "orderId": 1514090846268424192,
        #           "price": "0.5",
        #           "origQty": "10",
        #           "executedQty": "0",
        #           "cummulativeQuoteQty": "0",
        #           "status": "CANCELED",
        #           "type": "LIMIT",
        #           "side": "BUY"
        #       }
        #   }
        #
        # swap
        #
        #    {
        #        "code": 0,
        #        "msg": "",
        #        "data": {
        #          "order": {
        #            "symbol": "LINK-USDT",
        #            "orderId": 1597783850786750464,
        #            "side": "BUY",
        #            "positionSide": "LONG",
        #            "type": "TRIGGER_MARKET",
        #            "origQty": "5.0",
        #            "price": "5.0000",
        #            "executedQty": "0.0",
        #            "avgPrice": "0.0000",
        #            "cumQuote": "0",
        #            "stopPrice": "5.0000",
        #            "profit": "",
        #            "commission": "",
        #            "status": "CANCELLED",
        #            "time": 1669776330000,
        #            "updateTime": 1669776330000
        #          }
        #        }
        #    }
        #
        data = self.safe_value(response, 'data')
        first = self.safe_value(data, 'order', data)
        return self.parse_order(first, market)
def fetch_my_trades(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' fetchMyTrades() requires a symbol argument')
        self.load_markets()
        market = self.market(symbol)
        now = self.milliseconds()
        response = None
        request = {
            'symbol': market['id'],
        }
        if since is not None:
            startTimeReq = 'startTime' if market['spot'] else 'startTs'
            request[startTimeReq] = since
        elif market['swap']:
            request['startTs'] = now - 7776000000  # 90 days
        until = self.safe_integer(params, 'until')
        params = self.omit(params, 'until')
        if until is not None:
            endTimeReq = 'endTime' if market['spot'] else 'endTs'
            request[endTimeReq] = until
        elif market['swap']:
            request['endTs'] = now
        fills = None
        if market['spot']:
            response = self.spotV1PrivateGetTradeMyTrades(self.extend(request, params))
            data = self.safe_value(response, 'data', [])
            fills = self.safe_value(data, 'fills', [])
            #
            #     {
            #         "code": 0,
            #         "msg": "",
            #         "debugMsg": "",
            #         "data": {
            #             "fills": [
            #                 {
            #                     "symbol": "LTC-USDT",
            #                     "id": 36237072,
            #                     "orderId": 1674069326895775744,
            #                     "price": "85.891",
            #                     "qty": "0.0582",
            #                     "quoteQty": "4.9988562000000005",
            #                     "commission": -0.00005820000000000001,
            #                     "commissionAsset": "LTC",
            #                     "time": 1687964205000,
            #                     "isBuyer": True,
            #                     "isMaker": False
            #                 }
            #             ]
            #         }
            #     }
            #
        else:
            tradingUnit = self.safe_string_upper(params, 'tradingUnit', 'CONT')
            params = self.omit(params, 'tradingUnit')
            request['tradingUnit'] = tradingUnit
            response = self.swapV2PrivateGetTradeAllFillOrders(self.extend(request, params))
            data = self.safe_value(response, 'data', [])
            fills = self.safe_value(data, 'fill_orders', [])
            #
            #    {
            #       "code": "0",
            #       "msg": '',
            #       "data": {fill_orders: [
            #          {
            #              "volume": "0.1",
            #              "price": "106.75",
            #              "amount": "10.6750",
            #              "commission": "-0.0053",
            #              "currency": "USDT",
            #              "orderId": "1676213270274379776",
            #              "liquidatedPrice": "0.00",
            #              "liquidatedMarginRatio": "0.00",
            #              "filledTime": "2023-07-04T20:56:01.000+0800"
            #          }
            #        ]
            #      }
            #    }
            #
        return self.parse_trades(fills, market, since, limit, params)
def set_leverage(self, leverage: Int, symbol: Str = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' setLeverage() requires a symbol argument')
        side = self.safe_string_upper(params, 'side')
        self.check_required_argument('setLeverage', side, 'side', ['LONG', 'SHORT', 'BOTH'])
        self.load_markets()
        market = self.market(symbol)
        request = {
            'symbol': market['id'],
            'side': side,
            'leverage': leverage,
        }
        #
        #    {
        #        "code": 0,
        #        "msg": "",
        #        "data": {
        #            "leverage": 6,
        #            "symbol": "BTC-USDT"
        #        }
        #    }
        #
        return self.swapV2PrivatePostTradeLeverage(self.extend(request, params))
def close_position(self, symbol: str, side: OrderSide = None, params={}) -> Order:
        
        
        
        self.load_markets()
        positionId = self.safe_string(params, 'positionId')
        params = self.omit(params, 'positionId')
        response = None
        if positionId is not None:
            request: dict = {
                'positionId': positionId,
            }
            response = self.swapV1PrivatePostTradeClosePosition(self.extend(request, params))
        else:
            market = self.market(symbol)
            request: dict = {
                'symbol': market['id'],
            }
            response = self.swapV2PrivatePostTradeCloseAllPositions(self.extend(request, params))
        #
        # swapV1PrivatePostTradeClosePosition
        #
        #    {
        #        "code": 0,
        #        "msg": "",
        #        "timestamp": 1710992264190,
        #        "data": {
        #            "orderId": 1770656007907930112,
        #            "positionId": "1751667128353910784",
        #            "symbol": "LTC-USDT",
        #            "side": "Ask",
        #            "type": "MARKET",
        #            "positionSide": "Long",
        #            "origQty": "0.2"
        #        }
        #    }
        #
        # swapV2PrivatePostTradeCloseAllPositions
        #
        #    {
        #        "code": 0,
        #        "msg": "",
        #        "data": {
        #            "success": [
        #                1727686766700486656,
        #            ],
        #            "failed": null
        #        }
        #    }
        #
        data = self.safe_dict(response, 'data')
        return self.parse_order(data)
def enable_logging():
      
    environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
    tf_logger = get_tensorflow_logger()
    tf_logger.set_verbosity(tf_logger.ERROR)
def focused_targets(self):  # type: () -> t.Optional[t.List[str]]
        
        return self.focused_command_targets.get(self.command)
def cmd_export(conanfile_path, conanfile, reference, keep_source, output, client_cache):
     
    
    logger.debug("Exporting %s" % conanfile_path)
    output.highlight("Exporting package recipe")

    conan_linter(conanfile_path, output)
    for field in ["url", "license", "description"]:
        field_value = getattr(conanfile, field, None)
        if not field_value:
            output.warn("Conanfile doesn't have '%s'.\n"
                        "It is recommended to add it as attribute" % field)

    conan_ref_str = str(reference)
    # Maybe a platform check could be added, but depends on disk partition
    refs = search_recipes(client_cache, conan_ref_str, ignorecase=True)
    if refs and reference not in refs:
        raise ConanException("Cannot export package with same name but different case\n"
                             "You exported '%s' but already existing '%s'"
                             % (conan_ref_str, " ".join(str(s) for s in refs)))

    with client_cache.conanfile_write_lock(reference):
        _export_conanfile(conanfile_path, conanfile.output, client_cache, conanfile, reference,
                          keep_source)
def area_closing(image, area_threshold, connectivity=2,
                 parent=None, tree_traverser=None):
    
    
    # inversion of the input image
    image_inv = invert(image)
    output = image_inv.copy()

    if parent is None or tree_traverser is None:
        parent, tree_traverser = build_max_tree(image_inv, connectivity)

    area = _max_tree._compute_area(image_inv.ravel(), parent.ravel(), tree_traverser)

    _max_tree._direct_filter(image_inv.ravel(), output.ravel(), parent.ravel(),
                             tree_traverser, area, area_threshold)

    # inversion of the output image
    output = invert(output)

    return output
def local_maxima(image, label=False, connectivity=2,
                 parent=None, tree_traverser=None):
    
    

    output = np.ones(image.shape, dtype=np.uint64)

    if parent is None or tree_traverser is None:
        parent, tree_traverser = build_max_tree(image, connectivity)

    _max_tree._local_maxima(image.ravel(), output.ravel(), label, 
                            parent.ravel(), tree_traverser)

    return output
def max_tree_local_maxima(image, connectivity=2,
                          parent=None, tree_traverser=None):
    
    

    output = np.ones(image.shape, dtype=np.uint64)

    if parent is None or tree_traverser is None:
        parent, tree_traverser = max_tree(image, connectivity)

    _max_tree._local_maxima(image.ravel(), output.ravel(),
                            parent.ravel(), tree_traverser)

    return output
def __init__(self, start, goal, obstacle_list, rand_area,
                 goal_sample_rate=10,
                 max_iter=200,
                 robot_radius=0.0
                 ):
        
        

        
        self.start = self.Node(start[0], start[1], start[2])
        self.end = self.Node(goal[0], goal[1], goal[2])
        self.min_rand = rand_area[0]
        self.max_rand = rand_area[1]
        self.goal_sample_rate = goal_sample_rate
        self.max_iter = max_iter
        self.obstacle_list = obstacle_list

        self.curvature = 1.0  # for dubins path
        self.goal_yaw_th = np.deg2rad(1.0)
        self.goal_xy_th = 0.5
        self.robot_radius = robot_radius
def get_config_file(self, configfile):
          
        if configfile is not None:
            if not os.path.isfile(configfile):
                err = "Config file does not exist at: {}".format(configfile)
                logger.error(err)
                raise ValueError(err)
            return configfile
        dirname = os.path.dirname(sys.modules[self.__module__].__file__)
        folder, fname = os.path.split(dirname)
        retval = os.path.join(os.path.dirname(folder), "config", "{}.ini".format(fname))
        logger.debug("Config File location: '%s'", retval)
        return retval
def unregister_serializer(format):
    ""
    del _serializers[format]
def get_command_runner(self,
                           log_prefix,
                           node_id,
                           auth_config,
                           cluster_name,
                           process_runner,
                           use_internal_ip,
                           docker_config=None):
         
        
        common_args = {
            "log_prefix": log_prefix,
            "node_id": node_id,
            "provider": self,
            "auth_config": auth_config,
            "cluster_name": cluster_name,
            "process_runner": process_runner,
            "use_internal_ip": use_internal_ip
        }
        if docker_config and docker_config["container_name"] != "":
            return DockerCommandRunner(docker_config, **common_args)
        else:
            return SSHCommandRunner(**common_args)
def create_node(self, node_config: Dict[str, Any], tags: Dict[str, str],
                    count: int) -> Optional[Dict[str, Any]]:
        
        
        raise NotImplementedError
def terminate_node(self, node_id: str) -> Optional[Dict[str, Any]]:
        
        
        raise NotImplementedError
def forward(self, wav):
        
        
        STFT = self.params['compute_STFT'](wav)
        features = self.params['compute_spectrogram'](STFT)

        if self.feature_type in ['fbank', 'mfcc']:
            features = self.params['compute_fbanks'](features)
        if self.feature_type == 'mfcc':
            features = self.params['compute_mfccs'](features)

        if self.deltas:
            delta1 = self.params['compute_deltas'](features)
            delta2 = self.params['compute_deltas'](delta1)
            features = torch.cat([features, delta1, delta2], dim=-2)

        if self.context:
            features = self.params['context_window'](features)

        return features
def get_recommended_final_chunk_count(self, frames_per_chunk: int) -> int:
        
        

        return (
            upalign_value(self.get_required_padding(), frames_per_chunk)
            // frames_per_chunk
        )
def load_from_checkpoint(
        cls,
        checkpoint_path: Union[_PATH, IO],
        map_location: _MAP_LOCATION_TYPE = None,
        hparams_file: Optional[_PATH] = None,
        strict: Optional[bool] = None,
        **kwargs: Any,
    ) -> Self:
        

        
        loaded = _load_from_checkpoint(
            cls,  # type: ignore[arg-type]
            checkpoint_path,
            map_location,
            hparams_file,
            strict,
            **kwargs,
        )
        return cast(Self, loaded)
def predict_step(self, *args: Any, **kwargs: Any) -> Any:
        
        
        # For backwards compatibility
        batch = kwargs.get("batch", args[0])
        return self(batch)
def get_handle_for_endpoint(
        self, target_app_name: str
    ) -> Optional[Tuple[str, RayServeHandle, str, bool]]:
        
        
        for endpoint_tag, handle in self.handles.items():
            # If the target_app_name matches with the endpoint or if
            # there is only one endpoint.
            if target_app_name == endpoint_tag.app or len(self.handles) == 1:
                endpoint_info = self.endpoints[endpoint_tag]
                return (
                    endpoint_info.route,
                    handle,
                    endpoint_tag.app,
                    endpoint_info.app_is_cross_language,
                )

        return None
def getatom(self, atomends=None):
        
        atomlist = ['']
        if atomends is None:
            atomends = self.atomends

        while self.pos < len(self.field):
            if self.field[self.pos] in atomends:
                break
            else: atomlist.append(self.field[self.pos])
            self.pos = self.pos + 1

        return ''.join(atomlist)
def gotonext(self):
        
        wslist = []
        while self.pos < len(self.field):
            if self.field[self.pos] in self.LWS + '\n\r':
                if self.field[self.pos] not in '\n\r':
                    wslist.append(self.field[self.pos])
                self.pos += 1
            elif self.field[self.pos] == '(':
                self.commentlist.append(self.getcomment())
            else:
                break
        return EMPTYSTRING.join(wslist)
def mktime_tz(data):
    
    if data[9] is None:
        # No zone info, so localtime is better assumption than GMT
        return time.mktime(data[:8] + (-1,))
    else:
        t = calendar.timegm(data)
        return t - data[9]
def _setup_dataloader(
        self, dataloader: DataLoader, use_distributed_sampler: bool = True, move_to_device: bool = True
    ) -> DataLoader:
        
        
        sampler = dataloader.sampler
        if use_distributed_sampler and self._requires_distributed_sampler(dataloader):
            sampler = self._get_distributed_sampler(dataloader, **self._strategy.distributed_sampler_kwargs)

        # the dataloader needs to be re-instantiated because we want to update the input arguments (e.g., sampler)
        dataloader = _update_dataloader(dataloader, sampler)

        # add worker_init_fn for correct seeding in worker processes
        _auto_add_worker_init_fn(dataloader, self.global_rank)

        dataloader = self._strategy.process_dataloader(dataloader)
        device = self.device if move_to_device and not isinstance(self._strategy, XLAStrategy) else None
        lite_dataloader = _FabricDataLoader(dataloader=dataloader, device=device)
        lite_dataloader = cast(DataLoader, lite_dataloader)
        return lite_dataloader
def load(
        self,
        path: Union[str, Path],
        state: Optional[Dict[str, Union[nn.Module, Optimizer, Any]]] = None,
        strict: bool = True,
    ) -> Dict[str, Any]:
        
        
        unwrapped_state = _unwrap_objects(state)
        remainder = self._strategy.load_checkpoint(path=path, state=unwrapped_state, strict=strict)
        self.barrier()
        if state is not None:
            # We need to unwrap objects (see above) but this creates a new dictionary. In-place updates
            # (for user metadata) wouldn't show up in the original dict, so we need to copy the data back.
            for k in list(unwrapped_state.keys()):
                obj = _unwrap_compiled(state[k])
                if isinstance(obj, (_FabricModule, _FabricOptimizer, _FabricDataLoader)):
                    continue
                state[k] = unwrapped_state[k]
        return remainder
def sharded_model(self) -> Generator:
        
        
        rank_zero_deprecation("`Fabric.sharded_model()` is deprecated in favor of `Fabric.init_module()`.")
        with _old_sharded_model_context(self._strategy):
            yield
def init_module(self, empty_init: Optional[bool] = None) -> ContextManager:
        

        
        self._validate_launched()
        return self._strategy.module_init_context(empty_init=empty_init)
def save(
        self,
        path: Union[str, Path],
        state: Dict[str, Union[nn.Module, Optimizer, Any]],
        filter: Optional[Dict[str, Callable[[str, Any], bool]]] = None,
    ) -> None:
        
        
        if filter is not None:
            if not isinstance(filter, dict):
                raise TypeError(f"Filter should be a dictionary, given {filter!r}")
            if not set(filter).issubset(state):
                raise ValueError(
                    f"The filter keys {filter.keys() - state} are not present in the state keys {set(state)}."
                )
            for k, v in filter.items():
                if not callable(v):
                    raise TypeError(f"Expected `fabric.save(filter=...)` for key {k!r} to be a callable, given {v!r}")
        self._strategy.save_checkpoint(path=path, state=_unwrap_objects(state), filter=filter)
        self.barrier()
def setup(
        self,
        module: nn.Module,
        *optimizers: Optimizer,
        move_to_device: bool = True,
        _reapply_compile: Optional[bool] = None,
    ) -> Any:  # no specific return because the way we want our API to look does not play well with mypy
        

        
        self._validate_setup(module, optimizers)
        module, compile_kwargs = _unwrap_compiled(module) if _reapply_compile else (module, None)
        original_module = module

        module = self._precision.convert_module(module)

        if move_to_device:
            module = self._move_model_to_device(model=module, optimizers=list(optimizers))

        # Let accelerator/plugin wrap and connect the models and optimizers
        if optimizers:
            module, optimizers = self._strategy.setup_module_and_optimizers(  # type: ignore[assignment]
                module, list(optimizers)
            )
        else:
            module = self._strategy.setup_module(module)

        if compile_kwargs is not None:
            module = _to_compiled(module, compile_kwargs)
        module = _FabricModule(module, self._precision, original_module=original_module)

        # Update the _DeviceDtypeModuleMixin's device parameter
        # NOTE: for sharded strategies or manual device placement, there's no single root device
        _update_properties(
            module, device=self.device if move_to_device else next(module.parameters(), torch.tensor(0)).device
        )

        optimizers = [
            _FabricOptimizer(optimizer=optimizer, strategy=self._strategy, callbacks=self._callbacks)
            for optimizer in optimizers
        ]

        self._models_setup += 1

        if hasattr(original_module, "_fabric"):  # this is probably a LightningModule
            original_module._fabric = self
            original_module._fabric_optimizers = optimizers
            if original_module not in self._callbacks:
                self._callbacks.append(original_module)

        self.call("on_after_setup", fabric=self, module=module)

        if optimizers:
            # join both types in a tuple for API convenience
            return (module, *optimizers)
        return module
def minimize_using_explicit_allreduce(tape,
                                      optimizer,
                                      loss,
                                      trainable_variables,
                                      pre_allreduce_callbacks=None,
                                      post_allreduce_callbacks=None,
                                      allreduce_bytes_per_pack=0):
  
  
  if isinstance(optimizer,
                tf_keras.mixed_precision.LossScaleOptimizer):
    # FP16 GPU code path
    with tape:
      scaled_loss = optimizer.get_scaled_loss(loss)
    scaled_grads = tape.gradient(scaled_loss, trainable_variables)
    grads_and_vars = zip(scaled_grads, trainable_variables)
    if pre_allreduce_callbacks:
      grads_and_vars = _run_callbacks(pre_allreduce_callbacks, grads_and_vars)
    (allreduced_scaled_grads,
     filtered_training_vars) = _filter_and_allreduce_gradients(
         grads_and_vars,
         allreduce_precision="float16",
         bytes_per_pack=allreduce_bytes_per_pack)
    allreduced_unscaled_grads = optimizer.get_unscaled_gradients(
        allreduced_scaled_grads)
    grads_and_vars = zip(allreduced_unscaled_grads, filtered_training_vars)
  else:
    # TPU or FP32 GPU code path
    grads = tape.gradient(loss, trainable_variables)
    grads_and_vars = zip(grads, trainable_variables)
    if pre_allreduce_callbacks:
      grads_and_vars = _run_callbacks(pre_allreduce_callbacks, grads_and_vars)
    (allreduced_grads,
     filtered_training_vars) = _filter_and_allreduce_gradients(
         grads_and_vars,
         allreduce_precision="float32",
         bytes_per_pack=allreduce_bytes_per_pack)
    grads_and_vars = zip(allreduced_grads, filtered_training_vars)
  if post_allreduce_callbacks:
    grads_and_vars = _run_callbacks(post_allreduce_callbacks, grads_and_vars)
  optimizer.apply_gradients(
      grads_and_vars, experimental_aggregate_gradients=False)
def test_loading_own_specs():
    
    loader = CustomLoader({})
    # first test that specs remain intact
    sqs_query_description = loader.load_service_model("sqs", "service-2")
    assert sqs_query_description["metadata"]["protocol"] == "query"
    sqs_json_description = loader.load_service_model("sqs-json", "service-2")
    assert sqs_json_description["metadata"]["protocol"] == "json"
def validate_config(self, config: AlgorithmConfigDict) -> None:
        
        
        # Call super's validation method.
        super().validate_config(config)

        if isinstance(config["entropy_coeff"], int):
            config["entropy_coeff"] = float(config["entropy_coeff"])

        if config["entropy_coeff"] < 0.0:
            raise DeprecationWarning("entropy_coeff must be >= 0.0")

        # SGD minibatch size must be smaller than train_batch_size (b/c
        # we subsample a batch of `sgd_minibatch_size` from the train-batch for
        # each `num_sgd_iter`).
        # Note: Only check this if `train_batch_size` > 0 (DDPPO sets this
        # to -1 to auto-calculate the actual batch size later).
        if (
            config["train_batch_size"] > 0
            and config["sgd_minibatch_size"] > config["train_batch_size"]
        ):
            raise ValueError(
                "`sgd_minibatch_size` ({}) must be <= "
                "`train_batch_size` ({}).".format(
                    config["sgd_minibatch_size"], config["train_batch_size"]
                )
            )

        # Check for mismatches between `train_batch_size` and
        # `rollout_fragment_length` and auto-adjust `rollout_fragment_length`
        # if necessary.
        # Note: Only check this if `train_batch_size` > 0 (DDPPO sets this
        # to -1 to auto-calculate the actual batch size later).
        num_workers = config["num_workers"] or 1
        calculated_min_rollout_size = (
            num_workers
            * config["num_envs_per_worker"]
            * config["rollout_fragment_length"]
        )
        if (
            config["train_batch_size"] > 0
            and config["train_batch_size"] % calculated_min_rollout_size != 0
        ):
            new_rollout_fragment_length = math.ceil(
                config["train_batch_size"]
                / (num_workers * config["num_envs_per_worker"])
            )
            logger.warning(
                "`train_batch_size` ({}) cannot be achieved with your other "
                "settings (num_workers={} num_envs_per_worker={} "
                "rollout_fragment_length={})! Auto-adjusting "
                "`rollout_fragment_length` to {}.".format(
                    config["train_batch_size"],
                    config["num_workers"],
                    config["num_envs_per_worker"],
                    config["rollout_fragment_length"],
                    new_rollout_fragment_length,
                )
            )
            config["rollout_fragment_length"] = new_rollout_fragment_length

        # Episodes may only be truncated (and passed into PPO's
        # `postprocessing_fn`), iff generalized advantage estimation is used
        # (value function estimate at end of truncated episode to estimate
        # remaining value).
        if config["batch_mode"] == "truncate_episodes" and not config["use_gae"]:
            raise ValueError(
                "Episode truncation is not supported without a value "
                "function (to estimate the return at the end of the truncated"
                " trajectory). Consider setting "
                "batch_mode=complete_episodes."
            )

        # Multi-agent mode and multi-GPU optimizer.
        if config["multiagent"]["policies"] and not config["simple_optimizer"]:
            logger.info(
                "In multi-agent mode, policies will be optimized sequentially"
                " by the multi-GPU optimizer. Consider setting "
                "simple_optimizer=True if this doesn't work for you."
            )
def training(
        self,
        *,
        lr_schedule: Optional[List[List[Union[int, float]]]] = NotProvided,
        use_critic: Optional[bool] = NotProvided,
        use_gae: Optional[bool] = NotProvided,
        lambda_: Optional[float] = NotProvided,
        use_kl_loss: Optional[bool] = NotProvided,
        kl_coeff: Optional[float] = NotProvided,
        kl_target: Optional[float] = NotProvided,
        mini_batch_size_per_learner: Optional[int] = NotProvided,
        sgd_minibatch_size: Optional[int] = NotProvided,
        num_sgd_iter: Optional[int] = NotProvided,
        shuffle_sequences: Optional[bool] = NotProvided,
        vf_loss_coeff: Optional[float] = NotProvided,
        entropy_coeff: Optional[float] = NotProvided,
        entropy_coeff_schedule: Optional[List[List[Union[int, float]]]] = NotProvided,
        clip_param: Optional[float] = NotProvided,
        vf_clip_param: Optional[float] = NotProvided,
        grad_clip: Optional[float] = NotProvided,
        # Deprecated.
        vf_share_layers=DEPRECATED_VALUE,
        **kwargs,
    ) -> "PPOConfig":
        
        
        # Pass kwargs onto super's `training()` method.
        super().training(**kwargs)

        if use_critic is not NotProvided:
            self.use_critic = use_critic
            # TODO (Kourosh) This is experimental.
            #  Don't forget to remove .use_critic from algorithm config.
        if use_gae is not NotProvided:
            self.use_gae = use_gae
        if lambda_ is not NotProvided:
            self.lambda_ = lambda_
        if use_kl_loss is not NotProvided:
            self.use_kl_loss = use_kl_loss
        if kl_coeff is not NotProvided:
            self.kl_coeff = kl_coeff
        if kl_target is not NotProvided:
            self.kl_target = kl_target
        if mini_batch_size_per_learner is not NotProvided:
            self.mini_batch_size_per_learner = mini_batch_size_per_learner
        if sgd_minibatch_size is not NotProvided:
            self.sgd_minibatch_size = sgd_minibatch_size
        if num_sgd_iter is not NotProvided:
            self.num_sgd_iter = num_sgd_iter
        if shuffle_sequences is not NotProvided:
            self.shuffle_sequences = shuffle_sequences
        if vf_loss_coeff is not NotProvided:
            self.vf_loss_coeff = vf_loss_coeff
        if entropy_coeff is not NotProvided:
            self.entropy_coeff = entropy_coeff
        if clip_param is not NotProvided:
            self.clip_param = clip_param
        if vf_clip_param is not NotProvided:
            self.vf_clip_param = vf_clip_param
        if grad_clip is not NotProvided:
            self.grad_clip = grad_clip

        # TODO (sven): Remove these once new API stack is only option for PPO.
        if lr_schedule is not NotProvided:
            self.lr_schedule = lr_schedule
        if entropy_coeff_schedule is not NotProvided:
            self.entropy_coeff_schedule = entropy_coeff_schedule

        return self
def delete_dsstore(path, files_to_delete=('.DS_Store', '__MACOSX')):
    
    
    
    # Delete Apple .DS_store files
    for file in files_to_delete:
        matches = list(Path(path).rglob(file))
        LOGGER.info(f'Deleting {file} files: {matches}')
        for f in matches:
            f.unlink()
def safe_download(url,
                  file=None,
                  dir=None,
                  unzip=True,
                  delete=False,
                  curl=False,
                  retry=3,
                  min_bytes=1E0,
                  progress=True):
    
    
    

    # Check if the URL is a Google Drive link
    gdrive = url.startswith('https://drive.google.com/')
    if gdrive:
        url, file = get_google_drive_file_info(url)

    f = Path(dir or '.') / (file or url2file(url))  # URL converted to filename
    if not f.is_file():  # URL and file do not exist
        desc = f"Downloading {url if gdrive else clean_url(url)} to '{f}'"
        LOGGER.info(f'{desc}...')
        f.parent.mkdir(parents=True, exist_ok=True)  # make directory if missing
        check_disk_space(url)
        for i in range(retry + 1):
            try:
                if curl or i > 0:  # curl download with retry, continue
                    s = 'sS' * (not progress)  # silent
                    r = subprocess.run(['curl', '-#', f'-{s}L', url, '-o', f, '--retry', '3', '-C', '-']).returncode
                    assert r == 0, f'Curl return value {r}'
                else:  # urllib download
                    method = 'torch'
                    if method == 'torch':
                        torch.hub.download_url_to_file(url, f, progress=progress)
                    else:
                        with request.urlopen(url) as response, TQDM(total=int(response.getheader('Content-Length', 0)),
                                                                    desc=desc,
                                                                    disable=not progress,
                                                                    unit='B',
                                                                    unit_scale=True,
                                                                    unit_divisor=1024) as pbar:
                            with open(f, 'wb') as f_opened:
                                for data in response:
                                    f_opened.write(data)
                                    pbar.update(len(data))

                if f.exists():
                    if f.stat().st_size > min_bytes:
                        break  # success
                    f.unlink()  # remove partial downloads
            except Exception as e:
                if i == 0 and not is_online():
                    raise ConnectionError(emojis(f'❌  Download failure for {url}. Environment is not online.')) from e
                elif i >= retry:
                    raise ConnectionError(emojis(f'❌  Download failure for {url}. Retry limit reached.')) from e
                LOGGER.warning(f'⚠️ Download failure, retrying {i + 1}/{retry} {url}...')

    if unzip and f.exists() and f.suffix in ('', '.zip', '.tar', '.gz'):
        from zipfile import is_zipfile

        unzip_dir = dir or f.parent  # unzip to dir if provided else unzip in place
        if is_zipfile(f):
            unzip_dir = unzip_file(file=f, path=unzip_dir, progress=progress)  # unzip
        elif f.suffix in ('.tar', '.gz'):
            LOGGER.info(f'Unzipping {f} to {unzip_dir.resolve()}...')
            subprocess.run(['tar', 'xf' if f.suffix == '.tar' else 'xfz', f, '--directory', unzip_dir], check=True)
        if delete:
            f.unlink()  # remove zip
        return unzip_dir
def download(url, dir=Path.cwd(), unzip=True, delete=False, curl=False, threads=1, retry=3, exist_ok=False):
    
    
    
    dir = Path(dir)
    dir.mkdir(parents=True, exist_ok=True)  # make directory
    if threads > 1:
        with ThreadPool(threads) as pool:
            pool.map(
                lambda x: safe_download(url=x[0],
                                        dir=x[1],
                                        unzip=unzip,
                                        delete=delete,
                                        curl=curl,
                                        retry=retry,
                                        exist_ok=exist_ok,
                                        progress=threads <= 1), zip(url, repeat(dir)))
            pool.close()
            pool.join()
    else:
        for u in [url] if isinstance(url, (str, Path)) else url:
            safe_download(url=u, dir=dir, unzip=unzip, delete=delete, curl=curl, retry=retry, exist_ok=exist_ok)
def attempt_download_asset(file, repo="ultralytics/assets", release="v8.2.0", **kwargs):
    
    
    
    from ultralytics.utils import SETTINGS  # scoped for circular import

    # YOLOv3/5u updates
    file = str(file)
    file = checks.check_yolov5u_filename(file)
    file = Path(file.strip().replace("'", ""))
    if file.exists():
        return str(file)
    elif (SETTINGS["weights_dir"] / file).exists():
        return str(SETTINGS["weights_dir"] / file)
    else:
        # URL specified
        name = Path(parse.unquote(str(file))).name  # decode '%2F' to '/' etc.
        download_url = f"https://github.com/{repo}/releases/download"
        if str(file).startswith(("http:/", "https:/")):  # download
            url = str(file).replace(":/", "://")  # Pathlib turns :// -> :/
            file = url2file(name)  # parse authentication https://url.com/file.txt?auth...
            if Path(file).is_file():
                LOGGER.info(f"Found {clean_url(url)} locally at {file}")  # file already exists
            else:
                safe_download(url=url, file=file, min_bytes=1e5, **kwargs)

        elif repo == GITHUB_ASSETS_REPO and name in GITHUB_ASSETS_NAMES:
            safe_download(url=f"{download_url}/{release}/{name}", file=file, min_bytes=1e5, **kwargs)

        else:
            tag, assets = get_github_assets(repo, release)
            if not assets:
                tag, assets = get_github_assets(repo)  # latest release
            if name in assets:
                safe_download(url=f"{download_url}/{tag}/{name}", file=file, min_bytes=1e5, **kwargs)

        return str(file)
def lcs(word1, word2, i, j):
    
    
    
    if i == 0 or j == 0:
        return 0
    if word1[i - 1] == word2[j - 1]:
        return 1 + lcs(word1, word2, i - 1, j - 1)
    return max(lcs(word1, word2, i - 1, j), lcs(word1, word2, i, j - 1))
def build(self, build_type=None, target=None, cli_args=None, build_tool_args=None,
              stdout=None, stderr=None):
        

        
        
        self._conanfile.output.info("Running CMake.build()")
        self._build(build_type, target, cli_args, build_tool_args, stdout=stdout, stderr=stderr)
def _refresh_whitelist(self, whitelist: List[str]) -> List[str]:
        
        
        
        sanitized_whitelist = whitelist
        markets = exchange.get_markets()

        markets = [m for m in markets if m['quote'] == self.config['stake_currency']]
        known_pairs = set()
        for market in markets:
            pair = market['symbol']
            # pair is not int the generated dynamic market, or in the blacklist ... ignore it
            if pair not in whitelist or pair in self.config['exchange'].get('pair_blacklist', []):
                continue
            # else the pair is valid
            known_pairs.add(pair)
            # Market is not active
            if not market['active']:
                sanitized_whitelist.remove(pair)
                self.logger.info(
                    'Ignoring %s from whitelist. Market is not active.',
                    pair
                )

        # We need to remove pairs that are unknown
        final_list = [x for x in sanitized_whitelist if x in known_pairs]

        return final_list
def _gen_pair_whitelist(self, base_currency: str, key: str = 'quoteVolume') -> List[str]:
        
        
        

        if not exchange.exchange_has('fetchTickers'):
            raise OperationalException(
                'Exchange does not support dynamic whitelist.'
                'Please edit your config and restart the bot'
            )

        tickers = exchange.get_tickers()
        # check length so that we make sure that '/' is actually in the string
        tickers = [v for k, v in tickers.items()
                   if len(k.split('/')) == 2 and k.split('/')[1] == base_currency]

        sorted_tickers = sorted(tickers, reverse=True, key=lambda t: t[key])
        pairs = [s['symbol'] for s in sorted_tickers]
        return pairs
def get_real_amount(self, trade: Trade, order: Dict, order_amount: float = None) -> float:
        
        
        
        # Init variables
        if order_amount is None:
            order_amount = order['amount']
        # Only run for closed orders
        if trade.fee_updated(order.get('side', '')) or order['status'] == 'open':
            return order_amount

        trade_base_currency = self.exchange.get_pair_base_currency(trade.pair)
        # use fee from order-dict if possible
        if self.exchange.order_has_fee(order):
            fee_cost, fee_currency, fee_rate = self.exchange.extract_cost_curr_rate(order)
            logger.info(f"Fee for Trade {trade} [{order.get('side')}]: "
                        f"{fee_cost:.8g} {fee_currency} - rate: {fee_rate}")

            trade.update_fee(fee_cost, fee_currency, fee_rate, order.get('side', ''))
            if trade_base_currency == fee_currency:
                # Apply fee to amount
                return self.apply_fee_conditional(trade, trade_base_currency,
                                                  amount=order_amount, fee_abs=fee_cost)
            return order_amount
        return self.fee_detection_from_trades(trade, order, order_amount)
def _check_and_execute_exit(self, trade: Trade, exit_rate: float,
                                enter: bool, exit_: bool, exit_tag: Optional[str]) -> bool:
        
        
        
        should_exit: SellCheckTuple = self.strategy.should_exit(
            trade,
            exit_rate,
            datetime.now(timezone.utc),
            enter=enter,
            exit_=exit_,
            force_stoploss=self.edge.stoploss(trade.pair) if self.edge else 0
        )

        if should_exit.sell_flag:
            logger.info(f'Exit for {trade.pair} detected. Reason: {should_exit.sell_type}'
                        f'Tag: {exit_tag if exit_tag is not None else "None"}')
            self.execute_trade_exit(trade, exit_rate, should_exit, exit_tag=exit_tag)
            return True
        return False
def check_for_open_trades(self):
        
        
        
        open_trades = Trade.get_trades([Trade.is_open.is_(True)]).all()

        if len(open_trades) != 0 and self.state != State.RELOAD_CONFIG:
            msg = {
                'type': RPCMessageType.WARNING,
                'status':
                    f"{len(open_trades)} open trades active.\n\n"
                    f"Handle these trades manually on {self.exchange.name}, "
                    f"or '/start' the bot again and use '/stopbuy' "
                    f"to handle open trades gracefully. \n"
                    f"{'Note: Trades are simulated (dry run).' if self.config['dry_run'] else ''}",
            }
            self.rpc.send_msg(msg)
def _init_modules(self) -> None:
        
        
        
        # Initialize all modules

        persistence.init(self.config)
        exchange.init(self.config)

        # Set initial application state
        initial_state = self.config.get('initial_state')

        if initial_state:
            self.state = State[initial_state.upper()]
        else:
            self.state = State.STOPPED
def create_trade(self) -> bool:
        
        
        
        stake_amount = self.config['stake_amount']
        interval = self.analyze.get_ticker_interval()

        logger.info(
            'Checking buy signals to create a new trade with stake_amount: %f ...',
            stake_amount
        )
        whitelist = copy.deepcopy(self.config['exchange']['pair_whitelist'])
        # Check if stake_amount is fulfilled
        if exchange.get_balance(self.config['stake_currency']) < stake_amount:
            raise DependencyException(
                'stake amount is not fulfilled (currency={})'.format(self.config['stake_currency'])
            )

        # Remove currently opened and latest pairs from whitelist
        for trade in Trade.query.filter(Trade.is_open.is_(True)).all():
            if trade.pair in whitelist:
                whitelist.remove(trade.pair)
                logger.debug('Ignoring %s in pair whitelist', trade.pair)

        if not whitelist:
            raise DependencyException('No currency pairs in whitelist')

        # Pick pair based on buy signals
        for _pair in whitelist:
            (buy, sell) = self.analyze.get_signal(_pair, interval)
            if buy and not sell:
                pair = _pair
                break
        else:
            return False

        # Calculate amount
        buy_limit = self.get_target_bid(exchange.get_ticker(pair))
        amount = stake_amount / buy_limit

        order_id = exchange.buy(pair, buy_limit, amount)['id']

        stake_amount_fiat = self.fiat_converter.convert_amount(
            stake_amount,
            self.config['stake_currency'],
            self.config['fiat_display_currency']
        )

        # Create trade entity and return
        self.rpc.send_msg(
            '*{}:* Buying [{}]({}) with limit `{:.8f} ({:.6f} {}, {:.3f} {})` '
            .format(
                exchange.get_name(),
                pair.replace('_', '/'),
                exchange.get_pair_detail_url(pair),
                buy_limit,
                stake_amount,
                self.config['stake_currency'],
                stake_amount_fiat,
                self.config['fiat_display_currency']
            )
        )
        # Fee is applied twice because we make a LIMIT_BUY and LIMIT_SELL
        fee = exchange.get_fee(symbol=pair, taker_or_maker='maker')
        trade = Trade(
            pair=pair,
            stake_amount=stake_amount,
            amount=amount,
            fee_open=fee,
            fee_close=fee,
            open_rate=buy_limit,
            open_rate_requested=buy_limit,
            open_date=datetime.utcnow(),
            exchange=exchange.get_id(),
            open_order_id=order_id
        )
        Trade.session.add(trade)
        Trade.session.flush()
        return True
def execute_buy(self, pair: str, stake_amount: float, price: Optional[float] = None) -> bool:
        
        
        
        time_in_force = self.strategy.order_time_in_force['buy']

        if price:
            buy_limit_requested = price
        else:
            # Calculate price
            buy_limit_requested = self.get_buy_rate(pair, True)

        min_stake_amount = self._get_min_pair_stake_amount(pair, buy_limit_requested)
        if min_stake_amount is not None and min_stake_amount > stake_amount:
            logger.warning(
                f"Can't open a new trade for {pair}: stake amount "
                f"is too small ({stake_amount} < {min_stake_amount})"
            )
            return False

        amount = stake_amount / buy_limit_requested
        order_type = self.strategy.order_types['buy']
        order = self.exchange.buy(pair=pair, ordertype=order_type,
                                  amount=amount, rate=buy_limit_requested,
                                  time_in_force=time_in_force)
        order_id = order['id']
        order_status = order.get('status', None)

        # we assume the order is executed at the price requested
        buy_limit_filled_price = buy_limit_requested

        if order_status == 'expired' or order_status == 'rejected':
            order_tif = self.strategy.order_time_in_force['buy']

            # return false if the order is not filled
            if float(order['filled']) == 0:
                logger.warning('Buy %s order with time in force %s for %s is %s by %s.'
                               ' zero amount is fulfilled.',
                               order_tif, order_type, pair, order_status, self.exchange.name)
                return False
            else:
                # the order is partially fulfilled
                # in case of IOC orders we can check immediately
                # if the order is fulfilled fully or partially
                logger.warning('Buy %s order with time in force %s for %s is %s by %s.'
                               ' %s amount fulfilled out of %s (%s remaining which is canceled).',
                               order_tif, order_type, pair, order_status, self.exchange.name,
                               order['filled'], order['amount'], order['remaining']
                               )
                stake_amount = order['cost']
                amount = order['amount']
                buy_limit_filled_price = order['price']
                order_id = None

        # in case of FOK the order may be filled immediately and fully
        elif order_status == 'closed':
            stake_amount = order['cost']
            amount = order['amount']
            buy_limit_filled_price = order['price']

        # Fee is applied twice because we make a LIMIT_BUY and LIMIT_SELL
        fee = self.exchange.get_fee(symbol=pair, taker_or_maker='maker')
        trade = Trade(
            pair=pair,
            stake_amount=stake_amount,
            amount=amount,
            fee_open=fee,
            fee_close=fee,
            open_rate=buy_limit_filled_price,
            open_rate_requested=buy_limit_requested,
            open_date=datetime.utcnow(),
            exchange=self.exchange.id,
            open_order_id=order_id,
            strategy=self.strategy.get_strategy_name(),
            ticker_interval=timeframe_to_minutes(self.config['ticker_interval'])
        )

        # Update fees if order is closed
        if order_status == 'closed':
            self.update_trade_state(trade, order)

        Trade.session.add(trade)
        Trade.session.flush()

        # Updating wallets
        self.wallets.update()

        self._notify_buy(trade, order_type)

        return True
def execute_sell(self, trade: Trade, limit: float, sell_reason: SellType) -> None:
        
        
        
        # Execute sell and update trade record
        order_id = self.exchange.sell(str(trade.pair), limit, trade.amount)['id']
        trade.open_order_id = order_id
        trade.close_rate_requested = limit
        trade.sell_reason = sell_reason.value

        profit_trade = trade.calc_profit(rate=limit)
        current_rate = self.exchange.get_ticker(trade.pair)['bid']
        profit_percent = trade.calc_profit_percent(limit)
        pair_url = self.exchange.get_pair_detail_url(trade.pair)
        gain = "profit" if profit_percent > 0 else "loss"

        msg = {
            'type': RPCMessageType.SELL_NOTIFICATION,
            'exchange': trade.exchange.capitalize(),
            'pair': trade.pair,
            'gain': gain,
            'market_url': pair_url,
            'limit': limit,
            'amount': trade.amount,
            'open_rate': trade.open_rate,
            'current_rate': current_rate,
            'profit_amount': profit_trade,
            'profit_percent': profit_percent,
        }

        # For regular case, when the configuration exists
        if 'stake_currency' in self.config and 'fiat_display_currency' in self.config:
            stake_currency = self.config['stake_currency']
            fiat_currency = self.config['fiat_display_currency']
            msg.update({
                'stake_currency': stake_currency,
                'fiat_currency': fiat_currency,
            })

        # Send the message
        self.rpc.send_msg(msg)
        Trade.session.flush()
def _process(self) -> bool:
        
        
        
        state_changed = False
        try:
            nb_assets = self.config.get('dynamic_whitelist', None)
            # Refresh whitelist based on wallet maintenance
            sanitized_list = self._refresh_whitelist(
                self._gen_pair_whitelist(
                    self.config['stake_currency']
                ) if nb_assets else self.config['exchange']['pair_whitelist']
            )

            # Keep only the subsets of pairs wanted (up to nb_assets)
            self.active_pair_whitelist = sanitized_list[:nb_assets] if nb_assets else sanitized_list

            # Refreshing candles
            self.exchange.refresh_tickers(self.active_pair_whitelist, self.strategy.ticker_interval)

            # Query trades from persistence layer
            trades = Trade.query.filter(Trade.is_open.is_(True)).all()

            # First process current opened trades
            for trade in trades:
                state_changed |= self.process_maybe_execute_sell(trade)

            # Then looking for buy opportunities
            if len(trades) < self.config['max_open_trades']:
                state_changed = self.process_maybe_execute_buy()

            if 'unfilledtimeout' in self.config:
                # Check and handle any timed out open orders
                self.check_handle_timedout()
                Trade.session.flush()

        except TemporaryError as error:
            logger.warning('%s, retrying in 30 seconds...', error)
            time.sleep(constants.RETRY_TIMEOUT)
        except OperationalException:
            tb = traceback.format_exc()
            hint = 'Issue `/start` if you think it is safe to restart.'
            self.rpc.send_msg({
                'type': RPCMessageType.STATUS_NOTIFICATION,
                'status': f'OperationalException:\n```\n{tb}```{hint}'
            })
            logger.exception('OperationalException. Stopping trader ...')
            self.state = State.STOPPED
        return state_changed
def handle_stoploss_on_exchange(self, trade: Trade) -> bool:
        
        
        

        result = False

        # If trade is open and the buy order is fulfilled but there is no stoploss,
        # then we add a stoploss on exchange
        if not trade.open_order_id and not trade.stoploss_order_id:
            if self.edge:
                stoploss = self.edge.stoploss(pair=trade.pair)
            else:
                stoploss = self.strategy.stoploss

            stop_price = trade.open_rate * (1 + stoploss)

            # limit price should be less than stop price.
            # 0.99 is arbitrary here.
            limit_price = stop_price * 0.99

            stoploss_order_id = self.exchange.stoploss_limit(
                pair=trade.pair, amount=trade.amount, stop_price=stop_price, rate=limit_price
            )['id']
            trade.stoploss_order_id = str(stoploss_order_id)
            trade.stoploss_last_update = datetime.now()

        # Or the trade open and there is already a stoploss on exchange.
        # so we check if it is hit ...
        elif trade.stoploss_order_id:
            logger.debug('Handling stoploss on exchange %s ...', trade)
            order = self.exchange.get_order(trade.stoploss_order_id, trade.pair)
            if order['status'] == 'closed':
                trade.sell_reason = SellType.STOPLOSS_ON_EXCHANGE.value
                trade.update(order)
                result = True
            elif self.config.get('trailing_stop', False):
                # if trailing stoploss is enabled we check if stoploss value has changed
                # in which case we cancel stoploss order and put another one with new
                # value immediately
                self.handle_trailing_stoploss_on_exchange(trade, order)

        return result
def enter_positions(self) -> int:
        
        
        
        trades_created = 0

        whitelist = copy.deepcopy(self.active_pair_whitelist)
        if not whitelist:
            logger.info("Active pair whitelist is empty.")
            return trades_created
        # Remove pairs for currently opened trades from the whitelist
        for trade in Trade.get_open_trades():
            if trade.pair in whitelist:
                whitelist.remove(trade.pair)
                logger.debug('Ignoring %s in pair whitelist', trade.pair)

        if not whitelist:
            logger.info("No currency pair in active pair whitelist, "
                        "but checking to sell open trades.")
            return trades_created
        if PairLocks.is_global_lock():
            lock = PairLocks.get_pair_longest_lock('*')
            if lock:
                self.log_once(f"Global pairlock active until "
                              f"{lock.lock_end_time.strftime(constants.DATETIME_PRINT_FORMAT)}. "
                              f"Not creating new trades, reason: {lock.reason}.", logger.info)
            else:
                self.log_once("Global pairlock active. Not creating new trades.", logger.info)
            return trades_created
        # Create entity and execute trade for each pair from whitelist
        for pair in whitelist:
            try:
                trades_created += self.create_trade(pair)
            except DependencyException as exception:
                logger.warning('Unable to create trade for %s: %s', pair, exception)

        if not trades_created:
            logger.debug("Found no buy signals for whitelisted currencies. Trying again...")

        return trades_created
def get_target_bid(self, pair: str) -> float:
        
        
        
        config_bid_strategy = self.config.get('bid_strategy', {})
        if 'use_order_book' in config_bid_strategy and\
                config_bid_strategy.get('use_order_book', False):
            logger.info('Getting price from order book')
            order_book_top = config_bid_strategy.get('order_book_top', 1)
            order_book = self.exchange.get_order_book(pair, order_book_top)
            logger.debug('order_book %s', order_book)
            # top 1 = index 0
            order_book_rate = order_book['bids'][order_book_top - 1][0]
            logger.info('...top %s order book buy rate %0.8f', order_book_top, order_book_rate)
            used_rate = order_book_rate
        else:
            logger.info('Using Last Ask / Last Price')
            ticker = self.exchange.get_ticker(pair)
            if ticker['ask'] < ticker['last']:
                ticker_rate = ticker['ask']
            else:
                balance = self.config['bid_strategy']['ask_last_balance']
                ticker_rate = ticker['ask'] + balance * (ticker['last'] - ticker['ask'])
            used_rate = ticker_rate

        return used_rate
def get_sell_rate(self, pair: str, refresh: bool) -> float:
        
        
        
        if not refresh:
            rate = self._sell_rate_cache.get(pair)
            # Check if cache has been invalidated
            if rate:
                logger.info(f"Using cached sell rate for {pair}.")
                return rate

        ask_strategy = self.config.get('ask_strategy', {})
        if ask_strategy.get('use_order_book', False):
            # This code is only used for notifications, selling uses the generator directly
            logger.info(
                f"Getting price from order book {ask_strategy['price_side'].capitalize()} side."
            )
            rate = next(self._order_book_gen(pair, f"{ask_strategy['price_side']}s"))

        else:
            rate = self.exchange.fetch_ticker(pair)[ask_strategy['price_side']]
        self._sell_rate_cache[pair] = rate
        return rate
def get_buy_rate(self, pair: str, refresh: bool) -> float:
        
        
        
        if not refresh:
            rate = self._buy_rate_cache.get(pair)
            # Check if cache has been invalidated
            if rate:
                logger.info(f"Using cached buy rate for {pair}.")
                return rate

        config_bid_strategy = self.config.get('bid_strategy', {})
        if 'use_order_book' in config_bid_strategy and\
                config_bid_strategy.get('use_order_book', False):
            logger.info('Getting price from order book')
            order_book_top = config_bid_strategy.get('order_book_top', 1)
            order_book = self.exchange.get_order_book(pair, order_book_top)
            logger.debug('order_book %s', order_book)
            # top 1 = index 0
            order_book_rate = order_book['bids'][order_book_top - 1][0]
            logger.info('...top %s order book buy rate %0.8f', order_book_top, order_book_rate)
            used_rate = order_book_rate
        else:
            logger.info('Using Last Ask / Last Price')
            ticker = self.exchange.fetch_ticker(pair)
            if ticker['ask'] < ticker['last']:
                ticker_rate = ticker['ask']
            else:
                balance = self.config['bid_strategy']['ask_last_balance']
                ticker_rate = ticker['ask'] + balance * (ticker['last'] - ticker['ask'])
            used_rate = ticker_rate

        self._buy_rate_cache[pair] = used_rate

        return used_rate
def get_trade_stake_amount(self, pair) -> Optional[float]:
        
        
        
        if self.edge:
            return self.edge.stake_amount(
                pair,
                self.wallets.get_free(self.config['stake_currency']),
                self.wallets.get_total(self.config['stake_currency']),
                Trade.total_open_trades_stakes()
            )
        else:
            stake_amount = self.config['stake_amount']

        if stake_amount == constants.UNLIMITED_STAKE_AMOUNT:
            open_trades = len(Trade.get_open_trades())
            if open_trades >= self.config['max_open_trades']:
                logger.warning("Can't open a new trade: max number of trades is reached")
                return None
            available_amount = self.wallets.get_free(self.config['stake_currency'])
            return available_amount / (self.config['max_open_trades'] - open_trades)

        return self._check_available_stake_amount(stake_amount)
def _calculate_unlimited_stake_amount(self) -> float:
        
        
        
        free_open_trades = self.get_free_open_trades()
        if not free_open_trades:
            return 0

        available_amount = self._get_available_stake_amount()

        return available_amount / free_open_trades
def update_trade_state(self, trade: Trade, order_id: str, action_order: Dict[str, Any] = None,
                           stoploss_order: bool = False, send_msg: bool = True) -> bool:
        
        
        
        if not order_id:
            logger.warning(f'Orderid for trade {trade} is empty.')
            return False

        # Update trade with order values
        logger.info('Found open order for %s', trade)
        try:
            order = action_order or self.exchange.fetch_order_or_stoploss_order(order_id,
                                                                                trade.pair,
                                                                                stoploss_order)
        except InvalidOrderException as exception:
            logger.warning('Unable to fetch order %s: %s', order_id, exception)
            return False

        trade.update_order(order)

        if self.exchange.check_order_canceled_empty(order):
            # Trade has been cancelled on exchange
            # Handling of this will happen in check_handle_timedout.
            return True

        # Try update amount (binance-fix)
        try:
            new_amount = self.get_real_amount(trade, order)
            if not isclose(safe_value_fallback(order, 'filled', 'amount'), new_amount,
                           abs_tol=constants.MATH_CLOSE_PREC):
                order['amount'] = new_amount
                order.pop('filled', None)
                trade.recalc_open_trade_value()
        except DependencyException as exception:
            logger.warning("Could not update trade amount: %s", exception)

        trade.update(order)
        Trade.commit()

        # Updating wallets when order is closed
        if not trade.is_open:
            if send_msg and not stoploss_order and not trade.open_order_id:
                self._notify_exit(trade, '', True)
            self.handle_protections(trade.pair)
            self.wallets.update()
        elif send_msg and not trade.open_order_id:
            # Buy fill
            self._notify_enter(trade, fill=True)

        return False
def handle_timedout_limit_sell(self, trade: Trade, order: Dict) -> str:
        
        
        
        # if trade is not partially completed, just cancel the trade
        if order['remaining'] == order['amount'] or order.get('filled') == 0.0:
            if not self.exchange.check_order_canceled_empty(order):
                reason = "cancelled due to timeout"
                # if trade is not partially completed, just delete the trade
                self.exchange.cancel_order(trade.open_order_id, trade.pair)
                logger.info('Sell order %s for %s.', reason, trade)
            else:
                reason = "cancelled on exchange"
                logger.info('Sell order %s for %s.', reason, trade)

            trade.close_rate = None
            trade.close_rate_requested = None
            trade.close_profit = None
            trade.close_profit_abs = None
            trade.close_date = None
            trade.is_open = True
            trade.open_order_id = None

            return reason

        # TODO: figure out how to handle partially complete sell orders
        return 'partially filled - keeping order open'
def _order_has_fee(self, order: Dict) -> bool:
        
        
        
        if not isinstance(order, dict):
            return False
        return ('fee' in order and order['fee'] is not None
                and (order['fee'].keys() >= {'currency', 'cost'})
                and order['fee']['currency'] is not None
                and order['fee']['cost'] is not None
                )
def update_closed_trades_without_assigned_fees(self):
        
        
        
        trades: List[Trade] = Trade.get_sold_trades_without_assigned_fees()
        for trade in trades:

            if not trade.is_open and not trade.fee_updated('sell'):
                # Get sell fee
                order = trade.select_order('sell', 'closed')
                if order:
                    logger.info(f"Updating sell-fee on trade {trade} for order {order.order_id}.")
                    self.update_trade_state(trade, order.order_id,
                                            order.ft_order_side == 'stoploss')
def handle_insufficient_funds(self, trade: Trade):
        
        
        
        logger.info(f"Trying to refind lost order for {trade}")
        for order in trade.orders:
            logger.info(f"Trying to refind {order}")
            fo = None
            if not order.ft_is_open:
                logger.debug(f"Order {order} is no longer open.")
                continue
            try:
                fo = self.exchange.fetch_order_or_stoploss_order(order.order_id, order.ft_pair,
                                                                 order.ft_order_side == 'stoploss')
                if order.ft_order_side == 'stoploss':
                    if fo and fo['status'] == 'open':
                        # Assume this as the open stoploss order
                        trade.stoploss_order_id = order.order_id
                elif order.ft_order_side == trade.exit_side:
                    if fo and fo['status'] == 'open':
                        # Assume this as the open order
                        trade.open_order_id = order.order_id
                elif order.ft_order_side == trade.enter_side:
                    if fo and fo['status'] == 'open':
                        trade.open_order_id = order.order_id
                if fo:
                    logger.info(f"Found {order} for trade {trade}.")
                    self.update_trade_state(trade, order.order_id, fo,
                                            stoploss_order=order.ft_order_side == 'stoploss')

            except ExchangeError:
                logger.warning(f"Error updating {order.order_id}.")
def apply_fee_conditional(self, trade: Trade, trade_base_currency: str,
                              amount: float, fee_abs: float, order_obj: Order) -> Optional[float]:
        
        
        
        self.wallets.update()
        amount_ = trade.amount
        if order_obj.ft_order_side == trade.exit_side or order_obj.ft_order_side == 'stoploss':
            # check against remaining amount!
            amount_ = trade.amount - amount

        if trade.nr_of_successful_entries >= 1 and order_obj.ft_order_side == trade.entry_side:
            # In case of rebuy's, trade.amount doesn't contain the amount of the last entry.
            amount_ = trade.amount + amount

        if fee_abs != 0 and self.wallets.get_free(trade_base_currency) >= amount_:
            # Eat into dust if we own more than base currency
            logger.info(f"Fee amount for {trade} was in base currency - "
                        f"Eating Fee {fee_abs} into dust.")
        elif fee_abs != 0:
            logger.info(f"Applying fee on amount for {trade}, fee={fee_abs}.")
            return fee_abs
        return None
def handle_cancel_enter(
            self, trade: Trade, order: Dict, order_obj: Order,
            reason: str, replacing: Optional[bool] = False
    ) -> bool:
        
        
        
        was_trade_fully_canceled = False
        order_id = order_obj.order_id
        side = trade.entry_side.capitalize()

        if order['status'] not in constants.NON_OPEN_EXCHANGE_STATES:
            filled_val: float = order.get('filled', 0.0) or 0.0
            filled_stake = filled_val * trade.open_rate
            minstake = self.exchange.get_min_pair_stake_amount(
                trade.pair, trade.open_rate, self.strategy.stoploss)

            if filled_val > 0 and minstake and filled_stake < minstake:
                logger.warning(
                    f"Order {order_id} for {trade.pair} not cancelled, "
                    f"as the filled amount of {filled_val} would result in an unexitable trade.")
                return False
            corder = self.exchange.cancel_order_with_result(order_id, trade.pair, trade.amount)
            # if replacing, retry fetching the order 3 times if the status is not what we need
            if replacing:
                retry_count = 0
                while (
                    corder.get('status') not in constants.NON_OPEN_EXCHANGE_STATES
                    and retry_count < 3
                ):
                    sleep(0.5)
                    corder = self.exchange.fetch_order(order_id, trade.pair)
                    retry_count += 1

            # Avoid race condition where the order could not be cancelled coz its already filled.
            # Simply bailing here is the only safe way - as this order will then be
            # handled in the next iteration.
            if corder.get('status') not in constants.NON_OPEN_EXCHANGE_STATES:
                logger.warning(f"Order {order_id} for {trade.pair} not cancelled.")
                return False
        else:
            # Order was cancelled already, so we can reuse the existing dict
            corder = order
            reason = constants.CANCEL_REASON['CANCELLED_ON_EXCHANGE']

        logger.info(f'{side} order {reason} for {trade}.')

        # Using filled to determine the filled amount
        filled_amount = safe_value_fallback2(corder, order, 'filled', 'filled')
        if isclose(filled_amount, 0.0, abs_tol=constants.MATH_CLOSE_PREC):
            was_trade_fully_canceled = True
            # if trade is not partially completed and it's the only order, just delete the trade
            open_order_count = len([
                order for order in trade.orders if order.ft_is_open and order.order_id != order_id
                ])
            if open_order_count < 1 and trade.nr_of_successful_entries == 0 and not replacing:
                logger.info(f'{side} order fully cancelled. Removing {trade} from database.')
                trade.delete()
                reason += f", {constants.CANCEL_REASON['FULLY_CANCELLED']}"
            else:
                self.update_trade_state(trade, order_id, corder)
                logger.info(f'{side} Order timeout for {trade}.')
        else:
            # update_trade_state (and subsequently recalc_trade_from_orders) will handle updates
            # to the trade object
            self.update_trade_state(trade, order_id, corder)

            logger.info(f'Partial {trade.entry_side} order timeout for {trade}.')
            reason += f", {constants.CANCEL_REASON['PARTIALLY_FILLED']}"

        self.wallets.update()
        self._notify_enter_cancel(trade, order_type=self.strategy.order_types['entry'],
                                  reason=reason)
        return was_trade_fully_canceled
def test_non_square_precomputed_affinities():
    
    tsne = TSNE(affinity="precomputed")
    assert_raises_regexp(ValueError, "square affinity matrix", tsne.fit,
                         np.array([[0.0], [1.0]]))
def test_non_square_precomputed_affinities():
    
    tsne = TSNE(affinity="precomputed")
    assert_raises_regexp(ValueError, ".* square affinity matrix", tsne.fit,
                         np.array([[0.0], [1.0]]))
def test_pca_initialization_not_compatible_with_precomputed_kernel():
    
    tsne = TSNE(affinity="precomputed", init="pca")
    assert_raises_regexp(ValueError, "The parameter init=\"pca\" cannot be "
                         "used with affinity=\"precomputed\".",
                         tsne.fit_transform, np.array([[0.0], [1.0]]))
def test_pca_initialization_not_compatible_with_precomputed_kernel():
    
    tsne = TSNE(metric="precomputed", init="pca")
    assert_raises_regexp(ValueError, "The parameter init=\"pca\" cannot be "
                         "used with metric=\"precomputed\".",
                         tsne.fit_transform, np.array([[0.0], [1.0]]))
def check_uniform_grid(method, seeds=[0, 1, 2], n_iter=1000):
    
    
    for seed in seeds:
        tsne = TSNE(n_components=2, init='random', random_state=seed,
                    perplexity=20, n_iter=n_iter, method=method)
        Y = tsne.fit_transform(X_2d_grid)

        try_name = "{}_{}".format(method, seed)
        try:
            assert_uniform_grid(Y, try_name)
        except AssertionError:
            # If the test fails a first time, re-run with init=Y to see if
            # this was caused by a bad initialization. Note that this will
            # also run an early_exaggeration step.
            try_name += ":rerun"
            tsne.init = Y
            Y = tsne.fit_transform(X_2d_grid)
            assert_uniform_grid(Y, try_name)
def _update_legacy_config(self) -> bool:
         
        
        logger.debug("Checking for legacy state file update")
        priors = ["dssim_loss", "mask_type", "mask_type", "l2_reg_term", "clipnorm"]
        new_items = ["loss_function", "learn_mask", "mask_type", "loss_function_2",
                     "autoclip"]
        updated = False
        for old, new in zip(priors, new_items):
            if old not in self._config:
                logger.debug("Legacy item '%s' not in config. Skipping update", old)
                continue

            # dssim_loss > loss_function
            if old == "dssim_loss":
                self._config[new] = "ssim" if self._config[old] else "mae"
                del self._config[old]
                updated = True
                logger.info("Updated config from legacy dssim format. New config loss "
                            "function: '%s'", self._config[new])
                continue

            # Add learn mask option and set to True if model has "penalized_mask_loss" specified
            if old == "mask_type" and new == "learn_mask" and new not in self._config:
                self._config[new] = self._config["mask_type"] is not None
                updated = True
                logger.info("Added new 'learn_mask' config item for this model. Value set to: %s",
                            self._config[new])
                continue

            # Replace removed masks with most similar equivalent
            if old == "mask_type" and new == "mask_type" and self._config[old] in ("facehull",
                                                                                   "dfl_full"):
                old_mask = self._config[old]
                self._config[new] = "components"
                updated = True
                logger.info("Updated 'mask_type' from '%s' to '%s' for this model",
                            old_mask, self._config[new])

            # Replace l2_reg_term with the correct loss_2_function and update the value of
            # loss_2_weight
            if old == "l2_reg_term":
                self._config[new] = "mse"
                self._config["loss_weight_2"] = self._config[old]
                del self._config[old]
                updated = True
                logger.info("Updated config from legacy 'l2_reg_term' to 'loss_function_2'")

            # Replace clipnorm with correct gradient clipping type and value
            if old == "clipnorm":
                self._config[new] = self._config[old]
                del self._config[old]
                updated = True
                logger.info("Updated config from legacy '%s' to '%s'", old, new)

        logger.debug("State file updated for legacy config: %s", updated)
        return updated
def output_shapes(self) -> List[Tuple[None, int, int, int]]:
          
        shapes = [cast(Tuple[None, int, int, int], K.int_shape(output))
                  for output in self.model.outputs]
        return shapes
def build_model(self, inputs: T.List[tf.keras.layers.Input]) -> tf.keras.models.Model:
         
        
        raise NotImplementedError
def load(cls, fname):
        

        
        fname_dict = fname + '.d'
        with open(fname_dict, 'rb') as f:
            d = _pickle.load(f)
        index_params = d['index_params']
        query_time_params = d['query_time_params']
        nmslib_instance = cls(model=None, index_params=index_params, query_time_params=query_time_params)
        index = nmslib.init(method='hnsw', space='cosinesimil')
        index.loadIndex(fname)
        nmslib_instance.index = index
        nmslib_instance.labels = d['labels']
        return nmslib_instance
def __init__(self, block3_strides=True, name='DELF', pooling='avg',
               gem_power=3.0, embedding_layer=False, embedding_layer_dim=2048):
    
    
    super(Delf, self).__init__(name=name)

    # Backbone using Keras ResNet50.
    self.backbone = resnet.ResNet50(
        'channels_last',
        name='backbone',
        include_top=False,
        pooling=pooling,
        block3_strides=block3_strides,
        average_pooling=False,
        gem_power=gem_power,
        embedding_layer=embedding_layer,
        embedding_layer_dim=embedding_layer_dim)

    # Attention model.
    self.attention = AttentionModel(name='attention')
def _entry_is_complete(
    entry: config_entries.ConfigEntry, ssdp_rendering_control_location: str | None
) -> bool:
    
    
    return bool(
        entry.unique_id
        and entry.data.get(CONF_MAC)
        and (
            not ssdp_rendering_control_location
            or entry.data.get(CONF_SSDP_RENDERING_CONTROL_LOCATION)
        )
    )
def _read_register(self, register_type, register) -> Optional[float]:
        
        try:
            if register_type == DEFAULT_REGISTER_TYPE_INPUT:
                result = self._hub.read_input_registers(
                    self._slave, register, self._count
                )
            else:
                result = self._hub.read_holding_registers(
                    self._slave, register, self._count
                )
        except ConnectionException:
            self._set_unavailable(register)
            return

        if isinstance(result, (ModbusException, ExceptionResponse)):
            self._set_unavailable(register)
            return

        byte_string = b"".join(
            [x.to_bytes(2, byteorder="big") for x in result.registers]
        )
        val = struct.unpack(self._structure, byte_string)[0]
        register_value = format(
            (self._scale * val) + self._offset, f".{self._precision}f"
        )
        register_value = float(register_value)
        self._available = True

        return register_value
def __init__(
        self,
        hub: ModbusHub,
        config: Dict[str, Any],
    ):
        
        self._hub: ModbusHub = hub
        self._name = config[CONF_NAME]
        self._slave = config[CONF_SLAVE]
        self._target_temperature_register = config[CONF_TARGET_TEMP]
        self._current_temperature_register = config[CONF_CURRENT_TEMP]
        self._current_temperature_register_type = config[
            CONF_CURRENT_TEMP_REGISTER_TYPE
        ]
        self._target_temperature = None
        self._current_temperature = None
        self._data_type = config[CONF_DATA_TYPE]
        self._structure = config[CONF_STRUCTURE]
        self._count = config[CONF_DATA_COUNT]
        self._precision = config[CONF_PRECISION]
        self._scale = config[CONF_SCALE]
        self._scan_interval = timedelta(seconds=config[CONF_SCAN_INTERVAL])
        self._offset = config[CONF_OFFSET]
        self._unit = config[CONF_UNIT]
        self._max_temp = config[CONF_MAX_TEMP]
        self._min_temp = config[CONF_MIN_TEMP]
        self._temp_step = config[CONF_STEP]
        self._available = True
def listen(self, source, emitter):
        
        
        assert isinstance(source, AudioSource), "Source must be an AudioSource"

#        bytes_per_sec = source.SAMPLE_RATE * source.SAMPLE_WIDTH
        sec_per_buffer = float(source.CHUNK) / source.SAMPLE_RATE

        # Every time a new 'listen()' request begins, reset the threshold
        # used for silence detection.  This is as good of a reset point as
        # any, as we expect the user and Mycroft to not be talking.
        # NOTE: adjust_for_ambient_noise() doc claims it will stop early if
        #       speech is detected, but there is no code to actually do that.
        self.adjust_for_ambient_noise(source, 1.0)

        logger.debug("Waiting for wake word...")
        self._wait_until_wake_word(source, sec_per_buffer)

        logger.debug("Recording...")
        emitter.emit("recognizer_loop:record_begin")

        # If enabled, play a wave file with a short sound to audibly
        # indicate recording has begun.
        if config.get('confirm_listening'):
            file = resolve_resource_file(
                config.get('sounds').get('start_listening'))
            if file:
                play_wav(file)

        frame_data = self._record_phrase(source, sec_per_buffer)
        audio_data = self._create_audio_data(frame_data, source)
        emitter.emit("recognizer_loop:record_end")
        logger.debug("Thinking...")

        return audio_data
def mute(self):
        
        with self.read_lock:
            self.muted = True
            self.wrapped_stream.stop_stream()
def _send_wakeword_info(self, emitter):
        
        SessionManager.touch()
        payload = {'utterance': self.wake_word_name,
                   'session': SessionManager.get().session_id}
        emitter.emit("recognizer_loop:wakeword", payload)
def _send_wakeword_info(self, emitter):
        
        
        SessionManager.touch()
        payload = {'utterance': self.wake_word_name,
                   'session': SessionManager.get().session_id}
        emitter.emit("recognizer_loop:wakeword", payload)
def __init__(self, config: dict) -> None:
        
        
        
        self._conf.update(config)

        self._cached_ticker: Dict[str, Any] = {}

        # Holds last candle refreshed time of each pair
        self._pairs_last_refresh_time: Dict[Tuple[str, str], int] = {}

        # Holds candles
        self._klines: Dict[Tuple[str, str], DataFrame] = {}

        # Holds all open sell orders for dry_run
        self._dry_run_open_orders: Dict[str, Any] = {}

        if config['dry_run']:
            logger.info('Instance is running with dry_run enabled')

        exchange_config = config['exchange']
        self._api: ccxt.Exchange = self._init_ccxt(
            exchange_config, ccxt_kwargs=exchange_config.get('ccxt_config'))
        self._api_async: ccxt_async.Exchange = self._init_ccxt(
            exchange_config, ccxt_async, ccxt_kwargs=exchange_config.get('ccxt_async_config'))

        logger.info('Using Exchange "%s"', self.name)

        self.markets = self._load_markets()
        # Check if all pairs are available
        self.validate_pairs(config['exchange']['pair_whitelist'])
        self.validate_ordertypes(config.get('order_types', {}))
        self.validate_order_time_in_force(config.get('order_time_in_force', {}))
        if config.get('ticker_interval'):
            # Check if timeframe is available
            self.validate_timeframes(config['ticker_interval'])
def stoploss_limit(self, pair: str, amount: float, stop_price: float, rate: float) -> Dict:
        
        
        

        raise OperationalException(f"stoploss_limit is not implemented for {self.name}.")
def get_historic_trades(self, pair: str,
                            since: Optional[int] = None,
                            until: Optional[int] = None,
                            from_id: Optional[str] = None) -> List:
        
        
        

        if self._trades_pagination == 'time':
            return asyncio.get_event_loop().run_until_complete(
                self._async_get_trade_history(pair=pair, since=since, until=until))
        elif self._trades_pagination == 'id':
            # Use id-based trade-downloader
            return asyncio.get_event_loop().run_until_complete(
                self._async_get_trade_history_id(pair=pair, since=since,
                                                 until=until, from_id=from_id))
def get_historic_trades(self, pair: str,
                            since: Optional[int] = None,
                            until: Optional[int] = None,
                            from_id: Optional[str] = None) -> List:
        
        
        
        if not until:
            # Current milliseconds
            until = ccxt.Exchange.milliseconds()
        if self._trades_pagination == 'time':
            return asyncio.get_event_loop().run_until_complete(
                self._async_get_trade_history(pair=pair, since=since, until=until))

        elif self._trades_pagination == 'id':
            # Use id-based trade-downloader
            return asyncio.get_event_loop().run_until_complete(
                self._async_get_trade_history_id(pair=pair, since=since,
                                                 until=until, from_id=from_id))
def ohlcv_candle_limit(
            self, timeframe: str, candle_type: CandleType, since_ms: Optional[int]) -> int:
        
        
        
        return int(self._ft_has.get('ohlcv_candle_limit_per_timeframe', {}).get(
            timeframe, self._ft_has.get('ohlcv_candle_limit')))
def validate_stakecurrency(self, stake_currency: str) -> None:
        
        
        
        if not self._markets:
            raise OperationalException(
                'Could not load markets, therefore cannot start. '
                'Please investigate the above error for more details.'
            )
        quote_currencies = self.get_quote_currencies()
        if stake_currency not in quote_currencies:
            raise OperationalException(
                f"{stake_currency} is not available as stake on {self.name}. "
                f"Available currencies are: {', '.join(quote_currencies)}")
def get_valid_pair_combination(self, curr_1, curr_2) -> str:
        
        
        
        for pair in [f"{curr_1}/{curr_2}", f"{curr_2}/{curr_1}"]:
            if pair in self.markets and self.markets[pair].get('active'):
                return pair
        raise DependencyException(f"Could not combine {curr_1} and {curr_2} to get a valid pair.")
def timeframe_to_next_date(timeframe: str, date: datetime = None) -> datetime:
    
    
    
    if not date:
        date = datetime.utcnow()
    timeframe_secs = timeframe_to_seconds(timeframe)
    # Get offset to prev timeframe
    offset = date.timestamp() % timeframe_secs
    # Add remaining seconds to next timeframe
    new_timestamp = date.timestamp() + (timeframe_secs - offset)
    return datetime.fromtimestamp(new_timestamp, tz=timezone.utc)
def combine_funding_and_mark(funding_rates: DataFrame, mark_rates: DataFrame,
                                 futures_funding_rate: Optional[int] = None) -> DataFrame:
        
        
        
        if futures_funding_rate is None:
            return mark_rates.merge(
                funding_rates, on='date', how="inner", suffixes=["_mark", "_fund"])
        else:
            if len(funding_rates) == 0:
                # No funding rate candles - full fillup with fallback variable
                mark_rates['open_fund'] = futures_funding_rate
                return mark_rates.rename(
                        columns={'open': 'open_mark',
                                 'close': 'close_mark',
                                 'high': 'high_mark',
                                 'low': 'low_mark',
                                 'volume': 'volume_mark'})

            else:
                # Fill up missing funding_rate candles with fallback value
                combined = mark_rates.merge(
                    funding_rates, on='date', how="outer", suffixes=["_mark", "_fund"]
                    )
                combined['open_fund'] = combined['open_fund'].fillna(futures_funding_rate)
                return combined
def market_is_pair(market, base_currency: str = None, quote_currency: str = None):
    
    
    
    symbol_parts = market['symbol'].split('/')
    return (len(symbol_parts) == 2 and
            (symbol_parts[0] == base_currency if base_currency else len(symbol_parts[0]) > 0) and
            (symbol_parts[1] == quote_currency if quote_currency else len(symbol_parts[1]) > 0))
def calculate_fee_rate(
            self, fee: Dict, symbol: str, cost: float, amount: float) -> Optional[float]:
        
        
        
        if fee.get('rate') is not None:
            return fee.get('rate')
        fee_curr = fee['currency']
        # Calculate fee based on order details
        if fee_curr in self.get_pair_base_currency(symbol):
            # Base currency - divide by amount
            return round(fee['cost'] / amount, 8)
        elif fee_curr in self.get_pair_quote_currency(symbol):
            # Quote currency - divide by cost
            return round(self._contracts_to_amount(
                symbol, fee['cost']) / cost,
                8) if cost else None
        else:
            # If Fee currency is a different currency
            if not cost:
                # If cost is None or 0.0 -> falsy, return None
                return None
            try:
                comb = self.get_valid_pair_combination(fee_curr, self._config['stake_currency'])
                tick = self.fetch_ticker(comb)

                fee_to_quote_rate = safe_value_fallback2(tick, tick, 'last', 'ask')
            except ExchangeError:
                fee_to_quote_rate = self._config['exchange'].get('unknown_fee_rate', None)
                if not fee_to_quote_rate:
                    return None
            return round((self._contracts_to_amount(
                symbol, fee['cost']) * fee_to_quote_rate) / cost, 8)
def _load_markets(self) -> None:
          
        try:
            self._api.load_markets()
            self._load_async_markets()
            self._last_markets_refresh = arrow.utcnow().timestamp
        except ccxt.BaseError as e:
            logger.warning('Unable to initialize markets. Reason: %s', e)
def cancel_order_with_result(self, order_id: str, pair: str, amount: float) -> Dict:
        
        
        
        try:
            corder = self.cancel_order(order_id, pair)
            if self.is_cancel_order_result_suitable(corder):
                return corder
        except InvalidOrderException:
            logger.warning(f"Could not cancel order {order_id}.")
        try:
            order = self.fetch_order(order_id, pair)
        except InvalidOrderException:
            logger.warning(f"Could not fetch cancelled order {order_id}.")
            order = {'fee': {}, 'status': 'canceled', 'amount': amount, 'info': {}}

        return order
def get_next_limit_in_list(limit: int, limit_range: Optional[List[int]]):
        
        
        
        if not limit_range:
            return limit
        return min([x for x in limit_range if limit <= x])
def market_is_tradable(self, market: Dict[str, Any]) -> bool:
        
        
        
        return (
            market.get('quote', None) is not None
            and market.get('base', None) is not None
            and (self.trading_mode == TradingMode.SPOT and self.market_is_spot(market))
            or (self.trading_mode == TradingMode.MARGIN and self.market_is_margin(market))
            or (self.trading_mode == TradingMode.FUTURES and self.market_is_future(market))
        )
def dry_run_liquidation_price(
        self,
        pair: str,
        open_rate: float,   # Entry price of position
        is_short: bool,
        position: float,  # Absolute value of position size
        wallet_balance: float,  # Or margin balance
        mm_ex_1: float = 0.0,  # (Binance) Cross only
        upnl_ex_1: float = 0.0,  # (Binance) Cross only
    ) -> Optional[float]:
        
        
        

        market = self.markets[pair]
        taker_fee_rate = market['taker']
        mm_ratio, _ = self.get_maintenance_ratio_and_amt(pair, position)

        if self.trading_mode == TradingMode.FUTURES and self.margin_mode == MarginMode.ISOLATED:

            if market['inverse']:
                raise OperationalException(
                    "Freqtrade does not yet support inverse contracts")

            value = wallet_balance / position

            mm_ratio_taker = (mm_ratio + taker_fee_rate)
            if is_short:
                return (open_rate + value) / (1 + mm_ratio_taker)
            else:
                return (open_rate - value) / (1 - mm_ratio_taker)
        else:
            raise OperationalException(
                "Freqtrade only supports isolated futures for leverage trading")
def refresh_latest_ohlcv(self, pair_list: ListPairsWithTimeframes, *,
                             since_ms: Optional[int] = None, cache: bool = True,
                             drop_incomplete: bool = None
                             ) -> Dict[PairWithTimeframe, DataFrame]:
        
        
        
        logger.debug("Refreshing candle (OHLCV) data for %d pairs", len(pair_list))
        # TODO-lev: maybe depend this on candle type?
        drop_incomplete = self._ohlcv_partial_candle if drop_incomplete is None else drop_incomplete
        input_coroutines = []
        cached_pairs = []
        # Gather coroutines to run
        for pair, timeframe, candle_type in set(pair_list):
            if ((pair, timeframe, candle_type) not in self._klines or not cache
                    or self._now_is_time_to_refresh(pair, timeframe, candle_type)):
                if not since_ms and self.required_candle_call_count > 1:
                    # Multiple calls for one pair - to get more history
                    one_call = timeframe_to_msecs(timeframe) * self.ohlcv_candle_limit(timeframe)
                    move_to = one_call * self.required_candle_call_count
                    now = timeframe_to_next_date(timeframe)
                    since_ms = int((now - timedelta(seconds=move_to // 1000)).timestamp() * 1000)

                if since_ms:
                    input_coroutines.append(self._async_get_historic_ohlcv(
                        pair, timeframe, since_ms=since_ms, raise_=True, candle_type=candle_type))
                else:
                    # One call ... "regular" refresh
                    input_coroutines.append(self._async_get_candle_history(
                        pair, timeframe, since_ms=since_ms, candle_type=candle_type))
            else:
                logger.debug(
                    "Using cached candle (OHLCV) data for pair %s, timeframe %s, candleType %s ...",
                    pair, timeframe, candle_type
                )
                cached_pairs.append((pair, timeframe, candle_type))

        results_df = {}
        # Chunk requests into batches of 100 to avoid overwelming ccxt Throttling
        for input_coro in chunks(input_coroutines, 100):
            results = asyncio.get_event_loop().run_until_complete(
                asyncio.gather(*input_coro, return_exceptions=True))

            for res in results:
                if isinstance(res, Exception):
                    logger.warning(f"Async code raised an exception: {repr(res)}")
                    continue
                # Deconstruct tuple (has 4 elements)
                pair, timeframe, c_type, ticks = res
                # keeping last candle time as last refreshed time of the pair
                if ticks:
                    self._pairs_last_refresh_time[(pair, timeframe, c_type)] = ticks[-1][0] // 1000
                # keeping parsed dataframe in cache
                ohlcv_df = ohlcv_to_dataframe(
                    ticks, timeframe, pair=pair, fill_missing=True,
                    drop_incomplete=drop_incomplete)
                results_df[(pair, timeframe, c_type)] = ohlcv_df
                if cache:
                    self._klines[(pair, timeframe, c_type)] = ohlcv_df
        # Return cached klines
        for pair, timeframe, c_type in cached_pairs:
            results_df[(pair, timeframe, c_type)] = self.klines(
                (pair, timeframe, c_type),
                copy=False
            )

        return results_df
def amount_to_precision(self, pair: str, amount: float) -> float:
        
        

        
        return amount_to_precision(amount, self.get_precision_amount(pair), self.precisionMode)
def price_to_precision(self, pair: str, price: float, *, rounding_mode: int = ROUND) -> float:
        
        
        
        return price_to_precision(price, self.get_precision_price(pair),
                                  self.precisionMode, rounding_mode=rounding_mode)
def get_rate(self, pair: str, refresh: bool, side: str = "buy") -> float:
        
        
        
        cache_rate: TTLCache = self._buy_rate_cache if side == "buy" else self._sell_rate_cache
        [strat_name, name] = ['bid_strategy', 'Buy'] if side == "buy" else ['ask_strategy', 'Sell']

        if not refresh:
            rate = cache_rate.get(pair)
            # Check if cache has been invalidated
            if rate:
                logger.debug(f"Using cached {side} rate for {pair}.")
                return rate

        strategy = self._config.get(strat_name, {})

        if strategy.get('use_order_book', False) and ('use_order_book' in strategy):

            order_book_top = strategy.get('order_book_top', 1)
            order_book = self.fetch_l2_order_book(pair, order_book_top)
            logger.debug('order_book %s', order_book)
            # top 1 = index 0
            try:
                rate = order_book[f"{strategy['price_side']}s"][order_book_top - 1][0]
            except (IndexError, KeyError) as e:
                logger.warning(
                    f"{name} Price at location {order_book_top} from orderbook could not be "
                    f"determined. Orderbook: {order_book}"
                )
                raise PricingError from e

            logger.info(f"{name} price from orderbook {strategy['price_side'].capitalize()}"
                        f"side - top {order_book_top} order book {side} rate {rate:.8f}")
        else:
            logger.info(f"Using Last {strategy['price_side'].capitalize()} / Last Price")
            ticker = self.fetch_ticker(pair)
            ticker_rate = ticker[strategy['price_side']]
            if ticker['last']:
                if side == 'buy' and ticker_rate > ticker['last']:
                    balance = strategy['ask_last_balance']
                    ticker_rate = ticker_rate + balance * (ticker['last'] - ticker_rate)
                elif side == 'sell' and ticker_rate < ticker['last']:
                    balance = strategy.get('bid_last_balance', 0.0)
                    ticker_rate = ticker_rate - balance * (ticker_rate - ticker['last'])
            rate = ticker_rate

        if rate is None:
            raise PricingError(f"{name}-Rate for {pair} was empty.")
        cache_rate[pair] = rate

        return rate
def get_max_leverage(self, pair: str, stake_amount: float, price: float) -> float:
        
        
        

        raise OperationalException(f"Leverage is not available on {self.name} using freqtrade")
        return 1.0
def get_max_leverage(self, pair: Optional[str], nominal_value: Optional[float]) -> float:
        
            
        
        raise OperationalException(
            f"{self.name.capitalize()}.get_max_leverage has not been implemented.")
def _calculate_funding_fees(
        self,
        funding_rates: DataFrame,
        mark_rates: DataFrame,
        amount: float,
        open_date: datetime,
        close_date: Optional[datetime] = None
    ) -> float:
        
        
        
        fees: float = 0

        df = funding_rates.merge(mark_rates, on='date', how="inner", suffixes=["_fund", "_mark"])
        if not df.empty:
            df = df[(df['date'] >= open_date) & (df['date'] <= close_date)]
            fees = sum(df['open_fund'] * df['open_mark'] * amount)

        return fees
def _calculate_funding_fees(
        self,
        funding_rates: DataFrame,
        mark_rates: DataFrame,
        amount: float,
        open_date: datetime,
        close_date: Optional[datetime] = None,
        time_in_ratio: Optional[float] = None
    ) -> float:
        
        
        
        fees: float = 0

        df = funding_rates.merge(mark_rates, on='date', how="inner", suffixes=["_fund", "_mark"])
        if not df.empty:
            df = df[(df['date'] >= open_date) & (df['date'] <= close_date)]
            fees = sum(df['open_fund'] * df['open_mark'] * amount)

        return fees
def calculate_funding_fees(
        self,
        funding_rates: DataFrame,
        mark_rates: DataFrame,
        amount: float,
        open_date: datetime,
        close_date: Optional[datetime] = None,
        time_in_ratio: Optional[float] = None
    ) -> float:
        
        
        
        fees: float = 0

        df = funding_rates.merge(mark_rates, on='date', how="inner", suffixes=["_fund", "_mark"])
        if not df.empty:
            df = df[(df['date'] >= open_date) & (df['date'] <= close_date)]
            fees = sum(df['open_fund'] * df['open_mark'] * amount)

        return fees
def calculate_funding_fees(
        self,
        df: DataFrame,
        amount: float,
        open_date: datetime,
        close_date: Optional[datetime] = None,
        time_in_ratio: Optional[float] = None
    ) -> float:
        
        
        
        fees: float = 0

        if not df.empty:
            df = df[(df['date'] >= open_date) & (df['date'] <= close_date)]
            fees = sum(df['open_fund'] * df['open_mark'] * amount)

        return fees
def _fetch_and_calculate_funding_fees(
        self,
        pair: str,
        amount: float,
        is_short: bool,
        open_date: datetime,
        close_date: Optional[datetime] = None
    ) -> float:
        
        
        

        if self.funding_fee_cutoff(open_date):
            open_date += timedelta(hours=1)
        timeframe = self._ft_has['mark_ohlcv_timeframe']
        timeframe_ff = self._ft_has.get('funding_fee_timeframe',
                                        self._ft_has['mark_ohlcv_timeframe'])
        open_date = timeframe_to_prev_date(timeframe, open_date)

        if not close_date:
            close_date = datetime.now(timezone.utc)
        open_timestamp = int(open_date.timestamp()) * 1000
        # close_timestamp = int(close_date.timestamp()) * 1000

        mark_comb: PairWithTimeframe = (
            pair, timeframe, CandleType.from_string(self._ft_has["mark_ohlcv_price"]))

        funding_comb: PairWithTimeframe = (pair, timeframe_ff, CandleType.FUNDING_RATE)
        candle_histories = self.refresh_latest_ohlcv(
            [mark_comb, funding_comb],
            since_ms=open_timestamp,
            cache=False,
            drop_incomplete=False,
        )
        funding_rates = candle_histories[funding_comb]
        mark_rates = candle_histories[mark_comb]
        funding_mark_rates = self.combine_funding_and_mark(
            funding_rates=funding_rates, mark_rates=mark_rates)

        return self.calculate_funding_fees(
            funding_mark_rates,
            amount=amount,
            is_short=is_short,
            open_date=open_date,
            close_date=close_date
        )
def get_funding_fees(self, pair: str, amount: float, open_date: datetime) -> float:
        
        
        
        if self.trading_mode == TradingMode.FUTURES:
            if self._config['dry_run']:
                funding_fees = self._calculate_funding_fees(pair, amount, open_date)
            else:
                funding_fees = self._get_funding_fees_from_exchange(pair, open_date)
            return funding_fees
        else:
            return 0.0
def get_maintenance_ratio_and_amt(
        self,
        pair: str,
        nominal_value: Optional[float] = 0.0,
    ):
        
        
        
        # TODO-lev: return the real amounts
        return 0, 0.4
def get_maintenance_ratio_and_amt(
        self,
        pair: str,
        nominal_value: Optional[float] = 0.0,
    ) -> Tuple[float, Optional[float]]:
        
        
        
        # TODO-lev: return the real amounts
        return (0, 0.4)
def liquidation_price(
        self,
        open_rate: float,   # Entry price of position
        is_short: bool,
        leverage: float,
        mm_ratio: float,
        position: float,  # Absolute value of position size
        trading_mode: TradingMode,
        collateral: Optional[Collateral] = Collateral.ISOLATED,
        maintenance_amt: Optional[float] = None,  # (Binance)
        wallet_balance: Optional[float] = None,  # (Binance and Gateio)
        taker_fee_rate: Optional[float] = None,  # (Gateio & Okex)
        liability: Optional[float] = None,  # (Okex)
        interest: Optional[float] = None,  # (Okex)
        mm_ex_1: Optional[float] = 0.0,  # (Binance) Cross only
        upnl_ex_1: Optional[float] = 0.0,  # (Binance) Cross only
    ) -> Optional[float]:
        
        
        
        if trading_mode == TradingMode.SPOT:
            return None

        if not collateral:
            raise OperationalException(
                "Parameter collateral is required by liquidation_price when trading_mode is "
                f"{trading_mode}"
            )

        return self.liquidation_price_helper(
            open_rate,
            is_short,
            leverage,
            trading_mode,
            mm_ratio,
            collateral,
            maintenance_amt,
            position,
            wallet_balance,
            taker_fee_rate,
            liability,
            interest,
            mm_ex_1,
            upnl_ex_1,
        )
def liquidation_price(
        self,
        open_rate: float,   # Entry price of position
        is_short: bool,
        leverage: float,
        mm_ratio: float,
        position: float,  # Absolute value of position size
        trading_mode: TradingMode,
        collateral: Optional[Collateral] = Collateral.ISOLATED,
        maintenance_amt: Optional[float] = None,  # (Binance)
        wallet_balance: Optional[float] = None,  # (Binance and Gateio)
        taker_fee_rate: Optional[float] = None,  # (Gateio & Okex)
        mm_ex_1: Optional[float] = 0.0,  # (Binance) Cross only
        upnl_ex_1: Optional[float] = 0.0,  # (Binance) Cross only
    ) -> Optional[float]:
        
        
        
        if trading_mode == TradingMode.SPOT:
            return None

        if not collateral:
            raise OperationalException(
                "Parameter collateral is required by liquidation_price when trading_mode is "
                f"{trading_mode}"
            )

        return self.liquidation_price_helper(
            open_rate=open_rate,
            is_short=is_short,
            leverage=leverage,
            mm_ratio=mm_ratio,
            position=position,
            trading_mode=trading_mode,
            collateral=collateral,
            maintenance_amt=maintenance_amt,
            wallet_balance=wallet_balance,
            taker_fee_rate=taker_fee_rate,
            mm_ex_1=mm_ex_1,
            upnl_ex_1=upnl_ex_1,
        )
def fill_leverage_tiers(self) -> None:
        
        
        
        if self._api.has['fetchLeverageTiers']:
            if self.trading_mode == TradingMode.FUTURES:
                leverage_tiers = self.load_leverage_tiers()
                for pair, tiers in leverage_tiers.items():
                    tiers = []
                    for tier in tiers:
                        tiers.append(self.parse_leverage_tier(tier))
                    self._leverage_tiers[pair] = tiers
def fill_leverage_tiers(self) -> None:
        
        
        
        leverage_tiers = self.load_leverage_tiers()
        for pair, tiers in leverage_tiers.items():
            tiers = []
            for tier in tiers:
                tiers.append(self.parse_leverage_tier(tier))
            self._leverage_tiers[pair] = tiers
def _unlock_file(f):
    
    if fcntl:
        fcntl.lockf(f, fcntl.LOCK_UN)
    if os.path.exists(f.name + '.lock'):
        os.remove(f.name + '.lock')
def __init__(
        self,
        coordinator: SleepIQDataUpdateCoordinator,
        bed_id: str,
        side: str,
    ) -> None:
        
        super().__init__(coordinator, bed_id, side, SLEEP_NUMBER)
def async_schedule_install_addon(self, catch_error: bool = False) -> asyncio.Task:
        
        
        if not self._install_task or self._install_task.done():
            LOGGER.info("Z-Wave JS add-on is not installed. Installing add-on")
            self._install_task = self._async_schedule_addon_operation(
                self.async_install_addon, catch_error=catch_error
            )
        return self._install_task
def sock_connect(self, sock, address):
        
        
        if self._debug and sock.gettimeout() != 0:
            raise ValueError("the socket must be non-blocking")

        fut = self.create_future()
        if hasattr(socket, 'AF_UNIX') and sock.family == socket.AF_UNIX:
            self._sock_connect(fut, sock, address)
        else:
            resolved = base_events._ensure_resolved(address, loop=self)
            resolved.add_done_callback(
                lambda resolved: self._on_resolved(fut, sock, resolved))

        return fut
def _next_external_frame(frame, skip_file_prefixes):
    
    frame = frame.f_back
    while frame is not None and (
            _is_internal_filename(filename := frame.f_code.co_filename) or
            _is_filename_to_skip(filename, skip_file_prefixes)):
        frame = frame.f_back
    return frame
def create(
        self,
        arn: str,
        version: str,
        endpoint_options: Optional[DomainEndpointOptions] = None,
        security_options: Optional[SecurityOptions] = None,
        preferred_port: Optional[int] = None,
    ) -> Server:
        
        
        

        # determine custom domain endpoint
        custom_endpoint = determine_custom_endpoint(endpoint_options)

        # determine engine type
        engine_type = versions.get_engine_type(version)

        # build final endpoint and cluster url
        endpoint = build_cluster_endpoint(
            DomainKey.from_arn(arn), custom_endpoint, engine_type, preferred_port
        )
        url = f"http://{endpoint}" if "://" not in endpoint else endpoint

        # call abstract cluster factory
        cluster = self._create_cluster(arn, url, version, custom_endpoint, security_options)
        cluster.start()

        # save cluster into registry and return
        self.clusters[arn] = cluster
        return cluster
def test_pseudo_empty_fixtures(self):
        
        
        
        new_io = StringIO()
        management.call_command('loaddata', 'pets', stdout=new_io, stderr=new_io)
        command_output = new_io.getvalue().strip()
        # No objects will actually be loaded
        self.assertEqual(command_output, "Installed 0 object(s) (of 2) from 1 fixture(s)")
def predict(
        self,
        data: DataBatchType,
        dtype: Optional[Union[torch.dtype, Dict[str, torch.dtype]]] = None,
    ) -> DataBatchType:
        
        
        return super(TorchPredictor, self).predict(data=data, dtype=dtype)
def from_checkpoint(
        cls,
        checkpoint: Checkpoint,
        model: Optional[torch.nn.Module] = None,
        use_gpu: bool = False,
    ) -> "TorchPredictor":
        
        
        model, preprocessor = load_checkpoint(checkpoint, model)
        return TorchPredictor(model=model, preprocessor=preprocessor, use_gpu=use_gpu)
def read_stringnl_noescape_pair(f):
    
    
    

    return "%s %s" % (read_stringnl_noescape(f), read_stringnl_noescape(f))
def read_string1(f):
    
    

    n = read_uint1(f)
    assert n >= 0
    data = f.read(n)
    if len(data) == n:
        return data.decode("latin-1")
    raise ValueError("expected %d bytes in a string1, but only %d remain" %
                     (n, len(data)))
def genops(pickle):
    
    

    import cStringIO as StringIO

    if isinstance(pickle, str):
        pickle = StringIO.StringIO(pickle)

    if hasattr(pickle, "tell"):
        getpos = pickle.tell
    else:
        getpos = lambda: None

    while True:
        pos = getpos()
        code = pickle.read(1)
        opcode = code2op.get(code)
        if opcode is None:
            if code == "":
                raise ValueError("pickle exhausted before seeing STOP")
            else:
                raise ValueError("at position %s, opcode %r unknown" % (
                                 pos is None and "<unknown>" or pos,
                                 code))
        if opcode.arg is None:
            arg = None
        else:
            arg = opcode.arg.reader(pickle)
        yield opcode, arg, pos
        if code == '.':
            assert opcode.name == 'STOP'
            break
def read_decimalnl_short(f):
    
    

    s = read_stringnl(f, decode=False, stripquotes=False)

    # There's a hack for True and False here.
    if s == b"00":
        return False
    elif s == b"01":
        return True

    return int(s)
def maybe_start_scheduling_tasks(self):
        
        
        tasks_to_start = (
            self.target_num_scheduling_tasks - self.curr_num_scheduling_tasks
        )
        for _ in range(tasks_to_start):
            self._scheduling_tasks.add(
                self._loop.create_task(self.fulfill_pending_requests())
            )
def __init__(self, num_slots, replay_proportion: float):
        
        
        if replay_proportion > 0 and num_slots == 0:
            raise ValueError(
                "You must set num_slots > 0 if replay_proportion > 0.")
        self.replay_buffer = SimpleReplayBuffer(num_slots)
        self.replay_proportion = replay_proportion
def load_ip_bans_config(path: str):
    
    ip_list = []

    if not os.path.isfile(path):
        return ip_list

    try:
        list_ = load_yaml_config_file(path)
    except HomeAssistantError as err:
        _LOGGER.error('Unable to load %s: %s', path, str(err))
        return ip_list

    for ip_ban, ip_info in list_.items():
        try:
            ip_info = SCHEMA_IP_BAN_ENTRY(ip_info)
            ip_list.append(IpBan(ip_ban, ip_info['banned_at']))
        except vol.Invalid as err:
            _LOGGER.error("Failed to load IP ban %s: %s", ip_info, err)
            continue

    return ip_list
def clean_extension_url(extension: dict, manifest_data: dict):
    
    
    extension_url = extension["url"]
    target = extension.get("target") or AppExtensionTarget.POPUP
    if extension_url.startswith("/"):
        _clean_extension_url_with_only_path(manifest_data, target, extension_url)
    elif target == AppExtensionTarget.APP_PAGE:
        msg = "Url cannot start with protocol when target == APP_PAGE"
        logger.warning(msg)
        raise ValidationError(msg)
    else:
        _clean_app_url(extension_url)
def check_encoding(encoding):
    
    
    
    ET.XML("<?xml version='1.0' encoding='%s'?><xml />" % encoding)
def _read_results(results_file: Path, batch_size: int = 10) -> Iterator[List[Any]]:
        
        
        
        import rapidjson
        logger.info(f"Reading epochs from '{results_file}'")
        with results_file.open('r') as f:
            data = []
            for line in f:
                data += [rapidjson.loads(line)]
                if len(data) >= batch_size:
                    yield data
                    data = []
        yield data
def from_texts(
        cls,
        texts: List[str],
        embedding: Optional[Embeddings] = None,
        metadatas: Optional[List[dict]] = None,
        ids: Optional[List[str]] = None,
        dataset_path: str = _LANGCHAIN_DEFAULT_DEEPLAKE_PATH,
        **kwargs: Any,
    ) -> DeepLake:
        
        
        deeplake_dataset = cls(dataset_path=dataset_path, embedding=embedding, **kwargs)
        deeplake_dataset.add_texts(
            texts=texts,
            metadatas=metadatas,
            ids=ids,
        )
        return deeplake_dataset
def _search(
        self,
        query: Optional[str] = None,
        embedding: Optional[Union[List[float], np.ndarray]] = None,
        embedding_function: Optional[Callable] = None,
        k: int = 4,
        distance_metric: Optional[str] = None,
        use_maximal_marginal_relevance: bool = False,
        fetch_k: Optional[int] = 20,
        filter: Optional[Union[Dict, Callable]] = None,
        return_score: bool = False,
        exec_option: Optional[str] = None,
        deep_memory: bool = False,
        **kwargs: Any,
    ) -> Any[List[Document], List[Tuple[Document, float]]]:
        
        
        

        if kwargs.get("tql"):
            return self._search_tql(
                tql=kwargs["tql"],
                exec_option=exec_option,
                return_score=return_score,
                embedding=embedding,
                embedding_function=embedding_function,
                distance_metric=distance_metric,
                use_maximal_marginal_relevance=use_maximal_marginal_relevance,
                filter=filter,
            )

        if embedding_function:
            if isinstance(embedding_function, Embeddings):
                _embedding_function = embedding_function.embed_query
            else:
                _embedding_function = embedding_function
        elif self._embedding_function:
            _embedding_function = self._embedding_function.embed_query
        else:
            _embedding_function = None

        if embedding is None:
            if _embedding_function is None:
                raise ValueError(
                    "Either `embedding` or `embedding_function` needs to be"
                    " specified."
                )

            embedding = _embedding_function(query) if query else None

        if isinstance(embedding, list):
            embedding = np.array(embedding, dtype=np.float32)
            if len(embedding.shape) > 1:
                embedding = embedding[0]

        result = self.vectorstore.search(
            embedding=embedding,
            k=fetch_k if use_maximal_marginal_relevance else k,
            distance_metric=distance_metric,
            filter=filter,
            exec_option=exec_option,
            return_tensors=["embedding", "metadata", "text", "id"],
            deep_memory=deep_memory,
        )

        scores = result["score"]
        embeddings = result["embedding"]
        metadatas = result["metadata"]
        texts = result["text"]

        if use_maximal_marginal_relevance:
            lambda_mult = kwargs.get("lambda_mult", 0.5)
            indices = maximal_marginal_relevance(  # type: ignore
                embedding,  # type: ignore
                embeddings,
                k=min(k, len(texts)),
                lambda_mult=lambda_mult,
            )

            scores = [scores[i] for i in indices]
            texts = [texts[i] for i in indices]
            metadatas = [metadatas[i] for i in indices]

        docs = [
            Document(
                page_content=text,
                metadata=metadata,
            )
            for text, metadata in zip(texts, metadatas)
        ]

        if return_score:
            return [(doc, score) for doc, score in zip(docs, scores)]

        return docs
def component_translation_path(language: str, integration: Integration) -> pathlib.Path:
    

    
    return integration.file_path / "translations" / f"{language}.json"
def mode(data):
    

    
    data = iter(data)
    try:
        return Counter(data).most_common(1)[0][0]
    except IndexError:
        raise StatisticsError('no mode for empty data') from None
def quantiles(data, /, *, n=4, method='exclusive'):
    
    
    if n < 1:
        raise StatisticsError('n must be at least 1')
    data = sorted(data)
    ld = len(data)
    if ld < 2:
        raise StatisticsError('must have at least two data points')
    if method == 'inclusive':
        m = ld - 1
        result = []
        for i in range(1, n):
            j = i * m // n
            delta = i*m - j*n
            interpolated = (data[j] * (n - delta) + data[j+1] * delta) / n
            result.append(interpolated)
        return result
    if method == 'exclusive':
        m = ld + 1
        result = []
        for i in range(1, n):
            j = i * m // n                               # rescale i to m/n
            j = 1 if j < 1 else ld-1 if j > ld-1 else j  # clamp to 1 .. ld-1
            delta = i*m - j*n                            # exact integer math
            interpolated = (data[j-1] * (n - delta) + data[j] * delta) / n
            result.append(interpolated)
        return result
    raise ValueError(f'Unknown method: {method!r}')
def harmonic_mean(data, weights=None):
    
    
    if iter(data) is data:
        data = list(data)
    errmsg = 'harmonic mean does not support negative values'
    n = len(data)
    if n < 1:
        raise StatisticsError('harmonic_mean requires at least one data point')
    elif n == 1 and weights is None:
        x = data[0]
        if isinstance(x, (numbers.Real, Decimal)):
            if x < 0:
                raise StatisticsError(errmsg)
            return x
        else:
            raise TypeError('unsupported type')
    if weights is None:
        weights = repeat(1, n)
        sum_weights = n
    else:
        if iter(weights) is weights:
            weights = list(weights)
        if len(weights) != n:
            raise StatisticsError('Number of weights does not match data size')
        _, sum_weights, _ = _sum(w for w in _fail_neg(weights, errmsg))
    try:
        data = _fail_neg(data, errmsg)
        T, total, count = _sum(w / x if w else 0 for w, x in zip(weights, data))
    except ZeroDivisionError:
        return 0
    if total <= 0:
        raise StatisticsError('Weighted sum must be positive')
    return _convert(sum_weights / total, T)
def _rank(data, /, *, key=None, reverse=False, ties='average', start=1) -> list[float]:
    

    
    # If this function becomes public at some point, more thought
    # needs to be given to the signature.  A list of ints is
    # plausible when ties is "min" or "max".  When ties is "average",
    # either list[float] or list[Fraction] is plausible.

    # Default handling of ties matches scipy.stats.mstats.spearmanr.
    if ties != 'average':
        raise ValueError(f'Unknown tie resolution method: {ties!r}')
    if key is not None:
        data = map(key, data)
    val_pos = sorted(zip(data, count()), reverse=reverse)
    i = start - 1
    result = [0] * len(val_pos)
    for _, g in groupby(val_pos, key=itemgetter(0)):
        group = list(g)
        size = len(group)
        rank = i + (size + 1) / 2
        for value, orig_pos in group:
            result[orig_pos] = rank
        i += size
    return result
def test_affinity_propagation_convergence_warning_dense_sparse(centers, global_dtype):
    
    
    
    rng = np.random.RandomState(42)
    X = rng.rand(40, 10).astype(global_dtype, copy=False)
    y = (4 * rng.rand(40)).astype(int)
    ap = AffinityPropagation(random_state=46)
    ap.fit(X, y)
    ap.cluster_centers_ = centers
    with warnings.catch_warnings():
        warnings.simplefilter("error", ConvergenceWarning)
        assert_array_equal(ap.predict(X), np.zeros(X.shape[0], dtype=int))
def add(self, *new_mobjects):
        
        
        
        self.remove(*new_mobjects)
        self.mobjects += new_mobjects
        return self
def to_local(self, x, y, relative=False):
        
        
        if relative:
            return (x - self.x, y - self.y)
        return (x, y)
def cast_(self, features: Features):
        
        
        if sorted(features) != sorted(self._data.column_names):
            raise ValueError(
                f"The columns in features ({list(features)}) must be identical "
                f"as the columns in the dataset: {self._data.column_names}"
            )

        self._info.features = features
        type = features.type
        schema = pa.schema({col_name: type[col_name].type for col_name in self._data.column_names})
        self._data = self._data.cast(schema)
def flatten_(self, max_depth=16):
        
        for depth in range(1, max_depth):
            if any(isinstance(field.type, pa.StructType) for field in self._data.schema):
                self._data = self._data.flatten()
            else:
                break
        if self.info is not None:
            self.info.features = Features.from_arrow_schema(self._data.schema)
        logger.info(
            "Flattened dataset from depth {} to depth {}.".format(depth, 1 if depth + 1 < max_depth else "unknown")
        )
def filter(
        self,
        function: Optional[Callable] = None,
        with_indices=False,
        input_columns: Optional[Union[str, List[str]]] = None,
        batched: bool = False,
        batch_size: Optional[int] = 1000,
        keep_in_memory: bool = False,
        load_from_cache_file: Optional[bool] = None,
        cache_file_name: Optional[str] = None,
        writer_batch_size: Optional[int] = 1000,
        fn_kwargs: Optional[dict] = None,
        num_proc: Optional[int] = None,
        suffix_template: str = "_{rank:05d}_of_{num_proc:05d}",
        new_fingerprint: Optional[str] = None,
        desc: Optional[str] = None,
    ) -> "Dataset":
        
        
        if len(self.list_indexes()) > 0:
            raise DatasetTransformationNotAllowedError(
                "Using `.filter` on a dataset with attached indexes is not allowed. You can first run `.drop_index() to remove your index and then re-add it.`"
            )

        if function is None:
            function = lambda x: True  # noqa: E731

        if len(self) == 0:
            return self

        indices = self.map(
            function=partial(
                get_indices_from_mask_function, function, batched, with_indices, input_columns, self._indices
            ),
            with_indices=True,
            features=Features({"indices": Value("uint64")}),
            batched=True,
            batch_size=batch_size,
            remove_columns=self.column_names,
            keep_in_memory=keep_in_memory,
            load_from_cache_file=load_from_cache_file,
            cache_file_name=cache_file_name,
            writer_batch_size=writer_batch_size,
            fn_kwargs=fn_kwargs,
            num_proc=num_proc,
            suffix_template=suffix_template,
            new_fingerprint=new_fingerprint,
            input_columns=input_columns,
            desc=desc,
        )
        new_dataset = copy.deepcopy(self)
        new_dataset._indices = indices.data
        new_dataset._fingerprint = new_fingerprint
        return new_dataset
def load_from_disk(dataset_path: str, fs=None, keep_in_memory: Optional[bool] = None) -> "Dataset":
        
        
        
        # copies file from filesystem if it is remote filesystem to local filesystem and modifies dataset_path to temp directory containing local copies
        if is_remote_filesystem(fs):
            src_dataset_path = extract_path_from_uri(dataset_path)
            tmp_dir = tempfile.TemporaryDirectory()
            dataset_path = Path(tmp_dir.name, src_dataset_path)
            fs.download(src_dataset_path, dataset_path.as_posix(), recursive=True)

        with open(
            Path(dataset_path, config.DATASET_STATE_JSON_FILENAME).as_posix(), "r", encoding="utf-8"
        ) as state_file:
            state = json.load(state_file)
        with open(Path(dataset_path, DATASET_INFO_FILENAME).as_posix(), "r", encoding="utf-8") as dataset_info_file:
            dataset_info = DatasetInfo.from_dict(json.load(dataset_info_file))

        dataset_size = estimate_dataset_size(
            Path(dataset_path, data_file["filename"]) for data_file in state["_data_files"]
        )
        keep_in_memory = keep_in_memory if keep_in_memory is not None else is_small_dataset(dataset_size)
        table_cls = InMemoryTable if keep_in_memory else MemoryMappedTable
        arrow_table = concat_tables(
            table_cls.from_file(Path(dataset_path, data_file["filename"]).as_posix())
            for data_file in state["_data_files"]
        )
        if state.get("_indices_data_files"):
            indices_table = concat_tables(
                table_cls.from_file(Path(dataset_path, indices_file["filename"]).as_posix())
                for indices_file in state["_indices_data_files"]
            )
        else:
            indices_table = None

        split = state["_split"]
        split = NamedSplit(split) if split is not None else split

        return Dataset(
            arrow_table=arrow_table,
            indices_table=indices_table,
            info=dataset_info,
            split=split,
            fingerprint=state["_fingerprint"],
        )
def set_format(
        self,
        type: Optional[str] = None,
        columns: Optional[List] = None,
        output_all_columns: bool = False,
        **format_kwargs,
    ):
        
        
        format_kwargs.update(format_kwargs.pop("format_kwargs", {}))  # allow to use self.set_format(self.format)

        # Check that the format_type and format_kwargs are valid and make it possible to have a Formatter
        type = get_format_type_from_alias(type)
        _ = get_formatter(type, **format_kwargs)

        # Check filter column
        if isinstance(columns, str):
            columns = [columns]
        if columns is not None and any(col not in self._data.column_names for col in columns):
            raise ValueError(
                "Columns {} not in the dataset. Current columns in the dataset: {}".format(
                    list(filter(lambda col: col not in self._data.column_names, columns)), self._data.column_names
                )
            )

        self._format_type = type
        self._format_kwargs = format_kwargs
        self._format_columns = columns
        self._output_all_columns = output_all_columns
        logger.info(
            "Set __getitem__(key) output type to %s for %s columns "
            " (when key is int or slice) and %s output other (un-formatted) columns.",
            "python objects" if type is None else type,
            "no" if columns is None else str(columns),
            "do" if output_all_columns else "don't",
        )
def to_csv(
        self,
        path_or_buf: Union[PathLike, BinaryIO],
        batch_size: Optional[int] = None,
        num_proc: Optional[int] = None,
        **to_csv_kwargs,
    ) -> int:
        
        
        # Dynamic import to avoid circular dependency
        from .io.csv import CsvDatasetWriter

        return CsvDatasetWriter(self, path_or_buf, batch_size=batch_size, num_proc=num_proc, **to_csv_kwargs).write()
def to_dict(self, batch_size: Optional[int] = None, batched="deprecated") -> Union[dict, Iterator[dict]]:
        
        
        if batched != "deprecated":
            warnings.warn(
                "'batched' was deprecated in version 2.11.0 and will be removed in version 3.0.0. Use `.iter(batch_size=batch_size)` followed by `.to_dict()` on the individual batches instead.",
                FutureWarning,
            )
        else:
            batched = False

        if not batched:
            return query_table(
                table=self._data,
                key=slice(0, len(self)),
                indices=self._indices if self._indices is not None else None,
            ).to_pydict()
        else:
            batch_size = batch_size if batch_size else config.DEFAULT_MAX_BATCH_SIZE
            return (
                query_table(
                    table=self._data,
                    key=slice(offset, offset + batch_size),
                    indices=self._indices if self._indices is not None else None,
                ).to_pydict()
                for offset in range(0, len(self), batch_size)
            )
def cast_column(self, column: str, feature: FeatureType, new_fingerprint: str) -> "Dataset":
        
        
        if hasattr(feature, "cast_storage"):
            dataset = copy.deepcopy(self)
            dataset.features[column] = feature
            dataset._fingerprint = new_fingerprint
            dataset._data = dataset._data.cast(dataset.features.arrow_schema)
            dataset._data = update_metadata_with_features(dataset._data, dataset.features)
            return dataset
        else:
            features = self.features.copy()
            features[column] = feature
            return self.cast(features)
def from_sql(
        sql: Union[str, "sqlalchemy.sql.Selectable"],
        con: Union[str, "sqlalchemy.engine.Connection", "sqlalchemy.engine.Engine", "sqlite3.Connection"],
        features: Optional[Features] = None,
        cache_dir: str = None,
        keep_in_memory: bool = False,
        **kwargs,
    ):
        
        
        from .io.sql import SqlDatasetReader

        return SqlDatasetReader(
            sql,
            con,
            features=features,
            cache_dir=cache_dir,
            keep_in_memory=keep_in_memory,
            **kwargs,
        ).read()
def cache_files(self) -> List[dict]:
        
        cache_files = list_table_cache_files(self._data)
        if self._indices is not None:
            cache_files += list_table_cache_files(self._indices)
        return [{"filename": cache_filename} for cache_filename in cache_files]
def _get_output_signature(
        dataset: "Dataset",
        collate_fn: Callable,
        collate_fn_args: dict,
        batch_size: Optional[int] = None,
        num_test_batches: int = 10,
    ):
        
        
        if config.TF_AVAILABLE:
            import tensorflow as tf
        else:
            raise ImportError("Called a Tensorflow-specific function but Tensorflow is not installed.")

        if len(dataset) == 0:
            raise ValueError("Unable to get the output signature because the dataset is empty.")
        if batch_size is None:
            test_batch_size = min(len(dataset), 8)
        else:
            batch_size = min(len(dataset), batch_size)
            test_batch_size = batch_size

        test_batches = []
        for _ in range(num_test_batches):
            indices = sample(range(len(dataset)), test_batch_size)
            test_batch = dataset[indices]
            test_batch = [{key: value[i] for key, value in test_batch.items()} for i in range(test_batch_size)]
            test_batch = collate_fn(test_batch, **collate_fn_args)
            test_batches.append(test_batch)

        tf_columns_to_signatures = {}
        np_columns_to_dtypes = {}
        for column in test_batches[0].keys():
            raw_arrays = [batch[column] for batch in test_batches]
            # In case the collate_fn returns something strange
            np_arrays = []
            for array in raw_arrays:
                if isinstance(array, np.ndarray):
                    np_arrays.append(array)
                elif isinstance(array, tf.Tensor):
                    np_arrays.append(array.numpy())
                else:
                    np_arrays.append(np.array(array))

            if np.issubdtype(np_arrays[0].dtype, np.integer) or np_arrays[0].dtype == np.bool:
                tf_dtype = tf.int64
                np_dtype = np.int64
            elif np.issubdtype(np_arrays[0].dtype, np.number):
                tf_dtype = tf.float32
                np_dtype = np.float32
            elif np_arrays[0].dtype.kind == "U":  # Unicode strings
                np_dtype = np.unicode_
                tf_dtype = tf.string
            else:
                raise RuntimeError(
                    f"Unrecognized array dtype {np_arrays[0].dtype}. \n"
                    "Nested types and image/audio types are not supported yet."
                )
            shapes = [array.shape for array in np_arrays]
            static_shape = []
            for dim in range(len(shapes[0])):
                sizes = set([shape[dim] for shape in shapes])
                if dim == 0:
                    static_shape.append(batch_size)
                    continue
                if len(sizes) == 1:  # This dimension looks constant
                    static_shape.append(sizes.pop())
                else:  # Use None for variable dimensions
                    static_shape.append(None)
            tf_columns_to_signatures[column] = tf.TensorSpec(shape=static_shape, dtype=tf_dtype)
            np_columns_to_dtypes[column] = np_dtype

        return tf_columns_to_signatures, np_columns_to_dtypes
def __iter__(self):
        
        
        if self._indices is None and config.PYARROW_VERSION.major >= 8:
            # Fast iteration
            # Benchmark: https://gist.github.com/mariosasko/0248288a2e3a7556873969717c1fe52b (fast_iter_batch)
            format_kwargs = self._format_kwargs if self._format_kwargs is not None else {}
            formatter = get_formatter(self._format_type, features=self.features, **format_kwargs)
            batch_size = config.ARROW_READER_BATCH_SIZE_IN_DATASET_ITER
            for pa_subtable in table_iter(self.data, batch_size=batch_size):
                for i in range(pa_subtable.num_rows):
                    pa_subtable_ex = pa_subtable.slice(i, 1)
                    formatted_output = format_table(
                        pa_subtable_ex,
                        0,
                        formatter=formatter,
                        format_columns=self._format_columns,
                        output_all_columns=self._output_all_columns,
                    )
                    yield formatted_output
        else:
            for i in range(self.num_rows):
                yield self._getitem(
                    i,
                )
def load(self, query: str) -> List[Document]:
        
        
          # noqa: E501
        try:
            import fitz
        except ImportError:
            raise ImportError(
                "PyMuPDF package not found, please install it with "
                "`pip install pymupdf`"
            )

        try:
            results = self.arxiv_search(  # type: ignore
                query[: self.ARXIV_MAX_QUERY_LENGTH], max_results=self.load_max_docs
            ).results()
        except self.arxiv_exceptions as ex:
            logger.debug("Error on arxiv: %s", ex)
            return []

        docs: List[Document] = []
        for result in results:
            try:
                doc_file_name: str = result.download_pdf()
                with fitz.open(doc_file_name) as doc_file:
                    text: str = "".join(page.get_text() for page in doc_file)
            except FileNotFoundError as f_ex:
                logger.debug(f_ex)
                continue
            if self.load_all_available_meta:
                extra_metadata = {
                    "entry_id": result.entry_id,
                    "published_first_time": str(result.published.date()),
                    "comment": result.comment,
                    "journal_ref": result.journal_ref,
                    "doi": result.doi,
                    "primary_category": result.primary_category,
                    "categories": result.categories,
                    "links": [link.href for link in result.links],
                }
            else:
                extra_metadata = {}
            metadata = {
                "Published": str(result.updated.date()),
                "Title": result.title,
                "Authors": ", ".join(a.name for a in result.authors),
                "Summary": result.summary,
                **extra_metadata,
            }
            doc = Document(
                page_content=text[: self.doc_content_chars_max], metadata=metadata
            )
            docs.append(doc)
            os.remove(doc_file_name)
        return docs
def send_add_notification(self, request):
        
        send_team_add_email(team_member=self, request=request)
def load(
            _,
            path: Union[Path, str],
            offset: Optional[float] = None,
            duration: Optional[float] = None,
            sample_rate: Optional[float] = None,
            dtype: np.dtype = np.float32) -> Signal:
        
            
        
        if isinstance(path, Path):
            path = str(path)
        if not isinstance(path, str):
            path = path.decode()
        try:
            probe = ffmpeg.probe(path)
        except ffmpeg._run.Error as e:
            raise SpleeterError(
                'An error occurs with ffprobe (see ffprobe output below)\n\n{}'
                .format(e.stderr.decode()))
        if 'streams' not in probe or len(probe['streams']) == 0:
            raise SpleeterError('No stream was found with ffprobe')
        metadata = next(
            stream
            for stream in probe['streams']
            if stream['codec_type'] == 'audio')
        n_channels = metadata['channels']
        if sample_rate is None:
            sample_rate = metadata['sample_rate']
        output_kwargs = {'format': 'f32le', 'ar': sample_rate}
        if duration is not None:
            output_kwargs['t'] = str(dt.timedelta(seconds=duration))
        if offset is not None:
            output_kwargs['ss'] = str(dt.timedelta(seconds=offset))
        process = (
            ffmpeg
            .input(path)
            .output('pipe:', **output_kwargs)
            .run_async(pipe_stdout=True, pipe_stderr=True))
        buffer, _ = process.communicate()
        waveform = np.frombuffer(buffer, dtype='<f4').reshape(-1, n_channels)
        if not waveform.dtype == np.dtype(dtype):
            waveform = waveform.astype(dtype)
        return (waveform, sample_rate)
def unit_of_measurement(self) -> str:
        
        return TEMP_FAHRENHEIT
def _value(self, t: TensorType) -> TensorType:
        
        
        if self.framework == "torch" and torch and isinstance(t, torch.Tensor):
            t = t.float()
        return self.initial_p * \
            self.decay_rate ** (t / self.schedule_timesteps)
def test_splitpasswd(self):
        
        
        self.assertEqual(('user', 'ab'),urllib.parse.splitpasswd('user:ab'))
        self.assertEqual(('user', 'a\nb'),urllib.parse.splitpasswd('user:a\nb'))
        self.assertEqual(('user', 'a\tb'),urllib.parse.splitpasswd('user:a\tb'))
        self.assertEqual(('user', 'a\rb'),urllib.parse.splitpasswd('user:a\rb'))
        self.assertEqual(('user', 'a\fb'),urllib.parse.splitpasswd('user:a\fb'))
        self.assertEqual(('user', 'a\vb'),urllib.parse.splitpasswd('user:a\vb'))
        self.assertEqual(('user', 'a:b'),urllib.parse.splitpasswd('user:a:b'))
        self.assertEqual(('user', 'a b'),urllib.parse.splitpasswd('user:a b'))
        self.assertEqual(('user 2', 'ab'),urllib.parse.splitpasswd('user 2:ab'))
        self.assertEqual(('user+1', 'a+b'),urllib.parse.splitpasswd('user+1:a+b'))
def __init__(self,
               word_vocab_size,
               word_embed_size,
               type_vocab_size,
               output_embed_size,
               max_sequence_length=512,
               normalization_type='no_norm',
               initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),
               dropout_rate=0.1,
               **kwargs):
    
    
    super(MobileBertEmbedding, self).__init__(**kwargs)
    self.word_vocab_size = word_vocab_size
    self.word_embed_size = word_embed_size
    self.type_vocab_size = type_vocab_size
    self.output_embed_size = output_embed_size
    self.max_sequence_length = max_sequence_length
    self.normalization_type = normalization_type
    self.initializer = tf.keras.initializers.get(initializer)
    self.dropout_rate = dropout_rate

    self.word_embedding = keras_nlp.layers.OnDeviceEmbedding(
        self.word_vocab_size,
        self.word_embed_size,
        initializer=initializer,
        name='word_embedding')
    self.type_embedding = keras_nlp.layers.OnDeviceEmbedding(
        self.type_vocab_size,
        self.output_embed_size,
        use_one_hot=True,
        initializer=initializer,
        name='type_embedding')
    self.pos_embedding = keras_nlp.layers.PositionEmbedding(
        max_length=max_sequence_length,
        initializer=initializer,
        name='position_embedding')
    self.word_embedding_proj = tf.keras.layers.experimental.EinsumDense(
        'abc,cd->abd',
        output_shape=[None, self.output_embed_size],
        kernel_initializer=initializer,
        bias_axes='d',
        name='embedding_projection')
    self.layer_norm = _get_norm_layer(normalization_type, 'embedding_norm')
    self.dropout_layer = tf.keras.layers.Dropout(
        self.dropout_rate,
        name='embedding_dropout')
def test_encoding(categories, unknown_value, global_random_seed, smooth, target_type):
    
    

    n_categories = 3
    X_train_int_array = np.array([[0] * 20 + [1] * 30 + [2] * 40], dtype=np.int64).T
    X_test_int_array = np.array([[0, 1, 2]], dtype=np.int64).T
    n_samples = X_train_int_array.shape[0]

    if categories == "auto":
        X_train = X_train_int_array
        X_test = X_test_int_array
    else:
        X_train = categories[0][X_train_int_array]
        X_test = categories[0][X_test_int_array]

    X_test = np.concatenate((X_test, [[unknown_value]]))

    data_rng = np.random.RandomState(global_random_seed)
    n_splits = 3
    if target_type == "binary":
        y_int = data_rng.randint(low=0, high=2, size=n_samples)
        target_names = np.array(["cat", "dog"], dtype=object)
        y_train = target_names[y_int]

    else:  # target_type == continuous
        y_int = data_rng.uniform(low=-10, high=20, size=n_samples)
        y_train = y_int

    shuffled_idx = data_rng.permutation(n_samples)
    X_train_int_array = X_train_int_array[shuffled_idx]
    X_train = X_train[shuffled_idx]
    y_train = y_train[shuffled_idx]
    y_int = y_int[shuffled_idx]

    # Define our CV splitting strategy
    if target_type == "binary":
        cv = StratifiedKFold(
            n_splits=n_splits, random_state=global_random_seed, shuffle=True
        )
    else:
        cv = KFold(n_splits=n_splits, random_state=global_random_seed, shuffle=True)

    # Compute the expected values using our reference Python implementation of
    # target encoding:
    expected_X_fit_transform = np.empty_like(X_train_int_array, dtype=np.float64)

    for train_idx, test_idx in cv.split(X_train_int_array, y_train):
        X_, y_ = X_train_int_array[train_idx, 0], y_int[train_idx]
        cur_encodings = _encode_target(X_, y_, n_categories, smooth)
        expected_X_fit_transform[test_idx, 0] = cur_encodings[
            X_train_int_array[test_idx, 0]
        ]

    # Check that we can obtain the same encodings by calling `fit_transform` on
    # the estimator with the same CV parameters:
    target_encoder = TargetEncoder(
        smooth=smooth,
        categories=categories,
        cv=n_splits,
        random_state=global_random_seed,
    )

    X_fit_transform = target_encoder.fit_transform(X_train, y_train)

    assert target_encoder.target_type_ == target_type
    assert_allclose(X_fit_transform, expected_X_fit_transform)
    assert len(target_encoder.encodings_) == 1

    # compute encodings for all data to validate `transform`
    y_mean = np.mean(y_int)
    expected_encodings = _encode_target(
        X_train_int_array[:, 0], y_int, n_categories, smooth
    )
    assert_allclose(target_encoder.encodings_[0], expected_encodings)
    assert target_encoder.target_mean_ == pytest.approx(y_mean)

    # Transform on test data, the last value is unknown so it is encoded as the target
    # mean
    expected_X_test_transform = np.concatenate(
        (expected_encodings, np.array([y_mean]))
    ).reshape(-1, 1)

    X_test_transform = target_encoder.transform(X_test)
    assert_allclose(X_test_transform, expected_X_test_transform)
def displays(self):
		
		
		
		if not self.isStoryText():
			return []
		return re.findall(r'\<\<display\s+[\'"]?(.+?)[\'"]?\s?\>\>', self.text, re.IGNORECASE)
def update(self):
		
		
		
		if not self.isStoryText() and not self.isAnnotation() and not self.isStylesheet():
			self.displays = []
			self.links = []
			self.images = []
			self.macros = []
			return
        
		# <<display>>
		self.displays = re.findall(r'\<\<display\s+[\'"]?(.+?)[\'"]?\s?\>\>', self.text, re.IGNORECASE)
		
		self.macros = []
		# other macros (including shorthand <<display>>)
		for m in re.finditer(TweeLexer.MACRO_REGEX, self.text):
			# Exclude shorthand <<print>>
			if m.group(1) and m.group(1)[0] != '$':
				self.macros.append(m.group(1))

		# avoid duplicates by collecting links in a set
		links = set()

		# Regular hyperlinks (also matches wiki-style links inside macros)
		for m in re.finditer(TweeLexer.LINK_REGEX, self.text):
			links.add(m.group(2) or m.group(1))

		# Include images
		for m in re.finditer(TweeLexer.IMAGE_REGEX, self.text):
			if m.group(5):
				links.add(m.group(5))

		# <<choice passage_name [link_text]>>
		for block in re.findall(r'\<\<choice\s+(.*?)\s?\>\>', self.text):
			item = re.match(r'(?:"([^"]*)")|(?:\'([^\']*)\')|([^"\'\[\s]\S*)', block)
			if item:
				links.add(''.join(item.groups('')))

		# <<actions '' ''>>
		for block in re.findall(r'\<\<actions\s+(.*?)\s?\>\>', self.text):
			links.update(re.findall(r'[\'"](.*?)[\'"]', block))

		self.links = list(links)

		# Images

		images = set()
		for block in re.finditer(TweeLexer.IMAGE_REGEX, self.text):
			images.add(block.group(4))

		self.images = list(images)
def addHtml(self, source):
        
        
        order = []
        divs = re.search(r'<div\sid=["\']?store(?:A|-a)rea["\']?>(.*)</div>', source,
                        re.DOTALL)
        if divs:
            divs = divs.group(1);
            # HTML may be obfuscated.
            obfuscationkey = ''
            storysettings_re = r'[^>]*\stiddler=["\']?StorySettings["\']?[^>]*>.*?</div>'
            storysettings = re.search(r'<div'+storysettings_re, divs, re.DOTALL)
            if storysettings:
                ssTiddler = self.addTiddler(Tiddler(storysettings.group(0), 'html'))
                if re.search(r'obfuscate\s*:\s*swap\s*[\n$]', ssTiddler.text, re.I):
                    match = re.search(r'obfuscatekey\s*:\s*(\w*)\s*[\n$]', ssTiddler.text, re.I)
                    if match:
                        obfuscationkey = match.group(1)
                        nss = u''
                        for nsc in obfuscationkey:
                            if nss.find(nsc) == -1 and not nsc in ':\\\"n0':
                                nss = nss + nsc
                        obfuscationkey = nss
                divs = divs[:storysettings.start(0)] + divs[storysettings.end(0)+1:]

            for div in divs.split('<div'):
                div.strip()
                if div:
                    tiddler = Tiddler('<div' + div, 'html', obfuscationkey)
                    self.addTiddler(tiddler)
                    order.append(tiddler.title)
        return order
def copy(self,
             source_table,
             dest_table,
             create_disposition=CreateDisposition.CREATE_IF_NEEDED,
             write_disposition=WriteDisposition.WRITE_TRUNCATE):
        
        

        job = {
            "projectId": dest_table.project_id,
            "configuration": {
                "copy": {
                    "sourceTable": {
                        "projectId": source_table.project_id,
                        "datasetId": source_table.dataset_id,
                        "tableId": source_table.table_id,
                    },
                    "destinationTable": {
                        "projectId": dest_table.project_id,
                        "datasetId": dest_table.dataset_id,
                        "tableId": dest_table.table_id,
                    },
                    "createDisposition": create_disposition,
                    "writeDisposition": write_disposition,
                }
            }
        }

        self.run_job(dest_table.project_id, job, dataset=dest_table.dataset)
def run_job(self, project_id, body, dataset=None):
        
        

        if dataset and not self.dataset_exists(dataset):
            self.make_dataset(dataset)

        new_job = self.client.jobs().insert(projectId=project_id, body=body).execute()
        job_id = new_job['jobReference']['jobId']
        logger.info('Started import job %s:%s', project_id, job_id)
        while True:
            status = self.client.jobs().get(projectId=project_id, jobId=job_id).execute(num_retries=10)
            if status['status']['state'] == 'DONE':
                if status['status'].get('errorResult'):
                    raise BigQueryExecutionError(job_id, status['status']['errorResult'])
                return job_id

            logger.info('Waiting for job %s:%s to complete...', project_id, job_id)
            time.sleep(5)
def register_env(name, env_creator):
    
    

    if not callable(env_creator):
        raise TypeError("Second argument must be callable.", env_creator)
    _global_registry.register(ENV_CREATOR, name, env_creator)
def _make_key(prefix: str, category: str, key: str):
    
    
    return (
        b"TuneRegistry:"
        + prefix.encode("ascii")
        + b":"
        + category.encode("ascii")
        + b"/"
        + key.encode("ascii")
    )
def test_multipart(self):
          

        e = {}
        e['SERVER_PROTOCOL'] = "HTTP/1.1"
        e['REQUEST_METHOD'] = 'POST'
        e['CONTENT_TYPE'] = 'multipart/form-data; boundary=----------------314159265358979323846'
        e['wsgi.input']  = '------------------314159265358979323846\n'
        e['wsgi.input'] += 'Content-Disposition: form-data; name=test.txt; filename=test.txt\n'
        e['wsgi.input'] += 'Content-Type: application/octet-stream; charset=ISO-8859-1\n'
        e['wsgi.input'] += 'Content-Transfer-Encoding: binary\n'
        e['wsgi.input'] += 'This is a test.\n'
        e['wsgi.input'] += '------------------314159265358979323846\n'
        e['wsgi.input'] += 'Content-Disposition: form-data; name=sample.txt; filename=sample.txt\n'
        e['wsgi.input'] += 'Content-Type: text/plain; charset=ISO-8859-1\n'
        e['wsgi.input'] += 'Content-Transfer-Encoding: binary\n'
        e['wsgi.input'] += 'This is a sample\n'
        e['wsgi.input'] += '------------------314159265358979323846\n'
        e['wsgi.input'] += 'Content-Disposition: form-data; name=sample.txt; filename=sample2.txt\n'
        e['wsgi.input'] += 'Content-Type: text/plain; charset=ISO-8859-1\n'
        e['wsgi.input'] += 'Content-Transfer-Encoding: binary\n'
        e['wsgi.input'] += 'This is a second sample\n'
        e['wsgi.input'] += '------------------314159265358979323846--\n'
        e['CONTENT_LENGTH'] = len(e['wsgi.input'])
        e['wsgi.input'] = StringIO(e['wsgi.input'])
        e['wsgi.input'].seek(0)
        request.bind(e)
        self.assertTrue('test.txt' in request.POST)
        self.assertTrue('sample.txt' in request.POST)
        self.assertEqual('This is a test.', request.POST['test.txt'].file.read())
        self.assertEqual('test.txt', request.POST['test.txt'].filename)
        self.assertEqual(2, len(request.POST['sample.txt']))
        self.assertEqual('This is a sample', request.POST['sample.txt'][0].file.read())
        self.assertEqual('This is a second sample', request.POST['sample.txt'][1].file.read())
def __init__(self, context: str, context_len: int = 15, log_sse: bool = False,
                 fmt_str: str = None,
                 log_fluentd_config_path: str =
                 resource_filename('jina', '/'.join(('resources', 'logging.fluentd.yml'))),
                 **kwargs):
        

        
        super().__init__()
        from .. import __uptime__
        if not fmt_str:
            title = os.environ.get('JINA_POD_NAME', context)
            if 'JINA_LOG_LONG' in os.environ:
                fmt_str = f'{title[:context_len]:>{context_len}}@%(process)2d' \
                          f'[%(levelname).1s][%(filename).3s:%(funcName).3s:%(lineno)3d]:%(message)s'
            else:
                fmt_str = f'{title[:context_len]:>{context_len}}@%(process)2d' \
                          f'[%(levelname).1s]:%(message)s'

        timed_fmt_str = f'%(asctime)s:' + fmt_str

        verbose_level = LogVerbosity.from_string(os.environ.get('JINA_LOG_VERBOSITY', 'INFO'))

        # Remove all handlers associated with the root logger object.
        for handler in logging.root.handlers[:]:
            logging.root.removeHandler(handler)

        self.logger = logging.getLogger(context)
        self.logger.propagate = False
        self.logger.handlers = []
        self.logger.setLevel(verbose_level.value)

        if ('JINA_LOG_SSE' in os.environ) or log_sse:
            self.logger.addHandler(get_fluentd_handler(context, log_fluentd_config_path, False))

        if os.environ.get('JINA_LOG_FILE') == 'TXT':
            h = logging.FileHandler(f'jina-{__uptime__}.log', delay=True)
            h.setFormatter(PlainFormatter(timed_fmt_str))
            self.logger.addHandler(h)
        elif os.environ.get('JINA_LOG_FILE') == 'JSON':
            h = logging.FileHandler(f'jina-{__uptime__}.json', delay=True)
            h.setFormatter(JsonFormatter(timed_fmt_str))
            self.logger.addHandler(h)

        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(ColorFormatter(fmt_str))
        self.logger.addHandler(console_handler)

        success_level = LogVerbosity.SUCCESS.value  # between WARNING and INFO
        logging.addLevelName(success_level, 'SUCCESS')
        setattr(self.logger, 'success', lambda message: self.logger.log(success_level, message))
def success(self, *args, **kwargs):
        
        
        
        with ColorContext(color='green'):
            self.logger.info(*args, **kwargs)
def checkDbms(self):
        
        
        

        if not conf.extensiveFp and Backend.isDbmsWithin(PGSQL_ALIASES):
            setDbms(DBMS.PGSQL)

            self.getBanner()

            return True

        infoMsg = "testing %s" % DBMS.PGSQL
        logger.info(infoMsg)

        result = inject.checkBooleanExpression("[RANDNUM]::int=[RANDNUM]")

        if result:
            infoMsg = "confirming %s" % DBMS.PGSQL
            logger.info(infoMsg)

            result = inject.checkBooleanExpression("COALESCE([RANDNUM], NULL)=[RANDNUM]")

            if not result:
                warnMsg = "the back-end DBMS is not %s" % DBMS.PGSQL
                logger.warn(warnMsg)

                return False

            setDbms(DBMS.PGSQL)

            self.getBanner()

            if not conf.extensiveFp:
                return True

            infoMsg = "actively fingerprinting %s" % DBMS.PGSQL
            logger.info(infoMsg)

            if inject.checkBooleanExpression("XMLTABLE(NULL) IS NULL"):
                Backend.setVersion(">= 10.0")
            elif inject.checkBooleanExpression("SIND(0)=0"):
                Backend.setVersion(">= 9.6.0", "< 10.0")
            elif inject.checkBooleanExpression("TO_JSONB(1) IS NOT NULL"):
                Backend.setVersion(">= 9.5.0", "< 9.6.0")
            elif inject.checkBooleanExpression("JSON_TYPEOF(NULL) IS NULL"):
                Backend.setVersionList([">= 9.4.0", "< 9.5.0"])
            elif inject.checkBooleanExpression("ARRAY_REPLACE(NULL,1,1) IS NULL"):
                Backend.setVersionList([">= 9.3.0", "< 9.4.0"])
            elif inject.checkBooleanExpression("ROW_TO_JSON(NULL) IS NULL"):
                Backend.setVersionList([">= 9.2.0", "< 9.3.0"])
            elif inject.checkBooleanExpression("REVERSE('sqlmap')='pamlqs'"):
                Backend.setVersionList([">= 9.1.0", "< 9.2.0"])
            elif inject.checkBooleanExpression("LENGTH(TO_CHAR(1,'EEEE'))>0"):
                Backend.setVersionList([">= 9.0.0", "< 9.1.0"])
            elif inject.checkBooleanExpression("2=(SELECT DIV(6,3))"):
                Backend.setVersionList([">= 8.4.0", "< 9.0.0"])
            elif inject.checkBooleanExpression("EXTRACT(ISODOW FROM CURRENT_TIMESTAMP)<8"):
                Backend.setVersionList([">= 8.3.0", "< 8.4.0"])
            elif inject.checkBooleanExpression("ISFINITE(TRANSACTION_TIMESTAMP())"):
                Backend.setVersionList([">= 8.2.0", "< 8.3.0"])
            elif inject.checkBooleanExpression("9=(SELECT GREATEST(5,9,1))"):
                Backend.setVersionList([">= 8.1.0", "< 8.2.0"])
            elif inject.checkBooleanExpression("3=(SELECT WIDTH_BUCKET(5.35,0.024,10.06,5))"):
                Backend.setVersionList([">= 8.0.0", "< 8.1.0"])
            elif inject.checkBooleanExpression("'d'=(SELECT SUBSTR(MD5('sqlmap'),1,1))"):
                Backend.setVersionList([">= 7.4.0", "< 8.0.0"])
            elif inject.checkBooleanExpression("'p'=(SELECT SUBSTR(CURRENT_SCHEMA(),1,1))"):
                Backend.setVersionList([">= 7.3.0", "< 7.4.0"])
            elif inject.checkBooleanExpression("8=(SELECT BIT_LENGTH(1))"):
                Backend.setVersionList([">= 7.2.0", "< 7.3.0"])
            elif inject.checkBooleanExpression("'a'=(SELECT SUBSTR(QUOTE_LITERAL('a'),2,1))"):
                Backend.setVersionList([">= 7.1.0", "< 7.2.0"])
            elif inject.checkBooleanExpression("8=(SELECT POW(2,3))"):
                Backend.setVersionList([">= 7.0.0", "< 7.1.0"])
            elif inject.checkBooleanExpression("'a'=(SELECT MAX('a'))"):
                Backend.setVersionList([">= 6.5.0", "< 6.5.3"])
            elif inject.checkBooleanExpression("VERSION()=VERSION()"):
                Backend.setVersionList([">= 6.4.0", "< 6.5.0"])
            elif inject.checkBooleanExpression("2=(SELECT SUBSTR(CURRENT_DATE,1,1))"):
                Backend.setVersionList([">= 6.3.0", "< 6.4.0"])
            elif inject.checkBooleanExpression("'s'=(SELECT SUBSTRING('sqlmap',1,1))"):
                Backend.setVersionList([">= 6.2.0", "< 6.3.0"])
            else:
                Backend.setVersion("< 6.2.0")

            return True
        else:
            warnMsg = "the back-end DBMS is not %s" % DBMS.PGSQL
            logger.warn(warnMsg)

            return False
def __getitem__(self, idx) -> torch.Tensor:
        
        
        
        size = self.image_sizes[idx]
        return self.tensor[idx, ..., : size[0], : size[1]]
def from_tensors(
        tensors: List[torch.Tensor],
        size_divisibility: int = 0,
        pad_value: float = 0.0,
        padding_constraints: Optional[Dict[str, int]] = None,
    ) -> "ImageList":
        
        
        
        assert len(tensors) > 0
        assert isinstance(tensors, (tuple, list))
        for t in tensors:
            assert isinstance(t, torch.Tensor), type(t)
            assert t.shape[:-2] == tensors[0].shape[:-2], t.shape

        image_sizes = [(im.shape[-2], im.shape[-1]) for im in tensors]
        image_sizes_tensor = [shapes_to_tensor(x) for x in image_sizes]
        max_size = torch.stack(image_sizes_tensor).max(0).values

        if padding_constraints is not None:
            square_size = padding_constraints.get("square_size", 0)
            if square_size > 0:
                # pad to square.
                max_size[0] = max_size[1] = square_size
            if "size_divisibility" in padding_constraints:
                size_divisibility = padding_constraints["size_divisibility"]
        if size_divisibility > 1:
            stride = size_divisibility
            # the last two dims are H,W, both subject to divisibility requirement
            max_size = (max_size + (stride - 1)).div(stride, rounding_mode="floor") * stride

        # handle weirdness of scripting and tracing ...
        if torch.jit.is_scripting():
            max_size: List[int] = max_size.to(dtype=torch.long).tolist()
        else:
            if torch.jit.is_tracing():
                image_sizes = image_sizes_tensor

        if len(tensors) == 1:
            # This seems slightly (2%) faster.
            # TODO: check whether it's faster for multiple images as well
            image_size = image_sizes[0]
            padding_size = [0, max_size[-1] - image_size[1], 0, max_size[-2] - image_size[0]]
            batched_imgs = F.pad(tensors[0], padding_size, value=pad_value).unsqueeze_(0)
        else:
            # max_size can be a tensor in tracing mode, therefore convert to list
            batch_shape = [len(tensors)] + list(tensors[0].shape[:-2]) + list(max_size)
            device = (
                None if torch.jit.is_scripting() else ("cpu" if torch.jit.is_tracing() else None)
            )
            batched_imgs = tensors[0].new_full(batch_shape, pad_value, device=device)
            batched_imgs = move_device_like(batched_imgs, tensors[0])
            for img, pad_img in zip(tensors, batched_imgs):
                pad_img[..., : img.shape[-2], : img.shape[-1]].copy_(img)

        return ImageList(batched_imgs.contiguous(), image_sizes)
def _end_of_line(self, y):
        
        last = self.maxx
        while True:
            if ascii.ascii(self.win.inch(y, last)) != ascii.SP:
                last = min(self.maxx, last+1)
                break
            elif last == 0:
                break
            last = last - 1
        return last
def postcode(self) -> str:
        
        
        
        department = self.department_number()
        if department in ["2A", "2B"]:
            department = "20"
        return f"{department}{self.random_number(digits=5 - len(department), fix_len=True)}"
def __init__(self, **kwargs):
        
        
        
        postprocess_params = parse_args()
        postprocess_params.__dict__.update(**kwargs)

        # init model dir
        if postprocess_params.det_model_dir is None:
            postprocess_params.det_model_dir = os.path.join(BASE_DIR, 'det')
        if postprocess_params.rec_model_dir is None:
            postprocess_params.rec_model_dir = os.path.join(BASE_DIR, 'rec')
        print(postprocess_params)
        # download model
        maybe_download(postprocess_params.det_model_dir, model_params['det'])
        maybe_download(postprocess_params.rec_model_dir, model_params['rec'])

        if postprocess_params.det_algorithm not in SUPPORT_DET_MODEL:
            logger.error('det_algorithm must in {}'.format(SUPPORT_DET_MODEL))
            sys.exit(0)
        if postprocess_params.rec_algorithm not in SUPPORT_REC_MODEL:
            logger.error('rec_algorithm must in {}'.format(SUPPORT_REC_MODEL))
            sys.exit(0)

        postprocess_params.rec_char_dict_path = Path(
            __file__).parent / postprocess_params.rec_char_dict_path

        # init det_model and rec_model
        super().__init__(postprocess_params)
def check_img(img, alpha_color=(255, 255, 255)):
    
    
    
    flag_gif, flag_pdf = False, False
    if isinstance(img, bytes):
        img = img_decode(img)
    if isinstance(img, str):
        # download net image
        if is_link(img):
            download_with_progressbar(img, "tmp.jpg")
            img = "tmp.jpg"
        image_file = img
        img, flag_gif, flag_pdf = check_and_read(image_file)
        if not flag_gif and not flag_pdf:
            with open(image_file, "rb") as f:
                img_str = f.read()
                img = img_decode(img_str)
            if img is None:
                try:
                    buf = BytesIO()
                    image = BytesIO(img_str)
                    im = Image.open(image)
                    rgb = im.convert("RGB")
                    rgb.save(buf, "jpeg")
                    buf.seek(0)
                    image_bytes = buf.read()
                    data_base64 = str(base64.b64encode(image_bytes), encoding="utf-8")
                    image_decode = base64.b64decode(data_base64)
                    img_array = np.frombuffer(image_decode, np.uint8)
                    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
                except:
                    logger.error("error in loading image:{}".format(image_file))
                    return None, flag_gif, flag_pdf
        if img is None:
            logger.error("error in loading image:{}".format(image_file))
            return None, flag_gif, flag_pdf
    # single channel image array.shape:h,w
    if isinstance(img, np.ndarray) and len(img.shape) == 2:
        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
    # four channel image array.shape:h,w,c
    if isinstance(img, np.ndarray) and len(img.shape) == 3 and img.shape[2] == 4:
        img = alpha_to_color(img, alpha_color)
    return img, flag_gif, flag_pdf
def add(self, data, conn_type, squash=True):
        
        
        
        if data in self.children:
            return data
        data = self._prepare_data(data)
        if not squash:
            self.children.append(data)
            return data
        if self.connector == conn_type:
            # We can reuse self.children to append or squash the node other.
            if (isinstance(data, Node) and not data.negated
                    and (data.connector == conn_type or len(data) == 1)):
                # We can squash the other node's children directly into this
                # node. We are just doing (AB)(CD) == (ABCD) here, with the
                # addition that if the length of the other node is 1 the
                # connector doesn't matter. However, for the len(self) == 1
                # case we don't want to do the squashing, as it would alter
                # self.connector.
                self.children.extend(data.children)
                return self
            else:
                # We could use perhaps additional logic here to see if some
                # children could be used for pushdown here.
                self.children.append(data)
                return data
        else:
            obj = self._new_instance(self.children, self.connector,
                                     self.negated)
            self.connector = conn_type
            self.children = [obj, data]
            return data
def sofr(
        self,
        start_date: Annotated[
            Union[datetime.date, None, str],
            OpenBBCustomParameter(
                description="Start date of the data, in YYYY-MM-DD format."
            ),
        ] = None,
        end_date: Annotated[
            Union[datetime.date, None, str],
            OpenBBCustomParameter(
                description="End date of the data, in YYYY-MM-DD format."
            ),
        ] = None,
        provider: Optional[Literal["fred"]] = None,
        **kwargs
    ) -> OBBject:
        
          # noqa: E501

        return self._run(
            "/fixedincome/sofr",
            **filter_inputs(
                provider_choices={
                    "provider": self._get_provider(
                        provider,
                        "/fixedincome/sofr",
                        ("fred",),
                    )
                },
                standard_params={
                    "start_date": start_date,
                    "end_date": end_date,
                },
                extra_params=kwargs,
            )
        )
def form_to_protobuf(self):
        
        
        chain = Fw.ChainFilter.input()
        if self.comboDirection.currentIndex() == self.OUT or self.FORM_TYPE == self.FORM_TYPE_EXCLUDE_SERVICE:
            chain = Fw.ChainMangle.output()
        elif self.comboDirection.currentIndex() == self.IN or self.FORM_TYPE == self.FORM_TYPE_ALLOW_IN_SERVICE:
            chain = Fw.ChainFilter.input()

        rule = Fw.Rules.new(
            enabled=self.checkEnable.isChecked(),
            _uuid=self.uuid,
            description=self.lineDescription.text(),
            target=Fw.Verdicts.values()[self.comboVerdict.currentIndex()+1] # index 0 is ""
        )

        for k in self.statements:
            st_idx = self.statements[k]['what'].currentIndex()-1
            if st_idx == -1:
                return None, None, None, QC.translate("firewall", "select a statement.")

            statement = self.STATM_CONF[st_idx]['name']
            statem_keys = self.STATM_CONF[st_idx]['keys']
            statem_op = Fw.Operator.values()[self.statements[k]['op'].currentIndex()]
            statem_opts = self.statements[k]['opts'].currentText().lower()

            key_values = []
            for sk in statem_keys:
                if sk['values'] == None:
                    key_values.append((sk['key'], ""))
                else:
                    statem_value = self.statements[k]['value'].currentText()
                    val_idx = self.statements[k]['value'].currentIndex()

                    if statem_value == "" or statem_value == "0":
                        return None, None, None, QC.translate("firewall", "value cannot be 0 or empty.")

                    if st_idx == self.STATM_QUOTA:
                        if sk['key'] == Fw.ExprQuota.OVER.value:
                            if self.statements[k]['opts'].currentIndex() == 0:
                                key_values.append((sk['key'], ""))
                            continue
                        elif sk['key'] == Fw.ExprQuota.UNIT.value:
                            units = statem_value.split("/")
                            if len(units) != 2: # we expect the format key/value
                                return None, None, None, QC.translate("firewall", "the value format is 1024/kbytes (or bytes, mbytes, gbytes)")
                            if units[1] not in Fw.RateUnits.values():
                                return None, None, None, QC.translate("firewall", "the value format is 1024/kbytes (or bytes, mbytes, gbytes)")
                            sk['key'] = units[1]
                            statem_value = units[0]

                    elif st_idx == self.STATM_LIMIT:
                        if sk['key'] == Fw.ExprLimit.OVER.value:
                            if self.statements[k]['opts'].currentIndex() == 0:
                                key_values.append((sk['key'], ""))
                        elif sk['key'] == Fw.ExprLimit.UNITS.value:
                            units = statem_value.split("/")
                            if len(units) != 3: # we expect the format key/value
                                return None, None, None, QC.translate("firewall", "the value format is 1024/kbytes/second (or bytes, mbytes, gbytes)")

                            if units[1] not in Fw.RateUnits.values():
                                return None, None, None, QC.translate("firewall", "rate-limit not valid, use: bytes, kbytes, mbytes or gbytes.")
                            if units[2] not in Fw.TimeUnits.values():
                                return None, None, None, QC.translate("firewall", "time-limit not valid, use: second, minute, hour or day")
                            key_values.append((Fw.ExprLimit.UNITS.value, units[0]))
                            key_values.append((Fw.ExprLimit.RATE_UNITS.value, units[1]))
                            key_values.append((Fw.ExprLimit.TIME_UNITS.value, units[2]))

                        continue

                    elif st_idx == self.STATM_LOG:
                        key_values.append((Fw.ExprLog.LEVEL.value, statem_opts))

                    elif st_idx == self.STATM_META:
                        sk['key'] = self.statements[k]['opts'].currentText()

                    elif st_idx == self.STATM_IIFNAME or st_idx == self.STATM_OIFNAME:
                        # for these statements, the values is set in the Key
                        # field instead of Value. Value must be empty
                        sk['key'] = statem_value
                        statem_value = ""

                    elif st_idx == self.STATM_DEST_IP or \
                            st_idx == self.STATM_SOURCE_IP or \
                            st_idx == self.STATM_DPORT or \
                            st_idx == self.STATM_SPORT:
                        statement = statem_opts
                        try:
                            if "," in statem_value or "-" in statem_value or val_idx < 1:
                                raise ValueError("port entered is multiport or a port range")
                            statem_value = self.net_srv.port_by_index(val_idx)
                        except:
                            if (st_idx == self.STATM_DPORT or st_idx == self.STATM_SPORT) and \
                                    ("," not in statem_value and "-" not in statem_value):
                                try:
                                    t = int(statem_value)
                                except:
                                    return None, None, None, QC.translate("firewall", "port not valid.")

                    key_values.append((sk['key'], statem_value.replace(" ", "")))

            exprs = Fw.Expr.new(
                statem_op,
                statement,
                key_values,
            )
            rule.Expressions.extend([exprs])
        chain.Rules.extend([rule])

        node_addr = self.comboNodes.currentText()
        node = self._nodes.get_node(node_addr)
        return node_addr, node, chain, None
def join(self, *args, **kwargs):
        
        
        ...
def _build_mwf_output_waveform(self):
         
        
        import norbert  # pylint: disable=import-error
        output_dict = self.model_outputs
        x = self.stft_feature
        v = tf.stack(
            [
                pad_and_reshape(
                    output_dict[f'{instrument}_spectrogram'],
                    self._frame_length,
                    self._F)[:tf.shape(x)[0], ...]
                for instrument in self._instruments
            ],
            axis=3)
        input_args = [v, x]
        stft_function = tf.py_function(
            lambda v, x: norbert.wiener(v.numpy(), x.numpy()),
            input_args,
            tf.complex64),
        return {
            instrument: self._inverse_stft(stft_function[0][:, :, :, k])
            for k, instrument in enumerate(self._instruments)
        }
def _inverse_stft(self, stft_t, time_crop=None):
         
        
        inversed = inverse_stft(
            tf.transpose(stft_t, perm=[2, 0, 1]),
            self._frame_length,
            self._frame_step,
            window_fn=lambda frame_length, dtype: (
                hann_window(frame_length, periodic=True, dtype=dtype))
        ) * self.WINDOW_COMPENSATION_FACTOR
        reshaped = tf.transpose(inversed)
        if time_crop is None:
            time_crop = tf.shape(self._features['waveform'])[0]
        return reshaped[:time_crop, :]
def _build_loss(self, labels: Dict) -> Tuple[tf.Tensor, Dict]:
        
        
        
        output_dict = self.model_outputs
        loss_type = self._params.get("loss_type", self.L1_MASK)
        if loss_type == self.L1_MASK:
            losses = {
                name: tf.reduce_mean(tf.abs(output - labels[name]))
                for name, output in output_dict.items()
            }
        elif loss_type == self.WEIGHTED_L1_MASK:
            losses = {
                name: tf.reduce_mean(
                    tf.reduce_mean(labels[name], axis=[1, 2, 3], keep_dims=True)
                    * tf.abs(output - labels[name])
                )
                for name, output in output_dict.items()
            }
        else:
            raise ValueError(f"Unkwnown loss type: {loss_type}")
        loss = tf.reduce_sum(list(losses.values()))
        # Add metrics for monitoring each instrument.
        metrics = {k: tf.compat.v1.metrics.mean(v) for k, v in losses.items()}
        metrics["absolute_difference"] = tf.compat.v1.metrics.mean(loss)
        return loss, metrics
def __init__(self, features, params):
        
        
        

        self._features = features
        self._params = params
        # Get instrument name.
        self._mix_name = params["mix_name"]
        self._instruments = params["instrument_list"]
        # Get STFT/signals parameters
        self._n_channels = params["n_channels"]
        self._T = params["T"]
        self._F = params["F"]
        self._frame_length = params["frame_length"]
        self._frame_step = params["frame_step"]
def decode(input, output, header = 0):
    

    if a2b_qp is not None:
        data = input.read()
        odata = a2b_qp(data, header = header)
        output.write(odata)
        return

    new = ''
    while 1:
        line = input.readline()
        if not line: break
        i, n = 0, len(line)
        if n > 0 and line[n-1] == '\n':
            partial = 0; n = n-1
            # Strip trailing whitespace
            while n > 0 and line[n-1] in " \t\r":
                n = n-1
        else:
            partial = 1
        while i < n:
            c = line[i]
            if c == '_' and header:
                new = new + ' '; i = i+1
            elif c != ESCAPE:
                new = new + c; i = i+1
            elif i+1 == n and not partial:
                partial = 1; break
            elif i+1 < n and line[i+1] == ESCAPE:
                new = new + ESCAPE; i = i+2
            elif i+2 < n and ishex(line[i+1]) and ishex(line[i+2]):
                new = new + chr(unhex(line[i+1:i+3])); i = i+3
            else: # Bad escape sequence -- leave it in
                new = new + c; i = i+1
        if not partial:
            output.write(new + '\n')
            new = ''
    if new:
        output.write(new)
def ishex(c):
    
    assert isinstance(c, bytes)
    return b'0' <= c <= b'9' or b'a' <= c <= b'f' or b'A' <= c <= b'F'
def rgb2grey(rgb):
    
    
    return _convert(grey_from_rgb, rgb[:, :, :3])[..., 0]
def xyz2lab(xyz):
    
    
    arr = _prepare_colorarray(xyz)

    # scale by CIE XYZ tristimulus values of the reference white point
    arr = arr / lab_ref_white

    # Nonlinear distortion and linear transformation
    mask = arr > 0.008856
    arr[mask] = np.power(arr[mask], 1. / 3.)
    arr[~mask] = 7.787 * arr[~mask] + 16. / 116.

    x, y, z = arr[..., 0], arr[..., 1], arr[..., 2]

    # Vector scaling
    L = (116. * y) - 16.
    a = 500.0 * (x - y)
    b = 200.0 * (y - z)

    return np.concatenate(map(lambda x: x[..., np.newaxis], [L, a, b]), -1)
def lab2xyz(lab, illuminant="D65", observer=2):
    

    

    arr = _prepare_colorarray(lab).copy()

    L, a, b = arr[:, :, 0], arr[:, :, 1], arr[:, :, 2]
    y = (L + 16.) / 116.
    x = (a / 500.) + y
    z = y - (b / 200.)

    out = np.dstack([x, y, z])

    mask = out > 0.2068966
    out[mask] = np.power(out[mask], 3.)
    out[~mask] = (out[~mask] - 16.0 / 116.) / 7.787

    # rescale to the reference white (illuminant)
    xyz_ref_white = get_xyz_coords(illuminant, observer)
    out *= xyz_ref_white
    return out
def combine_stains(stains, conv_matrix, *, channel_axis=-1):
    
    
    stains = _prepare_colorarray(stains, channel_axis=-1)

    # log_adjust here is used to compensate the sum within separate_stains().
    log_adjust = -np.log(1E-6)
    log_rgb = -(stains * log_adjust) @ conv_matrix
    rgb = np.exp(log_rgb)

    return np.clip(rgb, a_min=0, a_max=1)
def luv2xyz(luv, illuminant="D65", observer="2", *, channel_axis=-1):
    
    
    arr = _prepare_colorarray(luv, channel_axis=-1).copy()

    L, u, v = arr[..., 0], arr[..., 1], arr[..., 2]

    eps = np.finfo(float).eps

    # compute y
    y = L.copy()
    mask = y > 7.999625
    y[mask] = np.power((y[mask] + 16.) / 116., 3.)
    y[~mask] = y[~mask] / 903.3
    xyz_ref_white = xyz_tristimulus_values(illuminant=illuminant, observer=observer)
    y *= xyz_ref_white[1]

    # reference white x,z
    uv_weights = np.array([1, 15, 3])
    u0 = 4 * xyz_ref_white[0] / (uv_weights @ xyz_ref_white)
    v0 = 9 * xyz_ref_white[1] / (uv_weights @ xyz_ref_white)

    # compute intermediate values
    a = u0 + u / (13. * L + eps)
    b = v0 + v / (13. * L + eps)
    c = 3 * y * (5 * b - 3)

    # compute x and z
    z = ((a - 4) * c - 15 * a * b * y) / (12 * b)
    x = -(c / b + 3. * z)

    return np.concatenate([q[..., np.newaxis] for q in [x, y, z]], axis=-1)
def gray2rgb(image):
    
    
    return np.stack(3 * (image,), axis=-1)
def rgb2hed(rgb):
    
    
    rgb = dtype.img_as_ubyte(rgb)
    arr = [1, 1, 1] - (np.log(rgb + [1, 1, 1]) / np.log(255))
    row = np.reshape(arr, (-1,3))
    scaled = np.dot(row, hed_from_rgb)
    scaled[scaled < 0] = 0
    scaled[scaled > 1] = 1
    return  np.reshape(scaled, rgb.shape)
def lab2lch(lab):
    
    
    lch = _prepare_lab_array(lab)

    a, b = lch[..., 1], lch[..., 2]
    lch[..., 1], lch[..., 2] = np.sqrt(a ** 2 + b ** 2), np.arctan2(b, a)

    H = lch[..., 2]
    H += np.where(H < 0, 2*np.pi, 0)  # (-pi, pi) -> (0, 2*pi)
    return lch
def convert_colorspace(arr, fromspace, tospace):
    
    
    fromdict = {'rgb': lambda im: im, 'hsv': hsv2rgb, 'rgb cie': rgbcie2rgb,
                'xyz': xyz2rgb, 'yuv': yuv2rgb, 'yiq': yiq2rgb,
                'ypbpr': ypbpr2rgb, 'ycbcr': ycbcr2rgb, 'ydbdr': ydbdr2rgb}
    todict = {'rgb': lambda im: im, 'hsv': rgb2hsv, 'rgb cie': rgb2rgbcie,
              'xyz': rgb2xyz, 'yuv': rgb2yuv, 'yiq': rgb2yiq,
              'ypbpr': rgb2ypbpr, 'ycbcr': rgb2ycbcr, 'ydbdr': rgb2ydbdr}

    fromspace = fromspace.lower()
    tospace = tospace.lower()
    if fromspace not in fromdict:
        msg = '`fromspace` has to be one of {}'.format(fromdict.keys())
        raise ValueError(msg)
    if tospace not in todict:
        msg = '`tospace` has to be one of {}'.format(todict.keys())
        raise ValueError(msg)

    return todict[tospace](fromdict[fromspace](arr))
def lab2rgb(lab, illuminant="D65", observer="2"):
    
    
    return xyz2rgb(lab2xyz(lab, illuminant, observer))
def _prepare_lab_array(arr, force_copy=True):
    
    
    arr = np.asarray(arr)
    shape = arr.shape
    if shape[-1] < 3:
        raise ValueError('Input image has less than 3 channels.')
    float_dtype = _supported_float_type(arr.dtype)
    if float_dtype == np.float32:
        _func = dtype.img_as_float32
    else:
        _func = dtype.img_as_float64
    return _func(arr, force_copy=force_copy)
def EncodedFile(file, data_encoding, file_encoding=None, errors='strict'):

     

    
    if file_encoding is None:
        file_encoding = data_encoding
    encode, decode = lookup(data_encoding)[:2]
    Reader, Writer = lookup(file_encoding)[2:]
    sr = StreamRecoder(file,
                       encode,decode,Reader,Writer,
                       errors)
    # Add attributes to simplify introspection
    sr.data_encoding = data_encoding
    sr.file_encoding = file_encoding
    return sr
def _generate_single_agent_episode(
        self,
        agent_id: str,
        agent_episode_ids: Optional[Dict[str, str]] = None,
        observations: Optional[List[MultiAgentDict]] = None,
        actions: Optional[List[MultiAgentDict]] = None,
        rewards: Optional[List[MultiAgentDict]] = None,
        infos: Optional[List[MultiAgentDict]] = None,
        is_terminateds: Union[MultiAgentDict, bool] = False,
        is_truncateds: Union[MultiAgentDict, bool] = False,
        extra_model_outputs: Optional[MultiAgentDict] = None,
    ) -> SingleAgentEpisode:
        
        

        # If an episode id for an agent episode was provided assign it.
        episode_id = None if agent_episode_ids is None else agent_episode_ids[agent_id]
        # We need the timestep mapping to create single agent's episode.
        if len(self.global_t_to_local_t) > 0:
            # Set to None if not provided.
            agent_observations = (
                None
                if observations is None
                else self._get_single_agent_data(
                    agent_id, observations, shift=-self.ts_carriage_return
                )
            )

            agent_actions = (
                None
                if actions is None
                else self._get_single_agent_data(
                    agent_id,
                    actions,
                    use_global_t_to_local_t=False,
                )
            )

            # Rewards are complicated in multi-agent scenarios, as agents could receive
            # a reward even though they did not get an observation or stepped at a
            # certain timestep.
            agent_rewards = (
                None
                if rewards is None
                else self._get_single_agent_data(
                    agent_id,
                    rewards,
                    use_global_t_to_local_t=False,
                )
            )
            # Like observations, infos start at timestep `t=0`, so we do not need to
            # shift or start later when using the global timestep mapping. But we
            # need to use tha timestep carriage in case the starting timestep is
            # different from the length of observations-after-initialization.
            agent_infos = (
                None
                if infos is None
                else self._get_single_agent_data(
                    agent_id, infos, shift=-self.ts_carriage_return
                )
            )

            agent_extra_model_outputs = (
                None
                if extra_model_outputs is None
                else self._get_single_agent_data(
                    agent_id,
                    extra_model_outputs,
                    use_global_t_to_local_t=False,
                )
            )

            agent_is_terminated = (
                [False]
                if is_terminateds is None
                else self._get_single_agent_data(
                    agent_id, is_terminateds, use_global_t_to_local_t=False
                )
                # else self._get_single_agent_data(
                #     agent_id, is_terminateds, start_index=1, shift=-1
                # )
            )
            # If a list the list could be empty, if the agent never stepped.
            agent_is_terminated = (
                False if not agent_is_terminated else agent_is_terminated[-1]
            )

            agent_is_truncated = (
                [False]
                if is_truncateds is None
                else self._get_single_agent_data(
                    agent_id,
                    is_truncateds,
                    use_global_t_to_local_t=False,
                )
            )
            # If a list the list could be empty, if the agent never stepped.
            agent_is_truncated = (
                False if not agent_is_truncated else agent_is_truncated[-1]
            )

            # If there are as many actions as observations we have to buffer.
            if (
                agent_actions
                and agent_observations
                and len(agent_observations) == len(agent_actions)
            ):
                # Assert then that the other data is in order.
                if agent_extra_model_outputs:
                    assert len(agent_extra_model_outputs) == len(
                        agent_actions
                    ), f"Agent {agent_id} has not as many extra model outputs as "
                    "actions."
                    # Put the last extra model outputs into the buffer.
                    self.agent_buffers[agent_id]["extra_model_outputs"].get_nowait()
                    self.agent_buffers[agent_id]["extra_model_outputs"].put_nowait(
                        agent_extra_model_outputs.pop()
                    )

                # Put the last action into the buffer.
                self.agent_buffers[agent_id]["actions"].put_nowait(agent_actions.pop())

            # TODO (simon): Check, if this can be refactored to a
            # `_generate_partial_rewards` method and can be done where
            # the global timestep  and global action timestep
            # mappings are created (__init__).
            # We have to take care of partial rewards when generating the
            # agent rewards:
            #   1. Rewards between different observations -> added up and
            #       assigned to next observation.
            #   2. Rewards after the last observation -> get buffered and added up
            #       in the buffer for the next observation.
            #   3. Rewards before the initial observation -> get buffered
            #       and added to the next observation.
            # All partial rewards are recorded in `partial_rewards` together
            # with their corresponding timesteps in `partial_rewards_t`.
            if agent_rewards and observations:
                partial_agent_rewards_t = _IndexMapping()
                partial_agent_rewards = []
                agent_rewards = []
                agent_reward = 0.0
                for t, reward in enumerate(rewards):
                    if agent_id in reward:
                        # Add the rewards
                        partial_agent_rewards.append(reward[agent_id])
                        # Then add the reward.
                        agent_reward += reward[agent_id]
                        # Note, rewards start at timestep 1 (there are no initial ones).
                        # TODO (simon): Check, if we need to use here also
                        # `ts_carriage_return`.
                        partial_agent_rewards_t.append(t + self.ts_carriage_return + 1)
                        if (t + 1) in self.global_t_to_local_t[agent_id][1:]:
                            agent_rewards.append(agent_reward)
                            agent_reward = 0.0
                            continue

                # If the agent reward is not zero, we must have rewards that came
                # after the last observation. Then we buffer this reward.
                self.agent_buffers[agent_id]["rewards"].put_nowait(
                    self.agent_buffers[agent_id]["rewards"].get_nowait() + agent_reward
                )
                # Now save away the original rewards and the reward timesteps.
                self.partial_rewards_t[agent_id] = partial_agent_rewards_t
                self.partial_rewards[agent_id] = partial_agent_rewards

            return SingleAgentEpisode(
                id_=episode_id,
                observations=agent_observations,
                actions=agent_actions,
                rewards=agent_rewards,
                infos=agent_infos,
                is_terminated=agent_is_terminated,
                is_truncated=agent_is_truncated,
                extra_model_outputs=agent_extra_model_outputs,
            )
        # Otherwise return empty `SingleAgentEpisode`.
        else:
            return SingleAgentEpisode(id_=episode_id)
def add_env_reset(
        self,
        *,
        observations: MultiAgentDict,
        infos: Optional[MultiAgentDict] = None,
        render_image: Optional[np.ndarray] = None,
    ) -> None:
        
        
        assert not self.is_done
        # Assume that this episode is completely empty and has not stepped yet.
        # Leave self.env_t (and self.env_t_started) at 0.
        assert self.env_t == self.env_t_started == 0
        infos = infos or {}

        # Note that we store the render images into the `MultiAgentEpisode`
        # instead into each `SingleAgentEpisode`.
        if render_image is not None:
            self.render_images.append(render_image)

        # Note, all agents will have an initial observation, some may have an initial
        # info dict as well.
        for agent_id, agent_obs in observations.items():
            # Update env_t_to_agent_t mapping (all agents that are part of the reset
            # obs have their first mapping 0 (env_t) -> 0 (agent_t)).
            self.env_t_to_agent_t[agent_id].append(0)
            # Create SingleAgentEpisode, if necessary.
            if agent_id not in self.agent_episodes:
                self.agent_episodes[agent_id] = SingleAgentEpisode(
                    observation_space=self.observation_space.get(agent_id),
                    action_space=self.action_space.get(agent_id),
                )
            # Add initial observations (and infos) to the agent's episode.
            self.agent_episodes[agent_id].add_env_reset(
                observation=agent_obs,
                infos=infos.get(agent_id),
            )
def _add_buttons(self) -> dict[T.Literal["mesh", "mask"], ttk.Button]:
         
        
        frame = ttk.Frame(self)
        frame.pack(side=tk.TOP, fill=tk.Y)
        buttons = {}
        for display in self.key_bindings.values():
            var = tk.BooleanVar()
            var.set(False)
            self._tk_vars[display] = var

            lookup = "landmarks" if display == "mesh" else display
            button = ttk.Button(frame,
                                image=get_images().icons[lookup],
                                command=T.cast(T.Callable, lambda t=display: self.on_click(t)),
                                style="display_deselected.TButton")
            button.state(["!pressed", "!focus"])
            button.pack()
            Tooltip(button, text=self._helptext[display])
            buttons[display] = button
        return buttons
def test_loss_on_specific_values(
    loss, y_true, raw_prediction, loss_true, gradient_true, hessian_true
):
    
    loss1 = loss(y_true=np.array([y_true]), raw_prediction=np.array([raw_prediction]))
    grad1 = loss.gradient(
        y_true=np.array([y_true]), raw_prediction=np.array([raw_prediction])
    )
    loss2, grad2 = loss.loss_gradient(
        y_true=np.array([y_true]), raw_prediction=np.array([raw_prediction])
    )
    grad3, hess = loss.gradient_hessian(
        y_true=np.array([y_true]), raw_prediction=np.array([raw_prediction])
    )

    assert loss1 == approx(loss_true, rel=1e-15, abs=1e-15)
    assert loss2 == approx(loss_true, rel=1e-15, abs=1e-15)

    if gradient_true is not None:
        assert grad1 == approx(gradient_true, rel=1e-15, abs=1e-15)
        assert grad2 == approx(gradient_true, rel=1e-15, abs=1e-15)
        assert grad3 == approx(gradient_true, rel=1e-15, abs=1e-15)

    if hessian_true is not None:
        assert hess == approx(hessian_true, rel=1e-15, abs=1e-15)
def update_lock(self, new_lock):
         
        
        for id_, node in new_lock._nodes.items():
            if node.modified:
                self._nodes[id_] = node
def h_maxima(image, h, selem=None):
    
    
    if np.issubdtype(image.dtype, 'half'):
        resolution = 2 * np.finfo(image.dtype).resolution
        if h < resolution:
            h = resolution
        h_corrected = h - resolution / 2.0
        shifted_img = image - h
    else:
        shifted_img = _subtract_constant_clip(image, h)
        h_corrected = h

    rec_img = greyreconstruct.reconstruction(shifted_img, image,
                                             method='dilation', selem=selem)
    residue_img = image - rec_img
    h_max = np.zeros(image.shape, dtype=np.uint8)
    h_max[residue_img >= h_corrected] = 1
    return h_max
def local_maxima(image, selem=None, connectivity=None, indices=False,
                 allow_borders=True):
    
    
    image = np.asarray(image, order="C")
    if image.size == 0:
        # Return early for empty input
        if indices:
            # Make sure that output is a tuple of 1 empty array per dimension
            return np.nonzero(image)
        else:
            return np.zeros(image.shape, dtype=np.bool)

    if allow_borders:
        # Ensure that local maxima are always at least one smaller sample away
        # from the image border
        image = _fast_pad(image, image.min())

    # Array of flags used to store the state of each pixel during evaluation.
    # See _extrema_cy.pyx for their meaning
    flags = np.zeros(image.shape, dtype=np.uint8)
    _set_edge_values_inplace(flags, value=3)

    if any(s < 3 for s in image.shape):
        # Warn and skip if any dimension is smaller than 3
        # -> no maxima can exist & structuring element can't be applied
        warn(
            "maxima can't exist for an image with any dimension smaller 3 "
            "if borders aren't allowed",
            stacklevel=3
        )
    else:
        selem = _resolve_neighborhood(selem, connectivity, image.ndim)
        neighbor_offsets = _offsets_to_raveled_neighbors(
            image.shape, selem, center=((1,) * image.ndim)
        )

        try:
            _local_maxima(image.ravel(), flags.ravel(), neighbor_offsets)
        except TypeError:
            if image.dtype == np.float16:
                # Provide the user with clearer error message
                raise TypeError("dtype of `image` is float16 which is not "
                                "supported, try upcasting to float32")
            else:
                raise  # Otherwise raise original message

    if allow_borders:
        # Revert padding performed at the beginning of the function
        flags = crop(flags, 1)
    else:
        # No padding was performed but set edge values back to 0
        _set_edge_values_inplace(flags, value=0)

    if indices:
        return np.nonzero(flags)
    else:
        return flags.view(np.bool)
def local_minima(image, selem=None):
    
    
    # find the minimal grey level difference
    h = _find_min_diff(image)
    if h == 0:
        return np.zeros(image.shape, np.uint8)
    if not np.issubdtype(image.dtype, 'half'):
        h = 1
    local_min = h_minima(image, h, selem=selem)
    return local_min
def _offset_to_raveled_neighbours(image, connectivity):
    
    
    connections = ndi.generate_binary_structure(image.ndim, connectivity)
    center = np.ones(connections.ndim, dtype=np.intp)
    # Center of kernel is not a neighbor
    connections[tuple(center)] = False

    connection_indices = np.transpose(np.nonzero(connections))
    offsets = (np.ravel_multi_index(connection_indices.T, image.shape) -
               np.ravel_multi_index(center.T, image.shape))

    return offsets
def _offset_to_raveled_neighbours(image_shape, selem):
    
    
    center = np.ones(selem.ndim, dtype=np.intp)
    # Center of kernel is not a neighbor
    selem[tuple(center)] = False
    connection_indices = np.transpose(np.nonzero(selem))
    offsets = (np.ravel_multi_index(connection_indices.T, image_shape) -
               np.ravel_multi_index(center.T, image_shape))

    return offsets
def _set_edge_values_inplace(image, value, edge_width=1):
    
    
    for axis in range(image.ndim):
        sl = [slice(None)] * image.ndim
        # Set edge in front
        sl[axis] = np.arange(edge_width)
        image[sl] = value
        # Set edge to the end
        sl[axis] = -(sl[axis] + 1)
        image[sl] = value
def _set_edge_values_inplace(image, value):
    
    
    for axis in range(image.ndim):
        sl = [slice(None)] * image.ndim
        # Set edge in front
        sl[axis] = 0
        image[sl] = value
        # Set edge to the end
        sl[axis] = -1
        image[sl] = value
def _fast_pad(image, value, *, order="C"):
    
    
    # Allocate padded image
    new_shape = np.array(image.shape) + 2
    new_image = np.empty(new_shape, dtype=image.dtype, order=order)

    # Copy old image into new space
    original_slice = tuple(slice(1, -1) for _ in range(image.ndim))
    new_image[original_slice] = image
    # and set the edge values
    _set_edge_values_inplace(new_image, value)

    return new_image
def train(self, unfiltered_dataframe: DataFrame, metadata: dict) -> Tuple[DataFrame, DataFrame]:
        
        
        

        return unfiltered_dataframe, unfiltered_dataframe
def train(self, unfiltered_dataframe: DataFrame, metadata: dict) -> Any:
        
        
        

        return Any
def check_if_feature_list_matches_strategy(self, dataframe: DataFrame,
                                               dk: FreqaiDataKitchen) -> None:
        
        
        
        dk.find_features(dataframe)
        if 'training_features_list_raw' in dk.data:
            feature_list = dk.data['training_features_list_raw']
        else:
            feature_list = dk.training_features_list
        if dk.training_features_list != feature_list:
            raise OperationalException("Trying to access pretrained model with `identifier` "
                                       "but found different features furnished by current strategy."
                                       "Change `identifer` to train from scratch, or ensure the"
                                       "strategy is furnishing the same features as the pretrained"
                                       "model")
def data_cleaning_train(self, dk: FreqaiDataKitchen) -> None:
        
        
        

        ft_params = self.freqai_info["feature_parameters"]

        if ft_params.get(
            "principal_component_analysis", False
        ):
            dk.principal_component_analysis()

        if ft_params.get("use_SVM_to_remove_outliers", False):
            dk.use_SVM_to_remove_outliers(predict=False)

        if ft_params.get("DI_threshold", 0):
            dk.data["avg_mean_dist"] = dk.compute_distances()

        if ft_params.get("use_DBSCAN_to_remove_outliers", False):
            if dk.pair in self.dd.old_DBSCAN_eps:
                eps = self.dd.old_DBSCAN_eps[dk.pair]
            else:
                eps = None
            dk.use_DBSCAN_to_remove_outliers(predict=False, eps=eps)
            self.dd.old_DBSCAN_eps[dk.pair] = dk.data['DBSCAN_eps']

        if ft_params.get('inlier_metric_window', 0):
            dk.compute_inlier_metric(set_='train')
            if self.freqai_info["data_split_parameters"]["test_size"] > 0:
                dk.compute_inlier_metric(set_='test')
def data_cleaning_train(self, dk: FreqaiDataKitchen) -> None:
        
        
        

        ft_params = self.freqai_info["feature_parameters"]

        if ft_params.get(
            "principal_component_analysis", False
        ):
            dk.principal_component_analysis()

        if ft_params.get("use_SVM_to_remove_outliers", False):
            dk.use_SVM_to_remove_outliers(predict=False)

        if ft_params.get("DI_threshold", 0):
            dk.data["avg_mean_dist"] = dk.compute_distances()

        if ft_params.get("use_DBSCAN_to_remove_outliers", False):
            if dk.pair in self.dd.old_DBSCAN_eps:
                eps = self.dd.old_DBSCAN_eps[dk.pair]
            else:
                eps = None
            dk.use_DBSCAN_to_remove_outliers(predict=False, eps=eps)
            self.dd.old_DBSCAN_eps[dk.pair] = dk.data['DBSCAN_eps']

        if ft_params.get('inlier_metric_window', 0):
            dk.compute_inlier_metric(set_='train')
            if self.freqai_info["data_split_parameters"]["test_size"] > 0:
                dk.compute_inlier_metric(set_='test')

        if self.freqai_info["feature_parameters"].get('noise_standard_deviation', 0):
            dk.add_noise_to_training_features()
def data_cleaning_predict(self, dk: FreqaiDataKitchen, dataframe: DataFrame) -> None:
        
        
        
        if self.freqai_info["feature_parameters"].get(
            "principal_component_analysis", False
        ):
            dk.pca_transform(dataframe)

        if self.freqai_info["feature_parameters"].get("use_SVM_to_remove_outliers", False):
            dk.use_SVM_to_remove_outliers(predict=True)

        if self.freqai_info["feature_parameters"].get("DI_threshold", 0):
            dk.check_if_pred_in_training_spaces()

        if self.freqai_info["feature_parameters"].get("use_DBSCAN_to_remove_outliers", False):
            dk.use_DBSCAN_to_remove_outliers(predict=True)
def start(self, dataframe: DataFrame, metadata: dict, strategy: IStrategy) -> DataFrame:
        
        
        

        self.live = strategy.dp.runmode in (RunMode.DRY_RUN, RunMode.LIVE)

        # FreqaiDataKitchen is reinstantiated for each coin
        self.dh = FreqaiDataKitchen(self.config, self.data_drawer, self.live, metadata["pair"])

        if self.live:
            # logger.info('testing live')
            self.start_live(dataframe, metadata, strategy)

            return (self.dh.full_predictions, self.dh.full_do_predict,
                    self.dh.full_target_mean, self.dh.full_target_std)

        logger.info(f'Training {len(self.dh.training_timeranges)} timeranges')

        # Loop enforcing the sliding window training/backtesting paradigm
        # tr_train is the training time range e.g. 1 historical month
        # tr_backtest is the backtesting time range e.g. the week directly
        # following tr_train. Both of these windows slide through the
        # entire backtest
        for tr_train, tr_backtest in zip(
            self.dh.training_timeranges, self.dh.backtesting_timeranges
        ):
            gc.collect()
            # self.config['timerange'] = tr_train
            self.dh.data = {}  # clean the pair specific data between models
            self.training_timerange = tr_train
            dataframe_train = self.dh.slice_dataframe(tr_train, dataframe)
            dataframe_backtest = self.dh.slice_dataframe(tr_backtest, dataframe)
            logger.info("training %s for %s", metadata["pair"], tr_train)
            trained_timestamp = TimeRange.parse_timerange(tr_train)
            self.dh.data_path = Path(self.dh.full_path /
                                     str("sub-train" + "-" + metadata['pair'].split("/")[0] +
                                         str(int(trained_timestamp.stopts))))
            if not self.model_exists(metadata["pair"], trained_timestamp=trained_timestamp.stopts):
                self.model = self.train(dataframe_train, metadata)
                self.dh.save_data(self.model)
            else:
                self.model = self.dh.load_data()
                # strategy_provided_features = self.dh.find_features(dataframe_train)
                # # TOFIX doesnt work with PCA
                # if strategy_provided_features != self.dh.training_features_list:
                #     logger.info("User changed input features, retraining model.")
                #     self.model = self.train(dataframe_train, metadata)
                #     self.dh.save_data(self.model)

            preds, do_preds = self.predict(dataframe_backtest, metadata)

            self.dh.append_predictions(preds, do_preds, len(dataframe_backtest))
            print('predictions', len(self.dh.full_predictions),
                  'do_predict', len(self.dh.full_do_predict))

        self.dh.fill_predictions(len(dataframe))

        return (self.dh.full_predictions, self.dh.full_do_predict,
                self.dh.full_target_mean, self.dh.full_target_std)
def return_values(self, dataframe: DataFrame) -> DataFrame:
        
        
        

        return
def backtest_prediction_exists(
        self,
        dk: FreqaiDataKitchen,
        scanning: bool = False,
    ) -> bool:
        
        
        
        if not self.live:
            prediction_file_name = dk.model_filename
            path_to_predictionfile = Path(dk.full_path /
                                          dk.backtesting_prediction_folder /
                                          f"{prediction_file_name}_prediction.h5")
            dk.backtesting_results_path = path_to_predictionfile

            file_exists = path_to_predictionfile.is_file()
            if file_exists and not scanning:
                logger.info("Found backtesting prediction file at %s", prediction_file_name)
            elif not scanning:
                logger.info(
                    "Could not find backtesting prediction file at %s", prediction_file_name
                )
            return file_exists
        else:
            return False
def extract_data_and_train_model(
        self,
        new_trained_timerange: TimeRange,
        pair: str,
        strategy: IStrategy,
        dk: FreqaiDataKitchen,
        data_load_timerange: TimeRange,
    ):
        
        
        

        corr_dataframes, base_dataframes = self.dd.get_base_and_corr_dataframes(
            data_load_timerange, pair, dk
        )

        unfiltered_dataframe = dk.use_strategy_to_populate_indicators(
            strategy, corr_dataframes, base_dataframes, pair
        )

        new_trained_timerange = dk.buffer_timerange(new_trained_timerange)

        unfiltered_dataframe = dk.slice_dataframe(new_trained_timerange, unfiltered_dataframe)

        # find the features indicated by strategy and store in datakitchen
        dk.find_features(unfiltered_dataframe)
        dk.find_labels(unfiltered_dataframe)

        model = self.train(unfiltered_dataframe, pair, dk)

        self.dd.pair_dict[pair]["trained_timestamp"] = new_trained_timerange.stopts
        dk.set_new_model_names(pair, new_trained_timerange.stopts)
        self.dd.save_data(model, pair, dk)

        if self.plot_features:
            plot_feature_importance(model, pair, dk, self.plot_features)

        self.dd.purge_old_models()
def s_text_s(self, tag, attrs):
        # Changed by Kovid to fix non breaking spaces being prepended to
        # element instead of being part of the text flow.
        # We don't use an entity for the nbsp as the contents of self.data will
        # be escaped on writeout.
         
        
        try:
            c = int(attrs.get((TEXTNS, 'c'), 1))
        except:
            c = 0
        if c > 0:
            self.data.append(u'\u00a0'*c)
def add_metrics_point(self, data_points: Dict[Hashable, float], timestamp: float):
        
        
        for name, value in data_points.items():
            # Using in-sort to insert while maintaining sorted ordering.
            bisect.insort(a=self.data[name], x=TimeStampedValue(timestamp, value))
def fetch_ohlcv(self, symbol, timeframe='5m', since=None, limit=None, params={}):
        
        
        
        self.load_markets()
        market = self.market(symbol)
        request = {
            'pair': market['id'],
            'type': self.safe_string(self.timeframes, timeframe, timeframe),
        }
        if limit is not None:
            request['limit'] = limit
        if since is not None:
            request['since'] = self.parse_to_int(since / 1000)
        response = self.publicGetChartsPairTypeChart(self.extend(request, params))
        #
        #     [
        #         {"time":1591296000,"open":0.024746,"close":0.024728,"low":0.024728,"high":0.024753,"volume":16.624},
        #         {"time":1591295700,"open":0.024718,"close":0.02475,"low":0.024711,"high":0.02475,"volume":31.645},
        #         {"time":1591295400,"open":0.024721,"close":0.024717,"low":0.024711,"high":0.02473,"volume":65.071}
        #     ]
        #
        return self.parse_ohlcvs(response, market, timeframe, since, limit)
def create_order(self, symbol: str, type: OrderType, side: OrderSide, amount, price=None, params={}):
        
        
        
        if type == 'market':
            raise InvalidOrder(self.id + ' only limits orders are supported')
        self.load_markets()
        market = self.market(symbol)
        request = {
            'pair': market['id'],
            'type': side,
            'amount': amount,
            'price': self.price_to_precision(symbol, price),
        }
        response = self.privatePostOrder(self.extend(request, params))
        if not response['success']:
            raise InvalidOrder(self.id + ' ' + self.json(response))
        order = self.parse_order(response, market)
        orderAmount = str(order['amount'])
        amount = order['amount'] if Precise.string_gt(orderAmount, '0') else amount
        return self.extend(order, {
            'amount': self.parse_number(amount),
        })
def move_folder_contents(conanfile, src_folder, dst_folder):
     
    
    # Remove potential "siblings" folders not wanted
    src_folder_name = os.path.basename(src_folder)
    for f in os.listdir(dst_folder):
        if f != src_folder_name:  # FIXME: Only works for 1st level subfolder
            dst = os.path.join(dst_folder, f)
            if os.path.isfile(dst):
                os.remove(dst)
            else:
                _internal_rmdir(dst)

    # Move all the contents
    for f in os.listdir(src_folder):
        src = os.path.join(src_folder, f)
        dst = os.path.join(dst_folder, f)
        if not os.path.exists(dst):
            shutil.move(src, dst_folder)
        else:
            for sub_src in os.listdir(src):
                shutil.move(os.path.join(src, sub_src), dst)
            _internal_rmdir(src)
    try:
        os.rmdir(src_folder)
    except OSError:
        pass
def _process_rpc_error(self, e: grpc.RpcError) -> bool:
        
        
        
        if self.client_worker._can_reconnect(e):
            if log_once("lost_reconnect_logs"):
                logger.warning(
                    "Log channel is reconnecting. Logs produced while "
                    "the connection was down can be found on the head "
                    "node of the cluster in "
                    "`ray_client_server_[port].out`")
            logger.info("Log channel dropped, retrying.")
            time.sleep(.5)
            return True
        logger.info("Shutting down log channel.")
        if not self.client_worker._in_shutdown:
            logger.exception("Unexpected exception:")
        return False
def set_args(self,
                 imw,
                 imh,
                 colormap=cv2.COLORMAP_JET,
                 heatmap_alpha=0.5,
                 view_img=False,
                 view_in_counts=True,
                 view_out_counts=True,
                 count_reg_pts=None,
                 count_txt_thickness=2,
                 count_txt_color=(0, 0, 0),
                 count_color=(255, 255, 255),
                 count_reg_color=(255, 0, 255),
                 region_thickness=5,
                 line_dist_thresh=15,
                 decay_factor=0.99,
                 shape='circle'):
        
        
        
        self.imw = imw
        self.imh = imh
        self.heatmap_alpha = heatmap_alpha
        self.view_img = view_img
        self.view_in_counts = view_in_counts
        self.view_out_counts = view_out_counts
        self.colormap = colormap

        # Region and line selection
        if count_reg_pts is not None:

            if len(count_reg_pts) == 2:
                print('Line Counter Initiated.')
                self.count_reg_pts = count_reg_pts
                self.counting_region = LineString(count_reg_pts)

            elif len(count_reg_pts) == 4:
                print('Region Counter Initiated.')
                self.count_reg_pts = count_reg_pts
                self.counting_region = Polygon(self.count_reg_pts)

            else:
                print('Region or line points Invalid, 2 or 4 points supported')
                print('Using Line Counter Now')
                self.counting_region = Polygon([(20, 400), (1260, 400)])  # dummy points

        # Heatmap new frame
        self.heatmap = np.zeros((int(self.imh), int(self.imw)), dtype=np.float32)

        self.count_txt_thickness = count_txt_thickness
        self.count_txt_color = count_txt_color
        self.count_color = count_color
        self.region_color = count_reg_color
        self.region_thickness = region_thickness
        self.decay_factor = decay_factor
        self.line_dist_thresh = line_dist_thresh
        self.shape = shape

        # shape of heatmap, if not selected
        if self.shape not in ['circle', 'rect']:
            print("Unknown shape value provided, 'circle' & 'rect' supported")
            print('Using Circular shape now')
            self.shape = 'circle'
def display_frames(self):
        
        cv2.imshow('Ultralytics Heatmap', self.im0)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            return
def upload_with_dedup(self, file_name, extension=None, precomputed_md5=None):
        
        
        

        # This construction of remote_path is critical to preventing duplicate
        # uploads of same object. Uploader will check if the file exists in S3
        # and re-upload only if necessary. So the template points to same file
        # in multiple places, this will upload only once
        filemd5 = precomputed_md5 or file_checksum(file_name)
        remote_path = filemd5
        if extension:
            remote_path = remote_path + "." + extension

        return self.upload(file_name, remote_path)
def eval_test_batch_from_scratch(
    encoder_decoder_asr,
    test_set,
    test_kwargs,
    reporter,
    pretrainer_load_audio=False,
):
    
    
    if "ckpt_prefix" in test_kwargs:
        del test_kwargs["ckpt_prefix"]
    if not (
        isinstance(test_set, DataLoader) or isinstance(test_set, LoopedLoader)
    ):
        test_set = make_dataloader(test_set, **test_kwargs)

    with torch.no_grad():
        for batch in tqdm(test_set, dynamic_ncols=True, disable=False):
            # instead of using batch.sig, we desire to see pretrained_hf_asr.load_audio in action
            wavs = []
            for audio_path in batch.wav:  # get the paths only
                if pretrainer_load_audio:
                    wavs.append(
                        encoder_decoder_asr.load_audio(
                            path=audio_path, silent_local_fetch=True
                        )
                    )
                else:
                    wavs.append(
                        read_audio(audio_path).to(encoder_decoder_asr.device)
                    )
            wavs, wav_lens = batch_pad_right(wavs)
            predictions = encoder_decoder_asr(wavs, wav_lens)

            # prepare for metric reporting
            predicted = [wrd.split(" ") for wrd in predictions[0]]
            targeted = [wrd.split(" ") for wrd in batch.words]
            ids = batch.id
            for metric in reporter.keys():
                reporter[metric]["tracker"].append(
                    ids=ids, predict=predicted, target=targeted
                )

        # Report summary
        ddp_barrier()
        eval_reporting(reports=reporter)
def hvac_mode(self):
        
        device = self._cubehandle.cube.device_by_rf(self._rf_address)
        if device.mode in [MAX_DEVICE_MODE_AUTOMATIC, MAX_DEVICE_MODE_BOOST]:
            return HVAC_MODE_AUTO
        if (
            device.mode == MAX_DEVICE_MODE_MANUAL
            and device.target_temperature == OFF_TEMPERATURE
        ):
            return HVAC_MODE_OFF

        return HVAC_MODE_HEAT
def shutdown(self, *args: Any, **kwargs: Any) -> None:
        
        super().shutdown(wait=False, cancel_futures=True)
        self.join_threads_or_timeout()
def _should_cache(req: ray_client_pb2.DataRequest) -> bool:
    
    
    
    req_type = req.WhichOneof("type")
    if req_type == "get" and req.get.asynchronous:
        return False
    if req_type == "put":
        return req.put.chunk_id == req.put.total_chunks - 1
    if req_type == "task":
        return req.task.chunk_id == req.task.total_chunks - 1
    return req_type not in ("acknowledge", "connection_cleanup")
def inception_resnet_v2_arg_scope(
    weight_decay=0.00004,
    batch_norm_decay=0.9997,
    batch_norm_epsilon=0.001,
    activation_fn=tf.nn.relu,
    batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS,
    batch_norm_scale=False):
  
  
  # Set weight_decay for weights in conv2d and fully_connected layers.
  with slim.arg_scope([slim.conv2d, slim.fully_connected],
                      weights_regularizer=slim.l2_regularizer(weight_decay),
                      biases_regularizer=slim.l2_regularizer(weight_decay)):

    batch_norm_params = {
        'decay': batch_norm_decay,
        'epsilon': batch_norm_epsilon,
        'updates_collections': batch_norm_updates_collections,
        'fused': None,  # Use fused batch norm if possible.
        'scale': batch_norm_scale,
    }
    # Set activation_fn and parameters for batch_norm.
    with slim.arg_scope([slim.conv2d], activation_fn=activation_fn,
                        normalizer_fn=slim.batch_norm,
                        normalizer_params=batch_norm_params) as scope:
      return scope
def __call__(self, trainer=None):
        

        
        # set up a reporter
        reporter = reporter_module.Reporter()
        if hasattr(self, 'name'):
            prefix = self.name + '/'
        else:
            prefix = ''
        for name, target in six.iteritems(self._targets):
            reporter.add_observer(prefix + name, target)
            reporter.add_observers(prefix + name,
                                   target.namedlinks(skipself=True))

        with reporter:
            result = self.evaluate()

        reporter_module.report(result)
        return result
def evaluate(self):
        

        
        iterator = self._iterators['main']
        eval_func = self.eval_func or self._targets['main']

        if self.eval_hook:
            self.eval_hook(self)

        if hasattr(iterator, 'reset'):
            iterator.reset()
            it = iterator
        else:
            it = copy.copy(iterator)

        summary = reporter_module.DictSummary()

        for batch in it:
            observation = {}
            with reporter_module.report_scope(observation):
                in_arrays = self.converter(batch, self.device)
                with function.no_backprop_mode():
                    if isinstance(in_arrays, tuple):
                        eval_func(*in_arrays)
                    elif isinstance(in_arrays, dict):
                        eval_func(**in_arrays)
                    else:
                        eval_func(in_arrays)

            summary.add(observation)

        return summary.compute_mean()
def build_finished(app, exception):
    
    if exception or not isinstance(app.builder, builders.StandaloneHTMLBuilder):
        return
    script_url = app.config.redoc_script_url
    output_filename = "script.js"

    cache_filepath = fetch_and_cache(script_url, output_filename)
    copy_file(cache_filepath, os.path.join(app.builder.outdir, '_static', "redoc.js"))
def __add_to_slice(self, s_result, result_start, result_stop, start, stop):
        
        

        
        if (result_stop - result_start) != (stop - start):
            raise ValueError(
                'Result start/stop range different than stop/start range (%s - %s vs. %s - %s)' % (
                    result_start, result_stop, start, stop,
                )
            )

        # Dense data: just copy using numpy's slice notation
        if not self.sparse_serialization:
            s_result[result_start:result_stop] = self.current_shard[start:stop]

            return s_result

        # A bit more difficult, we're using a different structure to build the
        # result.
        if s_result.shape != (result_start, self.dim):
            raise ValueError(
                'Assuption about sparse s_result shape invalid: %s expected rows, %s real rows.' % (
                    result_start, s_result.shape[0],
                )
            )

        tmp_matrix = self.current_shard[start:stop]
        s_result = sparse.vstack([s_result, tmp_matrix])
        return s_result
def dispatch_to_backend(
    context: RequestContext,
    http_request_dispatcher: Callable[[RequestContext], Response],
    include_response_metadata=False,
) -> ServiceResponse:
    
    
    
    http_response = http_request_dispatcher(context)
    parsed_response = parse_response(context.operation, http_response, include_response_metadata)
    raise_service_exception(http_response, parsed_response)
    return parsed_response
def _set_project(self):
          
        if self._options.get("project", None) is None:
            logger.debug("No project stored")
        else:
            logger.debug("Loading stored project")
            self._config.project.load(filename=self._options["project"], last_session=True)
def shape_i(var, i, fgraph=None):
    

    
    if fgraph is None and hasattr(var, 'fgraph'):
        fgraph = var.fgraph
    if fgraph and hasattr(fgraph, 'shape_feature'):
        if var not in fgraph.shape_feature.shape_of:
            # If var isn't in the ShapeFeature, add it.
            fgraph.shape_feature.on_import(fgraph, var.owner,
                                           'gof.ops.shape_i')
        return fgraph.shape_feature.shape_of[var][i]

    # If we are not able to use the shape feature, we should not put
    # Shape_i in the graph. Otherwise, the shape feature optimization
    # won't get applied.
    return var.shape[i]
def make_comment(content: str, *, preview: bool) -> str:
    
    
    content = content.rstrip()
    if not content:
        return "#"

    if content[0] == "#":
        content = content[1:]
    NON_BREAKING_SPACE = " "
    if (
        content
        and content[0] == NON_BREAKING_SPACE
        and not content.lstrip().startswith("type:")
    ):
        content = " " + content[1:]  # Replace NBSP by a simple space
    if content and content[0] not in COMMENT_EXCEPTIONS[preview]:
        content = " " + content
    return "#" + content
def localtime(value=None, timezone=None):
    
    
    
    if value is None:
        value = now()
    if timezone is None:
        timezone = get_current_timezone()
    # If `value` is naive, astimezone() will raise a ValueError,
    # so we don't need to perform a redundant check.
    value = value.astimezone(timezone)
    if hasattr(timezone, 'normalize'):
        # This method is available for pytz time zones.
        value = timezone.normalize(value)
    return value
def is_aware(value):
    
    
    
    return value.utcoffset() is not None
def activate(timezone):
    
    
    
    if isinstance(timezone, tzinfo):
        _active.value = timezone
    elif isinstance(timezone, six.string_types):
        _active.value = pytz.timezone(timezone)
    else:
        raise ValueError("Invalid timezone: %r" % timezone)
def _get_timezone_name(timezone):
    
    
    
    return timezone.tzname(None) or str(timezone)
def restore_dataset_context(request):
    
    original = copy.deepcopy(ray.data.context.DataContext.get_current())
    yield
    ray.data.context.DataContext._set_current(original)
def get_redirect_path_with_status(
        self, filename, path=None, language=None, version_slug=None, forced_only=False
    ):
        
        
        
        # Small optimization to skip executing the big query below.
        # TODO: use filter(enabled=True) once we have removed the null option from the field.
        if forced_only and not self.filter(force=True).exclude(enabled=False).exists():
            return None, None

        normalized_filename = self._normalize_path(filename)
        normalized_path = self._normalize_path(path)

        # Useful to allow redirects to match paths with or without trailling slash.
        # For example, ``/docs`` will match ``/docs/`` and ``/docs``.
        filename_without_trailling_slash = normalized_filename.rstrip("/")
        path_without_trailling_slash = normalized_path.rstrip("/")

        # Add extra fields with the ``filename`` and ``path`` to perform a
        # filter at db level instead with Python.
        queryset = self.annotate(
            filename=Value(
                filename,
                output_field=CharField(),
            ),
            path=Value(
                normalized_path,
                output_field=CharField(),
            ),
            filename_without_trailling_slash=Value(
                filename_without_trailling_slash,
                output_field=CharField(),
            ),
            path_without_trailling_slash=Value(
                path_without_trailling_slash,
                output_field=CharField(),
            ),
        )
        page = Q(
            redirect_type=PAGE_REDIRECT,
            from_url_without_rest__isnull=True,
            filename_without_trailling_slash__exact=F("from_url"),
        ) | Q(
            redirect_type=PAGE_REDIRECT,
            from_url_without_rest__isnull=False,
            filename__startswith=F("from_url_without_rest"),
        )
        exact = Q(
            redirect_type=EXACT_REDIRECT,
            from_url_without_rest__isnull=True,
            path_without_trailling_slash__exact=F("from_url"),
        ) | Q(
            redirect_type=EXACT_REDIRECT,
            from_url_without_rest__isnull=False,
            path__startswith=F("from_url_without_rest"),
        )
        clean_url_to_html = Q(redirect_type=CLEAN_URL_TO_HTML_REDIRECT)
        html_to_clean_url = Q(redirect_type=HTML_TO_CLEAN_URL_REDIRECT)

        if filename in ["/index.html", "/"]:
            # If the filename is a root index file (``/index.html`` or ``/``), we only need to match page and exact redirects,
            # since we don't have a filename to redirect to for clean_url_to_html and html_to_clean_url redirects.
            queryset = queryset.filter(page | exact)
        elif filename:
            if filename.endswith(("/index.html", "/")):
                queryset = queryset.filter(page | exact | clean_url_to_html)
            elif filename.endswith(".html"):
                queryset = queryset.filter(page | exact | html_to_clean_url)
            else:
                queryset = queryset.filter(page | exact)
        else:
            # If the filename is empty, we only need to match exact redirects.
            # Since the other types of redirects are not valid without a filename.
            queryset = queryset.filter(exact)

        # TODO: use filter(enabled=True) once we have removed the null option from the field.
        queryset = queryset.exclude(enabled=False)
        if forced_only:
            queryset = queryset.filter(force=True)

        redirect = queryset.select_related("project").first()
        if redirect:
            new_path = redirect.get_redirect_path(
                filename=normalized_filename,
                path=normalized_path,
                language=language,
                version_slug=version_slug,
            )
            return new_path, redirect.http_status
        return None, None
def __init__(
        self, forward_module: nn.Module, strategy: Strategy, original_module: Optional[nn.Module] = None
    ) -> None:
        

        
        super().__init__()
        self._forward_module = forward_module
        self._original_module = original_module or forward_module
        self._strategy = strategy
        self._fabric_module_initialized = True
def num_outputs_total(self) -> int:
        
        
        if self._estimated_output_blocks is not None:
            return self._estimated_output_blocks
        if len(self.input_dependencies) == 1:
            return self.input_dependencies[0].num_outputs_total()
        raise AttributeError
def add_input(self, refs: RefBundle, input_index: int) -> None:
        
        
        self._metrics.on_input_received(refs)
        self._add_input_inner(refs, input_index)
def completed(self) -> bool:
        
        
        if not self._execution_completed:
            if self._inputs_complete and self.num_active_tasks() == 0:
                # If all inputs are complete and there are no active tasks,
                # then the operator has completed execution.
                self._execution_completed = True
        return self._execution_completed and not self.has_next()
def on_data_ready(self, max_bytes_to_read: Optional[int]) -> int:
        
        
        bytes_read = 0
        while max_bytes_to_read is None or bytes_read < max_bytes_to_read:
            try:
                block_ref = self._streaming_gen._next_sync(0)
                if block_ref.is_nil():
                    # The generator currently doesn't have new output.
                    # And it's not stopped yet.
                    break
            except StopIteration:
                self._task_done_callback(None)
                break

            try:
                meta = ray.get(next(self._streaming_gen))
            except StopIteration:
                # The generator should always yield 2 values (block and metadata)
                # each time. If we get a StopIteration here, it means an error
                # happened in the task.
                # And in this case, the block_ref is the exception object.
                # TODO(hchen): Ray Core should have a better interface for
                # detecting and obtaining the exception.
                try:
                    ray.get(block_ref)
                    assert False, "Above ray.get should raise an exception."
                except Exception as ex:
                    self._task_done_callback(ex)
                    raise ex from None
            self._output_ready_callback(
                RefBundle([(block_ref, meta)], owns_blocks=True)
            )
            bytes_read += meta.size_bytes
        return bytes_read
def incremental_resource_usage(self) -> ExecutionResources:
        
        
        return ExecutionResources()
def preset_mode(self):
        

        if "presenceLocked" in self._tado_geofence_data:
            if not self._tado_geofence_data["presenceLocked"]:
                return PRESET_AUTO
        if self._tado_zone_data.is_away:
            return PRESET_AWAY
        return PRESET_HOME
def get_issue(self, issue_number: int) -> Dict[str, Any]:
        
        
        
        issue = self.github_repo_instance.get_issue(number=issue_number)
        page = 0
        comments: List[dict] = []
        while len(comments) <= 10:
            comments_page = issue.get_comments().get_page(page)
            if len(comments_page) == 0:
                break
            for comment in comments_page:
                comments.append({"body": comment.body, "user": comment.user.login})
            page += 1

        opened_by = None
        if issue.user and issue.user.login:
            opened_by = issue.user.login

        return {
            "number": issue_number,
            "title": issue.title,
            "body": issue.body,
            "comments": str(comments),
            "opened_by": str(opened_by),
        }
def async_activate(self):
        
        
        return self.hass.loop.run_in_executor(None, self.activate)
def gen_tgt_perms(self, tgt):
        
        
        max_num_chars = tgt.shape[1] - 2
        if max_num_chars == 1:
            return paddle.arange(end=3).unsqueeze(axis=0)
        perms = [paddle.arange(end=max_num_chars)] if self.perm_forward else []
        max_perms = math.factorial(max_num_chars)
        if self.perm_mirrored:
            max_perms //= 2
        num_gen_perms = min(self.max_gen_perms, max_perms)
        if max_num_chars < 5:
            if max_num_chars == 4 and self.perm_mirrored:
                selector = [0, 3, 4, 6, 9, 10, 12, 16, 17, 18, 19, 21]
            else:
                selector = list(range(max_perms))
            perm_pool = paddle.to_tensor(
                data=list(permutations(range(max_num_chars), max_num_chars)),
                place=self._device,
            )[selector]
            if self.perm_forward:
                perm_pool = perm_pool[1:]
            perms = paddle.stack(x=perms)
            if len(perm_pool):
                i = self.rng.choice(
                    len(perm_pool), size=num_gen_perms - len(perms), replace=False
                )
                perms = paddle.concat(x=[perms, perm_pool[i]])
        else:
            perms.extend(
                [
                    paddle.randperm(n=max_num_chars)
                    for _ in range(num_gen_perms - len(perms))
                ]
            )
            perms = paddle.stack(x=perms)
        if self.perm_mirrored:
            comp = perms.flip(axis=-1)
            x = paddle.stack(x=[perms, comp])
            perm_2 = list(range(x.ndim))
            perm_2[0] = 1
            perm_2[1] = 0
            perms = x.transpose(perm=perm_2).reshape((-1, max_num_chars))
        bos_idx = paddle.zeros(shape=(len(perms), 1), dtype=perms.dtype)
        eos_idx = paddle.full(
            shape=(len(perms), 1), fill_value=max_num_chars + 1, dtype=perms.dtype
        )
        perms = paddle.concat(x=[bos_idx, perms + 1, eos_idx], axis=1)
        if len(perms) > 1:
            perms[(1), 1:] = max_num_chars + 1 - paddle.arange(end=max_num_chars + 1)
        return perms
def _get_trial_info(trial, parameters, metrics):
    
    
    result = flatten_dict(trial.last_result)
    trial_info = [str(trial), trial.status, str(trial.address)]
    trial_info += [result.get(CONFIG_PREFIX + param) for param in parameters]
    trial_info += [result.get(metric) for metric in metrics]
    return trial_info
def __init__(self,
                 overwrite,
                 metric_columns=None,
                 max_progress_rows=20,
                 max_error_rows=20,
                 max_report_frequency=5):
        
        
        super(JupyterNotebookReporter,
              self).__init__(metric_columns, max_progress_rows, max_error_rows,
                             max_report_frequency)
        self._overwrite = overwrite
def trial_progress_str(
        trials: List[Trial],
        metric_columns: Union[List[str], Dict[str, str]],
        parameter_columns: Union[None, List[str], Dict[str, str]] = None,
        total_samples: int = 0,
        force_table: bool = False,
        fmt: str = "psql",
        max_rows: Optional[int] = None,
        done: bool = False,
        metric: Optional[str] = None,
        mode: Optional[str] = None,
        sort_by_metric: bool = False):
    
    
    messages = []
    delim = "<br>" if fmt == "html" else "\n"
    if len(trials) < 1:
        return delim.join(messages)

    num_trials = len(trials)
    trials_by_state = _get_trials_by_state(trials)

    for local_dir in sorted({t.local_dir for t in trials}):
        messages.append("Result logdir: {}".format(local_dir))

    num_trials_strs = [
        "{} {}".format(len(trials_by_state[state]), state)
        for state in sorted(trials_by_state)
    ]

    if total_samples and total_samples >= sys.maxsize:
        total_samples = "infinite"

    messages.append("Number of trials: {}{} ({})".format(
        num_trials, f"/{total_samples}"
        if total_samples else "", ", ".join(num_trials_strs)))

    if force_table or (has_verbosity(Verbosity.V2_TRIAL_NORM) and done):
        messages += trial_progress_table(trials, metric_columns,
                                         parameter_columns, fmt, max_rows,
                                         metric, mode, sort_by_metric)

    return delim.join(messages)
def report(self, trials, done, *sys_info):
        
        
        raise NotImplementedError
def is_on(self):
        
        if self.attribute == "gas":
            # Gas sensor value of Shelly Gas can be none/mild/heavy/test. We return True
            # when the value is mild or heavy.
            return getattr(self.block, self.attribute) in ["mild", "heavy"]
        return bool(getattr(self.block, self.attribute))
def __init__(self, logger,
            html_preprocessor=HTMLPreProcessor(),
            css_preprocessor=CSSPreProcessor(),
            encoding='utf-8', pretty_print=False):
        
        

        self.encoding = encoding
        self.html_preprocessor = html_preprocessor
        self.css_preprocessor = css_preprocessor
        self.pretty_print = pretty_print
        self.logger = self.log = logger
        self.version = '2.0'
        self.container = NullContainer()
        self.metadata = Metadata(self)
        self.uid = None
        self.manifest = Manifest(self)
        self.spine = Spine(self)
        self.guide = Guide(self)
        self.toc = TOC()
        self.pages = PageList()
def extra_state_attributes(self) -> dict[str, str | datetime]:
        
        if self.entity_description.key not in self.coordinator.data:
            return {}
        player = self.coordinator.data[self.entity_description.key]

        attrs: dict[str, str | datetime] = {}
        if game := player.get("gameextrainfo"):
            attrs["game"] = game
        if game_id := player.get("gameid"):
            attrs["game_id"] = game_id
            game_url = f"{STEAM_API_URL}{player['gameid']}/"
            attrs["game_image_header"] = f"{game_url}{STEAM_HEADER_IMAGE_FILE}"
            attrs["game_image_main"] = f"{game_url}{STEAM_MAIN_IMAGE_FILE}"
            if info := self._get_game_icon(player):
                attrs["game_icon"] = STEAM_ICON_URL % (
                    game_id,
                    info,
                )
        self._attr_name = player["personaname"]
        self._attr_entity_picture = player["avatarmedium"]
        if last_online := player.get("lastlogoff"):
            attrs["last_online"] = utc_from_timestamp(mktime(localtime(last_online)))
        if level := self.coordinator.data[self.entity_description.key]["level"]:
            attrs["level"] = level
        return attrs
def _unindex_entry_value(
        self, key: str, value: str, index: dict[str, list[str]]
    ) -> None:
        
        
        entries = index[value]
        entries.remove(key)
        if not entries:
            del index[value]
def get_related_field(self):
        
        
        
        data = self.to._meta.get_field_by_name(self.field_name)
        if not data[2]:
            raise FieldDoesNotExist("No related field named '%s'" %
                    self.field_name)
        return data[0]
def get_related_field(self):
        
        
        
        opts = self.through._meta
        if self.through_fields:
            field = opts.get_field(self.through_fields[0])
        else:
            for field in opts.fields:
                rel = getattr(field, 'rel', None)
                if rel and rel.to == self.to:
                    break
        return field.foreign_related_fields[0]
def _get_m2m_attr(self, related, attr):
        ""
        cache_attr = '_m2m_%s_cache' % attr
        if hasattr(self, cache_attr):
            return getattr(self, cache_attr)
        if self.rel.through_fields is not None:
            link_field_name = self.rel.through_fields[0]
        else:
            link_field_name = None
        for f in self.rel.through._meta.fields:
            if (f.is_relation and f.rel.to == related.related_model and
                    (link_field_name is None or link_field_name == f.name)):
                setattr(self, cache_attr, getattr(f, attr))
                return getattr(self, cache_attr)
def _get_m2m_attr(self, related, attr):
        
        
        
        cache_attr = '_m2m_%s_cache' % attr
        if hasattr(self, cache_attr):
            return getattr(self, cache_attr)
        if self.rel.through_fields is not None:
            link_field_name = self.rel.through_fields[0]
        else:
            link_field_name = None
        for f in self.rel.through._meta.fields:
            if (f.is_relation and f.rel.to == related.related_model and
                    (link_field_name is None or link_field_name == f.name)):
                setattr(self, cache_attr, getattr(f, attr))
                return getattr(self, cache_attr)
def __new__(cls, arr, **kwargs):
        

        
        x = np.asarray(arr).view(cls)
        x.tags = kwargs

        return x
def imshow(arr, plugin=None, **plugin_args):
    

    
    if isinstance(arr, basestring):
        arr = call_plugin('imread', arr, plugin=plugin)
    return call_plugin('imshow', arr, plugin=plugin, **plugin_args)
def imsave(fname, arr, plugin=None, check_contrast=True, **plugin_args):
    
    
    if plugin is None and hasattr(fname, 'lower'):
        if fname.lower().endswith(('.tiff', '.tif')):
            plugin = 'tifffile'
    if check_contrast and is_low_contrast(arr):
        warn('%s is a low contrast image' % fname)
    if arr.dtype == bool:
        warn('%s is a boolean image: setting True to 1 and False to 0' % fname)
    return call_plugin('imsave', fname, arr, plugin=plugin, **plugin_args)
def show():
    

    
    return call_plugin('_app_show')
def save_fixtures_to_db(self, fixtures_pack='generic', fixtures_dict=None,
                            use_object_ids=False):
        
        
        
        if fixtures_dict is None:
            fixtures_dict = {}

        fixtures_pack_path = self._validate_fixtures_pack(fixtures_pack)
        self._validate_fixture_dict(fixtures_dict, allowed=ALLOWED_DB_FIXTURES)

        db_models = {}
        for fixture_type, fixtures in six.iteritems(fixtures_dict):
            API_MODEL = FIXTURE_API_MODEL.get(fixture_type, None)
            PERSISTENCE_MODEL = FIXTURE_PERSISTENCE_MODEL.get(fixture_type, None)

            loaded_fixtures = {}
            for fixture in fixtures:
                # Guard against copy and type and similar typos
                if fixture in loaded_fixtures:
                    msg = 'Fixture "%s" is specified twice, probably a typo.' % (fixture)
                    raise ValueError(msg)

                fixture_dict = self.meta_loader.load(
                    self._get_fixture_file_path_abs(fixtures_pack_path, fixture_type, fixture))
                api_model = API_MODEL(**fixture_dict)
                db_model = API_MODEL.to_model(api_model)

                # Make sure we also set and use object id if that functionality is used
                if use_object_ids and 'id' in fixture_dict:
                    db_model.id = fixture_dict['id']

                db_model = PERSISTENCE_MODEL.add_or_update(db_model)
                loaded_fixtures[fixture] = db_model

            db_models[fixture_type] = loaded_fixtures

        return db_models
def permutation_importance(
    estimator,
    X,
    y,
    *,
    scoring=None,
    n_repeats=5,
    n_jobs=None,
    random_state=None,
    sample_weight=None,
    max_samples=1.0,
):
    
    
    if not hasattr(X, "iloc"):
        X = check_array(X, force_all_finite="allow-nan", dtype=None)

    # Precompute random seed from the random state to be used
    # to get a fresh independent RandomState instance for each
    # parallel call to _calculate_permutation_scores, irrespective of
    # the fact that variables are shared or not depending on the active
    # joblib backend (sequential, thread-based or process-based).
    random_state = check_random_state(random_state)
    random_seed = random_state.randint(np.iinfo(np.int32).max + 1)

    if not isinstance(max_samples, numbers.Integral):
        max_samples = int(max_samples * X.shape[0])
    elif not (0 < max_samples <= X.shape[0]):
        raise ValueError("max_samples must be in (0, n_samples]")

    if callable(scoring):
        scorer = scoring
    elif scoring is None or isinstance(scoring, str):
        scorer = check_scoring(estimator, scoring=scoring)
    else:
        scorers_dict = _check_multimetric_scoring(estimator, scoring)
        scorer = _MultimetricScorer(**scorers_dict)

    baseline_score = _weights_scorer(scorer, estimator, X, y, sample_weight)

    scores = Parallel(n_jobs=n_jobs)(
        delayed(_calculate_permutation_scores)(
            estimator,
            X,
            y,
            sample_weight,
            col_idx,
            random_seed,
            n_repeats,
            scorer,
            max_samples,
        )
        for col_idx in range(X.shape[1])
    )

    if isinstance(baseline_score, dict):
        return {
            name: _create_importances_bunch(
                baseline_score[name],
                # unpack the permuted scores
                np.array([scores[col_idx][name] for col_idx in range(X.shape[1])]),
            )
            for name in baseline_score
        }
    else:
        return _create_importances_bunch(baseline_score, np.array(scores))
def __init__(self,
               sample_rate=16000,
               window_ms=20.0,
               stride_ms=10.0):
    
    
    self.sample_rate = sample_rate
    self.window_ms = window_ms
    self.stride_ms = stride_ms
def perimeter(image, neighborhood=4):
    

    
    if image.ndim != 2:
        raise NotImplementedError('`perimeter` supports 2D images only')

    if neighborhood == 4:
        strel = STREL_4
    else:
        strel = STREL_8
    image = image.astype(np.uint8)
    eroded_image = ndi.binary_erosion(image, strel, border_value=0)
    border_image = image - eroded_image

    perimeter_weights = np.zeros(50, dtype=np.double)
    perimeter_weights[[5, 7, 15, 17, 25, 27]] = 1
    perimeter_weights[[21, 33]] = sqrt(2)
    perimeter_weights[[13, 23]] = (1 + sqrt(2)) / 2

    perimeter_image = ndi.convolve(border_image, np.array([[10, 2, 10],
                                                           [2, 1,  2],
                                                           [10, 2, 10]]),
                                   mode='constant', cval=0)

    # You can also write
    # return perimeter_weights[perimeter_image].sum()
    # but that was measured as taking much longer than bincount + np.dot (5x
    # as much time)
    perimeter_histogram = np.bincount(perimeter_image.ravel(), minlength=50)
    total_perimeter = perimeter_histogram @ perimeter_weights
    return total_perimeter
def fromFile(cls, pathOrFile):
        
        with maybe_open(pathOrFile, 'rb') as fp:
            value = load(fp)
        plist = cls()
        plist.update(value)
        return plist
def _data_to_save(self) -> dict:
        
        return {"items": list(self.data.values())}
def update_expectations(self):
        
        
        
        for w in xrange(self.m_W):
            self.m_lambda[:, w] *= np.exp(self.m_r[-1] -
                                          self.m_r[self.m_timestamp[w]])
        self.m_Elogbeta = sp.psi(self.m_eta + self.m_lambda) - \
            sp.psi(self.m_W*self.m_eta + self.m_lambda_sum[:, np.newaxis])

        self.m_timestamp[:] = self.m_updatect
        self.m_status_up_to_date = True
def __init__(self, corpus, id2word, max_chunks=None, max_time=None,
                 chunksize=256, kappa=1.0, tau=64.0, K=15, T=150, alpha=1,
                 gamma=1, eta=0.01, scale=1.0, var_converge=0.0001,
                 outputdir=None):
        
        
        
        self.corpus = corpus
        self.id2word = id2word
        self.chunksize = chunksize
        self.max_chunks = max_chunks
        self.max_time = max_time
        self.outputdir = outputdir

        self.m_W = len(id2word)
        self.m_D = len(corpus)

        self.m_T = T
        self.m_K = K
        self.m_alpha = alpha
        self.m_gamma = gamma

        self.m_var_sticks = np.zeros((2, T-1))
        self.m_var_sticks[0] = 1.0
        self.m_var_sticks[1] = range(T-1, 0, -1)
        self.m_varphi_ss = np.zeros(T)

        self.m_lambda = np.random.gamma(1.0, 1.0, (T, self.m_W)) * self.m_D*100/(T*self.m_W)-eta
        self.m_eta = eta
        self.m_Elogbeta = dirichlet_expectation(self.m_eta + self.m_lambda)

        self.m_tau = tau + 1
        self.m_kappa = kappa
        self.m_scale = scale
        self.m_updatect = 0
        self.m_status_up_to_date = True
        self.m_num_docs_processed = 0

        self.m_timestamp = np.zeros(self.m_W, dtype=int)
        self.m_r = [0]
        self.m_lambda_sum = np.sum(self.m_lambda, axis=1)

        self.m_var_converge = var_converge

        if self.outputdir:
            self.save_options()

        # if a training corpus was provided, start estimating the model right away
        if corpus is not None:
            self.update(corpus)
def device(self) -> AOSmithDevice:
        
        return self.coordinator.data[self.junction_id]
def list_fields(self):
        

        
        return [x[0] for x in self.list_fields_and_values()]
def preset_modes(self):
        
        presets = [PRESET_NONE, PRESET_BOOST]
        presets.extend(self._device_profile_names)
        return presets
def hvac_mode(self) -> str:
        
        if self._disabled_by_cooling_mode:
            return HVAC_MODE_OFF
        if self._device.boostMode:
            return HVAC_MODE_HEAT
        if self._device.controlMode == HMIP_MANUAL_CM:
            return HVAC_MODE_HEAT if self._heat_mode_enabled else HVAC_MODE_COOL

        return HVAC_MODE_AUTO
def __init__(self, coordinator: DataUpdateCoordinator, sensor_type: _SensorTypes):
        
        super().__init__(coordinator)
        self._type = sensor_type
def fetch_funding_rate_history(self, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):
        
        
        
        self.load_markets()
        market = self.market(symbol)
        paginate = False
        paginate, params = self.handle_option_and_params(params, 'fetchFundingRateHistory', 'paginate')
        if paginate:
            return self.fetch_paginated_call_deterministic('fetchFundingRateHistory', symbol, since, limit, '8h', params, 720)
        time = self.milliseconds()
        month = 30 * 24 * 60 * 60 * 1000
        if since is None:
            since = time - month
        request = {
            'instrument_name': market['id'],
            'start_timestamp': since,
            'end_timestamp': time,
        }
        response = self.publicGetGetFundingRateHistory(self.extend(request, params))
        #
        #    {
        #        "jsonrpc": "2.0",
        #        "id": 7617,
        #        "result": [
        #          {
        #            "timestamp": 1569891600000,
        #            "index_price": 8222.87,
        #            "prev_index_price": 8305.72,
        #            "interest_8h": -0.00009234260068476106,
        #            "interest_1h": -4.739622041017375e-7
        #          }
        #        ]
        #    }
        #
        rates = []
        result = self.safe_value(response, 'result', [])
        for i in range(0, len(result)):
            fr = result[i]
            rate = self.parse_funding_rate(fr, market)
            rates.append(rate)
        return self.filter_by_symbol_since_limit(rates, symbol, since, limit)
def fetch_my_liquidations(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' fetchMyLiquidations() requires a symbol argument')
        self.load_markets()
        market = self.market(symbol)
        if market['spot']:
            raise NotSupported(self.id + ' fetchMyLiquidations() does not support ' + market['type'] + ' markets')
        request = {
            'instrument_name': market['id'],
            'type': 'bankruptcy',
        }
        if since is not None:
            request['search_start_timestamp'] = since
        if limit is not None:
            request['count'] = limit
        response = self.privateGetGetSettlementHistoryByInstrument(self.extend(request, params))
        #
        #     {
        #         "jsonrpc": "2.0",
        #         "result": {
        #             "settlements": [
        #                 {
        #                     "type": "bankruptcy",
        #                     "timestamp": 1696579200041,
        #                     "funded": 10000.0,
        #                     "session_bankrupcy": 10000.0
        #                     "session_profit_loss": 112951.68715857354,
        #                     "session_tax": 0.15,
        #                     "session_tax_rate": 0.0015,
        #                     "socialized": 0.001,
        #                 },
        #             ],
        #             "continuation": "5dHzoGyD8Hs8KURoUhfgXgHpJTA5oyapoudSmNeAfEftqRbjNE6jNNUpo2oCu1khnZL9ao"
        #         },
        #         "usIn": 1696652052254890,
        #         "usOut": 1696652052255733,
        #         "usDiff": 843,
        #         "testnet": False
        #     }
        #
        result = self.safe_value(response, 'result', {})
        settlements = self.safe_value(result, 'settlements', [])
        return self.parse_liquidations(settlements, market, since, limit)
def fetch_positions(self, symbols: Strings = None, params={}):
        
        
        
        self.load_markets()
        kind = self.safe_string(params, 'kind')
        code = None
        if symbols is None:
            code = self.code_from_options('fetchPositions', params)
        elif isinstance(symbols, str):
            code = symbols
            symbols = None  # fix https://github.com/ccxt/ccxt/issues/13961
        else:
            if isinstance(symbols, list):
                length = len(symbols)
                if length != 1:
                    raise BadRequest(self.id + ' fetchPositions() symbols argument cannot contain more than 1 symbol')
                market = self.market(symbols[0])
                settle = market['settle']
                code = settle if (settle is not None) else market['base']
                kind = market['info']['kind']
        currency = self.currency(code)
        request = {
            'currency': currency['id'],
        }
        if kind is not None:
            request['kind'] = kind
        response = self.privateGetGetPositions(self.extend(request, params))
        #
        #     {
        #         "jsonrpc": "2.0",
        #         "id": 2236,
        #         "result": [
        #             {
        #                 "average_price": 7440.18,
        #                 "delta": 0.006687487,
        #                 "direction": "buy",
        #                 "estimated_liquidation_price": 1.74,
        #                 "floating_profit_loss": 0,
        #                 "index_price": 7466.79,
        #                 "initial_margin": 0.000197283,
        #                 "instrument_name": "BTC-PERPETUAL",
        #                 "kind": "future",
        #                 "leverage": 34,
        #                 "maintenance_margin": 0.000143783,
        #                 "mark_price": 7476.65,
        #                 "open_orders_margin": 0.000197288,
        #                 "realized_funding": -1e-8,
        #                 "realized_profit_loss": -9e-9,
        #                 "settlement_price": 7476.65,
        #                 "size": 50,
        #                 "size_currency": 0.006687487,
        #                 "total_profit_loss": 0.000032781
        #             },
        #         ]
        #     }
        #
        result = self.safe_value(response, 'result')
        return self.parse_positions(result, symbols)
def fetch_markets(self, params={}) -> List[Market]:
        
        
        
        instrumentsResponses = []
        result = []
        parsedMarkets = {}
        fetchAllMarkets = None
        fetchAllMarkets, params = self.handle_option_and_params(params, 'fetchMarkets', 'fetchAllMarkets', True)
        if fetchAllMarkets:
            instrumentsResponse = self.publicGetGetInstruments(params)
            instrumentsResponses.append(instrumentsResponse)
        else:
            currenciesResponse = self.publicGetGetCurrencies(params)
            #
            #     {
            #         "jsonrpc": "2.0",
            #         "result": [
            #             {
            #                 "withdrawal_priorities": [
            #                     {value: 0.15, name: "very_low"},
            #                     {value: 1.5, name: "very_high"},
            #                 ],
            #                 "withdrawal_fee": 0.0005,
            #                 "min_withdrawal_fee": 0.0005,
            #                 "min_confirmations": 1,
            #                 "fee_precision": 4,
            #                 "currency_long": "Bitcoin",
            #                 "currency": "BTC",
            #                 "coin_type": "BITCOIN"
            #             }
            #         ],
            #         "usIn": 1583761588590479,
            #         "usOut": 1583761588590544,
            #         "usDiff": 65,
            #         "testnet": False
            #     }
            #
            currenciesResult = self.safe_value(currenciesResponse, 'result', [])
            for i in range(0, len(currenciesResult)):
                currencyId = self.safe_string(currenciesResult[i], 'currency')
                request = {
                    'currency': currencyId,
                }
                instrumentsResponse = self.publicGetGetInstruments(self.extend(request, params))
                #
                #     {
                #         "jsonrpc":"2.0",
                #         "result":[
                #             {
                #                 "tick_size":0.0005,
                #                 "taker_commission":0.0003,
                #                 "strike":52000.0,
                #                 "settlement_period":"month",
                #                 "settlement_currency":"BTC",
                #                 "quote_currency":"BTC",
                #                 "option_type":"put",  # put, call
                #                 "min_trade_amount":0.1,
                #                 "maker_commission":0.0003,
                #                 "kind":"option",
                #                 "is_active":true,
                #                 "instrument_name":"BTC-24JUN22-52000-P",
                #                 "expiration_timestamp":1656057600000,
                #                 "creation_timestamp":1648199543000,
                #                 "counter_currency":"USD",
                #                 "contract_size":1.0,
                #                 "block_trade_commission":0.0003,
                #                 "base_currency":"BTC"
                #             },
                #             {
                #                 "tick_size":0.5,
                #                 "taker_commission":0.0005,
                #                 "settlement_period":"month",  # month, week
                #                 "settlement_currency":"BTC",
                #                 "quote_currency":"USD",
                #                 "min_trade_amount":10.0,
                #                 "max_liquidation_commission":0.0075,
                #                 "max_leverage":50,
                #                 "maker_commission":0.0,
                #                 "kind":"future",
                #                 "is_active":true,
                #                 "instrument_name":"BTC-27MAY22",
                #                 "future_type":"reversed",
                #                 "expiration_timestamp":1653638400000,
                #                 "creation_timestamp":1648195209000,
                #                 "counter_currency":"USD",
                #                 "contract_size":10.0,
                #                 "block_trade_commission":0.0001,
                #                 "base_currency":"BTC"
                #             },
                #             {
                #                 "tick_size":0.5,
                #                 "taker_commission":0.0005,
                #                 "settlement_period":"perpetual",
                #                 "settlement_currency":"BTC",
                #                 "quote_currency":"USD",
                #                 "min_trade_amount":10.0,
                #                 "max_liquidation_commission":0.0075,
                #                 "max_leverage":50,
                #                 "maker_commission":0.0,
                #                 "kind":"future",
                #                 "is_active":true,
                #                 "instrument_name":"BTC-PERPETUAL",
                #                 "future_type":"reversed",
                #                 "expiration_timestamp":32503708800000,
                #                 "creation_timestamp":1534242287000,
                #                 "counter_currency":"USD",
                #                 "contract_size":10.0,
                #                 "block_trade_commission":0.0001,
                #                 "base_currency":"BTC"
                #             },
                #         ],
                #         "usIn":1648691472831791,
                #         "usOut":1648691472831896,
                #         "usDiff":105,
                #         "testnet":false
                #     }
                #
                instrumentsResponses.append(instrumentsResponse)
        for i in range(0, len(instrumentsResponses)):
            instrumentsResult = self.safe_value(instrumentsResponses[i], 'result', [])
            for k in range(0, len(instrumentsResult)):
                market = instrumentsResult[k]
                kind = self.safe_string(market, 'kind')
                isSpot = (kind == 'spot')
                id = self.safe_string(market, 'instrument_name')
                baseId = self.safe_string(market, 'base_currency')
                quoteId = self.safe_string(market, 'counter_currency')
                settleId = self.safe_string(market, 'settlement_currency')
                base = self.safe_currency_code(baseId)
                quote = self.safe_currency_code(quoteId)
                settle = self.safe_currency_code(settleId)
                settlementPeriod = self.safe_value(market, 'settlement_period')
                swap = (settlementPeriod == 'perpetual')
                future = not swap and (kind.find('future') >= 0)
                option = (kind.find('option') >= 0)
                isComboMarket = kind.find('combo') >= 0
                expiry = self.safe_integer(market, 'expiration_timestamp')
                strike = None
                optionType = None
                symbol = id
                type = 'swap'
                if future:
                    type = 'future'
                elif option:
                    type = 'option'
                elif isSpot:
                    type = 'spot'
                if isSpot:
                    symbol = base + '/' + quote
                elif not isComboMarket:
                    symbol = base + '/' + quote + ':' + settle
                    if option or future:
                        symbol = symbol + '-' + self.yymmdd(expiry, '')
                        if option:
                            strike = self.safe_number(market, 'strike')
                            optionType = self.safe_string(market, 'option_type')
                            letter = 'C' if (optionType == 'call') else 'P'
                            symbol = symbol + '-' + self.number_to_string(strike) + '-' + letter
                parsedMarketValue = self.safe_value(parsedMarkets, symbol)
                if parsedMarketValue:
                    continue
                parsedMarkets[symbol] = True
                minTradeAmount = self.safe_number(market, 'min_trade_amount')
                tickSize = self.safe_number(market, 'tick_size')
                result.append({
                    'id': id,
                    'symbol': symbol,
                    'base': base,
                    'quote': quote,
                    'settle': settle,
                    'baseId': baseId,
                    'quoteId': quoteId,
                    'settleId': settleId,
                    'type': type,
                    'spot': isSpot,
                    'margin': False,
                    'swap': swap,
                    'future': future,
                    'option': option,
                    'active': self.safe_value(market, 'is_active'),
                    'contract': not isSpot,
                    'linear': (settle == quote),
                    'inverse': (settle != quote),
                    'taker': self.safe_number(market, 'taker_commission'),
                    'maker': self.safe_number(market, 'maker_commission'),
                    'contractSize': self.safe_number(market, 'contract_size'),
                    'expiry': expiry,
                    'expiryDatetime': self.iso8601(expiry),
                    'strike': strike,
                    'optionType': optionType,
                    'precision': {
                        'amount': minTradeAmount,
                        'price': tickSize,
                    },
                    'limits': {
                        'leverage': {
                            'min': None,
                            'max': None,
                        },
                        'amount': {
                            'min': minTradeAmount,
                            'max': None,
                        },
                        'price': {
                            'min': tickSize,
                            'max': None,
                        },
                        'cost': {
                            'min': None,
                            'max': None,
                        },
                    },
                    'created': self.safe_integer(market, 'creation_timestamp'),
                    'info': market,
                })
        return result
def fetch_tickers(self, symbols: Strings = None, params={}) -> Tickers:
        
        
        
        self.load_markets()
        symbols = self.market_symbols(symbols)
        code = self.safe_string_2(params, 'code', 'currency')
        params = self.omit(params, ['code'])
        if code is None:
            raise ArgumentsRequired(self.id + ' fetchTickers requires a currency/code(eg: BTC/ETH/USDT) parameter to fetch tickers for')
        currency = self.currency(code)
        request = {
            'currency': currency['id'],
        }
        response = self.publicGetGetBookSummaryByCurrency(self.extend(request, params))
        #
        #     {
        #         "jsonrpc": "2.0",
        #         "result": [
        #             {
        #                 "volume": 124.1,
        #                 "underlying_price": 7856.445926872601,
        #                 "underlying_index": "SYN.BTC-10MAR20",
        #                 "quote_currency": "USD",
        #                 "open_interest": 121.8,
        #                 "mid_price": 0.01975,
        #                 "mark_price": 0.01984559,
        #                 "low": 0.0095,
        #                 "last": 0.0205,
        #                 "interest_rate": 0,
        #                 "instrument_name": "BTC-10MAR20-7750-C",
        #                 "high": 0.0295,
        #                 "estimated_delivery_price": 7856.29,
        #                 "creation_timestamp": 1583783678366,
        #                 "bid_price": 0.0185,
        #                 "base_currency": "BTC",
        #                 "ask_price": 0.021
        #             },
        #         ],
        #         "usIn": 1583783678361966,
        #         "usOut": 1583783678372069,
        #         "usDiff": 10103,
        #         "testnet": False
        #     }
        #
        result = self.safe_list(response, 'result', [])
        tickers = {}
        for i in range(0, len(result)):
            ticker = self.parse_ticker(result[i])
            symbol = ticker['symbol']
            tickers[symbol] = ticker
        return self.filter_by_array_tickers(tickers, 'symbol', symbols)
def get_cloud_syncer(local_dir, remote_dir=None, sync_function=None):
    
    
    key = (local_dir, remote_dir)

    if key in _syncers:
        return _syncers[key]

    if not remote_dir:
        _syncers[key] = Syncer(local_dir, remote_dir, NOOP)
        return _syncers[key]

    client = get_sync_client(sync_function)

    if client:
        _syncers[key] = Syncer(local_dir, remote_dir, client)
        return _syncers[key]
    sync_client = get_cloud_sync_client(remote_dir)
    _syncers[key] = Syncer(local_dir, remote_dir, sync_client)
    return _syncers[key]
def get_node_syncer(local_dir, remote_dir=None, sync_function=None):
    
    
    key = (local_dir, remote_dir)
    if key in _syncers:
        return _syncers[key]
    elif not remote_dir or sync_function is False:
        sync_client = NOOP
    elif sync_function and sync_function is not True:
        sync_client = get_sync_client(sync_function)
    else:
        sync = log_sync_template()
        if sync:
            sync_client = CommandBasedClient(sync, sync)
            sync_client.set_logdir(local_dir)
        else:
            sync_client = NOOP

    _syncers[key] = NodeSyncer(local_dir, remote_dir, sync_client)
    return _syncers[key]
def __init__(self, local_dir: str, remote_dir: str, sync_client: SyncClient = NOOP):
        
        
        self._local_dir = os.path.join(local_dir, "") if local_dir else local_dir
        self._remote_dir = remote_dir
        self.last_sync_up_time = float("-inf")
        self.last_sync_down_time = float("-inf")
        self.sync_client = sync_client
def prepare(
        self,
        output_dir_path: str,
        iac_project_path: Optional[str] = None,
        debug: bool = False,
        aws_profile: Optional[str] = None,
        aws_region: Optional[str] = None,
        skip_prepare_infra: bool = False,
        plan_file: Optional[str] = None,
        project_root_dir: Optional[str] = None,
    ) -> str:
        
        
        
        LOG.info('Executing prepare hook of hook "%s"', self._hook_name)
        params = {
            "IACProjectPath": iac_project_path if iac_project_path else str(Path.cwd()),
            "OutputDirPath": output_dir_path,
            "Debug": debug,
            "SkipPrepareInfra": skip_prepare_infra,
        }
        if aws_profile:
            params["Profile"] = aws_profile
        if aws_region:
            params["Region"] = aws_region
        if plan_file:
            params["PlanFile"] = plan_file
        if project_root_dir:
            params["ProjectRootDir"] = project_root_dir

        output = self._execute("prepare", params)

        metadata_file_loc = None
        iac_applications: Dict[str, Dict] = output.get("iac_applications", {})
        if iac_applications and len(iac_applications) == 1:
            # NOTE: we assume there is only one application in the `iac_applications` dictionary,
            # which is the only case we support right now
            main_application = list(iac_applications.values())[0]
            metadata_file_loc = main_application.get("metadata_file")

        if not metadata_file_loc:
            raise InvalidHookWrapperException("Metadata file path not found in the prepare hook output")

        LOG.debug("Metadata file location - %s", metadata_file_loc)
        return cast(str, metadata_file_loc)
def random_crop_resize(frames: tf.Tensor, output_h: int, output_w: int,
                       num_frames: int, num_channels: int,
                       aspect_ratio: Tuple[float, float],
                       area_range: Tuple[float, float]) -> tf.Tensor:
  
  
  shape = tf.shape(frames)
  seq_len, _, _, channels = shape[0], shape[1], shape[2], shape[3]
  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])
  factor = output_w / output_h
  aspect_ratio = (aspect_ratio[0] * factor, aspect_ratio[1] * factor)
  sample_distorted_bbox = tf.image.sample_distorted_bounding_box(
      shape[1:],
      bounding_boxes=bbox,
      min_object_covered=0.1,
      aspect_ratio_range=aspect_ratio,
      area_range=area_range,
      max_attempts=100,
      use_image_if_no_bounding_boxes=True)
  bbox_begin, bbox_size, _ = sample_distorted_bbox
  offset_y, offset_x, _ = tf.unstack(bbox_begin)
  target_height, target_width, _ = tf.unstack(bbox_size)
  size = tf.convert_to_tensor((seq_len, target_height, target_width, channels))
  offset = tf.convert_to_tensor((0, offset_y, offset_x, 0))
  frames = tf.slice(frames, offset, size)
  frames = tf.cast(tf.image.resize(frames, (output_h, output_w)), frames.dtype)
  frames.set_shape((num_frames, output_h, output_w, num_channels))
  return frames
def resize_to_range(image,
                    label=None,
                    min_size=None,
                    max_size=None,
                    factor=None,
                    keep_aspect_ratio=True,
                    align_corners=True,
                    label_layout_is_chw=False,
                    scope=None,
                    method=tf.image.ResizeMethod.BILINEAR):
  
  
  with tf.name_scope(scope, 'resize_to_range', [image]):
    new_tensor_list = []
    min_size = tf.cast(min_size, tf.float32)
    if max_size is not None:
      max_size = tf.cast(max_size, tf.float32)
      # Modify the max_size to be a multiple of factor plus 1 and make sure the
      # max dimension after resizing is no larger than max_size.
      if factor is not None:
        max_size = (max_size - (max_size - 1) % factor)

    [orig_height, orig_width, _] = resolve_shape(image, rank=3)
    orig_height = tf.cast(orig_height, tf.float32)
    orig_width = tf.cast(orig_width, tf.float32)
    orig_min_size = tf.minimum(orig_height, orig_width)

    # Calculate the larger of the possible sizes
    large_scale_factor = min_size / orig_min_size
    large_height = tf.cast(tf.floor(orig_height * large_scale_factor), tf.int32)
    large_width = tf.cast(tf.floor(orig_width * large_scale_factor), tf.int32)
    large_size = tf.stack([large_height, large_width])

    new_size = large_size
    if max_size is not None:
      # Calculate the smaller of the possible sizes, use that if the larger
      # is too big.
      orig_max_size = tf.maximum(orig_height, orig_width)
      small_scale_factor = max_size / orig_max_size
      small_height = tf.cast(
          tf.floor(orig_height * small_scale_factor), tf.int32)
      small_width = tf.cast(tf.floor(orig_width * small_scale_factor), tf.int32)
      small_size = tf.stack([small_height, small_width])
      new_size = tf.cond(
          tf.cast(tf.reduce_max(large_size), tf.float32) > max_size,
          lambda: small_size,
          lambda: large_size)
    # Ensure that both output sides are multiples of factor plus one.
    if factor is not None:
      new_size += (factor - (new_size - 1) % factor) % factor
    if not keep_aspect_ratio:
      # If not keep the aspect ratio, we resize everything to max_size, allowing
      # us to do pre-processing without extra padding.
      new_size = [tf.reduce_max(new_size), tf.reduce_max(new_size)]
    new_tensor_list.append(tf.image.resize(
        image, new_size, method=method, align_corners=align_corners))
    if label is not None:
      if label_layout_is_chw:
        # Input label has shape [channel, height, width].
        resized_label = tf.expand_dims(label, 3)
        resized_label = tf.image.resize(
            resized_label,
            new_size,
            method=get_label_resize_method(label),
            align_corners=align_corners)
        resized_label = tf.squeeze(resized_label, 3)
      else:
        # Input label has shape [height, width, channel].
        resized_label = tf.image.resize(
            label,
            new_size,
            method=get_label_resize_method(label),
            align_corners=align_corners)
      new_tensor_list.append(resized_label)
    else:
      new_tensor_list.append(None)
    return new_tensor_list
def value_changed(self, packet):
        
        
        if packet.data[0] == 0xa5 and packet.data[1] == 0x02:
            val = packet.data[2]
            self._brightness = math.floor(val / 100.0 * 256.0)
            self._on_state = bool(val != 0)
            self.schedule_update_ha_state()
def fit(self, X, Y, **fit_params):
        
        
        super().fit(X, Y, **fit_params)
        return self
def predict_proba(self, X):
        
        
        check_is_fitted(self, 'estimators_')
        if not hasattr(self.estimator, "predict_proba"):
            raise ValueError("The base estimator should implement"
                             "predict_proba method")

        results = [estimator.predict_proba(X) for estimator in
                   self.estimators_]
        return results
def test_strategy_choice_ddp_fork_in_interactive():
    
    trainer = Trainer(accelerator="cpu", devices=2)
    assert isinstance(trainer.accelerator, CPUAccelerator)
    assert isinstance(trainer.strategy, DDPStrategy)
    assert isinstance(trainer.strategy.cluster_environment, LightningEnvironment)
    assert trainer.strategy.launcher._start_method == "fork"
def check_func_backward_inputs(func, grad_inputs):
    

    
    # Check if func_node returns any variables that should have their
    # data attributes copied into the static outputs array of the
    # backward schedule.
    forward_static_arrays_info = getattr(func, '_forward_static_arrays_info', None)
    if forward_static_arrays_info is not None:
        forward_schedule = get_static_schedule(func)
        backward_schedule = forward_schedule.get_backward_schedule_func()
        #print('Found static_arrays_list in backward(): ', forward_static_arrays_info)
        for func_arg_index, chain_arg_index in forward_static_arrays_info:
            # Need to make the chain_arg_index'th output array of the schedule refer
            # to the array of the variable in chain_arg_index'th position of the
            # grad_outputs tuple.
            # Note: if the input variable of the static chain was input to
            # multiple function in the forward pass, then the following
            # static array reference will be set multiple time for the same
            # variable. This is fine, though, since only the final reference
            # is needed for the output gradients from the static schedule.

            # assert backward_schedule._out_arrays[chain_arg_index] is None

            # Since the data array was allocated statically, we must return a copy.
            # Note: In Chainer, it is allowed for one or more of the grad inputs
            # to be None instead of a Variable.
            if grad_inputs[func_arg_index] is not None:
                backward_schedule._out_arrays[chain_arg_index] = grad_inputs[func_arg_index].data
                grad_inputs[func_arg_index].data = grad_inputs[func_arg_index].data.copy()
def generate_square_subsequent_mask(self, sz):
        
        
        mask = paddle.zeros([sz, sz], dtype="float32")
        mask_inf = paddle.triu(
            paddle.full(shape=[sz, sz], dtype="float32", fill_value="-inf"), diagonal=1
        )
        mask = mask + mask_inf
        return mask.unsqueeze([0, 1])
def glove2word2vec(glove_input_file, word2vec_output_file):
    

    
    num_lines, num_dims = get_glove_info(glove_input_file)
    logger.info("converting %i vectors from %s to %s", num_lines, glove_input_file, word2vec_output_file)
    with smart_open(word2vec_output_file, 'wb') as fout:
        fout.write("{0} {1}\n".format(num_lines, num_dims).encode('utf-8'))
        with smart_open(glove_input_file, 'rb') as fin:
            for line in fin:
                fout.write(line)
    return num_lines, num_dims
def start_killing_nodes(duration, kill_interval, kill_node_type):
    
    

    for kill_idx in range(1, int(duration / kill_interval)):
        while True:
            try:
                # kill
                if kill_node_type == TestScenario.KILL_HEAD_NODE:
                    kill_head()
                elif kill_node_type == TestScenario.KILL_WORKER_NODE:
                    kill_worker()
                break
            except Exception as e:
                from time import sleep

                print(f"Fail to kill node, retry in 5 seconds: {e}")
                sleep(5)

        time.sleep(kill_interval)
def show_result_pyplot(model,
                       img,
                       result,
                       score_thr=0.3,
                       title='result',
                       wait_time=0,
                       palette=None,
                       out_file=None):
    
    
    if hasattr(model, 'module'):
        model = model.module
    model.show_result(
        img,
        result,
        score_thr=score_thr,
        show=True,
        wait_time=wait_time,
        win_name=title,
        bbox_color=palette,
        text_color=(200, 200, 200),
        mask_color=palette,
        out_file=out_file)
def print_report(self, unit='auto', file=sys.stdout):
        
        
        entries = [[
            'FunctionName', 'UsedBytes', 'AcquiredBytes', 'Occurrence']]
        if unit == 'auto':
            max_used = max(
                record['used_bytes'] for record in self.summary().values())
            max_acquired = max(
                record['acquired_bytes'] for record in self.summary().values())
            denomi_used, unit_used = self._choose_unit(max_used)
            denomi_acquired, unit_acquired = self._choose_unit(max_acquired)
        elif unit != 'auto_foreach':
            denomi_used = denomi_acquired = self._table[unit]
            unit_used = unit_acquired = unit
        for function_name, record in self.summary().items():
            used_bytes = record['used_bytes']
            acquired_bytes = record['acquired_bytes']
            if unit == 'auto_foreach':
                denomi_used, unit_used = self._choose_unit(used_bytes)
                denomi_acquired, unit_acquired = self._choose_unit(acquired_bytes)
            used_bytes = '%3.2f%s' % (used_bytes / denomi_used, unit_used)
            acquired_bytes = '%3.2f%s' % (
                acquired_bytes / denomi_acquired, unit_acquired)
            occurrence = str(record['occurrence'])
            entries.append(
                [function_name, used_bytes, acquired_bytes, occurrence])
        entry_widths = []
        entry_widths.append(max(len(f) for f, _, _, _ in entries))
        entry_widths.append(max(len(u) for _, u, _, _ in entries))
        entry_widths.append(max(len(a) for _, _, a, _ in entries))
        entry_widths.append(max(len(o) for _, _, _, o in entries))
        template = '  '.join('{:>%d}' % w for w in entry_widths)
        for function_name, used_bytes, acquired_bytes, occurrence in entries:
            line = template.format(
                function_name, used_bytes, acquired_bytes, occurrence)
            file.write(line)
            file.write('\n')
        if hasattr(file, 'flush'):
            file.flush()
def _choose_unit(self, size):
        
        
        denomi = 1.0
        if size <= 0:
            return denomi, self._units[0]
        for unit in self._units[:-1]:
            if size / (denomi * 1024) < 1:
                return denomi, unit
            denomi *= 1024
        return denomi, self._units[-1]
def config_file_paths(self):
        
        
        paths = []

        if self.config_dir:
            paths.append(self.config_dir)

        paths.append(os.path.join(user_config_dir(), self.config_filename))
        paths.append(os.path.join(system_config_dir(), self.config_filename))

        return paths
def user_cache_dir():
    
    
    if WINDOWS:
        path = os.path.join(os.environ.get('LOCALAPPDATA') or os.environ.get('APPDATA'),
                            'glances', 'cache')
    elif MACOS:
        path = os.path.expanduser('~/Library/Caches/glances')
    else:
        path = os.path.join(os.environ.get('XDG_CACHE_HOME') or os.path.expanduser('~/.cache'),
                            'glances')

    return path
def ldelete(document: Document, layers, prob: Optional[float]) -> Document:
    
    

    lids = set(multiple_to_layer_ids(layers, document))

    for lid in lids:
        if prob is not None:
            lc = LineCollection()
            for line in document[lid]:
                if not random.random() < prob:
                    lc.append(line)

            if len(lc) == 0:
                document.pop(lid)
            else:
                document[lid] = lc
        else:
            document.pop(lid)

    return document
def lmove(document, sources, dest, prob: Optional[float], no_prop: bool):
    
    

    src_lids = vp.multiple_to_layer_ids(sources, document)
    dest_lid = vp.single_to_layer_id(dest, document)

    if dest_lid in src_lids:
        src_lids.remove(dest_lid)

    move_metadata = len(src_lids) == 1 and prob is None and not no_prop
    source_metadata = document.layers[src_lids[0]].metadata if move_metadata else {}

    for lid in src_lids:
        if prob is not None:
            # split lines with provided probability
            remaining_lines = vp.LineCollection()
            moving_lines = vp.LineCollection()
            for line in document.layers[lid]:
                if random.random() < prob:
                    moving_lines.append(line)
                else:
                    remaining_lines.append(line)

            if len(remaining_lines) > 0:
                document.replace(remaining_lines, lid)
            else:
                document.pop(lid)

            if len(moving_lines) > 0:
                document.add(moving_lines, dest_lid)
        else:
            document.add(document.pop(lid), dest_lid)
            if move_metadata:
                document.layers[dest_lid].metadata.update(source_metadata)

    return document
def _verify_ack_challenge(data, state, challenge):
    
    if not data.config.should_2fa(state):
        return
    if not challenge or not challenge.get("ack"):
        raise ChallengeNeeded(CHALLENGE_ACK_NEEDED)
def xyxy2xywh(x):
    
    
    
    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)
    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center
    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center
    y[..., 2] = x[..., 2] - x[..., 0]  # width
    y[..., 3] = x[..., 3] - x[..., 1]  # height
    return y
def clip_boxes(boxes, shape):
    
    
    
    if isinstance(boxes, torch.Tensor):  # faster individually (WARNING: inplace .clamp_() Apple MPS bug)
        boxes[..., 0] = boxes[..., 0].clamp(0, shape[1])  # x1
        boxes[..., 1] = boxes[..., 1].clamp(0, shape[0])  # y1
        boxes[..., 2] = boxes[..., 2].clamp(0, shape[1])  # x2
        boxes[..., 3] = boxes[..., 3].clamp(0, shape[0])  # y2
    else:  # np.array (faster grouped)
        boxes[..., [0, 2]] = boxes[..., [0, 2]].clip(0, shape[1])  # x1, x2
        boxes[..., [1, 3]] = boxes[..., [1, 3]].clip(0, shape[0])  # y1, y2
    return boxes
def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None, padding=True, xywh=False):
    
    
    
    if ratio_pad is None:  # calculate from img0_shape
        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new
        pad = round((img1_shape[1] - img0_shape[1] * gain) / 2 - 0.1), round(
            (img1_shape[0] - img0_shape[0] * gain) / 2 - 0.1)  # wh padding
    else:
        gain = ratio_pad[0][0]
        pad = ratio_pad[1]

    if padding:
        boxes[..., 0] -= pad[0]  # x padding
        boxes[..., 1] -= pad[1]  # y padding
        if not xywh:
            boxes[..., 2] -= pad[0]  # x padding
            boxes[..., 3] -= pad[1]  # y padding
    boxes[..., :4] /= gain
    return clip_boxes(boxes, img0_shape)
def __init__(self, t=0.0, device: torch.device = None):
        
        
        
        self.t = t
        self.device = device
        self.cuda = True if (device and str(device)[:4] == "cuda") else False
def get_one(self, resource_type, requester_user):
        
            
        
        rbac_utils.assert_user_is_admin(user_db=requester_user)

        all_permission_types = get_resource_permission_types_with_descriptions()
        permission_types = all_permission_types.get(resource_type, None)

        if permission_types is None:
            raise exc.HTTPNotFound('Invalid resource type: %s' % (resource_type))

        return permission_types
def checkerboard():
    

    
    return load("chessboard_GRAY_U8.png")
def ihc():
    

    
    return load("ihc.jpg")
def lena():
    

    
    raise RuntimeError("This image has been removed due to copyright concerns.")
def stereo_motorcycle():
    

    
    return (load("motorcycle_left.png"),
            load("motorcycle_right.png"),
            np.load(_os.path.join(data_dir, "motorcycle_disp.npz"))["arr_0"])
def brick():
    

    
    return load("brick.png")
def brick():
    

    
    return load("brick.png", as_gray=True)
def _fetch(data_filename):
    
    
    resolved_path = osp.join(data_dir, '..', data_filename)
    expected_hash = registry[data_filename]

    # Case 1:
    # The file may already be in the data_dir.
    # We may have decided to ship it in the scikit-image distribution.
    if _has_hash(resolved_path, expected_hash):
        # Nothing to be done, file is where it is expected to be
        return resolved_path

    # Case 2:
    # The user is using a cloned version of the github repo, which
    # contains both the publicly shipped data, and test data.
    # In this case, the file would be located relative to the
    # skimage_distribution_dir
    gh_repository_path = osp.join(skimage_distribution_dir, data_filename)
    if _has_hash(gh_repository_path, expected_hash):
        parent = osp.dirname(resolved_path)
        os.makedirs(parent, exist_ok=True)
        shutil.copy2(gh_repository_path, resolved_path)
        return resolved_path

    # Case 3:
    # Pooch not found.
    if image_fetcher is None:
        raise ModuleNotFoundError(
            "The requested file is part of the scikit-image distribution, "
            "but requires the installation of an optional dependency, pooch. "
            "To install pooch, use your preferred python package manager. "
            "Follow installation instruction found at "
            "https://scikit-image.org/docs/stable/install.html"
        )

    # Case 4:
    # Pooch needs to download the data. Let the image fetcher to search for
    # our data. A ConnectionError is raised if no internet connection is
    # available.
    try:
        resolved_path = image_fetcher.fetch(data_filename)
    except ConnectionError as err:
        # If we decide in the future to suppress the underlying 'requests'
        # error, change this to `raise ... from None`. See PEP 3134.
        raise ConnectionError(
            'Tried to download a scikit-image dataset, but no internet '
            'connection is available. To avoid this message in the '
            'future, try `skimage.data.download_all()` when you are '
            'connected to the internet.'
        ) from err
    return resolved_path
def _request_handler(self, **kwargs):
        
        
        
        route = self._get_current_route(request)

        try:
            event = self._construct_event(request, self.port, route.binary_types)
        except UnicodeDecodeError:
            return ServiceErrorResponses.lambda_failure_response()

        stdout_stream = io.BytesIO()
        stdout_stream_writer = StreamWriter(stdout_stream, self.is_debugging)

        try:
            self.lambda_runner.invoke(route.function_name, event, stdout=stdout_stream_writer, stderr=self.stderr)
        except FunctionNotFound:
            return ServiceErrorResponses.lambda_not_found_response()

        lambda_response, lambda_logs, _ = LambdaOutputParser.get_lambda_output(stdout_stream)

        if self.stderr and lambda_logs:
            # Write the logs to stderr if available.
            self.stderr.write(lambda_logs)

        try:
            (status_code, headers, body) = self._parse_lambda_output(lambda_response,
                                                                     route.binary_types,
                                                                     request)
        except (KeyError, TypeError, ValueError):
            LOG.error("Function returned an invalid response (must include one of: body, headers or "
                      "statusCode in the response object). Response received: %s", lambda_response)
            return ServiceErrorResponses.lambda_failure_response()

        return self.service_response(body, headers, status_code)
def _should_base64_decode_body(binary_types, flask_request, lamba_response_headers, is_base_64_encoded):
        
        

        
        best_match_mimetype = flask_request.accept_mimetypes.best_match(lamba_response_headers.get_all("Content-Type"))
        is_best_match_in_binary_types = best_match_mimetype in binary_types or '*/*' in binary_types

        return best_match_mimetype and is_best_match_in_binary_types and is_base_64_encoded
def __init__(
        self,
        api: Api,
        lambda_runner: LocalLambdaRunner,
        static_dir: Optional[str] = None,
        port: Optional[int] = None,
        host: Optional[str] = None,
        stderr: Optional[StreamWriter] = None,
        ssl_context: Optional[Tuple[str, str]] = None,
    ):
        
        
        
        super().__init__(lambda_runner.is_debugging(), port=port, host=host, ssl_context=ssl_context)
        self.api = api
        self.lambda_runner = lambda_runner
        self.static_dir = static_dir
        self._dict_of_routes: Dict[str, Route] = {}
        self.stderr = stderr

        self._click_session_id = None

        try:
            # save the session ID for telemetry event sending
            from samcli.cli.context import Context

            ctx = Context.get_current_context()

            if ctx:
                self._click_session_id = ctx.session_id
        except RuntimeError:
            LOG.debug("Not able to get click context in APIGW service")
def _construct_v_1_0_event(flask_request, port, binary_types, stage_name=None, stage_variables=None):
        
        
        
        # pylint: disable-msg=too-many-locals

        identity = ContextIdentity(source_ip=flask_request.remote_addr)

        endpoint = PathConverter.convert_path_to_api_gateway(flask_request.endpoint)
        method = flask_request.method
        protocol = flask_request.environ.get("SERVER_PROTOCOL", "HTTP/1.1")
        host = flask_request.host

        request_data = flask_request.get_data()

        request_mimetype = flask_request.mimetype

        is_base_64 = LocalApigwService._should_base64_encode(binary_types, request_mimetype)

        if is_base_64:
            LOG.debug("Incoming Request seems to be binary. Base64 encoding the request data before sending to Lambda.")
            request_data = base64.b64encode(request_data)

        if request_data:
            # Flask does not parse/decode the request data. We should do it ourselves
            # Note(xinhol): here we change request_data's type from bytes to str and confused mypy
            # We might want to consider to use a new variable here.
            request_data = request_data.decode("utf-8")

        query_string_dict, multi_value_query_string_dict = LocalApigwService._query_string_params(flask_request)

        context = RequestContext(
            resource_path=endpoint,
            http_method=method,
            stage=stage_name,
            identity=identity,
            path=endpoint,
            protocol=protocol,
            domain_name=host,
        )

        headers_dict, multi_value_headers_dict = LocalApigwService._event_headers(flask_request, port)

        event = ApiGatewayLambdaEvent(
            http_method=method,
            body=request_data,
            resource=endpoint,
            request_context=context,
            query_string_params=query_string_dict,
            multi_value_query_string_params=multi_value_query_string_dict,
            headers=headers_dict,
            multi_value_headers=multi_value_headers_dict,
            path_parameters=flask_request.view_args,
            path=flask_request.path,
            is_base_64_encoded=is_base_64,
            stage_variables=stage_variables,
        )

        event_str = json.dumps(event.to_dict(), sort_keys=True)
        LOG.debug("Constructed String representation of Event to invoke Lambda. Event: %s", event_str)
        return event_str
def _valid_identity_sources(self, request: Request, route: Route) -> bool:
        
        
        
        lambda_auth = route.authorizer_object

        if not isinstance(lambda_auth, LambdaAuthorizer):
            return False

        identity_sources = lambda_auth.identity_sources

        context = (
            self._build_v1_context(route)
            if lambda_auth.payload_version == LambdaAuthorizer.PAYLOAD_V1
            else self._build_v2_context(route)
        )

        kwargs = {
            "headers": request.headers,
            "querystring": request.query_string.decode("utf-8"),
            "context": context,
            "stageVariables": self.api.stage_variables,
            "validation_expression": lambda_auth.validation_string,
        }

        for validator in identity_sources:
            if not validator.is_valid(**kwargs):
                return False

        return True
def _invoke_lambda_function(self, lambda_function_name: str, event: dict) -> Union[str, bytes]:
        
        
        
        with StringIO() as stdout:
            event_str = json.dumps(event, sort_keys=True)
            stdout_writer = StreamWriter(stdout, auto_flush=True)

            self.lambda_runner.invoke(lambda_function_name, event_str, stdout=stdout_writer, stderr=self.stderr)
            lambda_response, is_lambda_user_error_response = LambdaOutputParser.get_lambda_output(stdout)
            if is_lambda_user_error_response:
                raise LambdaResponseParseException

        return lambda_response
def test_select_related_preserved(self):
        
        
        
        m = ChildAdmin(Child, admin.site)
        request = self.factory.get('/child/')
        cl = ChangeList(request, Child, m.list_display, m.list_display_links,
                m.list_filter, m.date_hierarchy, m.search_fields,
                m.list_select_related, m.list_per_page, m.list_max_show_all, m.list_editable, m)
        self.assertEqual(cl.queryset.query.select_related, {'parent': {'name': {}}})
def sensor_callback(self, var, value):
        
        _LOGGER.debug("Received update for %s: %s", var, value)
        dispatcher_send(
            self.hass, SIGNAL_COMFOCONNECT_UPDATE_RECEIVED.format(var), value
        )
def get_params(self, search_query, engine_category):
        
        
        # if paging is not supported, skip
        if search_query.pageno > 1 and not self.engine.paging:
            return None

        # if time_range is not supported, skip
        if search_query.time_range and not self.engine.time_range_support:
            return None

        params = {}
        params['category'] = engine_category
        params['pageno'] = search_query.pageno
        params['safesearch'] = search_query.safesearch
        params['time_range'] = search_query.time_range
        params['engine_data'] = search_query.engine_data.get(self.engine_name, {})
        params['searxng_locale'] = search_query.lang

        # deprecated / vintage --> use params['searxng_locale']
        #
        # Conditions related to engine's traits are implemented in engine.traits
        # module. Don't do 'locale' decissions here in the abstract layer of the
        # search processor, just pass the value from user's choice unchanged to
        # the engine request.

        if hasattr(self.engine, 'language') and self.engine.language:
            params['language'] = self.engine.language
        else:
            params['language'] = search_query.lang

        return params
def fetch_my_trades(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' fetchMyTrades() requires a symbol argument')
        self.load_markets()
        market = self.market(symbol)
        request = {
            'currency': market['id'],
        }
        response = self.privatePostOrderCompleteOrders(self.extend(request, params))
        #
        # despite the name of the endpoint it returns trades which may have a duplicate orderId
        # https://github.com/ccxt/ccxt/pull/7067
        #
        #     {
        #         "result": "success",
        #         "errorCode": "0",
        #         "completeOrders": [
        #             {
        #                 "timestamp": "1416561032",
        #                 "price": "419000.0",
        #                 "type": "bid",
        #                 "qty": "0.001",
        #                 "feeRate": "-0.0015",
        #                 "fee": "-0.0000015",
        #                 "orderId": "E84A1AC2-8088-4FA0-B093-A3BCDB9B3C85"
        #             }
        #         ]
        #     }
        #
        completeOrders = self.safe_value(response, 'completeOrders', [])
        return self.parse_trades(completeOrders, market, since, limit)
def fetch_balance(self, params={}) -> Balances:
        
        
        
        self.load_markets()
        response = self.v2PrivatePostAccountBalance(params)
        return self.parse_balance(response)
def dismiss(self, *_args, **kwargs):
         

        
        if not self._is_open:
            return
        self.dispatch('on_pre_dismiss')
        if self.dispatch('on_dismiss') is True:
            if kwargs.get('force', False) is not True:
                return
        if kwargs.get('animation', True):
            Animation(_anim_alpha=0., d=self._anim_duration).start(self)
        else:
            self._anim_alpha = 0
            self._real_remove_widget()
def top_viewed_pages(cls, project, since=None, limit=10):
        
        
        
        if since is None:
            since = timezone.now().date() - timezone.timedelta(days=30)

        queryset = (
            cls.objects
            .filter(project=project, date__gte=since)
            .values_list('path')
            .annotate(total_views=Sum('view_count'))
            .values_list('path', 'total_views')
            .order_by('-total_views')[:limit]
        )

        pages = []
        view_counts = []

        for data in queryset.iterator():
            pages.append(data[0])
            view_counts.append(data[1])

        final_data = {
            'pages': pages,
            'view_counts': view_counts,
        }

        return final_data
def IsPythonVersionCorrect( path ):
  
  from ycmd import utils

  if not EndsWithPython( path ):
    return False

  command = [ path,
              '-c',
              "import sys;"
              "major, minor = sys.version_info[ :2 ];"
              "good_python = ( major == 2 and minor >= 6 ) "
              "or ( major == 3 and minor >= 3 ) or major > 3;"
              # If this looks weird, remember that:
              #   int( True ) == 1
              #   int( False ) == 0
              "sys.exit( not good_python )" ]

  return utils.SafePopen( command ).wait() == 0
def test(
        cls, cfg: CfgNode, model: nn.Module, evaluators: Optional[List[DatasetEvaluator]] = None
    ):
        
        
        
        logger = logging.getLogger(__name__)
        if isinstance(evaluators, DatasetEvaluator):
            evaluators = [evaluators]
        if evaluators is not None:
            assert len(cfg.DATASETS.TEST) == len(evaluators), "{} != {}".format(
                len(cfg.DATASETS.TEST), len(evaluators)
            )

        results = OrderedDict()
        for idx, dataset_name in enumerate(cfg.DATASETS.TEST):
            data_loader = cls.build_test_loader(cfg, dataset_name)
            # When evaluators are passed in as arguments,
            # implicitly assume that evaluators can be created before data_loader.
            if evaluators is not None:
                evaluator = evaluators[idx]
            else:
                try:
                    embedder = cls.extract_embedder_from_model(model)
                    evaluator = cls.build_evaluator(cfg, dataset_name, embedder=embedder)
                except NotImplementedError:
                    logger.warn(
                        "No evaluator found. Use `DefaultTrainer.test(evaluators=)`, "
                        "or implement its `build_evaluator` method."
                    )
                    results[dataset_name] = {}
                    continue
            results_i = inference_on_dataset(model, data_loader, evaluator)
            results[dataset_name] = results_i
            if comm.is_main_process():
                assert isinstance(
                    results_i, dict
                ), "Evaluator must return a dict on the main process. Got {} instead.".format(
                    results_i
                )
                logger.info("Evaluation results for {} in csv format:".format(dataset_name))
                print_csv_format(results_i)

        if len(results) == 1:
            results = list(results.values())[0]
        return results
def test(
        cls,
        cfg: CfgNode,
        model: nn.Module,
        evaluators: Optional[Union[DatasetEvaluator, List[DatasetEvaluator]]] = None,
    ):
        
        
        
        logger = logging.getLogger(__name__)
        if isinstance(evaluators, DatasetEvaluator):
            evaluators = [evaluators]
        if evaluators is not None:
            assert len(cfg.DATASETS.TEST) == len(evaluators), "{} != {}".format(
                len(cfg.DATASETS.TEST), len(evaluators)
            )

        results = OrderedDict()
        for idx, dataset_name in enumerate(cfg.DATASETS.TEST):
            data_loader = cls.build_test_loader(cfg, dataset_name)
            # When evaluators are passed in as arguments,
            # implicitly assume that evaluators can be created before data_loader.
            if evaluators is not None:
                evaluator = evaluators[idx]
            else:
                try:
                    embedder = cls.extract_embedder_from_model(model)
                    evaluator = cls.build_evaluator(cfg, dataset_name, embedder=embedder)
                except NotImplementedError:
                    logger.warn(
                        "No evaluator found. Use `DefaultTrainer.test(evaluators=)`, "
                        "or implement its `build_evaluator` method."
                    )
                    results[dataset_name] = {}
                    continue
            results_i = inference_on_dataset(model, data_loader, evaluator)
            results[dataset_name] = results_i
            if comm.is_main_process():
                assert isinstance(
                    results_i, dict
                ), "Evaluator must return a dict on the main process. Got {} instead.".format(
                    results_i
                )
                logger.info("Evaluation results for {} in csv format:".format(dataset_name))
                print_csv_format(results_i)

        if len(results) == 1:
            results = list(results.values())[0]
        return results
def _maybe_disable_assert(disable_assert):
  
  if not disable_assert:
    yield
    return

  original_assert = control_flow_assert.Assert
  control_flow_assert.Assert = _dont_assert
  yield
  control_flow_assert.Assert = original_assert
def __init__(self,
               box_code_size,
               num_predictions_per_location,
               conv_hyperparams,
               kernel_size=3,
               use_depthwise=False,
               apply_conv_hyperparams_to_heads=False,
               box_encodings_clip_range=None,
               return_flat_predictions=True,
               name=None):
    
    
    if use_depthwise and (kernel_size == 1):
      raise ValueError('Should not use 1x1 kernel when using depthwise conv')

    super(WeightSharedConvolutionalBoxHead, self).__init__(name=name)
    self._box_code_size = box_code_size
    self._kernel_size = kernel_size
    self._num_predictions_per_location = num_predictions_per_location
    self._use_depthwise = use_depthwise
    self._apply_conv_hyperparams_to_heads = apply_conv_hyperparams_to_heads
    self._box_encodings_clip_range = box_encodings_clip_range
    self._return_flat_predictions = return_flat_predictions

    self._box_encoder_layers = []

    if self._use_depthwise:
      kwargs = conv_hyperparams.params(use_bias=True)
      if self._apply_conv_hyperparams_to_heads:
        kwargs['depthwise_regularizer'] = kwargs['kernel_regularizer']
        kwargs['depthwise_initializer'] = kwargs['kernel_initializer']
        kwargs['pointwise_regularizer'] = kwargs['kernel_regularizer']
        kwargs['pointwise_initializer'] = kwargs['kernel_initializer']
      self._box_encoder_layers.append(
          tf.keras.layers.SeparableConv2D(
              num_predictions_per_location * self._box_code_size,
              [self._kernel_size, self._kernel_size],
              padding='SAME',
              name='BoxPredictor',
              **kwargs))
    else:
      self._box_encoder_layers.append(
          tf.keras.layers.Conv2D(
              num_predictions_per_location * self._box_code_size,
              [self._kernel_size, self._kernel_size],
              padding='SAME',
              name='BoxPredictor',
              **conv_hyperparams.params(use_bias=True)))
def solution(n: int = 10) -> int:
    
    
    
    return sum(
        int("".join(map(str, num)))
        for num in permutations(range(n))
        if is_substring_divisible(num)
    )
def _generate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        stream: Optional[bool] = None,
        **kwargs: Any,
    ) -> LLMResult:
        
        
        if stop:
            if self.params:
                self.params.update({"stop_sequences": stop})
            else:
                self.params = {"stop_sequences": stop}
        should_stream = stream if stream is not None else self.streaming
        if should_stream:
            if len(prompts) > 1:
                raise ValueError(
                    f"WatsonxLLM currently only supports single prompt, got {prompts}"
                )
            generation = GenerationChunk(text="")
            stream_iter = self._stream(
                prompts[0], stop=stop, run_manager=run_manager, **kwargs
            )
            for chunk in stream_iter:
                if generation is None:
                    generation = chunk
                else:
                    generation += chunk
            assert generation is not None
            if isinstance(generation.generation_info, dict):
                llm_output = generation.generation_info.pop("llm_output")
                return LLMResult(generations=[[generation]], llm_output=llm_output)
            return LLMResult(generations=[[generation]])
        else:
            response = self.watsonx_model.generate(prompt=prompts, params=self.params)
            return self._create_llm_result(response)
def _get_relevant_documents(
        self,
        query: str,
        *,
        run_manager: CallbackManagerForRetrieverRun,
        where_filter: Optional[Dict[str, object]] = None,
        score: bool = False,
        hybrid_search_kwargs: Optional[Dict[str, object]] = None,
    ) -> List[Document]:
        
        
        query_obj = self.client.query.get(self.index_name, self.attributes)
        if where_filter:
            query_obj = query_obj.with_where(where_filter)

        if score:
            query_obj = query_obj.with_additional(["score", "explainScore"])

        if hybrid_search_kwargs is None:
            hybrid_search_kwargs = {}

        result = (
            query_obj.with_hybrid(query, alpha=self.alpha, **hybrid_search_kwargs)
            .with_limit(self.k)
            .do()
        )
        if "errors" in result:
            raise ValueError(f"Error during query: {result['errors']}")

        docs = []

        for res in result["data"]["Get"][self.index_name]:
            text = res.pop(self.text_key)
            docs.append(Document(page_content=text, metadata=res))
        return docs
def __init__(self, coordinator: AuroraDataUpdateCoordinator, name):
        
        super().__init__(coordinator=coordinator)

        self._name = name
        self.coordinator = coordinator
        self._unique_id = f"{self.coordinator.latitude}_{self.coordinator.longitude}"
def _session_report(self, trainer: "pl.Trainer", stage: str):
        
        

        # Align the frequency of checkpointing and logging
        if not self.is_checkpoint_step:
            return

        # Report latest logged metrics
        metrics = {LIGHTNING_REPORT_STAGE_KEY: stage}
        for k, v in self._monitor_candidates(trainer).items():
            if isinstance(v, torch.Tensor):
                metrics[k] = v.item()

        # Ensures all workers already finish writing their checkpoints
        trainer.strategy.barrier()

        # Create and report the latest checkpoint
        with tempfile.TemporaryDirectory() as tmpdir:
            src_model_path = os.path.expanduser(self.last_model_path)
            dst_model_path = os.path.join(tmpdir, MODEL_KEY)

            # Copy the lightning ckpt into a tmp directory
            # - File ckpt:       last.ckpt   -> checkpoint_00000x/model
            # - Directory ckpt:  last.ckpt/* -> checkpoint_00000x/model/*
            if self.is_report_rank:
                if os.path.isdir(src_model_path):
                    shutil.copytree(src_model_path, dst_model_path)
                elif os.path.isfile(src_model_path):
                    shutil.copy(src_model_path, dst_model_path)

            # Only the report_rank worker creates the actual checkpoints.
            # Other workers create placeholder checkpoints to prevent blocking.
            checkpoint = LightningCheckpoint.from_directory(tmpdir)
            train.report(metrics=metrics, checkpoint=checkpoint)

        self.is_checkpoint_step = False
def is_link_local(address: IPv4Address | IPv6Address) -> bool:
    
    return any(address in network for network in LINK_LOCAL_NETWORKS)
def _allPackageInfo(self) -> Generator[Dict[str, Any]]:
         

        manager = self._application.getPackageManager()

        # Get all the installed packages, add a section_title depending on package_type and user installed
        for package_type, packages in manager.getAllInstalledPackagesInfo().items():
            for package_data in packages:
                bundled_or_installed = "installed" if manager.isUserInstalledPackage(package_data["package_id"]) else "bundled"
                package_data["section_title"] = self.PACKAGE_SECTION_HEADER[bundled_or_installed][package_type]
                yield package_data

        # Get all to be removed package_info's. These packages are still used in the current session so the user might
        # to interact with these in the list
        for package_data in manager.getPackagesToRemove().values():
            yield package_data["package_info"]

        for package_data in manager.getPackagesToInstall().values():
            package_info = package_data["package_info"]
            package_type = package_info["package_type"]
            package_info["section_title"] = self.PACKAGE_SECTION_HEADER["installed"][package_type]
            yield package_info
def _allPackageInfo(self) -> Generator[PackageModel, None, None]:
         

        # Get all the installed packages, add a section_title depending on package_type and user installed
        for packages in self._manager.getAllInstalledPackagesInfo().values():
            for package_info in packages:
                yield self._makePackageModel(package_info)

        # Get all to be removed package_info's. These packages are still used in the current session so the user might
        # still want to interact with these.
        for package_data in self._manager.getPackagesToRemove().values():
            yield self._makePackageModel(package_data["package_info"])

        # Get all to be installed package_info's. Since the user might want to interact with these
        for package_data in self._manager.getPackagesToInstall().values():
            yield self._makePackageModel(package_data["package_info"])
def ellipsoid_stats(a, b, c):
    
    

    
    if (a <= 0) or (b <= 0) or (c <= 0):
        raise ValueError('Parameters a, b, and c must all be > 0')

    # Calculate volume & surface area
    # Surface calculation requires a >= b >= c and a != c.
    abc = [a, b, c]
    abc.sort(reverse=True)
    a = abc[0]
    b = abc[1]
    c = abc[2]

    # Volume
    vol = 4 / 3. * np.pi * a * b * c

    # Analytical ellipsoid surface area
    phi = np.arcsin((1. - (c ** 2 / (a ** 2.))) ** 0.5)
    d = float((a ** 2 - c ** 2) ** 0.5)
    m = (a ** 2 * (b ** 2 - c ** 2) /
         float(b ** 2 * (a ** 2 - c ** 2)))
    F = ellip_F(phi, m)
    E = ellip_E(phi, m)

    surf = 2 * np.pi * (c ** 2 +
                        b * c ** 2 / d * F +
                        b * d * E)

    return vol, surf
def test_HTTPError_interface_call(self):
        
        
        
        err = urllib.request.HTTPError(msg="something bad happened", url=None,
                                code=None, hdrs='Content-Length:42', fp=None)
        self.assertTrue(hasattr(err, 'reason'))
        assert hasattr(err, 'reason')
        assert hasattr(err, 'info')
        assert callable(err.info)
        try:
            err.info()
        except AttributeError:
            self.fail('err.info call failed.')
        self.assertEqual(err.info(), "Content-Length:42")
def preprocess_for_train(image, height, width, bbox,
                         fast_mode=True,
                         scope=None,
                         add_image_summaries=True):
  
  
  with tf.name_scope(scope, 'distort_image', [image, height, width, bbox]):
    if bbox is None:
      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],
                         dtype=tf.float32,
                         shape=[1, 1, 4])
    if image.dtype != tf.float32:
      image = tf.image.convert_image_dtype(image, dtype=tf.float32)
    # Each bounding box has shape [1, num_boxes, box coords] and
    # the coordinates are ordered [ymin, xmin, ymax, xmax].
    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),
                                                  bbox)
    if add_image_summaries:
      tf.summary.image('image_with_bounding_boxes', image_with_box)

    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)
    # Restore the shape since the dynamic slice based upon the bbox_size loses
    # the third dimension.
    distorted_image.set_shape([None, None, 3])
    image_with_distorted_box = tf.image.draw_bounding_boxes(
        tf.expand_dims(image, 0), distorted_bbox)
    if add_image_summaries:
      tf.summary.image('images_with_distorted_bounding_box',
                       image_with_distorted_box)

    # This resizing operation may distort the images because the aspect
    # ratio is not respected. We select a resize method in a round robin
    # fashion based on the thread number.
    # Note that ResizeMethod contains 4 enumerated resizing methods.

    # We select only 1 case for fast_mode bilinear.
    num_resize_cases = 1 if fast_mode else 4
    distorted_image = apply_with_random_selector(
        distorted_image,
        lambda x, method: tf.image.resize_images(x, [height, width], method),
        num_cases=num_resize_cases)

    if add_image_summaries:
      tf.summary.image('cropped_resized_image',
                       tf.expand_dims(distorted_image, 0))

    # Randomly flip the image horizontally.
    distorted_image = tf.image.random_flip_left_right(distorted_image)

    # Randomly distort the colors. There are 4 ways to do it.
    distorted_image = apply_with_random_selector(
        distorted_image,
        lambda x, ordering: distort_color(x, ordering, fast_mode),
        num_cases=4)

    if add_image_summaries:
      tf.summary.image('final_distorted_image',
                       tf.expand_dims(distorted_image, 0))
    distorted_image = tf.subtract(distorted_image, 0.5)
    distorted_image = tf.multiply(distorted_image, 2.0)
    return distorted_image
def preprocess_for_eval(image,
                        height,
                        width,
                        central_fraction=0.875,
                        scope=None,
                        central_crop=True,
                        use_grayscale=False):
  
  
  with tf.name_scope(scope, 'eval_image', [image, height, width]):
    if image.dtype != tf.float32:
      image = tf.image.convert_image_dtype(image, dtype=tf.float32)
    if use_grayscale:
      image = tf.image.rgb_to_grayscale(image)
    # Crop the central region of the image with an area containing 87.5% of
    # the original image.
    if central_crop and central_fraction:
      image = tf.image.central_crop(image, central_fraction=central_fraction)

    if height and width:
      # Resize the image to the specified height and width.
      image = tf.expand_dims(image, 0)
      image = tf.image.resize_bilinear(image, [height, width],
                                       align_corners=False)
      image = tf.squeeze(image, [0])
    image = tf.subtract(image, 0.5)
    image = tf.multiply(image, 2.0)
    return image
def get_template_artifacts_format(template_file):
    
    
    

    template_dict = get_template_data(template_file=template_file)

    # Get a list of Resources where the artifacts format matter for packaging.
    packageable_resources = get_packageable_resource_paths()

    artifacts = []
    for _, resource in template_dict.get("Resources", {}).items():
        # First check if the resources are part of package-able resource types.
        if resource.get("Type") in packageable_resources.keys():
            # Flatten list of locations per resource type.
            locations = list(itertools.chain(*packageable_resources.get(resource.get("Type"))))
            for location in locations:
                properties = resource.get("Properties", {})
                # Search for package-able location within resource properties.
                if jmespath.search(location, properties):
                    artifacts.append(properties.get("PackageType", ZIP))

    return artifacts
def get_tombstones():
  
  files = []
  if os.path.exists(APPORT_DIR):
    with os.scandir(APPORT_DIR) as d:
      # Loop over first 1000 directory entries
      for _, f in zip(range(1000), d, strict=False):
        if f.name.startswith("tombstone"):
          files.append((f.path, int(f.stat().st_ctime)))
        elif f.name.endswith(".crash") and f.stat().st_mode == 0o100640:
          files.append((f.path, int(f.stat().st_ctime)))
  return files
def outputs(self):
        
        # A copy is returned so the caller can iterate through the outputs
        # without concern about self._outputs being modified from another thread.
        return MappingProxyType(self._outputs.copy())
def endpoint_url(self, fmt):
        
        if fmt not in self._outputs:
            raise ValueError(f"Stream is not configured for format '{fmt}'")
        if not self.access_token:
            self.access_token = secrets.token_hex()
        return self.hass.data[DOMAIN][ATTR_ENDPOINTS][fmt].format(self.access_token)
def create_stream(
    hass: HomeAssistant,
    stream_source: str,
    options: dict[str, Any],
    stream_label: str | None = None,
) -> Stream:
    
    
    if DOMAIN not in hass.config.components:
        raise HomeAssistantError("Stream integration is not set up.")

    # Convert extra stream options into PyAV options
    pyav_options = convert_stream_options(options)
    # For RTSP streams, prefer TCP
    if isinstance(stream_source, str) and stream_source[:7] == "rtsp://":
        pyav_options = {
            "rtsp_flags": "prefer_tcp",
            "stimeout": "5000000",
            **pyav_options,
        }

    stream = Stream(
        hass, stream_source, options=pyav_options, stream_label=stream_label
    )
    hass.data[DOMAIN][ATTR_STREAMS].append(stream)
    return stream
def configure_optimizers(self):
        
        
        SUPPORTED_OPTIMIZERS = {
            "adam": Adam,
            "adamp": AdamP,
            "radam": RAdam,
            "adagrad": Adagrad,
            "adadelta": Adadelta,
            "adamax": Adamax,
            "adamw": AdamW,
            "sgd": SGD,
            "asgd": ASGD,
            "novograd": Novograd,
        }

        assert self.configs.model.optimizer in SUPPORTED_OPTIMIZERS.keys(), \
            f"Unsupported Optimizer: {self.configs.model.optimizer}\n" \
            f"Supported Optimizers: {SUPPORTED_OPTIMIZERS.keys()}"

        self.optimizer = SUPPORTED_OPTIMIZERS[self.configs.model.optimizer](
            self.parameters(),
            lr=self.configs.lr_scheduler.lr,
        )
        scheduler = SCHEDULER_REGISTRY[self.configs.lr_scheduler.scheduler_name](self.optimizer, self.configs)

        if self.configs.lr_scheduler.scheduler_name in ("reduce_lr_on_plateau", "warmup_reduce_lr_on_plateau"):
            lr_scheduler = {
                'scheduler': scheduler,
                'monitor': 'val_loss',
            }
        else:
            lr_scheduler = {
                'scheduler': scheduler
            }

        return {
            'optimizer': self.optimizer,
            'lr_scheduler': lr_scheduler
        }
def hybrid_a_star_planning(start, goal, ox, oy, xy_resolution, yaw_resolution):
    
    
    

    start[2], goal[2] = rs.pi_2_pi(start[2]), rs.pi_2_pi(goal[2])
    tox, toy = ox[:], oy[:]

    obstacle_kd_tree = KDTree(np.vstack((tox, toy)).T)

    config = Config(tox, toy, xy_resolution, yaw_resolution)

    start_node = Node(round(start[0] / xy_resolution),
                      round(start[1] / xy_resolution),
                      round(start[2] / yaw_resolution), True,
                      [start[0]], [start[1]], [start[2]], [True], cost=0)
    goal_node = Node(round(goal[0] / xy_resolution),
                     round(goal[1] / xy_resolution),
                     round(goal[2] / yaw_resolution), True,
                     [goal[0]], [goal[1]], [goal[2]], [True])

    openList, closedList = {}, {}

    _, _, h_dp = dp_planning(start_node.x_list[-1], start_node.y_list[-1],
                             goal_node.x_list[-1], goal_node.y_list[-1],
                             ox, oy, xy_resolution, VR)

    pq = []
    openList[calc_index(start_node, config)] = start_node
    heapq.heappush(pq, (calc_cost(start_node, h_dp, config),
                        calc_index(start_node, config)))
    final_path = None

    while True:
        if not openList:
            print("Error: Cannot find path, No open set")
            return [], [], []

        cost, c_id = heapq.heappop(pq)
        if c_id in openList:
            current = openList.pop(c_id)
            closedList[c_id] = current
        else:
            continue

        if show_animation:  # pragma: no cover
            plt.plot(current.x_list[-1], current.y_list[-1], "xc")
            # for stopping simulation with the esc key.
            plt.gcf().canvas.mpl_connect(
                'key_release_event',
                lambda event: [exit(0) if event.key == 'escape' else None])
            if len(closedList.keys()) % 10 == 0:
                plt.pause(0.001)

        is_updated, final_path = update_node_with_analytic_expansion(
            current, goal_node, config, ox, oy, obstacle_kd_tree)

        if is_updated:
            print("path found")
            break

        for neighbor in get_neighbors(current, config, ox, oy,
                                      obstacle_kd_tree):
            neighbor_index = calc_index(neighbor, config)
            if neighbor_index in closedList:
                continue
            if neighbor not in openList \
                    or openList[neighbor_index].cost > neighbor.cost:
                heapq.heappush(
                    pq, (calc_cost(neighbor, h_dp, config),
                         neighbor_index))
                openList[neighbor_index] = neighbor

    path = get_final_path(closedList, final_path)
    return path
def label_anchors(
      self,
      anchor_boxes: Dict[str, tf.Tensor],
      gt_boxes: tf.Tensor,
      gt_labels: tf.Tensor,
      gt_attributes: Optional[Dict[str, tf.Tensor]] = None,
      gt_weights: Optional[tf.Tensor] = None
  ) -> Tuple[Dict[str, tf.Tensor], Dict[str, tf.Tensor], Dict[str, Dict[
      str, tf.Tensor]], tf.Tensor, tf.Tensor]:
    
    
    flattened_anchor_boxes = []
    for anchors in anchor_boxes.values():
      flattened_anchor_boxes.append(tf.reshape(anchors, [-1, 4]))
    flattened_anchor_boxes = tf.concat(flattened_anchor_boxes, axis=0)
    similarity_matrix = self.similarity_calc(flattened_anchor_boxes, gt_boxes)
    match_indices, match_indicators = self.matcher(similarity_matrix)

    mask = tf.less_equal(match_indicators, 0)
    cls_mask = tf.expand_dims(mask, -1)
    cls_targets = self.target_gather(gt_labels, match_indices, cls_mask, -1)
    box_mask = tf.tile(cls_mask, [1, 4])
    box_targets = self.target_gather(gt_boxes, match_indices, box_mask)
    att_targets = {}
    if gt_attributes:
      for k, v in gt_attributes.items():
        att_size = v.get_shape().as_list()[-1]
        att_mask = tf.tile(cls_mask, [1, att_size])
        att_targets[k] = self.target_gather(v, match_indices, att_mask, 0.0)

    weights = tf.squeeze(tf.ones_like(gt_labels, dtype=tf.float32), -1)
    if gt_weights is not None:
      weights = tf.math.multiply(weights, gt_weights)
    box_weights = self.target_gather(weights, match_indices, mask)
    ignore_mask = tf.equal(match_indicators, -2)
    cls_weights = self.target_gather(weights, match_indices, ignore_mask)
    box_targets_list = box_list.BoxList(box_targets)
    anchor_box_list = box_list.BoxList(flattened_anchor_boxes)
    box_targets = self.box_coder.encode(box_targets_list, anchor_box_list)

    # Unpacks labels into multi-level representations.
    cls_targets_dict = unpack_targets(cls_targets, anchor_boxes)
    box_targets_dict = unpack_targets(box_targets, anchor_boxes)
    attribute_targets_dict = {}
    for k, v in att_targets.items():
      attribute_targets_dict[k] = unpack_targets(v, anchor_boxes)

    return cls_targets_dict, box_targets_dict, attribute_targets_dict, cls_weights, box_weights
def __init__(
      self,
      min_level,
      max_level,
      num_scales,
      aspect_ratios,
      anchor_size,
      image_size,
  ):
    
    self.min_level = min_level
    self.max_level = max_level
    self.num_scales = num_scales
    self.aspect_ratios = aspect_ratios
    self.anchor_size = anchor_size
    self.image_size = image_size
    self.multilevel_boxes = self._generate_multilevel_boxes()
def __init__(self, nanoleaf: Nanoleaf) -> None:
        
        super().__init__(nanoleaf)
        self._attr_unique_id = nanoleaf.serial_no
        self._attr_name = nanoleaf.name
        self._attr_min_mireds = math.ceil(1000000 / nanoleaf.color_temperature_max)
        self._attr_max_mireds = kelvin_to_mired(nanoleaf.color_temperature_min)
def release(self):
        
        
        self._value += 1
        self._locked = False
        for waiter in self._waiters:
            if not waiter.done():
                waiter.set_result(True)
                break
def notify(self, n=1):
        
        
        if not self.locked():
            raise RuntimeError('cannot notify on un-acquired lock')
        self._notify(n)
def print_graph_node_qualified_names(model: nn.Module):
    
    
    
    tracer = NodePathTracer()
    tracer.trace(model)
    pprint(list(tracer.node_to_qualname.values()))
def print_graph_node_qualified_names(
        model: nn.Module, tracer_kwargs: Dict = {}):
    
    
    
    train_tracer = NodePathTracer(**tracer_kwargs)
    train_tracer.trace(model.train())
    eval_tracer = NodePathTracer(**tracer_kwargs)
    eval_tracer.trace(model.eval())
    train_nodes = list(train_tracer.node_to_qualname.values())
    eval_nodes = list(eval_tracer.node_to_qualname.values())
    if len(train_nodes) == len(eval_nodes) and [
            t == e for t, e in zip(train_nodes, eval_nodes)]:
        # Nodes are aligned in train vs eval mode
        pprint(list(train_tracer.node_to_qualname.values()))
        return
    print("Nodes from train mode:")
    pprint(list(train_tracer.node_to_qualname.values()))
    print()
    print("Nodes from eval mode:")
    pprint(list(eval_tracer.node_to_qualname.values()))
    print()
    _warn_graph_differences(train_tracer, eval_tracer)
def extract_video_id(url: str) -> str:
    
    
    idregx = re.compile(r'[\w-]{11}$')
    url = str(url).strip()

    if idregx.match(url):
        return url # ID of video

    if '://' not in url:
        url = '//' + url
    parsedurl = urlparse(url)
    if parsedurl.netloc in ('youtube.com', 'www.youtube.com', 'm.youtube.com', 'gaming.youtube.com'):
        query = parse_qs(parsedurl.query)
        if 'v' in query and idregx.match(query['v'][0]):
            return query['v'][0]
    elif parsedurl.netloc in ('youtu.be', 'www.youtu.be'):
        vidid = parsedurl.path.split('/')[-1] if parsedurl.path else ''
        if idregx.match(vidid):
            return vidid

    err = "Need 11 character video id or the URL of the video. Got %s"
    raise ValueError(err % url)
def __init__(
        self,
        policy: Policy,
        gamma: float,
        repeat: int = 1,
        perturb_fn: Callable[[np.ndarray, int], None] = perturb_fn,
    ):
        
        
        super().__init__(policy, gamma)
        self.repeat = repeat
        self.perturb_fn = perturb_fn
def main(
    datasets: str = "wikitext,ptb,c4",
    *,
    # compilation fails as it does not support torch.complex64 for RoPE
    # compile: bool = False,
    accelerator: str = "auto",
    adapter_path: Path = Path("out/adapter/alpaca/lit-llama-adapter-finetuned.pth"),
    checkpoint_path: Path = Path("checkpoints/lit-llama/7B/lit-llama.pth"),
    tokenizer_path: Path = Path("checkpoints/lit-llama/tokenizer.model"),
    dtype: str = "float32",
    quantize: Optional[str] = None,
) -> None:
    
    
    assert adapter_path.is_file()
    assert checkpoint_path.is_file()
    assert tokenizer_path.is_file()

    fabric = L.Fabric(accelerator=accelerator, devices=1)

    dt = getattr(torch, dtype, None)
    if not isinstance(dt, torch.dtype):
        raise ValueError(f"{dtype} is not a valid dtype.")
    dtype = dt

    print("Loading model ...", file=sys.stderr)
    t0 = time.time()
    with lazy_load(checkpoint_path) as pretrained_checkpoint, lazy_load(adapter_path) as adapter_checkpoint:
        name = llama_model_lookup(pretrained_checkpoint)

        with EmptyInitOnDevice(
                device=fabric.device, dtype=dtype, quantization_mode=quantize
        ):
            model = LLaMA.from_name(name)

        # 1. Load the pretrained weights
        model.load_state_dict(pretrained_checkpoint, strict=False)
        # 2. Load the fine-tuned adapter weights
        model.load_state_dict(adapter_checkpoint, strict=False)

    print(f"Time to load model: {time.time() - t0:.02f} seconds.", file=sys.stderr)

    model.eval()

    # if compile:
    #     model = torch.compile(model)

    total_toks = 0
    model = fabric.setup_module(model)

    tokenizer = Tokenizer(tokenizer_path)

    for dsname in datasets.split(","):
        test_string = load_eval_data(dsname)

        sample = {"instruction": test_string, "input": input}
        test_string = generate_prompt(sample)

        encoded_text = tokenizer.encode(
            test_string, bos=True, eos=False, device=fabric.device
        )
        encoded_text = encoded_text[
            None, : 256 * model.config.block_size
        ]  # add batch dimension, trim like gptq implementation
        t0 = time.perf_counter()

        nlls = 0
        toks = 0
        block_size = 2048  # this is for compat with gptq, and indeed we get much worse beyond this (https://github.com/facebookresearch/llama/blob/57b0eb62de0636e75af471e49e2f1862d908d9d8/llama/model.py#L30)
        for i in tqdm.tqdm(range(0, encoded_text.shape[1], block_size)):
            inp = encoded_text[:, i : i + block_size]
            logits = model(inp)[0]
            nll = torch.nn.functional.cross_entropy(
                logits[:-1], inp[0, 1:].to(dtype=torch.long), reduction="sum"
            )
            toks += inp.size(1) - 1
            nlls += nll.item()

            print(encoded_text.shape, logits.shape)
            ppl = math.exp(nlls / toks)
            print(f"Perplexity on {dsname}: {ppl:.2f}")
            total_toks += toks

    t = time.perf_counter() - t0
    print(
        f"\n\nTime for inference: {t:.02f} sec total, {total_toks / t:.02f} tokens/sec",
        file=sys.stderr,
    )
    print(
        f"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB",
        file=sys.stderr,
    )
def applied_migrations(self):
        
        
        
        if self.has_table():
            return {(migration.app, migration.name): migration for migration in self.migration_qs}
        else:
            # If the django_migrations table doesn't exist, then no migrations
            # are applied.
            return {}
def keys(self) -> List[Any]:
        
        
        
        return list(self)
def unique_list(
    input_list: Union[List[T], Tuple[T, ...]],
    *,
    name_factory: Callable[[T], str] = str,
) -> List[T]:
    
    
    
    result: List[T] = []
    result_names: List[str] = []
    for v in input_list:
        v_name = name_factory(v)
        if v_name not in result_names:
            result_names.append(v_name)
            result.append(v)
        else:
            result[result_names.index(v_name)] = v

    return result
def get_dtype(dtype=None, map_mixed16=None):
    

    
    if dtype is None:
        dtype = config.dtype
    if dtype is mixed16 and map_mixed16 is not None:
        dtype = map_mixed16
    return numpy.dtype(dtype)
def hvac_action(self):
        
        
        if self.thermostat['equipmentStatus'] == "":
            return CURRENT_HVAC_IDLE

        actions = [
            ECOBEE_HVAC_ACTION_TO_HASS[status] for status in
            self.thermostat['equipmentStatus'].split(",")
            if ECOBEE_HVAC_ACTION_TO_HASS[status] is not None
        ]

        for action in (CURRENT_HVAC_HEAT, CURRENT_HVAC_COOL,
                       CURRENT_HVAC_DRY, CURRENT_HVAC_FAN):
            if action in actions:
                return action

        return CURRENT_HVAC_IDLE
def is_on(self):
        
        return self._automation.is_enabled
def _parse_due_date(data: DueDate, timezone_offset: int) -> datetime | None:
    
    
    if not (nowtime := dt.parse_datetime(data["date"])):
        return None
    if nowtime.tzinfo is None:
        nowtime = nowtime.replace(tzinfo=timezone(timedelta(hours=timezone_offset)))
    return dt.as_utc(nowtime)
def remove_indexed_files(model, version, build):
    
    
    

    if not DEDConfig.autosync_enabled():
        log.info(
            'Autosync disabled, skipping removal from the search index for: %s:%s',
            version.project.slug,
            version.slug,
        )
        return

    try:
        document = list(registry.get_documents(models=[model]))[0]
        log.info(
            'Deleting old files from search index for: %s:%s',
            version.project.slug,
            version.slug,
        )
        (
            document().search()
            .filter('term', project=version.project.slug)
            .filter('term', version=version.slug)
            .exclude('term', build=build)
            .delete()
        )
    except Exception:
        log.exception('Unable to delete a subset of files. Continuing.')
def _validate_luhn_checksum(number_as_string):
        
        
        

        s = number_as_string
        digits = len(s)
        number = int(s[0]) if digits & 1 else 0

        for i in range(digits - 2, -1, -2):
            number += Provider.luhn_lookup[s[i]] + int(s[i+1])
        return (number % 10) == 0
def _credit_card_type(cls, card_type=None):
          
        if card_type is None:
            card_type = cls.random_element(cls.credit_card_types.keys())
        elif isinstance(card_type, CreditCard):
            return card_type
        return cls.credit_card_types[card_type]
def decrypt(self, content: str, key: int) -> list[str]:
        
        
        

        # precondition
        assert isinstance(key, int)
        assert isinstance(content, str)

        key = key or self.__key or 1

        # make sure key is an appropriate size
        key %= 256

        return [chr(ord(ch) ^ key) for ch in content]
def get_cfn_attribute(self, attribute_name):
        
        return self.props.get(attribute_name)
def experimental(
        self,
        *,
        _enable_new_api_stack: Optional[bool] = NotProvided,
        _tf_policy_handles_more_than_one_loss: Optional[bool] = NotProvided,
        _disable_preprocessor_api: Optional[bool] = NotProvided,
        _disable_action_flattening: Optional[bool] = NotProvided,
        _disable_execution_plan_api: Optional[bool] = NotProvided,
        _disable_initialize_loss_from_dummy_batch: Optional[bool] = NotProvided,
    ) -> "AlgorithmConfig":
        
        
        if _enable_new_api_stack is not NotProvided:
            self._enable_new_api_stack = _enable_new_api_stack

            if _enable_new_api_stack is True and self.exploration_config:
                self.__prior_exploration_config = self.exploration_config
                self.exploration_config = {}

            elif _enable_new_api_stack is False and not self.exploration_config:
                if self.__prior_exploration_config is not None:
                    self.exploration_config = self.__prior_exploration_config
                    self.__prior_exploration_config = None
                else:
                    logger.warning(
                        "config._enable_new_api_stack was set to False, but no prior "
                        "exploration config was found to be restored."
                    )

        if _tf_policy_handles_more_than_one_loss is not NotProvided:
            self._tf_policy_handles_more_than_one_loss = (
                _tf_policy_handles_more_than_one_loss
            )
        if _disable_preprocessor_api is not NotProvided:
            self._disable_preprocessor_api = _disable_preprocessor_api
        if _disable_action_flattening is not NotProvided:
            self._disable_action_flattening = _disable_action_flattening
        if _disable_execution_plan_api is not NotProvided:
            self._disable_execution_plan_api = _disable_execution_plan_api
        if _disable_initialize_loss_from_dummy_batch is not NotProvided:
            self._disable_initialize_loss_from_dummy_batch = (
                _disable_initialize_loss_from_dummy_batch
            )

        return self
def get_learner_hyperparameters(self) -> LearnerHyperparameters:
        
        
        # Compile the per-module learner hyperparameter instances (if applicable).
        per_module_learner_hp_overrides = {}
        if self.algorithm_config_overrides_per_module:
            for (
                module_id,
                overrides,
            ) in self.algorithm_config_overrides_per_module.items():
                # Copy this AlgorithmConfig object (unfreeze copy), update copy from
                # the provided override dict for this module_id, then
                # create a new LearnerHyperparameter object from this altered
                # AlgorithmConfig.
                config_for_module = self.copy(copy_frozen=False).update_from_dict(
                    overrides
                )
                config_for_module.algorithm_config_overrides_per_module = None
                per_module_learner_hp_overrides[
                    module_id
                ] = config_for_module.get_learner_hyperparameters()

        return LearnerHyperparameters(
            learning_rate=self.lr,
            grad_clip=self.grad_clip,
            grad_clip_by=self.grad_clip_by,
            _per_module_overrides=per_module_learner_hp_overrides,
        )
def rollouts(
        self,
        *,
        env_runner_cls: Optional[type] = NotProvided,
        num_rollout_workers: Optional[int] = NotProvided,
        num_envs_per_worker: Optional[int] = NotProvided,
        create_env_on_local_worker: Optional[bool] = NotProvided,
        sample_collector: Optional[Type[SampleCollector]] = NotProvided,
        enable_connectors: Optional[bool] = NotProvided,
        env_to_module_connector: Optional[
            Callable[[EnvType], "ConnectorV2"]
        ] = NotProvided,
        module_to_env_connector: Optional[
            Callable[[EnvType, "RLModule"], "ConnectorV2"]
        ] = NotProvided,
        use_worker_filter_stats: Optional[bool] = NotProvided,
        update_worker_filter_stats: Optional[bool] = NotProvided,
        rollout_fragment_length: Optional[Union[int, str]] = NotProvided,
        batch_mode: Optional[str] = NotProvided,
        remote_worker_envs: Optional[bool] = NotProvided,
        remote_env_batch_wait_ms: Optional[float] = NotProvided,
        validate_workers_after_construction: Optional[bool] = NotProvided,
        preprocessor_pref: Optional[str] = NotProvided,
        observation_filter: Optional[str] = NotProvided,
        compress_observations: Optional[bool] = NotProvided,
        enable_tf1_exec_eagerly: Optional[bool] = NotProvided,
        sampler_perf_stats_ema_coef: Optional[float] = NotProvided,
        ignore_worker_failures=DEPRECATED_VALUE,
        recreate_failed_workers=DEPRECATED_VALUE,
        restart_failed_sub_environments=DEPRECATED_VALUE,
        num_consecutive_worker_failures_tolerance=DEPRECATED_VALUE,
        worker_health_probe_timeout_s=DEPRECATED_VALUE,
        worker_restore_timeout_s=DEPRECATED_VALUE,
        synchronize_filter=DEPRECATED_VALUE,
        sample_async=DEPRECATED_VALUE,
    ) -> "AlgorithmConfig":
        
        
        if env_runner_cls is not NotProvided:
            self.env_runner_cls = env_runner_cls
        if num_rollout_workers is not NotProvided:
            self.num_rollout_workers = num_rollout_workers
        if num_envs_per_worker is not NotProvided:
            self.num_envs_per_worker = num_envs_per_worker
        if sample_collector is not NotProvided:
            self.sample_collector = sample_collector
        if create_env_on_local_worker is not NotProvided:
            self.create_env_on_local_worker = create_env_on_local_worker
        if enable_connectors is not NotProvided:
            self.enable_connectors = enable_connectors
        if env_to_module_connector is not NotProvided:
            self._env_to_module_connector = env_to_module_connector
        if module_to_env_connector is not NotProvided:
            self._module_to_env_connector = module_to_env_connector
        if use_worker_filter_stats is not NotProvided:
            self.use_worker_filter_stats = use_worker_filter_stats
        if update_worker_filter_stats is not NotProvided:
            self.update_worker_filter_stats = update_worker_filter_stats
        if rollout_fragment_length is not NotProvided:
            if not (
                (
                    isinstance(rollout_fragment_length, int)
                    and rollout_fragment_length > 0
                )
                or rollout_fragment_length == "auto"
            ):
                raise ValueError("`rollout_fragment_length` must be int >0 or 'auto'!")
            self.rollout_fragment_length = rollout_fragment_length
        if batch_mode is not NotProvided:
            self.batch_mode = batch_mode
        if remote_worker_envs is not NotProvided:
            self.remote_worker_envs = remote_worker_envs
        if remote_env_batch_wait_ms is not NotProvided:
            self.remote_env_batch_wait_ms = remote_env_batch_wait_ms
        if validate_workers_after_construction is not NotProvided:
            self.validate_workers_after_construction = (
                validate_workers_after_construction
            )
        if preprocessor_pref is not NotProvided:
            self.preprocessor_pref = preprocessor_pref
        if observation_filter is not NotProvided:
            self.observation_filter = observation_filter
        if synchronize_filter is not NotProvided:
            self.synchronize_filters = synchronize_filter
        if compress_observations is not NotProvided:
            self.compress_observations = compress_observations
        if enable_tf1_exec_eagerly is not NotProvided:
            self.enable_tf1_exec_eagerly = enable_tf1_exec_eagerly
        if sampler_perf_stats_ema_coef is not NotProvided:
            self.sampler_perf_stats_ema_coef = sampler_perf_stats_ema_coef

        # Deprecated settings.
        if sample_async is True:
            deprecation_warning(
                old="AlgorithmConfig.rollouts(sample_async=True)",
                help="AsyncSampler is not supported anymore.",
                error=True,
            )
        if synchronize_filter != DEPRECATED_VALUE:
            deprecation_warning(
                old="AlgorithmConfig.rollouts(synchronize_filter=..)",
                new="AlgorithmConfig.rollouts(update_worker_filter_stats=..)",
                error=False,
            )
            self.update_worker_filter_stats = synchronize_filter
        if ignore_worker_failures != DEPRECATED_VALUE:
            deprecation_warning(
                old="ignore_worker_failures is deprecated, and will soon be a no-op",
                error=False,
            )
            self.ignore_worker_failures = ignore_worker_failures
        if recreate_failed_workers != DEPRECATED_VALUE:
            deprecation_warning(
                old="AlgorithmConfig.rollouts(recreate_failed_workers=..)",
                new="AlgorithmConfig.fault_tolerance(recreate_failed_workers=..)",
                error=False,
            )
            self.recreate_failed_workers = recreate_failed_workers
        if restart_failed_sub_environments != DEPRECATED_VALUE:
            deprecation_warning(
                old="AlgorithmConfig.rollouts(restart_failed_sub_environments=..)",
                new=(
                    "AlgorithmConfig.fault_tolerance("
                    "restart_failed_sub_environments=..)"
                ),
                error=False,
            )
            self.restart_failed_sub_environments = restart_failed_sub_environments
        if num_consecutive_worker_failures_tolerance != DEPRECATED_VALUE:
            deprecation_warning(
                old=(
                    "AlgorithmConfig.rollouts("
                    "num_consecutive_worker_failures_tolerance=..)"
                ),
                new=(
                    "AlgorithmConfig.fault_tolerance("
                    "num_consecutive_worker_failures_tolerance=..)"
                ),
                error=False,
            )
            self.num_consecutive_worker_failures_tolerance = (
                num_consecutive_worker_failures_tolerance
            )
        if worker_health_probe_timeout_s != DEPRECATED_VALUE:
            deprecation_warning(
                old="AlgorithmConfig.rollouts(worker_health_probe_timeout_s=..)",
                new="AlgorithmConfig.fault_tolerance(worker_health_probe_timeout_s=..)",
                error=False,
            )
            self.worker_health_probe_timeout_s = worker_health_probe_timeout_s
        if worker_restore_timeout_s != DEPRECATED_VALUE:
            deprecation_warning(
                old="AlgorithmConfig.rollouts(worker_restore_timeout_s=..)",
                new="AlgorithmConfig.fault_tolerance(worker_restore_timeout_s=..)",
                error=False,
            )
            self.worker_restore_timeout_s = worker_restore_timeout_s

        return self
def get_rollout_fragment_length(self, worker_index: int = 0) -> int:
        
        
        if self.rollout_fragment_length == "auto":
            # Example:
            # 2 workers, 2 envs per worker, 2000 train batch size:
            # -> 2000 / 4 -> 500
            # 4 workers, 3 envs per worker, 2500 train batch size:
            # -> 2500 / 12 -> 208.333 -> diff=4 (208 * 12 = 2496)
            # -> worker 1: 209, workers 2-4: 208
            rollout_fragment_length = self.total_train_batch_size / (
                self.num_envs_per_worker * (self.num_rollout_workers or 1)
            )
            if int(rollout_fragment_length) != rollout_fragment_length:
                diff = self.total_train_batch_size - int(
                    rollout_fragment_length
                ) * self.num_envs_per_worker * (self.num_rollout_workers or 1)
                if (worker_index * self.num_envs_per_worker) <= diff:
                    return int(rollout_fragment_length) + 1
            return int(rollout_fragment_length)
        else:
            return self.rollout_fragment_length
def debugging(
        self,
        *,
        logger_creator: Optional[Callable[[], Logger]] = NotProvided,
        logger_config: Optional[dict] = NotProvided,
        log_level: Optional[str] = NotProvided,
        log_sys_usage: Optional[bool] = NotProvided,
        fake_sampler: Optional[bool] = NotProvided,
        seed: Optional[int] = NotProvided,
        # deprecated
        worker_cls=None,
    ) -> "AlgorithmConfig":
        
        
        if worker_cls is not None:
            deprecation_warning(
                old="AlgorithmConfig.debugging(worker_cls=..)",
                new="AlgorithmConfig.rollouts(env_runner_cls=...)",
                error=True,
            )

        if logger_creator is not NotProvided:
            self.logger_creator = logger_creator
        if logger_config is not NotProvided:
            self.logger_config = logger_config
        if log_level is not NotProvided:
            self.log_level = log_level
        if log_sys_usage is not NotProvided:
            self.log_sys_usage = log_sys_usage
        if fake_sampler is not NotProvided:
            self.fake_sampler = fake_sampler
        if seed is not NotProvided:
            self.seed = seed

        return self
def translate_to_cfn(
    tf_json: dict, output_directory_path: str, terraform_application_dir: str, project_root_dir: str
) -> dict:
    
    
    
    # setup root_module and cfn dict
    root_module = tf_json.get("planned_values", {}).get("root_module")
    cfn_dict: dict = {"AWSTemplateFormatVersion": "2010-09-09", "Resources": {}}
    if not root_module:
        return cfn_dict

    LOG.debug("Mapping Lambda functions to their corresponding layers.")
    input_vars: Dict[str, Union[ConstantValue, References]] = {
        var_name: ConstantValue(value=var_value.get("value"))
        for var_name, var_value in tf_json.get("variables", {}).items()
    }
    root_tf_module = _build_module("", tf_json.get("configuration", {}).get("root_module"), input_vars, None)

    # to map s3 object sources to respective functions later
    # this dictionary will map between the hash value of the S3 Bucket attributes, and a tuple of the planned value
    # source code path, and the configuration value of the source code path.
    s3_hash_to_source: Dict[str, Tuple[str, List[Union[ConstantValue, ResolvedReference]]]] = {}

    # map code/imageuri to Lambda resources
    # the key is the hash value of lambda code/imageuri
    # the value is the list of pair of the resource logical id, and the lambda cfn resource dict
    lambda_resources_to_code_map: Dict[str, List[Tuple[Dict, str]]] = {}

    sam_metadata_resources: List[SamMetadataResource] = []

    resource_property_mapping: Dict[str, ResourceProperties] = get_resource_property_mapping()

    _check_unresolvable_values(root_module, root_tf_module)

    # create and iterate over queue of modules to handle child modules
    for curr_module, curr_tf_module in _get_modules(root_module, root_tf_module):
        curr_module_address = curr_module.get("address")

        # iterate over resources for current module
        resources = curr_module.get("resources", {})
        for resource in resources:
            resource_provider = resource.get("provider_name")
            resource_type = resource.get("type")
            resource_values = resource.get("values")
            resource_full_address = resource.get("address")
            resource_name = resource.get("name")
            resource_mode = resource.get("mode")

            resource_address = (
                f"data.{resource_type}.{resource_name}"
                if resource_mode == "data"
                else f"{resource_type}.{resource_name}"
            )
            config_resource_address = get_configuration_address(resource_address)
            if config_resource_address not in curr_tf_module.resources:
                raise PrepareHookException(
                    f"There is no configuration resource for resource address {resource_full_address} and "
                    f"configuration address {config_resource_address}"
                )

            config_resource = curr_tf_module.resources[config_resource_address]

            if (
                resource_provider == NULL_RESOURCE_PROVIDER_NAME
                and resource_type == SAM_METADATA_RESOURCE_TYPE
                and resource_name.startswith(SAM_METADATA_NAME_PREFIX)
            ):
                _add_metadata_resource_to_metadata_list(
                    SamMetadataResource(curr_module_address, resource, config_resource),
                    resource,
                    sam_metadata_resources,
                )
                continue

            # only process supported provider
            if resource_provider != AWS_PROVIDER_NAME:
                continue

            # store S3 sources
            if resource_type == "aws_s3_object":
                s3_bucket = (
                    resource_values.get("bucket")
                    if "bucket" in resource_values
                    else _resolve_resource_attribute(config_resource, "bucket")
                )
                s3_key = (
                    resource_values.get("key")
                    if "key" in resource_values
                    else _resolve_resource_attribute(config_resource, "key")
                )
                obj_hash = _get_s3_object_hash(s3_bucket, s3_key)
                code_artifact = resource_values.get("source")
                config_code_artifact = (
                    code_artifact if code_artifact else _resolve_resource_attribute(config_resource, "source")
                )
                s3_hash_to_source[obj_hash] = (code_artifact, config_code_artifact)

            resource_translator = RESOURCE_TRANSLATOR_MAPPING.get(resource_type)
            # resource type not supported
            if not resource_translator:
                continue

            # translate TF resource "values" to CFN properties
            LOG.debug("Processing resource %s", resource_full_address)
            translated_properties = _translate_properties(
                resource_values, resource_translator.property_builder_mapping, config_resource
            )
            translated_resource: Dict = {
                "Type": resource_translator.cfn_name,
                "Properties": translated_properties,
                "Metadata": {"SamResourceId": resource_full_address},
            }

            # Only set the SkipBuild metadata if it's a resource that can be built
            if resource_translator.cfn_name in CFN_CODE_PROPERTIES:
                translated_resource["Metadata"]["SkipBuild"] = True

            # build CFN logical ID from resource address
            logical_id = build_cfn_logical_id(resource_full_address)

            # Add resource to cfn dict
            if not translated_resource.get("Type", "").startswith(INTERNAL_PREFIX):
                # Internal resources are ones used for the purpose of translation, they are not real CFN resources.
                # These are usually resources that exist in other IaCs that don't map 1:1 with CFN resources, but their
                # properties need to be mapped to other, existing CFN resources.
                cfn_dict["Resources"][logical_id] = translated_resource

            resource_translation_properties = ResourceTranslationProperties(
                resource=resource,
                translated_resource=translated_resource,
                config_resource=config_resource,
                logical_id=logical_id,
                resource_full_address=resource_full_address,
            )
            if resource_type in resource_property_mapping:
                resource_properties: ResourceProperties = resource_property_mapping[resource_type]
                resource_properties.collect(resource_translation_properties)
                if isinstance(resource_properties, CodeResourceProperties):
                    resource_properties.add_lambda_resources_to_code_map(
                        resource_translation_properties, translated_properties, lambda_resources_to_code_map
                    )

            if resource_type in TRANSLATION_VALIDATORS:
                validator = TRANSLATION_VALIDATORS[resource_type](resource=resource, config_resource=config_resource)
                validator.validate()

    # map s3 object sources to corresponding functions
    LOG.debug("Mapping S3 object sources to corresponding functions")
    _map_s3_sources_to_functions(s3_hash_to_source, cfn_dict.get("Resources", {}), lambda_resources_to_code_map)

    _handle_linking(resource_property_mapping)

    add_integrations_to_methods(
        resource_property_mapping.get(TF_AWS_API_GATEWAY_METHOD, ResourceProperties()).cfn_resources,
        resource_property_mapping.get(TF_AWS_API_GATEWAY_INTEGRATION, ResourceProperties()).cfn_resources,
    )

    add_integration_responses_to_methods(
        resource_property_mapping.get(TF_AWS_API_GATEWAY_METHOD, ResourceProperties()).cfn_resources,
        resource_property_mapping.get(TF_AWS_API_GATEWAY_INTEGRATION_RESPONSE, ResourceProperties()).cfn_resources,
    )

    if sam_metadata_resources:
        LOG.debug("Enrich the mapped resources with the sam metadata information and generate Makefile")
        enrich_resources_and_generate_makefile(
            sam_metadata_resources,
            cfn_dict.get("Resources", {}),
            output_directory_path,
            terraform_application_dir,
            lambda_resources_to_code_map,
            project_root_dir,
        )
    else:
        LOG.debug("There is no sam metadata resources, no enrichment or Makefile is required")

    # check if there is still any dummy remote values for lambda resource imagesUri or S3 attributes
    _check_dummy_remote_values(cfn_dict.get("Resources", {}))

    return cfn_dict
def get_token(request):
    
    
    
    if "CSRF_COOKIE" not in request.META:
        request.META["CSRF_COOKIE"] = _get_new_csrf_key()
    request.META["CSRF_COOKIE_USED"] = True
    return request.META["CSRF_COOKIE"]
def from_params(cls, W, b=None, stride=1, pad=0, nobias=False, **kwargs):
        
        
        out_channels, in_channels, kw, kh = W.shape
        if b is not None:
            if out_channels != b.size:
                raise ValueError(
                    '`out_channels` does not match the size of `b`')

        link = cls(
            in_channels, out_channels, (kw, kh), stride, pad, nobias, **kwargs)
        return link
def from_params(cls, W, b=None, stride=1, pad=0, nobias=False, **kwargs):
        
        
        dilate, groups = argument.parse_kwargs(
            kwargs, ('dilate', 1), ('groups', 1))
        out_channels, _in_channels, kw, kh = W.shape
        in_channels = _in_channels * groups
        if b is not None:
            if out_channels != b.size:
                raise ValueError(
                    '`out_channels` does not match the size of `b`')

        link = cls(
            in_channels, out_channels, (kw, kh), stride, pad, nobias,
            initialW=variable.as_array(W), initial_bias=variable.as_array(b),
            dilate=dilate, groups=groups)
        return link
def _parse_json(self, config_file: str) -> str:
         
        
        formatted: str = ""
        with open(config_file, "r", encoding="utf-8", errors="replace") as cfile:
            conf_dict = json.load(cfile)
            for key in sorted(conf_dict.keys()):
                formatted += self._format_text(key, conf_dict[key])
        return formatted
def get_unique_resource_ids(
    stacks: List[Stack],
    resource_ids: Optional[Union[List[str]]],
    resource_types: Optional[Union[List[str]]],
) -> Set[ResourceIdentifier]:
    
    
    output_resource_ids: Set[ResourceIdentifier] = set()
    if resource_ids:
        for resources_id in resource_ids:
            output_resource_ids.add(ResourceIdentifier(resources_id))

    if resource_types:
        for resource_type in resource_types:
            resource_type_ids = get_resource_ids_by_type(stacks, resource_type)
            for resource_id in resource_type_ids:
                output_resource_ids.add(resource_id)
    return output_resource_ids
def test_generate_files_absolute_path(tmp_path):
    
    generate.generate_files(
        context={'cookiecutter': {'food': 'pizzä'}},
        repo_dir=Path('tests/test-generate-files').absolute(),
        output_dir=tmp_path,
    )
    assert Path(tmp_path, 'inputpizzä/simple.txt').is_file()
def run_command(
        args,  # type: CommonConfig
        cmd,  # type: t.Iterable[str]
        capture=False,  # type: bool
        env=None,  # type: t.Optional[t.Dict[str, str]]
        data=None,  # type: t.Optional[str]
        cwd=None,  # type: t.Optional[str]
        always=False,  # type: bool
        stdin=None,  # type: t.Optional[t.BinaryIO]
        stdout=None,  # type: t.Optional[t.BinaryIO]
        cmd_verbosity=1,  # type: int
        str_errors='strict',  # type: str
        error_callback=None,  # type: t.Optional[t.Callable[[SubprocessError], None]]
):  # type: (...) -> t.Tuple[t.Optional[str], t.Optional[str]]
    
    explain = args.explain and not always
    return raw_command(cmd, capture=capture, env=env, data=data, cwd=cwd, explain=explain, stdin=stdin, stdout=stdout,
                       cmd_verbosity=cmd_verbosity, str_errors=str_errors, error_callback=error_callback)
def named_temporary_file(args, prefix, suffix, directory, content):  # type: (CommonConfig, str, str, t.Optional[str], str) -> t.Iterator[str]
    
    if args.explain:
        yield os.path.join(directory or '/tmp', '%stemp%s' % (prefix, suffix))
    else:
        with tempfile.NamedTemporaryFile(prefix=prefix, suffix=suffix, dir=directory) as tempfile_fd:
            tempfile_fd.write(to_bytes(content))
            tempfile_fd.flush()

            yield tempfile_fd.name
def test_m2m_with_unicode_reference(self):
        
        
        
        m1 = StringReferenceModel.objects.create()
        m2 = StringReferenceModel.objects.create()
        m2.others.add(m1)  # used to cause an error (see ticket #6045)
        m2.save()
        list(m2.others.all())
def create_kubeconfig(token, ca, master_ip, api_port, filename, user):
    
    
    
    snap_path = os.environ.get("SNAP")
    config_template = "{}/{}".format(snap_path, "kubelet.config.template")
    config = "{}/credentials/{}".format(snapdata_path, filename)
    shutil.copyfile(config, "{}.backup".format(config))
    try_set_file_permissions("{}.backup".format(config))
    ca_line = ca_one_line(ca)
    with open(config_template, "r") as tfp:
        with open(config, "w+") as fp:
            config_txt = tfp.read()
            config_txt = config_txt.replace("CADATA", ca_line)
            config_txt = config_txt.replace("NAME", user)
            config_txt = config_txt.replace("TOKEN", token)
            config_txt = config_txt.replace("127.0.0.1", master_ip)
            config_txt = config_txt.replace("16443", api_port)
            fp.write(config_txt)
        try_set_file_permissions(config)
def join_dqlite_master_node(info, master_ip):
    
    
    

    # The cluster we want to join may be either token-auth based or x509-auth based.
    # The way to identify the cluster type is to look for the "admin_token" in the info
    # we got back from the cluster we try to join.
    # In the case of token-auth we need to:
    # - create the known_tokens.csv file (if it does not exist) with the admin token
    # - turn on token-auth on kube-apiserver
    # - create the token based admin kubeconfig
    # - recreate the kubelet, proxy, scheduler, controller kubeconfigs for the new ca
    # - restart kubelite
    # In the case of x509-auth we need to:
    # - recreate the admin/client, kubelet, proxy, scheduler, controller kubeconfigs for the new ca
    # - restart kubelite

    hostname_override = info["hostname_override"]
    store_cert("ca.crt", info["ca"])
    store_cert("ca.key", info["ca_key"])
    store_cert("serviceaccount.key", info["service_account_key"])

    if "admin_token" in info:
        # We try to join a cluster where token-auth is in place.
        rebuild_token_based_auth_configs(info)
    else:
        # We are joining a x509-auth based cluster
        rebuild_x509_auth_client_configs()

    if "api_authz_mode" in info:
        update_apiserver(info["api_authz_mode"])

    store_base_kubelet_args(info["kubelet_args"])
    update_kubelet_node_ip(info["kubelet_args"], hostname_override)
    update_kubelet_hostname_override(info["kubelet_args"])
    store_callback_token(info["callback_token"])
    update_dqlite(info["cluster_cert"], info["cluster_key"], info["voters"], hostname_override)
    # We want to update the local CNI yaml but we do not want to apply it.
    # The cni is applied already in the cluster we join
    try_initialise_cni_autodetect_for_clustering(master_ip, apply_cni=False)
    mark_no_cert_reissue()
def get_client_cert(master_ip, master_port, fname: str, token: str, subject: str, with_sans: bool):
    
    
    

    cert_crt = (snap_data() / "certs" / fname).with_suffix(".crt")
    cert_key = (snap_data() / "certs" / fname).with_suffix(".key")
    # generate csr
    script = "generate_csr_with_sans" if with_sans else "generate_csr"
    p = subprocess.run(
        [f"{snap()}/actions/common/utils.sh", script, subject, cert_key],
        check=True,
        capture_output=True,
    )
    csr = p.stdout.decode()

    req_data = {"token": token, "request": csr}
    # TODO: enable ssl verification
    signed = requests.post(
        "https://{}:{}/{}/sign-cert".format(master_ip, master_port, CLUSTER_API),
        json=req_data,
        verify=False,
    )
    if signed.status_code != 200:
        error = "Failed to sign {} certificate ({}).".format(fname, signed.status_code)
        try:
            if "error" in signed.json():
                error = "{} {}".format(error, format(signed.json()["error"]))
        except ValueError:
            print("Make sure the cluster you connect to supports joining worker nodes.")
        print(error)
        exit(1)
    info = signed.json()
    cert_crt.write_text(info["certificate"])
    try_set_file_permissions(cert_crt)
def update_cert_auth_kubeproxy(token, master_ip, master_port):
    
    
    
    proxy_token = "{}-proxy".format(token)
    get_client_cert(master_ip, master_port, "proxy", proxy_token, "/CN=system:kube-proxy", False)
    set_arg("--master", None, "kube-proxy")
    set_arg("--hostname-override", None, "kube-proxy")
def imitate_ping(self, dest_ip, timeout=3, count=5, psize=64):
        
        
        
        logger.info('receive request ping {}'.format(dest_ip))
        verbose_ping(dest_ip, timeout, count, psize, display=self.send_msg)
def insertion_sort(base, extended, errors):
    
    # This function raises UnicodeDecodeError with position in the extended.
    # Caller should add the offset.
    char = 0x80
    pos = -1
    bias = 72
    extpos = 0

    while extpos < len(extended):
        newpos, delta = decode_generalized_number(extended, extpos,
                                                  bias, errors)
        if delta is None:
            # There was an error in decoding. We can't continue because
            # synchronization is lost.
            return base
        pos += delta+1
        char += pos // (len(base) + 1)
        if char > 0x10FFFF:
            if errors == "strict":
                raise UnicodeDecodeError(
                    "punycode", extended, pos-1, pos,
                    f"Invalid character U+{char:x}")
            char = ord('?')
        pos = pos % (len(base) + 1)
        base = base[:pos] + chr(char) + base[pos:]
        bias = adapt(delta, (extpos == 0), len(base))
        extpos = newpos
    return base
def mobilenetv2_110d(pretrained=False, **kwargs):
     
    model = _gen_mobilenet_v2(
        'mobilenetv2_110d', 1.1, depth_multiplier=1.2, fix_stem_head=True, pretrained=pretrained, **kwargs)
    return model
def __init__(
        self,
        companion_stack: Optional[CompanionStack] = None,
        function_full_path: Optional[str] = None,
        logical_id: Optional[str] = None,
        physical_id: Optional[str] = None,
        output_logical_id: Optional[str] = None,
    ):
        
        
        
        self._function_full_path = (
            function_full_path.replace(posixpath.sep, "") if function_full_path else function_full_path
        )
        self._escaped_function_logical_id = (
            re.sub(r"[^a-z0-9]", "", self._function_full_path.lower()) if self._function_full_path is not None else None
        )
        self._function_md5 = str_checksum(self._function_full_path) if self._function_full_path is not None else None
        self._companion_stack = companion_stack

        self._logical_id = logical_id
        self._physical_id = physical_id
        self._output_logical_id = output_logical_id
def set_many(self, data, timeout=DEFAULT_TIMEOUT, version=None):
        
        
        
        for key, value in data.items():
            self.set(key, value, timeout=timeout, version=version)
        return []
def _check_input(self, shape):
        
        
        
        if len(shape) == 3:
            self.unsqueeze = True
            in_channels = 1

        elif len(shape) == 4:
            in_channels = shape[3]

        else:
            raise ValueError("Expected 3d or 4d inputs. Got " + len(shape))

        # Kernel size must be odd
        if self.kernel_size[0] % 2 == 0 or self.kernel_size[1] % 2 == 0:
            raise ValueError(
                "The field kernel size must be an odd number. Got %s."
                % (self.kernel_size)
            )

        return in_channels
def _manage_padding(self, x, kernel_size: int, dilation: int, stride: int):
        
        

        # Detecting input shape
        L_in = self.in_channels

        # Time padding
        padding = get_padding_elem(L_in, stride, kernel_size, dilation)

        # Applying padding
        x = F.pad(x, padding, mode=self.padding_mode)

        return x
def peek(self):
        
        
        
        if not self.stack:
            raise StackUnderflowError
        return self.stack[-1]
def mock_is_virtual_env() -> Generator[Mock, None, None]:
    
    with patch(
        "homeassistant.bootstrap.is_virtual_env", return_value=False
    ) as is_virtual_env:
        yield is_virtual_env
def remove_small_regions(masks, min_area=0, nms_thresh=0.7):
        
        
        
        if len(masks) == 0:
            return masks

        # Filter small disconnected regions and holes
        new_masks = []
        scores = []
        for mask in masks:
            mask = mask.cpu().numpy().astype(np.uint8)
            mask, changed = remove_small_regions(mask, min_area, mode='holes')
            unchanged = not changed
            mask, changed = remove_small_regions(mask, min_area, mode='islands')
            unchanged = unchanged and not changed

            new_masks.append(torch.as_tensor(mask).unsqueeze(0))
            # Give score=0 to changed masks and score=1 to unchanged masks
            # so NMS will prefer ones that didn't need postprocessing
            scores.append(float(unchanged))

        # Recalculate boxes and remove any new duplicates
        new_masks = torch.cat(new_masks, dim=0)
        boxes = batched_mask_to_box(new_masks)
        keep = torchvision.ops.nms(
            boxes.float(),
            torch.as_tensor(scores),
            nms_thresh,
        )

        return new_masks[keep].to(device=masks.device, dtype=masks.dtype), keep
def generate(self,
                 im,
                 crop_n_layers=0,
                 crop_overlap_ratio=512 / 1500,
                 crop_downscale_factor=1,
                 point_grids=None,
                 points_stride=32,
                 points_batch_size=64,
                 conf_thres=0.88,
                 stability_score_thresh=0.95,
                 stability_score_offset=0.95,
                 crop_nms_thresh=0.7):
        
        
        
        self.segment_all = True
        ih, iw = im.shape[2:]
        crop_regions, layer_idxs = generate_crop_boxes((ih, iw), crop_n_layers, crop_overlap_ratio)
        if point_grids is None:
            point_grids = build_all_layer_point_grids(points_stride, crop_n_layers, crop_downscale_factor)
        pred_masks, pred_scores, pred_bboxes, region_areas = [], [], [], []
        for crop_region, layer_idx in zip(crop_regions, layer_idxs):
            x1, y1, x2, y2 = crop_region
            w, h = x2 - x1, y2 - y1
            area = torch.tensor(w * h, device=im.device)
            points_scale = np.array([[w, h]])  # w, h
            # Crop image and interpolate to input size
            crop_im = F.interpolate(im[..., y1:y2, x1:x2], (ih, iw), mode='bilinear', align_corners=False)
            # (num_points, 2)
            points_for_image = point_grids[layer_idx] * points_scale
            crop_masks, crop_scores, crop_bboxes = [], [], []
            for (points, ) in batch_iterator(points_batch_size, points_for_image):
                pred_mask, pred_score = self.prompt_inference(crop_im, points=points, multimask_output=True)
                # Interpolate predicted masks to input size
                pred_mask = F.interpolate(pred_mask[None], (h, w), mode='bilinear', align_corners=False)[0]
                idx = pred_score > conf_thres
                pred_mask, pred_score = pred_mask[idx], pred_score[idx]

                stability_score = calculate_stability_score(pred_mask, self.model.mask_threshold,
                                                            stability_score_offset)
                idx = stability_score > stability_score_thresh
                pred_mask, pred_score = pred_mask[idx], pred_score[idx]
                # Bool type is much more memory-efficient.
                pred_mask = pred_mask > self.model.mask_threshold
                # (N, 4)
                pred_bbox = batched_mask_to_box(pred_mask).float()
                keep_mask = ~is_box_near_crop_edge(pred_bbox, crop_region, [0, 0, iw, ih])
                if not torch.all(keep_mask):
                    pred_bbox, pred_mask, pred_score = pred_bbox[keep_mask], pred_mask[keep_mask], pred_score[keep_mask]

                crop_masks.append(pred_mask)
                crop_bboxes.append(pred_bbox)
                crop_scores.append(pred_score)

            # Do nms within this crop
            crop_masks = torch.cat(crop_masks)
            crop_bboxes = torch.cat(crop_bboxes)
            crop_scores = torch.cat(crop_scores)
            keep = torchvision.ops.nms(crop_bboxes, crop_scores, self.args.iou)  # NMS
            crop_bboxes = uncrop_boxes_xyxy(crop_bboxes[keep], crop_region)
            crop_masks = uncrop_masks(crop_masks[keep], crop_region, ih, iw)
            crop_scores = crop_scores[keep]

            pred_masks.append(crop_masks)
            pred_bboxes.append(crop_bboxes)
            pred_scores.append(crop_scores)
            region_areas.append(area.expand(len(crop_masks)))

        pred_masks = torch.cat(pred_masks)
        pred_bboxes = torch.cat(pred_bboxes)
        pred_scores = torch.cat(pred_scores)
        region_areas = torch.cat(region_areas)

        # Remove duplicate masks between crops
        if len(crop_regions) > 1:
            scores = 1 / region_areas
            keep = torchvision.ops.nms(pred_bboxes, scores, crop_nms_thresh)
            pred_masks, pred_bboxes, pred_scores = pred_masks[keep], pred_bboxes[keep], pred_scores[keep]

        return pred_masks, pred_scores, pred_bboxes
def __compile__(self, input_storage=None, output_storage=None,
                    storage_map=None, keep_lock=False):
        

        
        error_storage = [None, None, None]
        if input_storage is None:
            input_storage = tuple([None] for variable in self.inputs)
        if output_storage is None:
            map = {}
            output_storage = []
            # Initialize the map with the inputs, as some outputs may
            # be inputs as well.
            for i, variable in enumerate(self.inputs):
                map[variable] = input_storage[i]
            for variable in self.outputs:
                if variable not in map:
                    map[variable] = [None]
                output_storage.append(map[variable])
        input_storage = tuple(input_storage)
        output_storage = tuple(output_storage)
        thunk = self.cthunk_factory(error_storage,
                                    input_storage,
                                    output_storage,
                                    storage_map,
                                    keep_lock=keep_lock)
        return (thunk,
                [link.Container(input, storage) for input, storage in
                 izip(self.fgraph.inputs, input_storage)],
                [link.Container(output, storage, True) for output, storage in
                 izip(self.fgraph.outputs, output_storage)],
                error_storage)
def headers(self):
        
        
        ret = []
        for x in [y.type for y in self.variables] + [y.op for y in self.node_order]:
            try: ret += x.c_headers()
            except utils.MethodNotDefined: pass
        return list(set(ret))
def code_gen(blocks):
    
    
    
    decl = ""
    head = ""
    tail = ""
    for block in blocks:
        decl += block.declare
        head = head + ("\n{\n%s" % block.behavior)
        tail = ("%s\n}\n" % block.cleanup) + tail
    return decl + head + tail
def failure_code(sub, use_goto=True):
    
    

    
    if use_goto:
        goto_statement = 'goto __label_%(id)i;' % sub
    else:
        goto_statement = ''
    return '''{
        %(failure_var)s = %(id)i;
        if (!PyErr_Occurred()) {
            PyErr_SetString(PyExc_RuntimeError,
                "Unexpected error in an Op's C code. "
                "No Python exception was set.");
        }
        %(goto_statement)s}''' % dict(sub, goto_statement=goto_statement)
def convert_until(status_dict: dict, until_key: str) -> str:
    
    if until_key in status_dict:  # only present for certain modes
        dt_utc_naive = dt_util.parse_datetime(status_dict[until_key])
        status_dict[until_key] = dt_util.as_local(dt_utc_naive).isoformat()
def scale(X, axis=0, with_mean=True, with_std=True, copy=True):
    
    
    if sp.issparse(X):
        X = X.tocsr()
        if with_mean:
            raise ValueError(
                "Cannot center sparse matrices: pass `with_mean=False` instead"
                " See docstring for motivation and alternatives.")
        warn_if_not_float(X, estimator='The scale function')
        if copy:
            X = X.copy()
        _, var = mean_variance_axis0(X)
        var[var==0.0] = 1.0
        inplace_csr_column_scale(X, np.sqrt(var))
    else:
        X = np.asarray(X)
        warn_if_not_float(X, estimator='The scale function')
        mean_, std_ = _mean_and_std(
            X, axis, with_mean=with_mean, with_std=with_std)
        if copy:
            X = X.copy()
        Xr = np.rollaxis(X, axis)
        if with_mean:
            Xr -= mean_
        if with_std:
            Xr /= std_
    return X
def fit(self, X, y=None):
        
        
        if sp.issparse(X):
            X = X.tocsr()
            if self.with_mean:
                raise ValueError(
                    "Cannot center sparse matrices: pass `with_mean=False` "
                    "instead See docstring for motivation and alternatives.")
            warn_if_not_float(X, estimator=self)
            if self.copy:
                X = X.copy()
            self.mean_ = None
            _, var = mean_variance_axis0(X)
            self.std_ = np.sqrt(var)
            self.std_[var == 0.0] = 1.0
            inplace_csr_column_scale(X, 1 / self.std_)
            return self
        else:
            X = np.asarray(X)
            warn_if_not_float(X, estimator=self)
            self.mean_, self.std_ = _mean_and_std(
                X, axis=0, with_mean=self.with_mean, with_std=self.with_std)
            return self
def _sync_ddp(result: Tensor, group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = None) -> Tensor:
    

    
    divide_by_world_size = False
    group = torch.distributed.group.WORLD if group is None else group

    op: Optional[ReduceOp]
    if isinstance(reduce_op, str):
        reduce_op = "avg" if reduce_op == "mean" else reduce_op
        if reduce_op.lower() == "avg" and torch.distributed.get_backend(group) == "gloo":
            # The GLOO backend does not support the `ReduceOp.AVG` operation
            op = ReduceOp.SUM  # type: ignore[assignment]
            divide_by_world_size = True
        else:
            op = getattr(ReduceOp, reduce_op.upper())
    else:
        op = reduce_op

    # HPU doesn't support Long types, forcefully set it to float
    # TODO: move this to the `lightning_habana` package
    if (
        package_available("habana_frameworks")
        and os.environ.get("HCCL_DISTRIBUTED_BACKEND") == "1"
        and result.type()
        in (
            "torch.LongTensor",
            "torch.hpu.LongTensor",
        )
    ):
        rank_zero_info("Long tensor unsupported on HPU, casting to float")
        result = result.float()

    # Sync all processes before reduction
    torch.distributed.barrier(group=group)
    torch.distributed.all_reduce(result, op=op, group=group, async_op=False)
    world_size = torch.distributed.get_world_size(group)

    if not divide_by_world_size:
        return result
    # `torch.distributed.all_reduce` is in-place, so we should do the division in-place to leave the modified tensors
    # with the expected value
    if not torch.is_floating_point(result):
        return result.copy_(result / world_size)
    return result.div_(world_size)
def contextmanager(func):
    
    

    sourcefile = inspect.getsourcefile(func)
    _, linenumber = inspect.getsourcelines(func)

    wrapper = contextlib.contextmanager(func)

    if sourcefile is not None:
        wrapper.__chainer_wrapped_sourcefile__ = sourcefile
        wrapper.__chainer_wrapped_linenumber__ = linenumber
    return wrapper
def contextmanager(func):
    
    

    wrapper = contextlib.contextmanager(func)

    sourcefile = inspect.getsourcefile(func)
    _, linenumber = inspect.getsourcelines(func)

    # Note: these attributes are used in docs/source/conf.py.
    if sourcefile is not None:
        wrapper.__chainer_wrapped_sourcefile__ = sourcefile
        wrapper.__chainer_wrapped_linenumber__ = linenumber
    return wrapper
def test_sgd_proba(self):
        

        # hinge loss does not allow for conditional prob estimate
        clf = self.factory(loss="hinge", alpha=0.01, n_iter=10).fit(X, Y)
        assert_raises(NotImplementedError, clf.predict_proba, [3, 2])

        # the log and modified_huber losses can output "probability" estimates
        for loss in ("log", "modified_huber"):
            clf = self.factory(loss=loss, alpha=0.01, n_iter=10).fit(X, Y)
            p = clf.predict_proba([3, 2])
            assert_true(p > 0.5)
            p = clf.predict_proba([-1, -1])
            assert_true(p < 0.5)
def unique_id(self) -> str:
        
        return self._unique_id
def __init__(self,
               is_training,
               depth_multiplier,
               min_depth,
               pad_to_multiple,
               conv_hyperparams_fn,
               fpn_min_level=3,
               fpn_max_level=7,
               additional_layer_depth=256,
               reuse_weights=None,
               use_explicit_padding=False,
               use_depthwise=False,
               use_native_resize_op=False,
               override_base_feature_extractor_hyperparams=False):
    
    
    super(SSDResnet101V1FpnFeatureExtractor, self).__init__(
        is_training,
        depth_multiplier,
        min_depth,
        pad_to_multiple,
        conv_hyperparams_fn,
        resnet_v1.resnet_v1_101,
        'resnet_v1_101',
        'fpn',
        fpn_min_level,
        fpn_max_level,
        additional_layer_depth,
        reuse_weights=reuse_weights,
        use_explicit_padding=use_explicit_padding,
        use_depthwise=use_depthwise,
        use_native_resize_op=use_native_resize_op,
        override_base_feature_extractor_hyperparams=
        override_base_feature_extractor_hyperparams)
def preprocess(self, resized_inputs):
    
    
    if resized_inputs.shape.as_list()[3] == 3:
      channel_means = [123.68, 116.779, 103.939]
      return resized_inputs - [[channel_means]]
    else:
      return resized_inputs
def build_alpharep_fixture():
    
    
    
    data = io.BytesIO()
    zf = zipfile.ZipFile(data, "w")
    zf.writestr("a.txt", b"content of a")
    zf.writestr("b/c.txt", b"content of c")
    zf.writestr("b/d/e.txt", b"content of e")
    zf.writestr("b/f.txt", b"content of f")
    zf.writestr("g/h/i.txt", b"content of i")
    zf.writestr("j/k.bin", b"content of k")
    zf.writestr("j/l.baz", b"content of l")
    zf.writestr("j/m.bar", b"content of m")
    zf.filename = "alpharep.zip"
    return zf
def move_to_device(item, device, criterion_func):
    
    
    
    if criterion_func(item):
        device_copy = item.to(device)
        item.data = device_copy.data
        return item
    elif isinstance(item, list):
        return [move_to_device(v, device, criterion_func) for v in item]
    elif isinstance(item, tuple):
        return tuple([move_to_device(v, device, criterion_func) for v in item])
    elif isinstance(item, dict):
        return {k: move_to_device(v, device, criterion_func) for k, v in item.items()}
    else:
        return item
def build_nasfpn_decoder(
    input_specs: Mapping[str, tf.TensorShape],
    model_config: hyperparams.Config,
    l2_regularizer: Optional[tf_keras.regularizers.Regularizer] = None
) -> tf_keras.Model:
  
  
  decoder_type = model_config.decoder.type
  decoder_cfg = model_config.decoder.get()
  if decoder_type != 'nasfpn':
    raise ValueError(f'Inconsistent decoder type {decoder_type}. '
                     'Need to be `nasfpn`.')

  norm_activation_config = model_config.norm_activation
  return NASFPN(
      input_specs=input_specs,
      min_level=model_config.min_level,
      max_level=model_config.max_level,
      num_filters=decoder_cfg.num_filters,
      num_repeats=decoder_cfg.num_repeats,
      use_separable_conv=decoder_cfg.use_separable_conv,
      activation=norm_activation_config.activation,
      use_sync_bn=norm_activation_config.use_sync_bn,
      norm_momentum=norm_activation_config.norm_momentum,
      norm_epsilon=norm_activation_config.norm_epsilon,
      kernel_regularizer=l2_regularizer)
def __init__(
        self, rachio: Rachio, data: dict[str, Any], coordinator: RachioUpdateCoordinator
    ) -> None:
        
        self.rachio = rachio
        self._id = data[KEY_ID]
        self.coordinator = coordinator
def replace_named_groups(pattern):
    
    
    named_group_indices = [
        (m.start(0), m.end(0), m.group(1))
        for m in named_group_matcher.finditer(pattern)
    ]
    # Tuples of (named capture group pattern, group name).
    group_pattern_and_name = []
    # Loop over the groups and their start and end indices.
    for start, end, group_name in named_group_indices:
        # Handle nested parentheses, e.g. '^(?P<a>(x|y))/b'.
        unmatched_open_brackets, prev_char = 1, None
        for idx, val in enumerate(pattern[end:]):
            # Check for unescaped `(` and `)`. They mark the start and end of a
            # nested group.
            if val == '(' and prev_char != '\\':
                unmatched_open_brackets += 1
            elif val == ')' and prev_char != '\\':
                unmatched_open_brackets -= 1
            prev_char = val
            # If brackets are balanced, the end of the string for the current
            # named capture group pattern has been reached.
            if unmatched_open_brackets == 0:
                group_pattern_and_name.append((pattern[start:end + idx + 1], group_name))
                break

    # Replace the string for named capture groups with their group names.
    for group_pattern, group_name in group_pattern_and_name:
        pattern = pattern.replace(group_pattern, group_name)
    return pattern
def test_lda_dense_input():
    
    
    
    rng = np.random.RandomState(0)
    X = rng.randint(5, size=(20, 10))
    n_topics = 3
    alpha0 = eta0 = 1. / n_topics
    lda = OnlineLDA(n_topics=n_topics, alpha=alpha0, eta=eta0,
                    random_state=rng)

    X_trans = lda.fit_transform(X)
    assert_true((X_trans > 0.0).any())
def test_lda_dense_input():
    
    
    
    rng = np.random.RandomState(0)
    X = rng.randint(5, size=(20, 10))
    n_topics = 3
    doc_topic_prior = 1. / n_topics
    topic_word_prior = 1. / n_topics
    lda = LatentDirichletAllocation(n_topics=n_topics, doc_topic_prior=doc_topic_prior,
                                    topic_word_prior=topic_word_prior, random_state=rng)
    X_trans = lda.fit_transform(X)
    assert_true((X_trans > 0.0).any())
def from_documents(
        cls: Type[CVST],
        documents: List[Document],
        embedding: Embeddings,
        *,
        session: Session = _NOT_SET,
        keyspace: str = "",
        table_name: str = "",
        ids: Optional[List[str]] = None,
        batch_size: int = 16,
        ttl_seconds: Optional[int] = None,
        body_index_options: Optional[List[Tuple[str, Any]]] = None,
        **kwargs: Any,
    ) -> CVST:
        
        
        texts = [doc.page_content for doc in documents]
        metadatas = [doc.metadata for doc in documents]
        return cls.from_texts(
            texts=texts,
            embedding=embedding,
            metadatas=metadatas,
            session=session,
            keyspace=keyspace,
            table_name=table_name,
            ids=ids,
            batch_size=batch_size,
            ttl_seconds=ttl_seconds,
            body_index_options=body_index_options,
            **kwargs,
        )
def test_bare_secret_accepted_and_replaced(self):
        
        
        
        req = self._get_POST_request_with_token(cookie=TEST_SECRET)
        mw = CsrfViewMiddleware(token_view)
        mw.process_request(req)
        resp = mw.process_view(req, token_view, (), {})
        self.assertIsNone(resp)
        resp = mw(req)
        csrf_cookie = self._read_csrf_cookie(req, resp)
        self.assertEqual(len(csrf_cookie), CSRF_TOKEN_LENGTH)
        self._check_token_present(resp, csrf_cookie)
def test_bare_secret_accepted_and_replaced(self):
        
        
        
        req = self._get_POST_request_with_token(cookie=TEST_SECRET)
        mw = CsrfViewMiddleware(token_view)
        mw.process_request(req)
        resp = mw.process_view(req, token_view, (), {})
        self.assertIsNone(resp)
        resp = mw(req)
        csrf_cookie = self._read_csrf_cookie(req, resp)
        # This also checks that csrf_cookie now has length CSRF_TOKEN_LENGTH.
        self.assertMaskedSecretCorrect(csrf_cookie, TEST_SECRET)
        self._check_token_present(resp, csrf_cookie)
def async_connection_success(self) -> None:
        
        
        self.async_db_connected.set_result(True)
def _setup_recorder(self) -> bool:
        
        tries = 1

        while tries <= self.db_max_retries:
            try:
                self._setup_connection()
                return True
            except UnsupportedDialect:
                break
            except Exception as err:  # pylint: disable=broad-except
                _LOGGER.exception(
                    "Error during connection setup: %s (retrying in %s seconds)",
                    err,
                    self.db_retry_wait,
                )
            tries += 1
            time.sleep(self.db_retry_wait)

        return False
def run(self) -> None:
        
        try:
            self._run()
        finally:
            # Ensure shutdown happens cleanly if
            # anything goes wrong in the run loop
            self._shutdown()
def new(arg=None):
    
    

    crypto = mssha1()
    if arg:
        crypto.update(arg)

    return crypto
def normalize_string_prefix(s: str) -> str:
    
    match = STRING_PREFIX_RE.match(s)
    assert match is not None, f"failed to match string {s!r}"
    orig_prefix = match.group(1)
    new_prefix = (
        orig_prefix.replace("F", "f")
        .replace("B", "b")
        .replace("U", "")
        .replace("u", "")
    )
    return f"{new_prefix}{match.group(2)}"
def cross_validation(model, horizon, period=None, initial=None, parallel=None, cutoffs=None, disable_tqdm=False, extra_output_columns=None):
    
    
    
    if model.history is None:
        raise Exception('Model has not been fit. Fitting the model provides contextual parameters for cross validation.')
    
    df = model.history.copy().reset_index(drop=True)
    horizon = pd.Timedelta(horizon)
    predict_columns = ['ds', 'yhat']
        
    if model.uncertainty_samples:
        predict_columns.extend(['yhat_lower', 'yhat_upper'])

    if extra_output_columns is not None:
        if isinstance(extra_output_columns, str):
            extra_output_columns = [extra_output_columns]
        predict_columns.extend([c for c in extra_output_columns if c not in predict_columns])
        
    # Identify largest seasonality period
    period_max = 0.
    for s in model.seasonalities.values():
        period_max = max(period_max, s['period'])
    seasonality_dt = pd.Timedelta(str(period_max) + ' days')    

    if cutoffs is None:
        # Set period
        period = 0.5 * horizon if period is None else pd.Timedelta(period)

        # Set initial
        initial = (
            max(3 * horizon, seasonality_dt) if initial is None
            else pd.Timedelta(initial)
        )

        # Compute Cutoffs
        cutoffs = generate_cutoffs(df, horizon, initial, period)
    else:
        # add validation of the cutoff to make sure that the min cutoff is strictly greater than the min date in the history
        if min(cutoffs) <= df['ds'].min(): 
            raise ValueError("Minimum cutoff value is not strictly greater than min date in history")
        # max value of cutoffs is <= (end date minus horizon)
        end_date_minus_horizon = df['ds'].max() - horizon 
        if max(cutoffs) > end_date_minus_horizon: 
            raise ValueError("Maximum cutoff value is greater than end date minus horizon, no value for cross-validation remaining")
        initial = cutoffs[0] - df['ds'].min()
        
    # Check if the initial window 
    # (that is, the amount of time between the start of the history and the first cutoff)
    # is less than the maximum seasonality period
    if initial < seasonality_dt:
            msg = 'Seasonality has period of {} days '.format(period_max)
            msg += 'which is larger than initial window. '
            msg += 'Consider increasing initial.'
            logger.warning(msg)

    if parallel:
        valid = {"threads", "processes", "dask"}

        if parallel == "threads":
            pool = concurrent.futures.ThreadPoolExecutor()
        elif parallel == "processes":
            pool = concurrent.futures.ProcessPoolExecutor()
        elif parallel == "dask":
            try:
                from dask.distributed import get_client
            except ImportError as e:
                raise ImportError("parallel='dask' requires the optional "
                                  "dependency dask.") from e
            pool = get_client()
            # delay df and model to avoid large objects in task graph.
            df, model = pool.scatter([df, model])
        elif hasattr(parallel, "map"):
            pool = parallel
        else:
            msg = ("'parallel' should be one of {} for an instance with a "
                   "'map' method".format(', '.join(valid)))
            raise ValueError(msg)

        iterables = ((df, model, cutoff, horizon, predict_columns)
                     for cutoff in cutoffs)
        iterables = zip(*iterables)

        logger.info("Applying in parallel with %s", pool)
        predicts = pool.map(single_cutoff_forecast, *iterables)
        if parallel == "dask":
            # convert Futures to DataFrames
            predicts = pool.gather(predicts)

    else:
        predicts = [
            single_cutoff_forecast(df, model, cutoff, horizon, predict_columns) 
            for cutoff in (tqdm(cutoffs) if not disable_tqdm else cutoffs)
        ]

    # Combine all predicted pd.DataFrame into one pd.DataFrame
    return pd.concat(predicts, axis=0).reset_index(drop=True)
def erase(root: Node, value: int) -> Node:
    
    
    
    left, right = split(root, value-1)
    _, right = split(right, value)
    return merge(left, right)
def get_group_permissions(self, obj=None):  # noqa: D205, D212, D400, D415
        
        
        return _user_get_permissions(self, obj, "group")
def resize_pos_embed(
        posemb: torch.Tensor,
        posemb_new: torch.Tensor,
        num_prefix_tokens: int = 1,
        gs_new: Tuple[int, int] = (),
        interpolation: str = 'bicubic',
        antialias: bool = False,
) -> torch.Tensor:
     
    
    ntok_new = posemb_new.shape[1] - num_prefix_tokens
    ntok_old = posemb.shape[1] - num_prefix_tokens
    gs_old = [int(math.sqrt(ntok_old))] * 2
    if not len(gs_new):  # backwards compatibility
        gs_new = [int(math.sqrt(ntok_new))] * 2
    return resample_abs_pos_embed(
        posemb, gs_new, gs_old,
        num_prefix_tokens=num_prefix_tokens,
        interpolation=interpolation,
        antialias=antialias,
        verbose=True,
    )
def get_intermediate_layers(
            self,
            x: torch.Tensor,
            n: Union[int, List[int], Tuple[int]] = 1,
            reshape: bool = False,
            return_prefix_tokens: bool = False,
            norm: bool = False,
    ) -> List[torch.Tensor]:
         
        
        return self.forward_intermediates(
            x, n,
            return_prefix_tokens=return_prefix_tokens,
            norm=norm,
            output_fmt='NCHW' if reshape else 'NLC',
            features_only=True,
        )
def filter(self, value):
        
        
        self._filter_input = value
        if value is None:
            self._filter = None
            self._filter_key = None
        else:
            new_filter = value.split(':')
            if len(new_filter) == 1:
                self._filter = new_filter[0]
                self._filter_key = None
            else:
                self._filter = new_filter[1]
                self._filter_key = new_filter[0]

        self._filter_re = None
        if self.filter is not None:
            logger.info("Set filter to {0} on key {1}".format(self.filter, self.filter_key))
            # Compute the regular expression
            try:
                self._filter_re = re.compile(self.filter)
                logger.debug("Filter regex compilation OK: {0}".format(self.filter))
            except Exception as e:
                logger.error("Cannot compile filter regex: {0} ({1})".format(self.filter, e))
                self._filter = None
                self._filter_re = None
                self._filter_key = None
def image_has_mask(input_image: np.ndarray) -> bool:
    
    
        
    return (
        input_image.ndim == 3 and 
        input_image.shape[2] == 4 and 
        np.max(input_image[:, :, 3]) > 127
    )
def temperature_unit(self):
        
        return TEMP_CELSIUS
def hvac_mode(self) -> str:
        
        return HVAC_MODE_HEAT
def elastic_project_search(request, project_slug):
    
    queryset = Project.objects.protected(request.user)
    project = get_object_or_404(queryset, slug=project_slug)
    version_slug = request.GET.get('version', LATEST)
    query = request.GET.get('q', None)
    if query:
        user = ''
        if request.user.is_authenticated():
            user = request.user
        log.info(LOG_TEMPLATE.format(
            user=user,
            project=project or '',
            type='inproject',
            version=version_slug or '',
            language='',
            msg=query or '',
        ))

    if query:

        kwargs = {}
        body = {
            "query": {
                "bool": {
                    "should": [
                        {"match": {"title": {"query": query, "boost": 10}}},
                        {"match": {"headers": {"query": query, "boost": 5}}},
                        {"match": {"content": {"query": query}}},
                    ]
                }
            },
            "highlight": {
                "fields": {
                    "title": {},
                    "headers": {},
                    "content": {},
                }
            },
            "fields": ["title", "project", "version", "path"],
            "filter": {
                "and": [
                    {"term": {"project": project_slug}},
                    {"term": {"version": version_slug}},
                ]
            },
            "size": 50  # TODO: Support pagination.
        }

        # Add routing to optimize search by hitting the right shard.
        kwargs['routing'] = project_slug

        results = PageIndex().search(body, **kwargs)
    else:
        results = {}

    if results:
        # pre and post 1.0 compat
        for num, hit in enumerate(results['hits']['hits']):
            for key, val in hit['fields'].items():
                if isinstance(val, list):
                    results['hits']['hits'][num]['fields'][key] = val[0]

    return render_to_response(
        'search/elastic_project_search.html',
        {
            'project': project,
            'query': query,
            'results': results,
        },
        context_instance=RequestContext(request),
    )
def project_download_media(request, project_slug, type_, version_slug):
    
    
    
    # Do private project auth checks
    queryset = Project.objects.protected(request.user).filter(slug=project_slug)
    if not queryset.exists():
        raise Http404
    privacy_level = getattr(settings, 'DEFAULT_PRIVACY_LEVEL', 'public')
    if privacy_level == 'public' or settings.DEBUG:
        path = os.path.join(settings.MEDIA_URL, type_, project_slug, version_slug,
                            '%s.%s' % (project_slug, type_.replace('htmlzip', 'zip')))
        return HttpResponseRedirect(path)
    else:
        # Get relative media path
        path = (queryset[0]
                .get_production_media_path(
                    type_=type_, version_slug=version_slug)
                .replace(settings.PRODUCTION_ROOT, '/prod_artifacts'))
        content_type, encoding = mimetypes.guess_type(path)
        content_type = content_type or 'application/octet-stream'
        response = HttpResponse(content_type=content_type)
        if encoding:
            response["Content-Encoding"] = encoding
        response['X-Accel-Redirect'] = path
        # Include version in filename; this fixes a long-standing bug
        filename = "%s-%s.%s" % (project_slug, version_slug, path.split('.')[-1])
        response['Content-Disposition'] = 'filename=%s' % filename
        return response
def filings(
        self,
        start_date: Annotated[
            Union[datetime.date, None, str],
            OpenBBCustomParameter(
                description="Start date of the data, in YYYY-MM-DD format."
            ),
        ] = None,
        end_date: Annotated[
            Union[datetime.date, None, str],
            OpenBBCustomParameter(
                description="End date of the data, in YYYY-MM-DD format."
            ),
        ] = None,
        form_type: Annotated[
            Optional[str],
            OpenBBCustomParameter(
                description="Filter by form type. Visit https://www.sec.gov/forms for a list of supported form types."
            ),
        ] = None,
        limit: Annotated[
            int,
            OpenBBCustomParameter(description="The number of data entries to return."),
        ] = 100,
        provider: Optional[Literal["fmp"]] = None,
        **kwargs
    ) -> OBBject[List[Data]]:
        
          # noqa: E501

        inputs = filter_inputs(
            provider_choices={
                "provider": provider,
            },
            standard_params={
                "start_date": start_date,
                "end_date": end_date,
                "form_type": form_type,
                "limit": limit,
            },
            extra_params=kwargs,
        )

        return self._run(
            "/equity/discovery/filings",
            **inputs,
        )
def from_embeddings(
        cls,
        text_embeddings: Iterable[Tuple[str, List[float]]],
        embedding: Embeddings,
        metadatas: Optional[Iterable[dict]] = None,
        ids: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> FAISS:
        
        
        texts = [t[0] for t in text_embeddings]
        embeddings = [t[1] for t in text_embeddings]
        return cls.__from(
            texts,
            embeddings,
            embedding,
            metadatas=metadatas,
            ids=ids,
            **kwargs,
        )
def create_trade(stake_amount: float) -> bool:
    
    
    
    logger.info(
        'Checking buy signals to create a new trade with stake_amount: %f ...',
        stake_amount
    )
    whitelist = copy.deepcopy(_CONF['exchange']['pair_whitelist'])
    # Check if stake_amount is fulfilled
    if exchange.get_balance(_CONF['stake_currency']) < stake_amount:
        raise DependencyException(
            'stake amount is not fulfilled (currency={})'.format(_CONF['stake_currency'])
        )

    # Remove currently opened and latest pairs from whitelist
    for trade in Trade.query.filter(Trade.is_open.is_(True)).all():
        if trade.pair in whitelist:
            whitelist.remove(trade.pair)
            logger.debug('Ignoring %s in pair whitelist', trade.pair)
    if not whitelist:
        raise DependencyException('No pair in whitelist')

    # Pick pair based on StochRSI buy signals
    for _pair in whitelist:
        if get_signal(_pair, SignalType.BUY):
            pair = _pair
            break
    else:
        return False

    # Calculate amount and subtract fee
    fee = exchange.get_fee()
    buy_limit = get_target_bid(exchange.get_ticker(pair))
    amount = (1 - fee) * stake_amount / buy_limit

    order_id = exchange.buy(pair, buy_limit, amount)
    # Create trade entity and return
    rpc.send_msg('*{}:* Buying [{}]({}) with limit `{:.8f}`'.format(
        exchange.get_name().upper(),
        pair.replace('_', '/'),
        exchange.get_pair_detail_url(pair),
        buy_limit
    ))
    # Fee is applied twice because we make a LIMIT_BUY and LIMIT_SELL
    trade = Trade(
        pair=pair,
        stake_amount=stake_amount,
        amount=amount,
        fee=fee * 2,
        open_rate=buy_limit,
        open_date=datetime.utcnow(),
        exchange=exchange.get_name().upper(),
        open_order_id=order_id
    )
    Trade.session.add(trade)
    Trade.session.flush()
    return True
def execute_sell(trade: Trade, limit: float) -> None:
    
    
    
    # Execute sell and update trade record
    order_id = exchange.sell(str(trade.pair), limit, trade.amount)
    trade.open_order_id = order_id
    trade.close_date = datetime.utcnow()

    exp_profit = round(trade.calc_profit(limit), 2)
    message = '*{}:* Selling [{}]({}) with limit `{:f} (profit: ~{}%)`'.format(
        trade.exchange,
        trade.pair.replace('_', '/'),
        exchange.get_pair_detail_url(trade.pair),
        limit,
        exp_profit
    )
    logger.info(message)
    telegram.send_msg(message)
def handle_trade(trade: Trade) -> bool:
    
    
    
    if not trade.is_open:
        raise ValueError('attempt to handle closed trade: {}'.format(trade))

    logger.debug('Handling %s ...', trade)
    current_rate = exchange.get_ticker(trade.pair)['bid']
    if should_sell(trade, current_rate, datetime.utcnow()):
        execute_sell(trade, current_rate)
        return True
    return False
def _process(nb_assets: Optional[int] = 0) -> bool:
    
    
    
    state_changed = False
    try:
        # Refresh whitelist based on wallet maintenance
        sanitized_list = refresh_whitelist(
            gen_pair_whitelist(
                _CONF['stake_currency']
            ) if nb_assets else _CONF['exchange']['pair_whitelist']
        )

        # Keep only the subsets of pairs wanted (up to nb_assets)
        final_list = sanitized_list[:nb_assets] if nb_assets else sanitized_list
        _CONF['exchange']['pair_whitelist'] = final_list

        # Query trades from persistence layer
        trades = Trade.query.filter(Trade.is_open.is_(True)).all()
        if len(trades) < _CONF['max_open_trades']:
            try:
                # Create entity and execute trade
                state_changed = create_trade(float(_CONF['stake_amount']))
                if not state_changed:
                    logger.info(
                        'Checked all whitelisted currencies. '
                        'Found no suitable entry positions for buying. Will keep looking ...'
                    )
            except DependencyException as exception:
                logger.warning('Unable to create trade: %s', exception)

        for trade in trades:
            # Get order details for actual price per unit
            if trade.open_order_id:
                # Update trade with order values
                logger.info('Got open order for %s', trade)
                trade.update(exchange.get_order(trade.open_order_id))

            if trade.is_open and trade.open_order_id is None:
                # Check if we can sell our current pair
                state_changed = handle_trade(trade) or state_changed

            Trade.session.flush()
    except (requests.exceptions.RequestException, json.JSONDecodeError) as error:
        logger.warning(
            'Got %s in _process(), retrying in 30 seconds...',
            error
        )
        time.sleep(30)
    except OperationalException:
        rpc.send_msg('*Status:* Got OperationalException:\n```\n{traceback}```{hint}'.format(
            traceback=traceback.format_exc(),
            hint='Issue `/start` if you think it is safe to restart.'
        ))
        logger.exception('Got OperationalException. Stopping trader ...')
        update_state(State.STOPPED)
    return state_changed
def main():
    
    
    
    logger.info('Starting freqtrade %s', __version__)

    global _CONF
    with open('config.json') as file:
        _CONF = json.load(file)

    logger.info('Validating configuration ...')
    validate(_CONF, CONF_SCHEMA)

    init(_CONF)
    old_state = get_state()
    logger.info('Initial State: %s', old_state)
    telegram.send_msg('*Status:* `{}`'.format(old_state.name.lower()))
    while True:
        new_state = get_state()
        # Log state transition
        if new_state != old_state:
            telegram.send_msg('*Status:* `{}`'.format(new_state.name.lower()))
            logging.info('Changing state to: %s', new_state.name)

        if new_state == State.STOPPED:
            time.sleep(1)
        elif new_state == State.RUNNING:
            _process()
            # We need to sleep here because otherwise we would run into bittrex rate limit
            time.sleep(exchange.get_sleep_time())
        old_state = new_state
def gen_pair_whitelist(base_currency: str, topn: int = 20, key: str = 'BaseVolume') -> List[str]:
    
    
    
    summaries = sorted(
        (s for s in exchange.get_market_summaries() if s['MarketName'].startswith(base_currency)),
        key=lambda s: s.get(key) or 0.0,
        reverse=True
    )

    # topn must be greater than 0
    if not topn > 0:
        topn = 20

    return [s['MarketName'].replace('-', '_') for s in summaries[:topn]]
def check_handle_timedout(timeoutvalue: int) -> None:
    
    
    
    timeoutthreashold = arrow.utcnow().shift(minutes=-timeoutvalue).datetime

    for trade in Trade.query.filter(Trade.open_order_id.isnot(None)).all():
        order = exchange.get_order(trade.open_order_id)
        if order['type'] == "LIMIT_BUY" and order['opened'] < timeoutthreashold:
            # Buy timeout - cancel order
            exchange.cancel_order(trade.open_order_id)
            if order['remaining'] == order['amount']:
                # if trade is not partially completed, just delete the trade
                Trade.session.delete(trade)
                Trade.session.flush()
                logger.info('Buy order timeout for %s.', trade)
            else:
                # if trade is partially complete, edit the stake details for the trade
                # and close the order
                trade.amount = order['amount'] - order['remaining']
                trade.stake_amount = trade.amount * trade.open_rate
                trade.open_order_id = None
                logger.info('Partial buy order timeout for %s.', trade)
        elif order['type'] == "LIMIT_SELL" and order['opened'] < timeoutthreashold:
            # Sell timeout - cancel order and update trade
            if order['remaining'] == order['amount']:
                # if trade is not partially completed, just cancel the trade
                exchange.cancel_order(trade.open_order_id)
                trade.close_rate = None
                trade.close_profit = None
                trade.close_date = None
                trade.is_open = True
                trade.open_order_id = None
                logger.info('Sell order timeout for %s.', trade)
                return True
            else:
                # TODO: figure out how to handle partially complete sell orders
                pass
def call(self, x, padding_mask: Optional[tf.Tensor] = None) -> tf.Tensor:
    
    

    seq_len, _, embed_dim = x.shape
    assert embed_dim == self.embed_dim
    if seq_len is None:
      seq_len = 1

    # L x B x D
    residual = x * self.residual_weight

    # L x B x D -> B x D x L
    x = tf.transpose(x, perm=(1, 2, 0))

    # Masking of the tensor
    if padding_mask is not None:
      x = x * tf.cast(tf.expand_dims(padding_mask, axis=1), x.dtype)

    k = self.kernel(seq_len)

    kernel_size = k.shape[1]
    fft_len = seq_len
    s = 0

    if self.bidirectional:
      k1, k2 = tf.split(k, [self.embed_dim, self.embed_dim], axis=0)
      # D x 2*L-1
      padding_l = tf.constant([[0, 0], [kernel_size - 1, 0]])
      padding_r = tf.constant([[0, 0], [0, kernel_size - 1]])
      padding_x = tf.constant([[0, 0], [0, 0], [kernel_size - 1, 0]])
      k = tf.pad(k1, padding_l) + tf.pad(tf.reverse(k2, axis=[-1]), padding_r)
      x = tf.pad(x, padding_x)
      fft_len = fft_len + kernel_size - 1
      s = 2 * kernel_size - 2

    k_f = tf.signal.rfft(
        k, fft_length=tf.constant([2 * fft_len], dtype=tf.int32)
    )
    x_f = tf.signal.rfft(
        x, fft_length=tf.constant([2 * fft_len], dtype=tf.int32)
    )
    # B x D x L
    out = tf.signal.irfft(
        x_f * k_f, fft_length=tf.constant([2 * fft_len], dtype=tf.int32)
    )[..., s : s + seq_len]

    # B x D x L -> L x B x D
    out = tf.nn.silu(tf.transpose(out, perm=(2, 0, 1)) + residual)
    return out
def __init__(self, search_query: SearchQuery):
        
        # init vars
        super().__init__()
        self.search_query = search_query
        self.result_container = ResultContainer()
        self.start_time = None
        self.actual_timeout = None
def extra_state_attributes(self) -> Mapping[str, Any] | None:
        
        if self.entity_description.key != "air_quality":
            return None
        attrs: dict[str, Any] = {}
        attrs[ATTR_TIME] = self.coordinator.data.measured_at
        attrs[ATTR_DOMINENTPOL] = self.coordinator.data.dominant_pollutant

        iaqi = self.coordinator.data.extended_air_quality

        attribute = {
            ATTR_PM2_5: iaqi.pm25,
            ATTR_PM10: iaqi.pm10,
            ATTR_HUMIDITY: iaqi.humidity,
            ATTR_PRESSURE: iaqi.pressure,
            ATTR_TEMPERATURE: iaqi.temperature,
            ATTR_OZONE: iaqi.ozone,
            ATTR_NITROGEN_DIOXIDE: iaqi.nitrogen_dioxide,
            ATTR_SULFUR_DIOXIDE: iaqi.sulfur_dioxide,
        }
        res_attributes = {k: v for k, v in attribute.items() if v is not None}
        return {**attrs, **res_attributes}
def copy(array, out=None, out_device=None, stream=None):
    

    
    _check_cuda_available()
    assert stream is None  # TODO(beam2d): FIX IT

    if out is None:
        if out_device is None:
            out_device = array
        with get_device(out_device):
            out = cupy.empty_like(array)

    with get_device(array):
        cupy.copyto(out, array)

    return out
def get_device(*args):
    

    
    warnings.warn('get_device is deprecated. Please use get_device_from_id or'
                  ' get_device_from_array instead.', DeprecationWarning)

    for arg in args:
        if type(arg) in _integer_types:
            check_cuda_available()
            return Device(arg)
        if isinstance(arg, ndarray):
            if arg.device is None:
                continue
            return arg.device
        if available and isinstance(arg, Device):
            return arg

    return DummyDevice
def get_context(arg=None):
    

    
    device = get_device(arg)
    if device is None:
        return None
    return _contexts[device]
def using_device(*args):
    

    
    for arg in args:
        user = DeviceUser(arg)
        if user.is_active:
            return user
    return DeviceUser(None)
def use_device(arg, pop=True):
    

    
    device = get_device(arg)
    if device is None:
        return
    device.use()
def use_device(arg):
    

    
    device = get_device(arg)
    if device is None:
        return
    device.use()
def empty_like(array):
    
    _check_cuda_available()
    if isinstance(array, cupy.ndarray):
        return cupy.empty_like(array)
    return cupy.empty(array.shape, dtype=array.dtype)
def empty_like(array):
    

    
    _check_cuda_available()
    if isinstance(array, cupy.ndarray):
        return cupy.empty_like(array)
    return cupy.empty(array.shape, dtype=array.dtype)
def to_gpu(array, device=None, stream=None):
    

    
    check_cuda_available()
    if type(device) in _integer_types:
        device = get_device_from_id(device)
    with device:
        array_dev = get_device_from_array(array)
        if array_dev.id == cupy.cuda.device.get_device_id():
            return array

        if stream is not None:
            ret = cupy.empty_like(array)
            mem = None
            if array_dev.id == -1:
                # cpu to gpu
                mem = cupy.cuda.alloc_pinned_memory(array.nbytes)
                src = numpy.frombuffer(
                    mem, array.dtype, array.size).reshape(array.shape)
                src[...] = array
                ret.set(src, stream)
            else:
                # gpu to gpu
                with array_dev:
                    src = array.copy()
                    event = cupy.cuda.Event()
                    event.record()
                stream.wait_event(event)
                ret.data.copy_from_device_async(src.data, src.nbytes, stream)

            # to hold a reference until the end of the asynchronous memcpy
            stream.add_callback(lambda *x: None, (src, mem, ret))

            return ret

        if array_dev.id == -1:
            return cupy.asarray(array)

        # Need to make a copy when an array is copied to another device
        return cupy.array(array, copy=True)
def to_cpu(array, stream=None):
    

    
    if isinstance(array, (list, tuple)):
        d = {}
        ret = []
        for arr in array:
            if arr is None:
                ret.append(None)
            else:
                arr2 = d.get(id(arr))
                if arr2 is None:
                    arr2 = _array_to_cpu(arr, stream)
                    d[id(arr)] = arr2
                ret.append(arr2)
        return type(array)(ret)
    else:
        return _array_to_cpu(array, stream)
def elementwise(in_params, out_params, operation, name, **kwargs):
    

    
    _check_cuda_available()
    return cupy.ElementwiseKernel(
        in_params, out_params, operation, name, **kwargs)
def get_array_module(*args):
    

    
    if available:
        args = [arg.data if isinstance(arg, chainer.Variable) else arg
                for arg in args]
        return cupy.get_array_module(*args)
    else:
        return numpy
def get_entry_meta(
        self,
        entry: str,
        working_dir: Path,
        **kwargs,
    ) -> Dict[str, str]:
        
        target_path = working_dir / PROMPT_FLOW_DIR_NAME / FLOW_META_JSON

        if target_path.is_file():
            entry_meta = read_json_content(target_path, "flow metadata")
            for key in ["inputs", "outputs", "init"]:
                if key not in entry_meta:
                    continue
                for port_name, port in entry_meta[key].items():
                    if "type" in port and isinstance(port["type"], list) and len(port["type"]) == 1:
                        port["type"] = port["type"][0]
            entry_meta.pop("framework", None)
            return entry_meta
        raise UserErrorException("Flow metadata not found.")
def __init__(
        self,
        net,
        in_feature,
        out_channels,
        scale_factors,
        top_block=None,
        norm="LN",
        square_pad=0,
    ):
        
        
        
        super(SimpleFeaturePyramid, self).__init__()
        assert isinstance(net, Backbone)

        self.scale_factors = scale_factors

        input_shapes = net.output_shape()
        strides = [int(input_shapes[in_feature].stride / scale) for scale in scale_factors]
        _assert_strides_are_log2_contiguous(strides)

        dim = input_shapes[in_feature].channels
        self.stages = []
        use_bias = norm == ""
        for idx, scale in enumerate(scale_factors):
            out_dim = dim
            if scale == 4.0:
                layers = [
                    nn.ConvTranspose2d(dim, dim // 2, kernel_size=2, stride=2),
                    get_norm(norm, dim // 2),
                    nn.GELU(),
                    nn.ConvTranspose2d(dim // 2, dim // 4, kernel_size=2, stride=2),
                ]
                out_dim = dim // 4
            elif scale == 2.0:
                layers = [nn.ConvTranspose2d(dim, dim // 2, kernel_size=2, stride=2)]
                out_dim = dim // 2
            elif scale == 1.0:
                layers = []
            elif scale == 0.5:
                layers = [nn.MaxPool2d(kernel_size=2, stride=2)]
            else:
                raise NotImplementedError(f"scale_factor={scale} is not supported yet.")

            layers.extend(
                [
                    Conv2d(
                        out_dim,
                        out_channels,
                        kernel_size=1,
                        bias=use_bias,
                        norm=get_norm(norm, out_channels),
                    ),
                    Conv2d(
                        out_channels,
                        out_channels,
                        kernel_size=3,
                        padding=1,
                        bias=use_bias,
                        norm=get_norm(norm, out_channels),
                    ),
                ]
            )
            layers = nn.Sequential(*layers)

            stage = int(math.log2(strides[idx]))
            self.add_module(f"simfp_{stage}", layers)
            self.stages.append(layers)

        self.net = net
        self.in_feature = in_feature
        self.top_block = top_block
        # Return feature names are "p<stage>", like ["p2", "p3", ..., "p6"]
        self._out_feature_strides = {"p{}".format(int(math.log2(s))): s for s in strides}
        # top block output feature maps.
        if self.top_block is not None:
            for s in range(stage, stage + self.top_block.num_levels):
                self._out_feature_strides["p{}".format(s + 1)] = 2 ** (s + 1)

        self._out_features = list(self._out_feature_strides.keys())
        self._out_feature_channels = {k: out_channels for k in self._out_features}
        self._size_divisibility = strides[-1]
        self._square_pad = square_pad
def technical_404_response(request, exception):
    ""
    try:
        tried = exception.args[0]['tried']
    except (IndexError, TypeError):
        tried = []
    else:
        if not tried:
            # tried exists but is an empty list. The URLconf must've been empty.
            return empty_urlconf(request)

    t = Template(TECHNICAL_404_TEMPLATE)
    c = Context({
        'root_urlconf': settings.ROOT_URLCONF,
        'urlpatterns': tried,
        'reason': str(exception),
        'request': request,
        'request_protocol': os.environ.get("HTTPS") == "on" and "https" or "http",
        'settings': get_safe_settings(),
    })
    return HttpResponseNotFound(t.render(c), mimetype='text/html')
def get_safe_settings():
    
    
    
    settings_dict = {}
    for k in dir(settings):
        if k.isupper():
            settings_dict[k] = cleanse_setting(k, getattr(settings, k))
    return settings_dict
def expand_1_axis(w: np.ndarray,
                  epsilon: float,
                  axis: int) -> np.ndarray:
  
  
  assert axis in (0, -1), (
      "Only support expanding the first or the last dimension. "
      "Got: {}".format(axis))

  rank = len(w.shape)

  d_w = np.random.normal(np.zeros_like(w), np.fabs(w) * epsilon, w.shape)
  d_w = np.repeat(d_w, 2, axis=axis)

  sign_flip = np.array([1, -1])
  for _ in range(rank - 1):
    sign_flip = np.expand_dims(sign_flip, axis=-1 if axis == 0 else 0)
  sign_flip = np.tile(sign_flip,
                      [w.shape[0]] + [1] * (rank - 2) + [w.shape[-1]])

  d_w *= sign_flip
  w_expand = (np.repeat(w, 2, axis=axis) + d_w) / 2
  return w_expand
def event(
        self,
        event: str,
        timeout: Optional[Union[int, float]] = None,
        *,
        condition: Optional[Dict[str, Any]] = None,
    ):
        
        
        if condition is None:
            condition = {}
        condition.update({"__blueprint__": self.name})

        waiters = []
        for app in self.apps:
            waiter = app.signal_router.get_waiter(
                event, condition, exclusive=False
            )
            if not waiter:
                raise NotFound("Could not find signal %s" % event)
            waiters.append(waiter)

        return self._event(waiters, timeout)
def set_login(self, login, remote_url):
        
        try:
            statement = self.connection.cursor()
            statement.execute("INSERT OR REPLACE INTO %s (remote_url, user, token) "
                              "VALUES (?, ?, ?)" % REMOTES_USER_TABLE,
                              (remote_url, login[0], login[1]))
            self.connection.commit()
        except Exception as e:
            raise ConanException("Could not store credentials %s" % str(e))
def get_login(self, remote_url):
          
        with self._connect() as connection:
            try:
                statement = connection.cursor()
                statement.execute('select user, token from %s where remote_url="%s"'
                                  % (REMOTES_USER_TABLE, remote_url))
                rs = statement.fetchone()
                if not rs:
                    return None, None
                name = rs[0]
                token = rs[1]
                return name, token
            except Exception:
                raise ConanException("Couldn't read login\n Try removing '%s' file" % self.dbfile)
def make_atom(self, child, qn, connection):
        
        
        
        lvalue, lookup_type, value_annotation, params_or_value = child
        if isinstance(lvalue, Constraint):
            try:
                lvalue, params = lvalue.process(lookup_type, params_or_value, connection)
            except EmptyShortCircuit:
                raise EmptyResultSet
        elif isinstance(lvalue, Aggregate):
            params = lvalue.field.get_db_prep_lookup(lookup_type, params_or_value, connection)
        else:
            raise TypeError("'make_atom' expects a Constraint or an Aggregate "
                            "as the first item of its 'child' argument.")

        if isinstance(lvalue, tuple):
            # A direct database column lookup.
            field_sql = self.sql_for_columns(lvalue, qn, connection)
        else:
            # A smart object with an as_sql() method.
            field_sql = lvalue.as_sql(qn, connection)

        if value_annotation is datetime.datetime:
            cast_sql = connection.ops.datetime_cast_sql()
        else:
            cast_sql = '%s'

        if hasattr(params, 'as_sql'):
            extra, params = params.as_sql(qn, connection)
            cast_sql = ''
        else:
            extra = ''

        if (len(params) == 1 and params[0] == '' and lookup_type == 'exact'
            and connection.features.interprets_empty_strings_as_nulls):
            lookup_type = 'isnull'
            value_annotation = True

        if lookup_type in connection.operators:
            format = "%s %%s %%s" % (connection.ops.lookup_cast(lookup_type),)
            return (format % (field_sql,
                              connection.operators[lookup_type] % cast_sql,
                              extra), params)

        if lookup_type == 'in':
            if not value_annotation:
                raise EmptyResultSet
            if extra:
                return ('%s IN %s' % (field_sql, extra), params)
            max_in_list_size = connection.ops.max_in_list_size()
            if max_in_list_size and len(params) > max_in_list_size:
                # Break up the params list into an OR of manageable chunks.
                in_clause_elements = ['(']
                for offset in xrange(0, len(params), max_in_list_size):
                    if offset > 0:
                        in_clause_elements.append(' OR ')
                    in_clause_elements.append('%s IN (' % field_sql)
                    group_size = min(len(params) - offset, max_in_list_size)
                    param_group = ', '.join(repeat('%s', group_size))
                    in_clause_elements.append(param_group)
                    in_clause_elements.append(')')
                in_clause_elements.append(')')
                return ''.join(in_clause_elements), params
            else:
                return ('%s IN (%s)' % (field_sql,
                                        ', '.join(repeat('%s', len(params)))),
                        params)
        elif lookup_type in ('range', 'year'):
            return ('%s BETWEEN %%s and %%s' % field_sql, params)
        elif lookup_type in ('month', 'day', 'week_day'):
            return ('%s = %%s' % connection.ops.date_extract_sql(lookup_type, field_sql),
                    params)
        elif lookup_type == 'isnull':
            return ('%s IS %sNULL' % (field_sql,
                (not value_annotation and 'NOT ' or '')), ())
        elif lookup_type == 'search':
            return (connection.ops.fulltext_search_sql(field_sql), params)
        elif lookup_type in ('regex', 'iregex'):
            return connection.ops.regex_lookup(lookup_type) % (field_sql, cast_sql), params

        raise TypeError('Invalid lookup_type: %r' % lookup_type)
def check_required_conan_version(global_conf):
     
    
    required_range_new = global_conf.get("core:required_conan_version")
    if required_range_new:
        validate_conan_version(required_range_new)
def __init__(
      self,
      min_level: int = 2,
      max_level: int = 5,
      target_level: int = 2,
      num_filters: int = 128,
      num_fpn_filters: int = 256,
      activation: str = 'relu',
      kernel_regularizer: Optional[tf_keras.regularizers.Regularizer] = None,
      bias_regularizer: Optional[tf_keras.regularizers.Regularizer] = None,
      **kwargs):

    
    
    if target_level > max_level:
      raise ValueError('target_level should be less than max_level')

    self._config_dict = {
        'min_level': min_level,
        'max_level': max_level,
        'target_level': target_level,
        'num_filters': num_filters,
        'num_fpn_filters': num_fpn_filters,
        'activation': activation,
        'kernel_regularizer': kernel_regularizer,
        'bias_regularizer': bias_regularizer,
    }
    norm = tf_keras.layers.GroupNormalization
    conv2d = tf_keras.layers.Conv2D
    activation_fn = tf_utils.get_activation(activation)
    if tf_keras.backend.image_data_format() == 'channels_last':
      norm_axis = -1
    else:
      norm_axis = 1
    inputs = self._build_inputs(num_fpn_filters, min_level, max_level)

    upscaled_features = []
    for level in range(min_level, max_level + 1):
      num_conv_layers = max(1, level - target_level)
      x = inputs[str(level)]
      for i in range(num_conv_layers):
        x = conv2d(
            filters=num_filters,
            kernel_size=3,
            padding='same',
            kernel_initializer=tf_keras.initializers.VarianceScaling(),
            kernel_regularizer=kernel_regularizer,
            bias_regularizer=bias_regularizer)(x)
        x = norm(groups=32, axis=norm_axis)(x)
        x = activation_fn(x)
        if level != target_level:
          x = spatial_transform_ops.nearest_upsampling(x, scale=2)
      upscaled_features.append(x)

    fused_features = tf.math.add_n(upscaled_features)
    self._output_specs = {str(target_level): fused_features.get_shape()}

    super(PanopticFPNFusion, self).__init__(
        inputs=inputs, outputs=fused_features, **kwargs)
def populate_any_indicators(
        self, pair, df, tf, informative=None, set_generalized_indicators=False
    ):
        
        
        

        coin = pair.split('/')[0]

        with self.freqai.lock:
            if informative is None:
                informative = self.dp.get_pair_dataframe(pair, tf)

            # first loop is automatically duplicating indicators for time periods
            for t in self.freqai_info["feature_parameters"]["indicator_periods_candles"]:

                t = int(t)
                informative[f"%-{coin}rsi-period_{t}"] = ta.RSI(informative, timeperiod=t)
                informative[f"%-{coin}mfi-period_{t}"] = ta.MFI(informative, timeperiod=t)
                informative[f"%-{coin}adx-period_{t}"] = ta.ADX(informative, window=t)
                informative[f"{coin}sma-period_{t}"] = ta.SMA(informative, timeperiod=t)
                informative[f"{coin}ema-period_{t}"] = ta.EMA(informative, timeperiod=t)

                informative[f"%-{coin}mfi-period_{t}"] = ta.MFI(informative, timeperiod=t)

                bollinger = qtpylib.bollinger_bands(
                    qtpylib.typical_price(informative), window=t, stds=2.2
                )
                informative[f"{coin}bb_lowerband-period_{t}"] = bollinger["lower"]
                informative[f"{coin}bb_middleband-period_{t}"] = bollinger["mid"]
                informative[f"{coin}bb_upperband-period_{t}"] = bollinger["upper"]

                informative[f"%-{coin}bb_width-period_{t}"] = (
                    informative[f"{coin}bb_upperband-period_{t}"]
                    - informative[f"{coin}bb_lowerband-period_{t}"]
                ) / informative[f"{coin}bb_middleband-period_{t}"]
                informative[f"%-{coin}close-bb_lower-period_{t}"] = (
                    informative["close"] / informative[f"{coin}bb_lowerband-period_{t}"]
                )

                informative[f"%-{coin}roc-period_{t}"] = ta.ROC(informative, timeperiod=t)

                informative[f"%-{coin}relative_volume-period_{t}"] = (
                    informative["volume"] / informative["volume"].rolling(t).mean()
                )

            informative[f"%-{coin}pct-change"] = informative["close"].pct_change()
            informative[f"%-{coin}raw_volume"] = informative["volume"]
            informative[f"%-{coin}raw_price"] = informative["close"]

            indicators = [col for col in informative if col.startswith("%")]
            # This loop duplicates and shifts all indicators to add a sense of recency to data
            for n in range(self.freqai_info["feature_parameters"]["include_shifted_candles"] + 1):
                if n == 0:
                    continue
                informative_shift = informative[indicators].shift(n)
                informative_shift = informative_shift.add_suffix("_shift-" + str(n))
                informative = pd.concat((informative, informative_shift), axis=1)

            df = merge_informative_pair(df, informative, self.config["timeframe"], tf, ffill=True)
            skip_columns = [
                (s + "_" + tf) for s in ["date", "open", "high", "low", "close", "volume"]
            ]
            df = df.drop(columns=skip_columns)

            # Add generalized indicators here (because in live, it will call this
            # function to populate indicators during training). Notice how we ensure not to
            # add them multiple times
            if set_generalized_indicators:
                df["%-day_of_week"] = (df["date"].dt.dayofweek + 1) / 7
                df["%-hour_of_day"] = (df["date"].dt.hour + 1) / 25

                # user adds targets here by prepending them with &- (see convention below)
                df["&-s_close"] = (
                    df["close"]
                    .shift(-self.freqai_info["feature_parameters"]["label_period_candles"])
                    .rolling(self.freqai_info["feature_parameters"]["label_period_candles"])
                    .mean()
                    / df["close"]
                    - 1
                )

                # If user wishes to use multiple targets, they can add more by
                # appending more columns with '&'. User should keep in mind that multi targets
                # requires a multioutput prediction model such as
                # templates/CatboostPredictionMultiModel.py,

                # df["&-s_range"] = (
                #     df["close"]
                #     .shift(-self.freqai_info["feature_parameters"]["label_period_candles"])
                #     .rolling(self.freqai_info["feature_parameters"]["label_period_candles"])
                #     .max()
                #     -
                #     df["close"]
                #     .shift(-self.freqai_info["feature_parameters"]["label_period_candles"])
                #     .rolling(self.freqai_info["feature_parameters"]["label_period_candles"])
                #     .min()
                # )

        return df
def feature_engineering_expand_basic(self, dataframe: DataFrame, metadata: Dict, **kwargs):
        
        
        
        dataframe["%-pct-change"] = dataframe["close"].pct_change()
        dataframe["%-raw_volume"] = dataframe["volume"]
        dataframe["%-raw_price"] = dataframe["close"]
        return dataframe
def get_array_module(*args):
    

    
    return chainer.backend.get_array_module(*args)
def get_device_from_array(*arrays):
    
    
    for array in arrays:
        if isinstance(array, ndarray) and array.device is not None:
            return array.device
    return DummyDevice
def to_gpu(array, device=None, stream=None):
    

    
    if stream is not None:
        warnings.warn(
            'The stream option is deprecated in chainer.backends.cuda.to_gpu. '
            'Please remove it.', DeprecationWarning)

    check_cuda_available()
    if device is DummyDevice:
        device = cuda.Device()
    else:
        device = _get_device_or_current(device)

    return backend._convert_arrays(
        array, lambda arr: _array_to_gpu(arr, device, stream))
def run_experiments(experiments=None,
                    search_alg=None,
                    scheduler=None,
                    with_server=False,
                    server_port=TuneServer.DEFAULT_PORT,
                    verbose=True,
                    queue_trials=False,
                    trial_executor=None,
                    raise_on_failed_trial=True):
    

    

    if scheduler is None:
        scheduler = FIFOScheduler()

    if search_alg is None:
        search_alg = BasicVariantGenerator()

    search_alg.add_configurations(experiments)

    runner = TrialRunner(
        search_alg,
        scheduler=scheduler,
        launch_web_server=with_server,
        server_port=server_port,
        verbose=verbose,
        queue_trials=queue_trials,
        trial_executor=trial_executor)

    logger.info(runner.debug_string(max_debug=99999))

    last_debug = 0
    while not runner.is_finished():
        runner.step()
        if time.time() - last_debug > DEBUG_PRINT_INTERVAL:
            logger.info(runner.debug_string())
            last_debug = time.time()

    logger.info(runner.debug_string(max_debug=99999))

    wait_for_log_sync()

    errored_trials = []
    for trial in runner.get_trials():
        if trial.status != Trial.TERMINATED:
            errored_trials += [trial]

    if errored_trials:
        if raise_on_failed_trial:
            raise TuneError("Trials did not complete", errored_trials)
        else:
            logger.error("Trials did not complete: %s", errored_trials)

    return runner.get_trials()
def run(run_or_experiment,
        name=None,
        stop=None,
        config=None,
        resources_per_trial=None,
        num_samples=1,
        local_dir=None,
        upload_dir=None,
        trial_name_creator=None,
        loggers=None,
        sync_to_cloud=None,
        sync_to_driver=None,
        checkpoint_freq=0,
        checkpoint_at_end=False,
        export_formats=None,
        max_failures=3,
        restore=None,
        search_alg=None,
        scheduler=None,
        with_server=False,
        server_port=TuneServer.DEFAULT_PORT,
        verbose=2,
        resume=False,
        queue_trials=False,
        reuse_actors=True,
        trial_executor=None,
        raise_on_failed_trial=True,
        return_trials=True,
        ray_auto_init=True,
        sync_function=None):
    
    
    trial_executor = trial_executor or RayTrialExecutor(
        queue_trials=queue_trials,
        reuse_actors=reuse_actors,
        ray_auto_init=ray_auto_init)
    experiment = run_or_experiment
    if not isinstance(run_or_experiment, Experiment):
        run_identifier = Experiment._register_if_needed(run_or_experiment)
        experiment = Experiment(
            name=name,
            run=run_identifier,
            stop=stop,
            config=config,
            resources_per_trial=resources_per_trial,
            num_samples=num_samples,
            local_dir=local_dir,
            upload_dir=upload_dir,
            sync_to_driver=sync_to_driver,
            trial_name_creator=trial_name_creator,
            loggers=loggers,
            checkpoint_freq=checkpoint_freq,
            checkpoint_at_end=checkpoint_at_end,
            export_formats=export_formats,
            max_failures=max_failures,
            restore=restore,
            sync_function=sync_function)
    else:
        logger.debug("Ignoring some parameters passed into tune.run.")

    if sync_to_cloud:
        assert experiment.remote_checkpoint_dir, (
            "Need `upload_dir` if `sync_to_cloud` given.")

    runner = TrialRunner(
        search_alg=search_alg or BasicVariantGenerator(),
        scheduler=scheduler or FIFOScheduler(),
        local_checkpoint_dir=experiment.checkpoint_dir,
        remote_checkpoint_dir=experiment.remote_checkpoint_dir,
        sync_to_cloud=sync_to_cloud,
        resume=resume,
        launch_web_server=with_server,
        server_port=server_port,
        verbose=bool(verbose > 1),
        trial_executor=trial_executor)

    runner.add_experiment(experiment)

    if verbose:
        print(runner.debug_string(max_debug=99999))

    last_debug = 0
    while not runner.is_finished():
        runner.step()
        if time.time() - last_debug > DEBUG_PRINT_INTERVAL:
            if verbose:
                print(runner.debug_string())
            last_debug = time.time()

    if verbose:
        print(runner.debug_string(max_debug=99999))

    wait_for_sync()

    errored_trials = []
    for trial in runner.get_trials():
        if trial.status != Trial.TERMINATED:
            errored_trials += [trial]

    if errored_trials:
        if raise_on_failed_trial:
            raise TuneError("Trials did not complete", errored_trials)
        else:
            logger.error("Trials did not complete: %s", errored_trials)

    if return_trials:
        return runner.get_trials()
    return ExperimentAnalysis(experiment.checkpoint_dir)
def _report_progress(
    runner: TrialRunner, reporter: ProgressReporter, done: bool = False
):
    
    
    trials = runner.get_trials()
    if reporter.should_report(trials, done=done):
        sched_debug_str = runner.scheduler_alg.debug_string()
        executor_debug_str = runner.trial_executor.debug_string()
        reporter.report(trials, done, sched_debug_str, executor_debug_str)
def densenet161(pretrained=False, **kwargs):
    
    
    model = _densenet(
        'densenet161', growth_rate=48, block_config=(6, 12, 36, 24), pretrained=pretrained, **kwargs)
    return model
def __init__(
      self,
      channel_dims_per_stage: List[int],
      blocks_per_stage: List[int],
      strides: int = 1,
      use_sync_bn: bool = True,
      norm_momentum: float = 0.1,
      norm_epsilon: float = 1e-5,
      kernel_initializer: str = 'VarianceScaling',
      kernel_regularizer: Optional[tf_keras.regularizers.Regularizer] = None,
      bias_regularizer: Optional[tf_keras.regularizers.Regularizer] = None,
      **kwargs):
    
    
    super(HourglassBlock, self).__init__(**kwargs)

    if len(channel_dims_per_stage) != len(blocks_per_stage):
      raise ValueError('filter size and residual block repetition '
                       'lists must have the same length')

    self._num_stages = len(channel_dims_per_stage) - 1
    self._channel_dims_per_stage = channel_dims_per_stage
    self._blocks_per_stage = blocks_per_stage
    self._strides = strides
    self._use_sync_bn = use_sync_bn
    self._norm_momentum = norm_momentum
    self._norm_epsilon = norm_epsilon
    self._kernel_initializer = kernel_initializer
    self._kernel_regularizer = kernel_regularizer
    self._bias_regularizer = bias_regularizer

    self._filters = channel_dims_per_stage[0]
    if self._num_stages > 0:
      self._filters_downsampled = channel_dims_per_stage[1]

    self._reps = blocks_per_stage[0]
def fib_iterative(n: int) -> list[int]:
    
    
    
    if n < 0:
        raise Exception("n is negative")
    if n == 0:
        return [0]
    fib = [0, 1]
    for _ in range(n - 1):
        fib.append(fib[-1] + fib[-2])
    return fib
def fib_binet(n: int) -> list[int]:
    
    
    
    if n < 0:
        raise ValueError("n is negative")
    if n >= 1475:
        raise ValueError("n is too large")
    sqrt_5 = sqrt(5)
    phi = (1 + sqrt_5) / 2
    return [round(phi**i / sqrt_5) for i in range(n + 1)]
def _put_object(
        self,
        data: Union[bytes, bytearray],
        client_ref_id: bytes,
        client_id: str,
        owner_id: bytes,
        context=None,
    ):
        
        
        try:
            obj = loads_from_client(data, self)

            if owner_id:
                owner = self.actor_refs[owner_id]
            else:
                owner = None
            with disable_client_hook():
                objectref = ray.put(obj, _owner=owner)
        except Exception as e:
            logger.exception("Put failed:")
            return ray_client_pb2.PutResponse(
                id=b"", valid=False, error=cloudpickle.dumps(e)
            )

        self.object_refs[client_id][objectref.binary()] = objectref
        if len(client_ref_id) > 0:
            self.client_side_ref_map[client_id][client_ref_id] = objectref.binary()
        logger.debug("put: %s" % objectref)
        return ray_client_pb2.PutResponse(id=objectref.binary(), valid=True)
def _on_device_update(self) -> None:
        
        self._on_entry_data_changed()
        if not self._entry_data.available:
            # Only write state if the device has gone unavailable
            # since _on_state_update will be called if the device
            # is available when the full state arrives
            # through the next entity state packet.
            self.async_write_ha_state()
def parse_content_header(value: str) -> Tuple[str, Options]:
    
    
    pos = value.find(";")
    if pos == -1:
        options: Dict[str, Union[int, str]] = {}
    else:
        options = {
            m.group(1)
            .lower(): (m.group(2) or m.group(3))
            .replace("%22", '"')
            .replace("%0D%0A", "\n")
            for m in _param.finditer(value[pos:])
        }
        value = value[:pos]
    return value.strip().lower(), options
def position_encode(embed, sentences):
    

    

    xp = backends.get_array_module(sentences)
    e = embed(sentences)
    n_words, n_units = e.shape[-2:]

    # To avoid 0/0, we use max(length, 1) here.
    # Note that when the length is zero, its embedding is always zero and
    # is ignored.
    length = xp.maximum(
        xp.sum((sentences != 0).astype('f'), axis=-1), 1)
    length = length.reshape((length.shape + (1, 1)))
    k = xp.arange(1, n_units + 1, dtype=numpy.float32) / n_units
    i = xp.arange(1, n_words + 1, dtype=numpy.float32)[:, None]
    coeff = (1 - i / length) - k * (1 - 2.0 * i / length)
    e = coeff * e
    s = F.sum(e, axis=-2)
    return s
def inverse_transform(self, X):
        
        
        return self._transform(X, func=self.inverse_func,
                               kw_args=self.inv_kw_args)
def calc_grid_map_config(ox, oy, xy_resolution):
    
    
    
    min_x = round(min(ox) - EXTEND_AREA / 2.0)
    min_y = round(min(oy) - EXTEND_AREA / 2.0)
    max_x = round(max(ox) + EXTEND_AREA / 2.0)
    max_y = round(max(oy) + EXTEND_AREA / 2.0)
    xw = int(round((max_x - min_x) / xy_resolution))
    yw = int(round((max_y - min_y) / xy_resolution))
    print("The grid map is ", xw, "x", yw, ".")
    return min_x, min_y, max_x, max_y, xw, yw
def shuffle(
        self,
        seeds: Optional[Union[int, Dict[str, int]]] = None,
        seed: Optional[int] = None,
        generators: Optional[Dict[str, np.random.Generator]] = None,
        keep_in_memory: bool = False,
        load_from_cache_file: bool = True,
        indices_cache_file_names: Optional[Dict[str, str]] = None,
        writer_batch_size: Optional[int] = 1000,
    ):
        
        
        self._check_values_type()
        if seed is not None and seeds is not None:
            raise ValueError("Please specify seed or seeds, but not both")
        seeds = seed if seed is not None else seeds
        if seeds is None:
            seeds = {k: None for k in self}
        elif not isinstance(seeds, dict):
            seeds = {k: seeds for k in self}
        if generators is None:
            generators = {k: None for k in self}
        if indices_cache_file_names is None:
            indices_cache_file_names = {k: None for k in self}
        return DatasetDict(
            {
                k: dataset.shuffle(
                    seed=seeds[k],
                    generator=generators[k],
                    keep_in_memory=keep_in_memory,
                    load_from_cache_file=load_from_cache_file,
                    indices_cache_file_name=indices_cache_file_names[k],
                    writer_batch_size=writer_batch_size,
                )
                for k, dataset in self.items()
            }
        )
def load_from_disk(dataset_dict_path: str, fs=None) -> "DatasetDict":
        
        

        
        dataset_dict = DatasetDict()
        if is_remote_filesystem(fs):
            dest_dataset_dict_path = extract_path_from_uri(dataset_dict_path)
        else:
            fs = fsspec.filesystem("file")
            dest_dataset_dict_path = dataset_dict_path
        for k in json.load(
            fs.open(Path(dest_dataset_dict_path).joinpath("dataset_dict.json").as_posix(), "r", encoding="utf-8")
        )["splits"]:
            dataset_dict_split_path = (
                dataset_dict_path.split("://")[0] + "://" + Path(dest_dataset_dict_path).joinpath(k).as_posix()
                if is_remote_filesystem(fs)
                else Path(dest_dataset_dict_path).joinpath(k).as_posix()
            )
            dataset_dict[k] = Dataset.load_from_disk(dataset_dict_split_path, fs)
        return dataset_dict
def push_to_hub(
        self,
        repo_id,
        private: Optional[bool] = False,
        token: Optional[str] = None,
        branch: Optional[None] = None,
        max_shard_size: Union[int, str] = "500MB",
        shard_size: Optional[int] = "deprecated",
        embed_external_files: bool = True,
    ):
        
        
        if shard_size != "deprecated":
            warnings.warn(
                "'shard_size' was renamed to 'max_shard_size' in version 2.1.1 and will be removed in 2.4.0.",
                FutureWarning,
            )
            max_shard_size = shard_size

        self._check_values_type()
        total_uploaded_size = 0
        total_dataset_nbytes = 0
        info_to_dump: DatasetInfo = next(iter(self.values())).info.copy()
        dataset_name = repo_id.split("/")[-1]
        info_to_dump.splits = SplitDict(dataset_name=dataset_name)
        for split in self.keys():
            logger.warning(f"Pushing split {split} to the Hub.")
            # The split=key needs to be removed before merging
            repo_id, split, uploaded_size, dataset_nbytes = self[split]._push_parquet_shards_to_hub(
                repo_id,
                split=split,
                private=private,
                token=token,
                branch=branch,
                max_shard_size=max_shard_size,
                embed_external_files=embed_external_files,
            )
            total_uploaded_size += uploaded_size
            total_dataset_nbytes += dataset_nbytes
            info_to_dump.splits[split] = SplitInfo(
                str(split), num_bytes=dataset_nbytes, num_examples=len(self[split]), dataset_name=dataset_name
            )
        organization, dataset_name = repo_id.split("/")
        info_to_dump.download_checksums = None
        info_to_dump.download_size = total_uploaded_size
        info_to_dump.dataset_size = total_dataset_nbytes
        info_to_dump.size_in_bytes = total_uploaded_size + total_dataset_nbytes
        buffer = BytesIO()
        buffer.write(f'{{"{organization}--{dataset_name}": '.encode())
        info_to_dump._dump_info(buffer)
        buffer.write(b"}")
        HfApi(endpoint=config.HF_ENDPOINT).upload_file(
            path_or_fileobj=buffer.getvalue(),
            path_in_repo=config.DATASETDICT_INFOS_FILENAME,
            repo_id=repo_id,
            token=token,
            repo_type="dataset",
            revision=branch,
            identical_ok=True,
        )
def cast_(self, features: Features):
        
        
        self._check_values_type()
        new_dataset_dict = {k: dataset.cast(features=features) for k, dataset in self.items()}
        self.update(new_dataset_dict)
def inverse_transform(self, X=None, *, Xt=None):
        
        
        
        X = _deprecate_Xt_in_inverse_transform(X, Xt)

        check_is_fitted(self)

        unil, inverse = np.unique(self.labels_, return_inverse=True)
        return X[..., inverse]
def _fetch_keyspaces(self, keyspaces: Optional[List[str]] = None) -> List[str]:
        
        
        
        all_keyspaces = self.fetch_all(
            "SELECT keyspace_name FROM system_schema.keyspaces"
        )

        # Filtering keyspaces based on 'keyspace_list' and '_exclude_keyspaces'
        filtered_keyspaces = []
        for ks in all_keyspaces:
            if not isinstance(ks, Dict):
                continue  # Skip if the row is not a dictionary.

            keyspace_name = ks["keyspace_name"]
            if keyspaces and keyspace_name in keyspaces:
                filtered_keyspaces.append(keyspace_name)
            elif not keyspaces and keyspace_name not in self._exclude_keyspaces:
                filtered_keyspaces.append(keyspace_name)

        return filtered_keyspaces
def __init__(
        self,
        embedding: Embeddings,
        *,
        distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,
        table_name: str = "embeddings",
        content_field: str = "content",
        metadata_field: str = "metadata",
        vector_field: str = "vector",
        id_field: str = "id",
        use_vector_index: bool = False,
        vector_index_name: str = "",
        vector_index_options: Optional[dict] = None,
        vector_size: int = 1536,
        use_full_text_search: bool = False,
        pool_size: int = 5,
        max_overflow: int = 10,
        timeout: float = 30,
        **kwargs: Any,
    ):
        
        

        self.embedding = embedding
        self.distance_strategy = distance_strategy
        self.table_name = self._sanitize_input(table_name)
        self.content_field = self._sanitize_input(content_field)
        self.metadata_field = self._sanitize_input(metadata_field)
        self.vector_field = self._sanitize_input(vector_field)
        self.id_field = self._sanitize_input(id_field)

        self.use_vector_index = bool(use_vector_index)
        self.vector_index_name = self._sanitize_input(vector_index_name)
        self.vector_index_options = dict(vector_index_options or {})
        self.vector_index_options["metric_type"] = self.distance_strategy
        self.vector_size = int(vector_size)

        self.use_full_text_search = bool(use_full_text_search)

        # Pass the rest of the kwargs to the connection.
        self.connection_kwargs = kwargs

        # Add program name and version to connection attributes.
        if "conn_attrs" not in self.connection_kwargs:
            self.connection_kwargs["conn_attrs"] = dict()

        self.connection_kwargs["conn_attrs"]["_connector_name"] = "langchain python sdk"
        self.connection_kwargs["conn_attrs"]["_connector_version"] = "2.0.0"

        # Create connection pool.
        self.connection_pool = QueuePool(
            self._get_connection,
            max_overflow=max_overflow,
            pool_size=pool_size,
            timeout=timeout,
        )
        self._create_table()
def load_checkpoint(
        self,
        path: _PATH,
        state: Optional[Union[Module, Optimizer, Dict[str, Union[Module, Optimizer, Any]]]] = None,
        strict: bool = True,
    ) -> Dict[str, Any]:
        
        if not state:
            raise ValueError(
                f"Got FSDPStrategy.load_checkpoint(..., state={state!r}) but a state with at least "
                f" a model instance to reload is required. Pass it in like so:"
                " FSDPStrategy.load_checkpoint(..., state={'model': model, ...})"
            )
        # broadcast the path from rank 0 to ensure all the states are loaded from a common path
        path = Path(self.broadcast(path))

        if isinstance(state, Module):
            from lightning.fabric.strategies.model_parallel import _load_raw_module_state_from_path

            _load_raw_module_state_from_path(path, module=state, world_size=self.world_size, strict=strict)
            return {}

        if isinstance(state, Optimizer):
            raise NotImplementedError(
                "Loading a single optimizer object from a checkpoint is not supported yet with the FSDP strategy."
            )

        from torch.distributed.checkpoint.optimizer import load_sharded_optimizer_state_dict
        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
        from torch.distributed.fsdp import OptimStateKeyType

        modules = {key: module for key, module in state.items() if _has_fsdp_modules(module)}
        if len(modules) == 0:
            raise ValueError(
                "Could not find a FSDP model in the provided checkpoint state. Please provide the model as"
                " part of the state like so: `load_checkpoint(..., state={'model': model, ...})`. Make sure"
                " you set up the model (and optimizers if any) through the strategy before loading the checkpoint."
            )
        optimizers = {key: optim for key, optim in state.items() if isinstance(optim, Optimizer)}
        if len(modules) > 1:
            raise ValueError(
                "Found multiple FSDP models in the given state. Loading checkpoints with FSDP is"
                " currently limited to a single model per checkpoint. To load multiple models, call the"
                " load method for each model separately with a different path."
            )
        module_key, module = list(modules.items())[0]

        if _is_sharded_checkpoint(path):
            state_dict_ctx = _get_sharded_state_dict_context(module)

            with state_dict_ctx:
                module_state = {module_key: module.state_dict()}
                _distributed_checkpoint_load(module_state, path)
                module.load_state_dict(module_state[module_key], strict=strict)

                if optimizers:
                    from torch.distributed.checkpoint import FileSystemReader

                    # TODO: replace with newer APIs
                    # https://github.com/pytorch/pytorch/issues/119800#issuecomment-1942156271
                    reader = FileSystemReader(path=path)
                    # the optimizer states must be loaded separately
                    for optim_key, optim in optimizers.items():
                        optim_state = load_sharded_optimizer_state_dict(
                            model_state_dict=module_state[module_key],
                            optimizer_key=optim_key,
                            storage_reader=reader,
                        )
                        flattened_osd = FSDP.optim_state_dict_to_load(
                            optim_state_dict=optim_state[optim_key],
                            model=module,
                            optim=optim,
                        )
                        optim.load_state_dict(flattened_osd)

            # Load metadata (anything not a module or optimizer)
            metadata = torch.load(path / _METADATA_FILENAME)
            requested_metadata_keys = state.keys() - modules.keys() - optimizers.keys()
            _validate_keys_for_strict_loading(requested_metadata_keys, metadata.keys(), strict=strict)
            for key in requested_metadata_keys:
                if key not in metadata:
                    continue
                state[key] = metadata.pop(key)

            # return the remaining metadata that wasn't requested as part of `state`
            return metadata

        if _is_full_checkpoint(path):
            checkpoint = _lazy_load(path)

            from lightning.fabric.strategies.model_parallel import _load_raw_module_state

            _load_raw_module_state(checkpoint.pop(module_key), module=module, world_size=self.world_size, strict=strict)

            if isinstance(state, Module):
                return {}

            # Materialize lazy tensors if there are any left in the checkpoint
            # The `torch.Optimizer.load_state_dict` method can't load lazy tensors because of deepcopy pickle issues
            checkpoint = _materialize_tensors(checkpoint)

            # Load optimizer states
            for optim_key, optim in optimizers.items():
                # rank0_only should be false because we need to load the optimizer state on all ranks
                with _get_full_state_dict_context(module, world_size=self.world_size, rank0_only=False):
                    temp_state_dict = checkpoint.pop(optim_key)

                    # Handling the case where the optimizer state is saved from a normal optimizer
                    if isinstance(list(temp_state_dict["state"].keys())[0], int):
                        temp_state_dict = FSDP.rekey_optim_state_dict(
                            temp_state_dict, OptimStateKeyType.PARAM_NAME, module
                        )

                    optim_state_dict = FSDP.optim_state_dict_to_load(
                        optim_state_dict=temp_state_dict,
                        model=module,
                        optim=optim,
                    )
                    optim.load_state_dict(optim_state_dict)

            requested_metadata_keys = state.keys() - modules.keys() - optimizers.keys()
            _validate_keys_for_strict_loading(requested_metadata_keys, checkpoint.keys(), strict=strict)

            # Load metadata (anything not a module or optimizer)
            _move_state_into(source=checkpoint, destination=state, keys=requested_metadata_keys)

            # return the remaining metadata that wasn't requested as part of `state`
            return checkpoint

        raise ValueError(
            f"The path {str(path)!r} does not point to a valid checkpoint. Make sure the path points to either a"
            " directory with FSDP checkpoint shards, or a single file with a full checkpoint."
        )
def save_checkpoint(
        self, path: _PATH, state: Dict[str, Union[Module, Optimizer, Any]], storage_options: Optional[Any] = None
    ) -> None:
        
        
        if not _TORCH_GREATER_EQUAL_2_0:
            raise NotImplementedError(
                "Saving and loading checkpoints with the `FSDPStrategy` is not supported in PyTorch < 2.0."
                " Please upgrade `torch` or file an issue: `https://github.com/Lightning-AI/lightning/issues`."
            )
        if storage_options is not None:
            raise TypeError(
                "`FSDPStrategy.save_checkpoint(..., storage_options=...)` is not supported because"
                " `FSDPStrategy` does not use the `CheckpointIO`."
            )
        # broadcast the path from rank 0 to ensure all the states are saved in a common path
        path = Path(self.broadcast(path))
        if path.is_dir() and os.listdir(path):
            raise FileExistsError(f"The checkpoint directory already exists and is not empty: {path}")

        from torch.distributed.checkpoint import FileSystemWriter, save_state_dict
        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP

        modules = [module for module in state.values() if isinstance(module, FSDP)]
        if len(modules) == 0:
            raise ValueError(
                "Could not find a FSDP model in the provided checkpoint state. Please provide the model as"
                " part of the state like so: `save_checkpoint(..., state={'model': model, ...})`. Make sure"
                " you set up the model (and optimizers if any) through the strategy before saving the checkpoint."
            )
        if len(modules) > 1:
            raise ValueError(
                "Found multiple FSDP modules in the given state. Saving checkpoints with FSDP is"
                " currently limited to a single model per checkpoint. To save multiple models, call the"
                " save method for each model separately with a different path."
            )

        module = modules[0]

        if self._state_dict_type == "sharded":
            path.mkdir(parents=True, exist_ok=True)
            state_dict_ctx = _get_sharded_state_dict_context(module)

            # replace the modules and optimizer objects in the state with their local state dict
            # and separate the user's metadata
            converted_state = {}
            metadata = {}
            with state_dict_ctx:
                for key, obj in state.items():
                    if isinstance(obj, FSDP):
                        converted_state[key] = obj.state_dict()
                    elif isinstance(obj, Optimizer):
                        converted_state[key] = FSDP.optim_state_dict(module, obj)
                    else:  # everything not a module or optimizer is considered metadata
                        metadata[key] = obj

            # FSDP's FileSystemWriter streams the tensors to disk to minimize memory peaks
            writer = FileSystemWriter(path=path, single_file_per_rank=True)
            save_state_dict(converted_state, writer)

            if self.global_rank == 0:
                torch.save(metadata, path / "meta.pt")

        elif self._state_dict_type == "full":
            state_dict_ctx = _get_full_state_dict_context(module)
            full_state = {}
            with state_dict_ctx:
                for key, obj in state.items():
                    if isinstance(obj, FSDP):
                        full_state[key] = obj.state_dict()
                    elif isinstance(obj, Optimizer):
                        full_state[key] = FSDP.optim_state_dict(module, obj)
                    else:  # everything not a module or optimizer is considered metadata
                        full_state[key] = obj  # type: ignore[assignment]

            if self.global_rank == 0:
                torch.save(full_state, path)
        else:
            raise ValueError(f"Unknown state_dict_type: {self._state_dict_type}")
def _load_raw_module_state(state_dict: Dict[str, Any], module: Module, strict: bool = True) -> None:
    

    with _get_full_state_dict_context(module, rank0_only=False):
        module.load_state_dict(state_dict, strict=strict)
def export(self, out_f=None, format='mp3', codec=None, bitrate=None, parameters=None, tags=None, id3v2_version='4',
               cover=None):
        
        
        
        id3v2_allowed_versions = ['3', '4']

        if format == "raw" and (codec is not None or parameters is not None):
            raise AttributeError(
                    'Can not invoke ffmpeg when export format is "raw"; '
                    'specify an ffmpeg raw format like format="s16le" instead '
                    'or call export(format="raw") with no codec or parameters')

        out_f, _ = _fd_or_path_or_tempfile(out_f, 'wb+')
        out_f.seek(0)

        if format == "raw":
            out_f.write(self._data)
            out_f.seek(0)
            return out_f

        # wav with no ffmpeg parameters can just be written directly to out_f
        easy_wav = format == "wav" and codec is None and parameters is None

        if easy_wav:
            data = out_f
        else:
            data = NamedTemporaryFile(mode="wb", delete=False)

        pcm_for_wav = self._data
        if self.sample_width == 1:
            # convert to unsigned integers for wav
            pcm_for_wav = audioop.bias(self._data, 1, 128)

        wave_data = wave.open(data, 'wb')
        wave_data.setnchannels(self.channels)
        wave_data.setsampwidth(self.sample_width)
        wave_data.setframerate(self.frame_rate)
        # For some reason packing the wave header struct with
        # a float in python 2 doesn't throw an exception
        wave_data.setnframes(int(self.frame_count()))
        wave_data.writeframesraw(pcm_for_wav)
        wave_data.close()

        # for easy wav files, we're done (wav data is written directly to out_f)
        if easy_wav:
            return out_f

        output = NamedTemporaryFile(mode="w+b", delete=False)

        # build converter command to export
        conversion_command = [
            self.converter,
            '-y',  # always overwrite existing files
            "-f", "wav", "-i", data.name,  # input options (filename last)
        ]

        if codec is None:
            codec = self.DEFAULT_CODECS.get(format, None)

        if cover is not None:
            if cover.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')) and format == "mp3":
                conversion_command.extend(["-i", cover, "-map", "0", "-map", "1", "-c:v", "mjpeg"])
            else:
                raise AttributeError(
                    "Currently cover images are only supported by MP3 files. The allowed image formats are: .tif, .jpg, .bmp, .jpeg and .png.")

        if codec is not None:
            # force audio encoder
            conversion_command.extend(["-acodec", codec])

        if bitrate is not None:
            conversion_command.extend(["-b:a", bitrate])

        if parameters is not None:
            # extend arguments with arbitrary set
            conversion_command.extend(parameters)

        if tags is not None:
            if not isinstance(tags, dict):
                raise InvalidTag("Tags must be a dictionary.")
            else:
                # Extend converter command with tags
                # print(tags)
                for key, value in tags.items():
                    conversion_command.extend(
                        ['-metadata', '{0}={1}'.format(key, value)])

                if format == 'mp3':
                    # set id3v2 tag version
                    if id3v2_version not in id3v2_allowed_versions:
                        raise InvalidID3TagVersion(
                            "id3v2_version not allowed, allowed versions: %s" % id3v2_allowed_versions)
                    conversion_command.extend([
                        "-id3v2_version", id3v2_version
                    ])

        if sys.platform == 'darwin' and codec == 'mp3':
            conversion_command.extend(["-write_xing", "0"])

        conversion_command.extend([
            "-f", format, output.name,  # output options (filename last)
        ])

        log_conversion(conversion_command)

        # read stdin / write stdout
        with open(os.devnull, 'rb') as devnull:
            p = subprocess.Popen(conversion_command, stdin=devnull, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        p_out, p_err = p.communicate()

        log_subprocess_output(p_out)
        log_subprocess_output(p_err)

        if p.returncode != 0:
            raise CouldntEncodeError(
                "Encoding failed. ffmpeg/avlib returned error code: {0}\n\nCommand:{1}\n\nOutput from ffmpeg/avlib:\n\n{2}".format(
                    p.returncode, conversion_command, p_err.decode(errors='ignore') ))

        output.seek(0)
        out_f.write(output.read())

        data.close()
        output.close()

        os.unlink(data.name)
        os.unlink(output.name)

        out_f.seek(0)
        return out_f
def overlay(self, seg, position=0, loop=False, times=None, gain_during_overlay=None):
        
        
        

        if loop:
            # match loop=True's behavior with new times (count) mechinism.
            times = -1
        elif times is None:
            # no times specified, just once through
            times = 1
        elif times == 0:
            # it's a no-op, make a copy since we never mutate
            return self._spawn(self._data)

        output = StringIO()

        seg1, seg2 = AudioSegment._sync(self, seg)
        sample_width = seg1.sample_width
        spawn = seg1._spawn

        output.write(seg1[:position]._data)

        # drop down to the raw data
        seg1 = seg1[position:]._data
        seg2 = seg2._data
        pos = 0
        seg1_len = len(seg1)
        seg2_len = len(seg2)
        while times:
            remaining = max(0, seg1_len - pos)
            if seg2_len >= remaining:
                seg2 = seg2[:remaining]
                seg2_len = remaining
                # we've hit the end, we're done looping (if we were) and this
                # is our last go-around
                times = 1

            if gain_during_overlay:
                seg1_overlaid = seg1[pos:pos + seg2_len]
                seg1_adjusted_gain = audioop.mul(seg1_overlaid, self.sample_width,
                                               db_to_float(float(gain_during_overlay)))
                output.write(audioop.add(seg1_adjusted_gain, seg2, sample_width))
            else:
                output.write(audioop.add(seg1[pos:pos + seg2_len], seg2,
                                     sample_width))
            pos += seg2_len

            # dec times to break our while loop (eventually)
            times -= 1

        output.write(seg1[pos:])

        return spawn(data=output)
def silent(cls, duration=1000, frame_rate=11025):
        
        
        
        frames = int(frame_rate * (duration / 1000.0))
        data = b"\0\0" * frames
        return cls(data, metadata={"channels": 1,
                                   "sample_width": 2,
                                   "frame_rate": frame_rate,
                                   "frame_width": 2})
def validate_api_key(api_key):
    
    
    
    api_key_db = ApiKey.get(api_key)

    if not api_key_db.enabled:
        raise exceptions.ApiKeyDisabledError('API key is disabled.')

    LOG.audit('API key with id "%s" is validated.' % (api_key_db.id))

    return api_key_db
def generate_api_key():
    
    
    
    # 256bit seed from urandom
    seed = os.urandom(256)

    # since urandom does not provide sufficient entropy hash, base64encode and salt.
    # The resulting value is now large and should be hard to predict.
    hashed_seed = hashlib.sha256(seed).hexdigest()

    base64_encoded = base64.b64encode(
        six.b(hashed_seed),
        six.b(random.choice(['rA', 'aZ', 'gQ', 'hH', 'hG', 'aR', 'DD']))).rstrip(b'==')
    base64_encoded = base64_encoded.decode()
    return base64_encoded
def get(
        self,
        indices: Optional[Union[int, slice, List[int]]] = None,
        *,
        neg_indices_left_of_zero: bool = False,
        fill: Optional[Any] = None,
        one_hot_discrete: bool = False,
        _ignore_last_ts: bool = False,
        _add_last_ts_value: Optional[Any] = None,
    ):
        
        
        if indices is None:
            data = self._get_all_data(
                one_hot_discrete=one_hot_discrete,
                _ignore_last_ts=_ignore_last_ts,
            )
        elif isinstance(indices, slice):
            data = self._get_slice(
                indices,
                fill=fill,
                neg_indices_left_of_zero=neg_indices_left_of_zero,
                one_hot_discrete=one_hot_discrete,
                _ignore_last_ts=_ignore_last_ts,
                _add_last_ts_value=_add_last_ts_value,
            )
        elif isinstance(indices, list):
            data = [
                self._get_int_index(
                    idx,
                    fill=fill,
                    neg_indices_left_of_zero=neg_indices_left_of_zero,
                    one_hot_discrete=one_hot_discrete,
                    _ignore_last_ts=_ignore_last_ts,
                    _add_last_ts_value=_add_last_ts_value,
                )
                for idx in indices
            ]
            if self.finalized:
                data = batch(data)
        else:
            assert isinstance(indices, int)
            data = self._get_int_index(
                indices,
                fill=fill,
                neg_indices_left_of_zero=neg_indices_left_of_zero,
                one_hot_discrete=one_hot_discrete,
                _ignore_last_ts=_ignore_last_ts,
                _add_last_ts_value=_add_last_ts_value,
            )

        return data
def __add__(
        self, other: Union[List, "InfiniteLookbackBuffer", int, float, complex]
    ) -> "InfiniteLookbackBuffer":
        
        

        if self.finalized:
            raise RuntimeError(f"Cannot `add` to a finalized {type(self).__name__}.")
        else:
            # If `other` is an int, simply add it to all our values (if possible) and
            # use the result as the underlying data for the returned buffer.
            if isinstance(other, (int, float, complex)):
                data = [
                    (d + other) if isinstance(d, (int, float, complex)) else d
                    for d in self.data
                ]
            # If `other` is a InfiniteLookbackBuffer itself, do NOT include its
            # lookback buffer anymore. We assume that `other`'s lookback buffer i
            # already at the end of `self`.
            elif isinstance(other, InfiniteLookbackBuffer):
                data = self.data + other.data[other.lookback :]
            # `other` is a list, simply concat the two lists and use the result as
            # the underlying data for the returned buffer.
            else:
                data = self.data + other

            return InfiniteLookbackBuffer(
                data=data,
                lookback=self.lookback,
                space=self.space,
            )
def load_environment_vars(self, prefix=SANIC_PREFIX):
        
        
        
        for key, value in environ.items():
            if not key.startswith(prefix) or not key.isupper():
                continue

            _, config_key = key.split(prefix, 1)

            for converter in reversed(self._converters):
                try:
                    self[config_key] = converter(value)
                    break
                except ValueError:
                    pass
def from_object(self, obj):
        
        
        if isinstance(obj, str):
            obj = import_string(obj)
        for key in dir(obj):
            if key.isupper():
                self[key] = getattr(obj, key)
def update(self, *args, **kwargs):
        
        super().update(*args, **kwargs)
        self.save()
def get_git_branch():
    
    
    
    if is_git_dir():
        with contextlib.suppress(subprocess.CalledProcessError):
            origin = subprocess.check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])
            return origin.decode().strip()
def yaml_save(file='data.yaml', data=None, header=''):
    
    
    
    if data is None:
        data = {}
    file = Path(file)
    if not file.parent.exists():
        # Create parent directories if they don't exist
        file.parent.mkdir(parents=True, exist_ok=True)

    # Convert Path objects to strings
    for k, v in data.items():
        if isinstance(v, Path):
            data[k] = str(v)

    # Dump data to file in YAML format
    with open(file, 'w') as f:
        if header:
            f.write(header)
        yaml.safe_dump(data, f, sort_keys=False, allow_unicode=True)
def colorstr(*input):
    
    
    
    *args, string = input if len(input) > 1 else ('blue', 'bold', input[0])  # color arguments, string
    colors = {
        'black': '\033[30m',  # basic colors
        'red': '\033[31m',
        'green': '\033[32m',
        'yellow': '\033[33m',
        'blue': '\033[34m',
        'magenta': '\033[35m',
        'cyan': '\033[36m',
        'white': '\033[37m',
        'bright_black': '\033[90m',  # bright colors
        'bright_red': '\033[91m',
        'bright_green': '\033[92m',
        'bright_yellow': '\033[93m',
        'bright_blue': '\033[94m',
        'bright_magenta': '\033[95m',
        'bright_cyan': '\033[96m',
        'bright_white': '\033[97m',
        'end': '\033[0m',  # misc
        'bold': '\033[1m',
        'underline': '\033[4m'}
    return ''.join(colors[x] for x in args) + f'{string}' + colors['end']
def _generate_filter_for_columns(
        self, columns: Iterable[Column], encoder: Callable[[Any], Any]
    ) -> ColumnElement:
        
        
        i_domains = _domain_matcher(self.included_domains, columns, encoder)
        i_entities = _entity_matcher(self.included_entities, columns, encoder)
        i_entity_globs = _globs_to_like(self.included_entity_globs, columns, encoder)
        includes = [i_domains, i_entities, i_entity_globs]

        e_domains = _domain_matcher(self.excluded_domains, columns, encoder)
        e_entities = _entity_matcher(self.excluded_entities, columns, encoder)
        e_entity_globs = _globs_to_like(self.excluded_entity_globs, columns, encoder)
        excludes = [e_domains, e_entities, e_entity_globs]

        have_exclude = self._have_exclude
        have_include = self._have_include

        # Case 1 - No filter
        # - All entities included
        if not have_include and not have_exclude:
            raise RuntimeError(
                "No filter configuration provided, check has_config before calling this method."
            )

        # Case 2 - Only includes
        # - Entity listed in entities include: include
        # - Otherwise, entity matches domain include: include
        # - Otherwise, entity matches glob include: include
        # - Otherwise: exclude
        if have_include and not have_exclude:
            return or_(*includes).self_group()

        # Case 3 - Only excludes
        # - Entity listed in exclude: exclude
        # - Otherwise, entity matches domain exclude: exclude
        # - Otherwise, entity matches glob exclude: exclude
        # - Otherwise: include
        if not have_include and have_exclude:
            return not_(or_(*excludes).self_group())

        # Case 4 - Domain and/or glob includes (may also have excludes)
        # - Entity listed in entities include: include
        # - Otherwise, entity listed in entities exclude: exclude
        # - Otherwise, entity matches glob include: include
        # - Otherwise, entity matches glob exclude: exclude
        # - Otherwise, entity matches domain include: include
        # - Otherwise: exclude
        if self.included_domains or self.included_entity_globs:
            return or_(
                i_entities,
                # https://github.com/sqlalchemy/sqlalchemy/issues/9190
                # pylint: disable-next=invalid-unary-operand-type
                (~e_entities & (i_entity_globs | (~e_entity_globs & i_domains))),
            ).self_group()

        # Case 5 - Domain and/or glob excludes (no domain and/or glob includes)
        # - Entity listed in entities include: include
        # - Otherwise, entity listed in exclude: exclude
        # - Otherwise, entity matches glob exclude: exclude
        # - Otherwise, entity matches domain exclude: exclude
        # - Otherwise: include
        if self.excluded_domains or self.excluded_entity_globs:
            return (not_(or_(*excludes)) | i_entities).self_group()  # type: ignore[no-any-return, no-untyped-call]

        # Case 6 - No Domain and/or glob includes or excludes
        # - Entity listed in entities include: include
        # - Otherwise: exclude
        return i_entities
def _compute_second_stage_input_feature_maps(self, features_to_crop,
                                               proposal_boxes_normalized,
                                               image_shape,
                                               num_proposals,
                                               context_features,
                                               valid_context_size):
    
    
    del image_shape
    box_features = self._crop_and_resize_fn(
        features_to_crop, proposal_boxes_normalized, None,
        [self._initial_crop_size, self._initial_crop_size])

    flattened_box_features = self._flatten_first_two_dimensions(box_features)

    flattened_box_features = self._maxpool_layer(flattened_box_features)

    if self._attention_position == (
        faster_rcnn_pb2.AttentionPosition.POST_RPN):
      attention_features = self._context_feature_extract_fn(
          box_features=flattened_box_features,
          num_proposals=num_proposals,
          context_features=context_features,
          valid_context_size=valid_context_size)

      # Adds box features with attention features.
      flattened_box_features += self._flatten_first_two_dimensions(
          attention_features)

    return flattened_box_features
def _sample_drop_mask(self, w):
        
        

        if self.training:

            # Sample new masks when needed
            if self.drop_mask_cnt + self.batch_size > self.N_drop_masks:
                self.drop_mask_cnt = 0
                self.drop_masks = self.drop(
                    torch.ones(
                        self.N_drop_masks, self.hidden_size * 2, device=w.device
                    )
                ).data

            # Sampling the mask
            drop_mask = self.drop_masks[
                self.drop_mask_cnt : self.drop_mask_cnt + self.batch_size
            ]
            self.drop_mask_cnt = self.drop_mask_cnt + self.batch_size

        else:
            self.drop_mask_te = self.drop_mask_te.to(w.device)
            drop_mask = self.drop_mask_te

        return drop_mask
def _makeArchive(root_path: str) -> Optional[bytes]:
        
        
        
        parent_folder = os.path.dirname(root_path)
        contents = os.walk(root_path)
        try:
            buffer = io.BytesIO()
            archive = ZipFile(buffer, "w", ZIP_DEFLATED)
            for root, folders, files in contents:
                for folder_name in folders:
                    # Add all folders, even empty ones.
                    absolute_path = os.path.join(root, folder_name)
                    relative_path = absolute_path.replace(parent_folder + '\\', '')
                    archive.write(relative_path)
                for file_name in files:
                    # Add all files.
                    absolute_path = os.path.join(root, file_name)
                    relative_path = absolute_path.replace(parent_folder + '\\', '')
                    archive.write(relative_path)
            archive.close()
            return buffer.getvalue()
        except (IOError, OSError, BadZipfile) as error:
            Logger.log("e", "Could not create archive from user data directory: %s", error)
            return None
def reducer_override(self, obj):
        
        
        if isinstance(obj, self.source_type):
            index = len(self._found)
            self._found.append(obj)
            return _get_node, (id(self), index)

        return super().reducer_override(obj)
def submit(cluster_config_file, docker, screen, tmux, stop, start,
           cluster_name, port_forward, script, args, script_args):
    
    
    assert not (screen and tmux), "Can specify only one of `screen` or `tmux`."
    assert not (script_args and args), "Use -- --arg1 --arg2 for script args."

    if args:
        logger.warning(
            "ray submit [yaml] [script.py] --args=... is deprecated and "
            "will be removed in a future version of Ray. Use "
            "`ray submit [yaml] script.py -- --arg1 --arg2` instead.")

    if start:
        create_or_update_cluster(cluster_config_file, None, None, False, False,
                                 True, cluster_name)

    target = os.path.join("~", os.path.basename(script))
    rsync(cluster_config_file, script, target, cluster_name, down=False)

    command_parts = ["python", target]
    if script_args:
        command_parts += list(script_args)
    elif args is not None:
        command_parts += [args]

    port_forward = [(port, port) for port in list(port_forward)]
    cmd = " ".join(command_parts)
    exec_cluster(
        cluster_config_file,
        cmd,
        docker,
        screen,
        tmux,
        stop,
        start=False,
        override_cluster_name=cluster_name,
        port_forward=port_forward)
def __init__(self, config_entry):
        
        self.config_entry = config_entry
        self.updated_config = {}
def purge_task_execution(logger, timestamp, purge_incomplete=False):
    
    
    
    if not timestamp:
        raise ValueError("Specify a valid timestamp to purge.")

    logger.info(
        "Purging executions older than timestamp: %s"
        % timestamp.strftime("%Y-%m-%dT%H:%M:%S.%fZ")
    )

    filters = {}

    if purge_incomplete:
        filters["start_timestamp__lt"] = timestamp
    else:
        filters["end_timestamp__lt"] = timestamp
        filters["start_timestamp__lt"] = timestamp
        filters["status"] = {"$in": DONE_STATES}

    exec_filters = copy.copy(filters)
    try:
        deleted_count = TaskExecution.delete_by_query(**exec_filters)
    except InvalidQueryError as e:
        msg = (
            "Bad query (%s) used to delete execution instances: %s"
            "Please contact support." % (exec_filters, six.text_type(e))
        )
        raise InvalidQueryError(msg)
    except:
        logger.exception(
            "Deletion of execution models failed for query with filters: %s.",
            exec_filters,
        )
    else:
        logger.info("Deleted %s task execution objects" % deleted_count)

    zombie_execution_instances = len(
        TaskExecution.query(only_fields=["id"], no_dereference=True, **exec_filters)
    )

    if zombie_execution_instances > 0:
        logger.error("Zombie execution instances left: %d.", zombie_execution_instances)

    # Print stats
    logger.info("All execution models older than timestamp %s were deleted.", timestamp)
def _setup_entities(
    hass: HomeAssistant, entry: ConfigEntry, device_ids: list[str]
) -> list[TuyaHaFan]:
    
    device_manager = hass.data[DOMAIN][entry.entry_id][TUYA_DEVICE_MANAGER]
    entities = []
    for device_id in device_ids:
        device = device_manager.device_map[device_id]
        if device is None:
            continue
        entities.append(TuyaHaFan(device, device_manager))
        hass.data[DOMAIN][entry.entry_id][TUYA_HA_DEVICES].add(device_id)
    return entities
def repackage_hidden(h):
    
    if isinstance(h, torch.Tensor):
        return h.detach()
    else:
        return tuple(repackage_hidden(v) for v in h)
def _validate_pair_loc(self, pair: str, pct_change: float) -> bool:
        
        
        

        result = True
        if pct_change < self._min_rate_of_change:
            self.log_once(f"Removed {pair} from whitelist, because rate of change "
                          f"over {self._days} {plural(self._days, 'day')} is {pct_change:.3f}, "
                          f"which is below the threshold of {self._min_rate_of_change}.",
                          logger.info)
            result = False
        if self._max_rate_of_change:
            if pct_change > self._max_rate_of_change:
                self.log_once(
                    f"Removed {pair} from whitelist, because rate of change "
                    f"over {self._days} {plural(self._days, 'day')} is {pct_change:.3f}, "
                    f"which is above the threshold of {self._max_rate_of_change}.",
                    logger.info)
                result = False
        return result
def can_run(conanfile):
    
    
    
    # Issue related: https://github.com/conan-io/conan/issues/11035
    allowed = conanfile.conf.get("tools.build.cross_building:can_run", check_type=bool)
    if allowed is None:
        return not cross_building(conanfile)
    return allowed
def add_node(self, key=None, data=None):
        
        
        
        if key is None:
            key = self.get_unique_key()
        elif key in self._manually_added_keys:
            raise ValueError("Adding duplicate node: {key}".format(key=key))
        else:
            self._manually_added_keys.append(key)
        if key in self.key2ind:  # Implicitly added already; don't add again.
            ind = self.key2ind[key]
            node = self.digraph[ind]
            # All that this operation can do is add data:
            self.digraph[ind] = DGNode(node.key, node.edges, data)
            return key
        self.key2ind[key] = len(self.digraph)
        self.digraph.append(DGNode(key, [], data))
        return key
def post_process(self, bboxlist):
         
        
        retval = list()
        for i in range(len(bboxlist) // 2):
            bboxlist[i * 2] = self.softmax(bboxlist[i * 2], axis=1)
        for i in range(len(bboxlist) // 2):
            ocls, oreg = bboxlist[i * 2], bboxlist[i * 2 + 1]
            stride = 2 ** (i + 2)    # 4,8,16,32,64,128
            poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))
            for _, hindex, windex in poss:
                axc, ayc = stride / 2 + windex * stride, stride / 2 + hindex * stride
                score = ocls[0, 1, hindex, windex]
                loc = np.ascontiguousarray(oreg[0, :, hindex, windex]).reshape((1, 4))
                priors = np.array([[axc / 1.0, ayc / 1.0, stride * 4 / 1.0, stride * 4 / 1.0]])
                variances = [0.1, 0.2]
                box = self.decode(loc, priors, variances)
                x_1, y_1, x_2, y_2 = box[0] * 1.0
                retval.append([x_1, y_1, x_2, y_2, score])
        retval = np.array(retval)
        if len(retval) == 0:
            retval = np.zeros((1, 5))
        return retval
def decode(location, priors):
        
        
        variances = [0.1, 0.2]
        boxes = np.concatenate((priors[:, :2] + location[:, :2] * variances[0] * priors[:, 2:],
                                priors[:, 2:] * np.exp(location[:, 2:] * variances[1])), axis=1)
        boxes[:, :2] -= boxes[:, 2:] / 2
        boxes[:, 2:] += boxes[:, :2]
        return boxes
def finalize_predictions(self, bounding_boxes_scales):
         
        
        ret = list()
        batch_size = range(bounding_boxes_scales[0].shape[0])
        for img in batch_size:
            bboxlist = [scale[img:img+1] for scale in bounding_boxes_scales]
            boxes = self._post_process(bboxlist)
            bboxlist = self._nms(boxes, 0.5)
            ret.append(bboxlist)
        return ret
def _initial_imputation(self, X, in_fit=False):
        
        
        if is_scalar_nan(self.missing_values):
            force_all_finite = "allow-nan"
        else:
            force_all_finite = True

        X = self._validate_data(
            X,
            dtype=FLOAT_DTYPES,
            order="F",
            reset=in_fit,
            force_all_finite=force_all_finite,
        )
        _check_inputs_dtype(X, self.missing_values)

        X_missing_mask = _get_mask(X, self.missing_values)
        mask_missing_values = X_missing_mask.copy()
        if self.initial_imputer_ is None:
            self.initial_imputer_ = SimpleImputer(
                missing_values=self.missing_values,
                strategy=self.initial_strategy,
                keep_empty_features=self.keep_empty_features,
            )
            X_filled = self.initial_imputer_.fit_transform(X)
        else:
            X_filled = self.initial_imputer_.transform(X)

        valid_mask = np.flatnonzero(
            np.logical_not(np.isnan(self.initial_imputer_.statistics_))
        )

        if not self.keep_empty_features:
            # drop empty features
            Xt = X[:, valid_mask]
            mask_missing_values = mask_missing_values[:, valid_mask]
        else:
            # mark empty features as not missing and keep the original
            # imputation
            mask_missing_values[:, valid_mask] = True
            Xt = X

        return Xt, X_filled, mask_missing_values, X_missing_mask
def _impute_one_feature(
        self,
        X_filled,
        mask_missing_values,
        feat_idx,
        neighbor_feat_idx,
        estimator=None,
        fit_mode=True,
        params=None,
    ):
        
        
        if estimator is None and fit_mode is False:
            raise ValueError(
                "If fit_mode is False, then an already-fitted "
                "estimator should be passed in."
            )

        if estimator is None:
            estimator = clone(self._estimator)

        missing_row_mask = mask_missing_values[:, feat_idx]
        if fit_mode:
            X_train = _safe_indexing(
                _safe_indexing(X_filled, neighbor_feat_idx, axis=1),
                ~missing_row_mask,
                axis=0,
            )
            y_train = _safe_indexing(
                _safe_indexing(X_filled, feat_idx, axis=1),
                ~missing_row_mask,
                axis=0,
            )
            estimator.fit(X_train, y_train, **params)

        # if no missing values, don't predict
        if np.sum(missing_row_mask) == 0:
            return X_filled, estimator

        # get posterior samples if there is at least one missing value
        X_test = _safe_indexing(
            _safe_indexing(X_filled, neighbor_feat_idx, axis=1),
            missing_row_mask,
            axis=0,
        )
        if self.sample_posterior:
            mus, sigmas = estimator.predict(X_test, return_std=True)
            imputed_values = np.zeros(mus.shape, dtype=X_filled.dtype)
            # two types of problems: (1) non-positive sigmas
            # (2) mus outside legal range of min_value and max_value
            # (results in inf sample)
            positive_sigmas = sigmas > 0
            imputed_values[~positive_sigmas] = mus[~positive_sigmas]
            mus_too_low = mus < self._min_value[feat_idx]
            imputed_values[mus_too_low] = self._min_value[feat_idx]
            mus_too_high = mus > self._max_value[feat_idx]
            imputed_values[mus_too_high] = self._max_value[feat_idx]
            # the rest can be sampled without statistical issues
            inrange_mask = positive_sigmas & ~mus_too_low & ~mus_too_high
            mus = mus[inrange_mask]
            sigmas = sigmas[inrange_mask]
            a = (self._min_value[feat_idx] - mus) / sigmas
            b = (self._max_value[feat_idx] - mus) / sigmas

            truncated_normal = stats.truncnorm(a=a, b=b, loc=mus, scale=sigmas)
            imputed_values[inrange_mask] = truncated_normal.rvs(
                random_state=self.random_state_
            )
        else:
            imputed_values = estimator.predict(X_test)
            imputed_values = np.clip(
                imputed_values, self._min_value[feat_idx], self._max_value[feat_idx]
            )

        # update the feature
        _safe_assign(
            X_filled,
            imputed_values,
            row_indexer=missing_row_mask,
            column_indexer=feat_idx,
        )
        return X_filled, estimator
def load_data(
        self, url: Optional[str] = None, query: Optional[str] = None
    ) -> List[Document]:
        
        
        if url is None and query is None:
            raise ValueError("Either url or query must be provided.")
        if url is not None and query is not None:
            raise ValueError("Only one of url or query must be provided.")

        documents = []

        if self.mode == "scrape":
            firecrawl_docs = self.firecrawl.scrape_url(url, params=self.params)
            documents.append(
                Document(
                    page_content=firecrawl_docs.get("markdown", ""),
                    metadata=firecrawl_docs.get("metadata", {}),
                )
            )
        elif self.mode == "crawl":
            firecrawl_docs = self.firecrawl.crawl_url(url, params=self.params)
            for doc in firecrawl_docs:
                documents.append(
                    Document(
                        page_content=doc.get("markdown", ""),
                        metadata=doc.get("metadata", {}),
                    )
                )
        elif self.mode == "search":
            firecrawl_docs = self.firecrawl.search(query, params=self.params)
            for doc in firecrawl_docs:
                documents.append(
                    Document(
                        page_content=doc.get("markdown", ""),
                        metadata=doc.get("metadata", {}),
                    )
                )
        else:
            raise ValueError(
                "Invalid mode. Please choose 'scrape', 'crawl' or 'search'."
            )

        return documents
def venv_bin(self, prefixes, filename=None):
        
        
        
        if filename is not None:
            prefixes.append(filename)
        return os.path.join(*prefixes)
def save_environment_json(self):
        
        
        
        data = {
            'python': {
                'version': self.config.python_full_version,
            },
            'env_vars_hash': self._get_env_vars_hash(),
        }

        if isinstance(self.build_env, DockerBuildEnvironment):
            build_image = self.config.build.image or DOCKER_IMAGE
            data.update({
                'build': {
                    'image': build_image,
                    'hash': self.build_env.image_hash,
                },
            })

        with open(self.environment_json_path(), 'w') as fpath:
            # Compatibility for Py2 and Py3. ``io.TextIOWrapper`` expects
            # unicode but ``json.dumps`` returns str in Py2.
            fpath.write(str(json.dumps(data)))
def _get_env_vars_hash(self):
        
        
        
        env_vars = self._get_env_vars()
        m = hashlib.sha256()
        if env_vars:
            for env_var in env_vars:
                hash_str = '_{var}_{value}_'.format(
                    var=env_var[0],
                    value=env_var[1]
                )
                m.update(hash_str.encode('utf-8'))
        else:
            m.update('None'.encode('utf-8'))

        return m.hexdigest()
def _get_env_vars(self):
        
        env_vars = self.version.project.environmentvariable_set.values_list('name', 'value')
        return tuple(env_vars)
def _pip_cache_cmd_argument(self):
        
        
        
        if (
            # Cache is going to be removed anyways
            settings.RTD_CLEAN_AFTER_BUILD or
            self.project.has_feature(Feature.CLEAN_AFTER_BUILD) or
            # Cache will be pushed/pulled each time and won't be used because
            # packages are already installed in the environment
            self.project.has_feature(Feature.CACHED_ENVIRONMENT)
        ):
            return [
                '--no-cache-dir',
            ]
        return [
            '--cache-dir',
            self.project.pip_cache_path,
        ]
def conda_bin_name(self):
        
        
        
        # Config file using ``build.tools.python``
        if self.config.using_build_tools:
            return self.config.python_interpreter
        return 'conda'
def sample(self, mask: Optional[Tuple[Optional[np.ndarray]]] = None) -> tuple:
        
        
        if mask is not None:
            assert isinstance(
                mask, tuple
            ), f"Expected type of mask is tuple, actual type: {type(mask)}"
            assert len(mask) == len(
                self.spaces
            ), f"Expected length of mask is {len(self.spaces)}, actual length: {len(mask)}"

            return tuple(
                space.sample(mask=sub_mask)
                for space, sub_mask in zip(self.spaces, mask)
            )

        return tuple(space.sample() for space in self.spaces)
def seed(self, seed: Optional[Union[int, List[int]]] = None) -> list:
        
        
        seeds = []

        if isinstance(seed, list):
            for i, space in enumerate(self.spaces):
                seeds += space.seed(seed[i])
        elif isinstance(seed, int):
            seeds = super().seed(seed)
            subseeds = self.np_random.integers(
                np.iinfo(np.int32).max, size=len(self.spaces)
            )
            for subspace, subseed in zip(self.spaces, subseeds):
                seeds.append(subspace.seed(int(subseed))[0])
        elif seed is None:
            for space in self.spaces:
                seeds += space.seed(seed)
        else:
            raise TypeError("Passed seed not of an expected type: list or int or None")

        return seeds
def assert_allclose(x, y, rtol=1e-7, atol=0, equal_nan=True, err_msg='', verbose=True):
    
    
    x = _preprocess_input(x)
    y = _preprocess_input(y)

    numpy.testing.assert_allclose(x, y, rtol=rtol, atol=atol, equal_nan=equal_nan, err_msg=err_msg, verbose=verbose)
def assert_allclose(
        x, y, rtol=1e-7, atol=0, equal_nan=True, err_msg='', verbose=True):
    
    
    x = _preprocess_input(x)
    y = _preprocess_input(y)

    numpy.testing.assert_allclose(
        x, y, rtol=rtol, atol=atol, equal_nan=equal_nan, err_msg=err_msg,
        verbose=verbose)
def assert_allclose_ex(x, y, rtol=1e-7, atol=0, equal_nan=True, err_msg='',
                       verbose=True, **kwargs):
    
    
    dtype_check = kwargs.pop('dtype_check', None)
    strides_check = kwargs.pop('strides_check', None)
    atol = kwargs.pop(x.dtype.name + '_atol', atol)
    rtol = kwargs.pop(x.dtype.name + '_rtol', rtol)

    assert_allclose(x, y, rtol, atol, equal_nan, err_msg, verbose)
    _check_dtype_and_strides(x, y, dtype_check, strides_check)
def parse_service_exception(
    response: Response, parsed_response: Dict
) -> Optional[ServiceException]:
    
    
    
    if response.status_code < 301 or "Error" not in parsed_response:
        return None
    error = parsed_response["Error"]
    service_exception = CommonServiceException(
        code=error.get("Code", f"'{response.status_code}'"),
        status_code=response.status_code,
        message=error.get("Message", ""),
        sender_fault=error.get("Type") == "Sender",
    )
    # Add all additional fields in the parsed response as members of the exception
    for key, value in parsed_response.items():
        if key.lower() not in ["code", "message", "type", "error"] and not hasattr(
            service_exception, key
        ):
            setattr(service_exception, key, value)
    return service_exception
def do_block_translate(parser, token):
    
    
    
    bits = token.split_contents()

    options = {}
    remaining_bits = bits[1:]
    asvar = None
    while remaining_bits:
        option = remaining_bits.pop(0)
        if option in options:
            raise TemplateSyntaxError('The %r option was specified more '
                                      'than once.' % option)
        if option == 'with':
            value = token_kwargs(remaining_bits, parser, support_legacy=True)
            if not value:
                raise TemplateSyntaxError('"with" in %r tag needs at least '
                                          'one keyword argument.' % bits[0])
        elif option == 'count':
            value = token_kwargs(remaining_bits, parser, support_legacy=True)
            if len(value) != 1:
                raise TemplateSyntaxError('"count" in %r tag expected exactly '
                                          'one keyword argument.' % bits[0])
        elif option == "context":
            try:
                value = remaining_bits.pop(0)
                value = parser.compile_filter(value)
            except Exception:
                msg = (
                    '"context" in %r tag expected '
                    'exactly one argument.') % bits[0]
                six.reraise(TemplateSyntaxError, TemplateSyntaxError(msg), sys.exc_info()[2])
        elif option == "trimmed":
            value = True
        elif option == "asvar":
            try:
                value = remaining_bits.pop(0)
            except IndexError:
                msg = "No argument provided to the '%s' tag for the asvar option." % bits[0]
                six.reraise(TemplateSyntaxError, TemplateSyntaxError(msg), sys.exc_info()[2])
            asvar = value
        else:
            raise TemplateSyntaxError('Unknown argument for %r tag: %r.' %
                                      (bits[0], option))
        options[option] = value

    if 'count' in options:
        countervar, counter = list(options['count'].items())[0]
    else:
        countervar, counter = None, None
    if 'context' in options:
        message_context = options['context']
    else:
        message_context = None
    extra_context = options.get('with', {})

    trimmed = options.get("trimmed", False)

    singular = []
    plural = []
    while parser.tokens:
        token = parser.next_token()
        if token.token_type in (TOKEN_VAR, TOKEN_TEXT):
            singular.append(token)
        else:
            break
    if countervar and counter:
        if token.contents.strip() != 'plural':
            raise TemplateSyntaxError("'blocktrans' doesn't allow other block tags inside it")
        while parser.tokens:
            token = parser.next_token()
            if token.token_type in (TOKEN_VAR, TOKEN_TEXT):
                plural.append(token)
            else:
                break
    if token.contents.strip() != 'endblocktrans':
        raise TemplateSyntaxError("'blocktrans' doesn't allow other block tags (seen %r) inside it" % token.contents)

    return BlockTranslateNode(extra_context, singular, plural, countervar,
                              counter, message_context, trimmed=trimmed,
                              asvar=asvar)
def pronounce_number(number, lang="en-us", places=2, short_scale=True):
    
    
    
    lang_lower = str(lang).lower()
    if lang_lower.startswith("en"):
        return pronounce_number_en(number, places=places,
                                   short_scale=short_scale)
    elif lang_lower.startswith("it"):
        return pronounce_number_it(number, places=places)
    elif lang_lower.startswith("fr"):
        return pronounce_number_fr(number, places=places)
    elif lang_lower.startswith("de"):
        return pronounce_number_de(number, places=places)

    # Default to just returning the numeric value
    return str(number)
def nice_duration(duration, lang=None, speech=True, use_years=True,
                  resolution=TimeResolution.SECONDS):
     
    
    return _duration_handler(duration, lang=lang, speech=speech,
                             use_years=use_years, resolution=resolution)
def _duration_handler(time1, lang=None, speech=True, *, time2=None,
                      use_years=True, resolution=TimeResolution.SECONDS):
     
    
    _leapdays = 0
    _input_resolution = resolution
    milliseconds = 0

    type1 = type(time1)

    if time2:
        type2 = type(time2)
        if type1 is not type2:
            raise Exception("nice_duration(" + str(time1) + ", " +
                            str(time2) + "): \n\t can't "
                            "combine data types: " + str(type(time1)) +
                            " and " + str(type(time2)))
        elif type1 is datetime.datetime:
            duration = time1 - time2
            _leapdays = (abs(leapdays(time1.year, time2.year)))

            # when operating on datetimes, refuse resolutions that
            # would result in bunches of trailing zeroes
            if all([time1.second == 0, time2.second == 0,
                    resolution.value >= 5]):
                resolution = TimeResolution.MINUTES
            if all([time1.minute == 0, time2.minute == 0,
                    resolution.value == 4]):
                resolution = TimeResolution.HOURS
            if all([time1.hour == 0, time2.hour == 0,
                    resolution.value == 3]):
                resolution = TimeResolution.DAYS

        else:
            _tmp = warnings.formatwarning
            warnings.formatwarning = lambda msg, * \
                args, **kwargs: "{}\n".format(msg)
            warning = ("WARN: mycroft.util.format.nice_duration_dt() can't "
                       "subtract " + str(type1) + ". Ignoring 2nd "
                       "argument '" + str(time2) + "'.")
            warnings.warn(warning)
            warnings.formatwarning = _tmp
            duration = time1
    else:
        duration = time1

    # Pull decimal portion of seconds, if present, to use for milliseconds
    if isinstance(duration, float):
        milliseconds = str(duration).split('.')[1]
        if speech:
            milliseconds = milliseconds[:2]
        else:
            milliseconds = milliseconds[:3]
        milliseconds = float("0." + milliseconds)

    # Cast duration to datetime.timedelta for human-friendliness
    if not isinstance(duration, datetime.timedelta):
        duration = datetime.timedelta(seconds=duration)

    days = duration.days
    if use_years:
        days -= _leapdays if days > 365 else 0
        years = days // 365
    else:
        years = 0
    days = days % 365 if years > 0 else days

    # We already stored milliseconds. Now we want the integer part.
    seconds = duration.seconds
    minutes = seconds // 60
    seconds %= 60
    hours = minutes // 60
    minutes %= 60

    if speech:
        out = ""
        if years > 0:
            out += pronounce_number(years, lang) + " "
            out += _translate_word("year" if years == 1 else "years", lang)

        if days > 0 and resolution.value > TimeResolution.YEARS.value:
            if out:
                out += " "
            out += pronounce_number(days, lang) + " "
            out += _translate_word("day" if days == 1 else "days", lang)

        if hours > 0 and resolution.value > TimeResolution.DAYS.value:
            if out:
                out += " "
            out += pronounce_number(hours, lang) + " "
            out += _translate_word("hour" if hours == 1 else "hours", lang)

        if minutes > 0 and resolution.value > TimeResolution.HOURS.value:
            if out:
                out += " "
            out += pronounce_number(minutes, lang) + " "
            out += _translate_word("minute" if minutes ==
                                   1 else "minutes", lang)

        if ((seconds > 0 and resolution.value >=
             TimeResolution.SECONDS.value) or
            (milliseconds > 0 and resolution.value ==
             TimeResolution.MILLISECONDS.value)):

            if resolution.value == TimeResolution.MILLISECONDS.value:
                seconds += milliseconds
            if out:
                out += " "
                # Throw "and" between minutes and seconds if duration < 1 hour
                if len(out.split()) > 3 or seconds < 1:
                    out += _translate_word("and", lang) + " "
            # speaking "zero point five seconds" is better than "point five"
            if seconds < 1:
                out += pronounce_number(0, lang)
            out += pronounce_number(seconds, lang) + " "
            out += _translate_word("second" if seconds ==
                                   1 else "seconds", lang)

    else:
        # M:SS, MM:SS, H:MM:SS, Dd H:MM:SS format

        _seconds_str = ("0" + str(seconds)) if seconds < 10 else str(seconds)

        out = ""
        if years > 0:
            out = str(years) + "y "
        if days > 0 and resolution.value > TimeResolution.YEARS.value:
            out += str(days) + "d "
        if hours > 0 and resolution.value > TimeResolution.DAYS.value:
            out += str(hours)

        if resolution.value == TimeResolution.MINUTES.value:
            out += (("h " + str(minutes) + "m") if hours > 0
                    else str(minutes) + "m")
        elif minutes > 0 and resolution.value > TimeResolution.HOURS.value:
            if hours != 0:
                out += ":"
                if minutes < 10:
                    out += "0"
            out += str(minutes) + ":"
            if seconds > 0 and resolution.value > TimeResolution.MINUTES.value:
                out += _seconds_str
            else:
                out += "00"
        # if we have seconds but no minutes...
        elif seconds > 0 and resolution.value > TimeResolution.MINUTES.value:
            # check if output ends in hours
            try:
                if str(hours) == out.split()[-1]:
                    out += ":"
            except IndexError:
                pass
            out += ("00:" if hours > 0 else "0:") + _seconds_str

        if milliseconds > 0 and resolution.value \
                == TimeResolution.MILLISECONDS.value:
            _mill = str(milliseconds).split(".")[1]
            # right-pad milliseconds to three decimal places
            while len(_mill) < 3:
                _mill += "0"
            # make sure output < 1s still formats correctly
            if out == "":
                out = "0:00"
            else:
                if (str(hours) == out.split()[-1]) and ":" not in out:
                    out += ":00:00"
            # only append milliseconds to output that contains
            # minutes and/or seconds
            if ":" in out:
                out += "." + _mill

        # If this evaluates True, out currently ends in hours: "1d 12"
        if out and all([resolution.value >= TimeResolution.HOURS.value,
                        ":" not in out, out[-1] != "m", hours > 0]):
            # to "1d 12h"
            out += "h"
        out = out.strip()

    if not out:
        if _input_resolution == TimeResolution.YEARS:
            out = "zero years" if speech else "0y"
        elif _input_resolution == TimeResolution.DAYS:
            out = "zero days" if speech else "0d"
        elif _input_resolution == TimeResolution.HOURS:
            out = "zero hours" if speech else "0h"
        elif _input_resolution == TimeResolution.MINUTES:
            if speech:
                out = "under a minute" if seconds > 0 else "zero minutes"
            else:
                out = "0m"
        else:
            out = "zero seconds" if speech else "0:00"

    return out
def _duration_handler(time1, lang=None, speech=True, *, time2=None,
                      use_years=True, clock=False,
                      resolution=TimeResolution.SECONDS):
     
    
    _leapdays = 0
    _input_resolution = resolution
    milliseconds = 0

    type1 = type(time1)

    if time2:
        type2 = type(time2)
        if type1 is not type2:
            raise Exception("nice_duration(" + str(time1) + ", " +
                            str(time2) + "): \n\t can't "
                            "combine data types: " + str(type(time1)) +
                            " and " + str(type(time2)))
        elif type1 is datetime.datetime:
            duration = time1 - time2
            _leapdays = (abs(leapdays(time1.year, time2.year)))

            # when operating on datetimes, refuse resolutions that
            # would result in bunches of trailing zeroes
            if all([time1.second == 0, time2.second == 0,
                    resolution.value >= 5]):
                resolution = TimeResolution.MINUTES
            if all([time1.minute == 0, time2.minute == 0,
                    resolution.value == 4]):
                resolution = TimeResolution.HOURS
            if all([time1.hour == 0, time2.hour == 0,
                    resolution.value == 3]):
                resolution = TimeResolution.DAYS

        else:
            _tmp = warnings.formatwarning
            warnings.formatwarning = lambda msg, * \
                args, **kwargs: "{}\n".format(msg)
            warning = ("WARN: mycroft.util.format.nice_duration_dt() can't "
                       "subtract " + str(type1) + ". Ignoring 2nd "
                       "argument '" + str(time2) + "'.")
            warnings.warn(warning)
            warnings.formatwarning = _tmp
            duration = time1
    else:
        duration = time1

    # Pull decimal portion of seconds, if present, to use for milliseconds
    if isinstance(duration, float):
        milliseconds = str(duration).split('.')[1]
        if speech:
            milliseconds = milliseconds[:2]
        else:
            milliseconds = milliseconds[:3]
        milliseconds = float("0." + milliseconds)

    # Cast duration to datetime.timedelta for human-friendliness
    if not isinstance(duration, datetime.timedelta):
        duration = datetime.timedelta(seconds=duration)

    days = duration.days
    if use_years:
        days -= _leapdays if days > 365 else 0
        years = days // 365
    else:
        years = 0
    days = days % 365 if years > 0 else days

    # We already stored milliseconds. Now we want the integer part.
    seconds = duration.seconds
    minutes = seconds // 60
    seconds %= 60
    hours = minutes // 60
    minutes %= 60

    if speech:
        out = ""
        if years > 0:
            out += pronounce_number(years, lang) + " "
            out += _translate_word("year" if years == 1 else "years", lang)

        if days > 0 and resolution.value > TimeResolution.YEARS.value:
            if out:
                out += " "
            out += pronounce_number(days, lang) + " "
            out += _translate_word("day" if days == 1 else "days", lang)

        if hours > 0 and resolution.value > TimeResolution.DAYS.value:
            if out:
                out += " "
            out += pronounce_number(hours, lang) + " "
            out += _translate_word("hour" if hours == 1 else "hours", lang)

        if minutes > 0 and resolution.value > TimeResolution.HOURS.value:
            if out:
                out += " "
            out += pronounce_number(minutes, lang) + " "
            out += _translate_word("minute" if minutes ==
                                   1 else "minutes", lang)

        if ((seconds > 0 and resolution.value >=
             TimeResolution.SECONDS.value) or
            (milliseconds > 0 and resolution.value ==
             TimeResolution.MILLISECONDS.value)):

            if resolution.value == TimeResolution.MILLISECONDS.value:
                seconds += milliseconds
            if out:
                out += " "
                # Throw "and" between minutes and seconds if duration < 1 hour
                if len(out.split()) > 3 or seconds < 1:
                    out += _translate_word("and", lang) + " "
            # speaking "zero point five seconds" is better than "point five"
            if seconds < 1:
                out += pronounce_number(0, lang)
            out += pronounce_number(seconds, lang) + " "
            out += _translate_word("second" if seconds ==
                                   1 else "seconds", lang)

    else:
        # M:SS, MM:SS, H:MM:SS, Dd H:MM:SS format

        _seconds_str = ("0" + str(seconds)) if seconds < 10 else str(seconds)

        out = ""
        if years > 0:
            out = str(years) + "y "
        if days > 0 and resolution.value > TimeResolution.YEARS.value:
            out += str(days) + "d "
        if (hours > 0 and resolution.value > TimeResolution.DAYS.value) or \
                (clock and resolution is TimeResolution.HOURS):
            out += str(hours)

        if resolution.value == TimeResolution.MINUTES.value and not clock:
            out += (("h " + str(minutes) + "m") if hours > 0
                    else str(minutes) + "m")
        elif (minutes > 0 and resolution.value > TimeResolution.HOURS.value) \
                or (clock and resolution.value >= TimeResolution.HOURS.value):
            if hours != 0 or (clock and resolution is TimeResolution.HOURS):
                out += ":"
                if minutes < 10:
                    out += "0"
            out += str(minutes) + ":"
            if (seconds > 0 and resolution.value >
                    TimeResolution.MINUTES.value) or clock:
                out += _seconds_str
            else:
                out += "00"
        # if we have seconds but no minutes...
        elif (seconds > 0 or clock) and resolution.value > \
                TimeResolution.MINUTES.value:
            # check if output ends in hours
            try:
                if str(hours) == out.split()[-1]:
                    out += ":"
            except IndexError:
                pass
            out += ("00:" if hours > 0 else "0:") + _seconds_str

        if (milliseconds > 0 or clock) and resolution.value \
                == TimeResolution.MILLISECONDS.value:
            _mill = str(milliseconds).split(".")[1]
            # right-pad milliseconds to three decimal places
            while len(_mill) < 3:
                _mill += "0"
            # make sure output < 1s still formats correctly
            if out == "":
                out = "0:00"
            else:
                if (str(hours) == out.split()[-1]) and ":" not in out:
                    out += ":00:00"
            # only append milliseconds to output that contains
            # minutes and/or seconds
            if ":" in out:
                out += "." + _mill

        # If this evaluates True, out currently ends in hours: "1d 12"
        if out and all([resolution.value >= TimeResolution.HOURS.value,
                        ":" not in out, out[-1] != "m", hours > 0]):
            # to "1d 12h"
            out += "h"
        out = out.strip()

    if not out:
        if _input_resolution == TimeResolution.YEARS:
            out = "zero years" if speech else "0y"
        elif _input_resolution == TimeResolution.DAYS:
            out = "zero days" if speech else "0d"
        elif _input_resolution == TimeResolution.HOURS:
            out = "zero hours" if speech else "0h"
        elif _input_resolution == TimeResolution.MINUTES:
            if speech:
                out = "under a minute" if seconds > 0 else "zero minutes"
            else:
                out = "0m"
        else:
            out = "zero seconds" if speech else "0:00"

    return out
def _do_policy_eval(
        *,
        to_eval: Dict[PolicyID, List[PolicyEvalData]],
        policies: Dict[PolicyID, Policy],
        active_episodes: Dict[str, MultiAgentEpisode],
        tf_sess=None,
) -> Dict[PolicyID, Tuple[TensorStructType, StateBatch, dict]]:
    
    

    eval_results: Dict[PolicyID, TensorStructType] = {}

    if tf_sess:
        builder = TFRunBuilder(tf_sess, "policy_eval")
        pending_fetches: Dict[PolicyID, Any] = {}
    else:
        builder = None

    if log_once("compute_actions_input"):
        logger.info("Inputs to compute_actions():\n\n{}\n".format(
            summarize(to_eval)))

    # type: PolicyID, PolicyEvalData
    for policy_id, eval_data in to_eval.items():
        policy: Policy = _get_or_raise(policies, policy_id)
        # If tf (non eager) AND TFPolicy's compute_action method has not
        # been overridden -> Use `policy._build_compute_actions()`.
        if builder and (policy.compute_actions.__code__ is
                        TFPolicy.compute_actions.__code__):

            obs_batch: List[EnvObsType] = [t.obs for t in eval_data]
            state_batches: StateBatch = _to_column_format(
                [t.rnn_state for t in eval_data])
            # TODO(ekl): how can we make info batch available to TF code?
            prev_action_batch = [t.prev_action for t in eval_data]
            prev_reward_batch = [t.prev_reward for t in eval_data]

            pending_fetches[policy_id] = policy._build_compute_actions(
                builder,
                obs_batch=obs_batch,
                state_batches=state_batches,
                prev_action_batch=prev_action_batch,
                prev_reward_batch=prev_reward_batch,
                timestep=policy.global_timestep)
        else:
            rnn_in = [t.rnn_state for t in eval_data]
            rnn_in_cols: StateBatch = [
                np.stack([row[i] for row in rnn_in])
                for i in range(len(rnn_in[0]))
            ]
            eval_results[policy_id] = policy.compute_actions(
                [t.obs for t in eval_data],
                state_batches=rnn_in_cols,
                prev_action_batch=[t.prev_action for t in eval_data],
                prev_reward_batch=[t.prev_reward for t in eval_data],
                info_batch=[t.info for t in eval_data],
                episodes=[active_episodes[t.env_id] for t in eval_data],
                timestep=policy.global_timestep)

    if builder:
        # type: PolicyID, Tuple[TensorStructType, StateBatch, dict]
        for pid, v in pending_fetches.items():
            eval_results[pid] = builder.get(v)

    if log_once("compute_actions_result"):
        logger.info("Outputs of compute_actions():\n\n{}\n".format(
            summarize(eval_results)))

    return eval_results
def _do_policy_eval_w_trajectory_view_api(
        *,
        to_eval: Dict[PolicyID, List[PolicyEvalData]],
        policies: Dict[PolicyID, Policy],
        sample_collector,
        active_episodes: Dict[str, MultiAgentEpisode],
        tf_sess: Optional["tf.Session"] = None,
) -> Dict[PolicyID, Tuple[TensorStructType, StateBatch, dict]]:
    
    

    eval_results: Dict[PolicyID, TensorStructType] = {}

    if tf_sess:
        builder = TFRunBuilder(tf_sess, "policy_eval")
        pending_fetches: Dict[PolicyID, Any] = {}
    else:
        builder = None

    if log_once("compute_actions_input"):
        logger.info("Inputs to compute_actions():\n\n{}\n".format(
            summarize(to_eval)))

    for policy_id, eval_data in to_eval.items():
        policy: Policy = _get_or_raise(policies, policy_id)
        input_dict = sample_collector.get_inference_input_dict(policy_id)
        eval_results[policy_id] = \
            policy.compute_actions_from_input_dict(
                input_dict,
                timestep=policy.global_timestep,
                episodes=[active_episodes[t.env_id] for t in eval_data])

    if builder:
        # type: PolicyID, Tuple[TensorStructType, StateBatch, dict]
        for pid, v in pending_fetches.items():
            eval_results[pid] = builder.get(v)

    if log_once("compute_actions_result"):
        logger.info("Outputs of compute_actions():\n\n{}\n".format(
            summarize(eval_results)))

    return eval_results
def test_app_locales(self):
        
        
        
        with app_cache._empty(), app_cache._with_app('django.contrib.admin'):
            filenames = list(gen_filenames())
        self.assertIn(os.path.join(os.path.dirname(admin.__file__), 'locale',
                                   'nl', 'LC_MESSAGES', 'django.mo'),
                      filenames)
def test_check_errors(self):
        
        
        
        dirname = tempfile.mkdtemp()
        filename = os.path.join(dirname, 'test_syntax_error.py')
        self.addCleanup(shutil.rmtree, dirname)
        with open(filename, 'w') as f:
            f.write("Ceci n'est pas du Python.")

        with extend_sys_path(dirname):
            with self.assertRaises(SyntaxError):
                autoreload.check_errors(import_module)('test_syntax_error')
        self.assertFileFound(filename)
def get_table_list(self, cursor):
        
        
        
        cursor.execute("SELECT TABLE_NAME, 't' FROM USER_TABLES UNION ALL "
                       "SELECT VIEW_NAME, 'v' FROM USER_VIEWS")
        return [TableInfo(row[0].lower(), row[1]) for row in cursor.fetchall()]
def __init__(
        self,
        embedding: Embeddings,
        index_uri: str,
        metric: str,
        *,
        vector_index_uri: str = "",
        docs_array_uri: str = "",
        config: Optional[Mapping[str, Any]] = None,
        timestamp: Any = None,
        allow_dangerous_deserialization: bool = False,
        **kwargs: Any,
    ):
        
        
        if not allow_dangerous_deserialization:
            raise ValueError(
                "TileDB relies on pickle for serialization and deserialization. "
                "This can be dangerous if the data is intercepted and/or modified "
                "by malicious actors prior to being de-serialized. "
                "If you are sure that the data is safe from modification, you can "
                " set allow_dangerous_deserialization=True to proceed. "
                "Loading of compromised data using pickle can result in execution of "
                "arbitrary code on your machine."
            )
        self.embedding = embedding
        self.embedding_function = embedding.embed_query
        self.index_uri = index_uri
        self.metric = metric
        self.config = config

        tiledb_vs, tiledb = dependable_tiledb_import()
        with tiledb.scope_ctx(ctx_or_config=config):
            index_group = tiledb.Group(self.index_uri, "r")
            self.vector_index_uri = (
                vector_index_uri
                if vector_index_uri != ""
                else get_vector_index_uri_from_group(index_group)
            )
            self.docs_array_uri = (
                docs_array_uri
                if docs_array_uri != ""
                else get_documents_array_uri_from_group(index_group)
            )
            index_group.close()
            group = tiledb.Group(self.vector_index_uri, "r")
            self.index_type = group.meta.get("index_type")
            group.close()
            self.timestamp = timestamp
            if self.index_type == "FLAT":
                self.vector_index = tiledb_vs.flat_index.FlatIndex(
                    uri=self.vector_index_uri,
                    config=self.config,
                    timestamp=self.timestamp,
                    **kwargs,
                )
            elif self.index_type == "IVF_FLAT":
                self.vector_index = tiledb_vs.ivf_flat_index.IVFFlatIndex(
                    uri=self.vector_index_uri,
                    config=self.config,
                    timestamp=self.timestamp,
                    **kwargs,
                )
def apply_augmentations(augmentations: List[Union[Transform, Augmentation]], inputs):
    
    
    
    if isinstance(inputs, np.ndarray):
        # handle the common case of image-only Augmentation, also for backward compatibility
        image_only = True
        inputs = AugInput(inputs)
    else:
        image_only = False
    tfms = inputs.apply_augmentations(augmentations)
    return inputs.image if image_only else inputs, tfms
def get_transform(self, *args) -> Transform:
        
        
        
        raise NotImplementedError
def resources(self):
        
        return dict(self.template_resources)
def available(self) -> bool:
        
        return self.coordinator.last_update_success
def test_closes_connection_without_content_length(self):
        
        
        
        conn = HTTPConnection(LiveServerViews.server_thread.host, LiveServerViews.server_thread.port, timeout=1)
        try:
            conn.request('GET', '/streaming_example_view/', headers={'Connection': 'keep-alive'})
            response = conn.getresponse()
            self.assertTrue(response.will_close)
            self.assertEqual(response.read(), b'Iamastream')
            self.assertEqual(response.status, 200)
            self.assertEqual(response.getheader('Connection'), 'close')

            conn.request('GET', '/streaming_example_view/', headers={'Connection': 'close'})
            response = conn.getresponse()
            self.assertTrue(response.will_close)
            self.assertEqual(response.read(), b'Iamastream')
            self.assertEqual(response.status, 200)
            self.assertEqual(response.getheader('Connection'), 'close')
        finally:
            conn.close()
def _i18n_cache_key_suffix(request, cache_key):
    
    if settings.USE_I18N or settings.USE_L10N:
        # first check if LocaleMiddleware or another middleware added
        # LANGUAGE_CODE to request, then fall back to the active language
        # which in turn can also fall back to settings.LANGUAGE_CODE
        cache_key += '.%s' % getattr(request, 'LANGUAGE_CODE', get_language())
    if settings.USE_TZ:
        cache_key += '.%s' % get_current_timezone_name()
    return cache_key
def patch_response_headers(response, cache_timeout=None):
    
    
    
    if cache_timeout is None:
        cache_timeout = settings.CACHE_MIDDLEWARE_SECONDS
    if cache_timeout < 0:
        cache_timeout = 0  # Can't have max-age negative
    if settings.USE_ETAGS and not response.has_header('ETag'):
        if hasattr(response, 'render') and callable(response.render):
            response.add_post_render_callback(set_response_etag)
        else:
            response = set_response_etag(response)
    if not response.has_header('Expires'):
        response['Expires'] = http_date(time.time() + cache_timeout)
    patch_cache_control(response, max_age=cache_timeout)
def patch_vary_headers(response, newheaders):
    
    
    
    # Note that we need to keep the original order intact, because cache
    # implementations may rely on the order of the Vary contents in, say,
    # computing an MD5 hash.
    if response.has_header('Vary'):
        vary_headers = cc_delim_re.split(response['Vary'])
    else:
        vary_headers = []
    # Use .lower() here so we treat headers as case-insensitive.
    existing_headers = {header.lower() for header in vary_headers}
    additional_headers = [newheader for newheader in newheaders
                          if newheader.lower() not in existing_headers]
    vary_headers += additional_headers
    if '*' in vary_headers:
        response['Vary'] = '*'
    else:
        response['Vary'] = ', '.join(vary_headers)
def test_reference_match(self, n, top, runs, max_devs):
    
    
    boxes = random_boxes([runs, n])
    scores = tf.random.uniform(shape=[runs, n])
    reference, valid = tf.image.non_max_suppression_padded(
        boxes, scores, top, pad_to_max_output_size=True)
    for refinements, max_deviation in enumerate(max_devs):
      optimized = edgetpu.non_max_suppression_padded(
          boxes, scores, top, refinements=refinements)
      deviation, margin = _deviation_and_margin(reference, valid, optimized)
      self.assertLess(
          deviation,
          max_deviation,
          msg='Deviation rate between optimized and reference implementations is '
          'higher than expected. If you are tuning the test, recommended safe '
          'deviation rate is '
          f'{deviation} + {margin} = {deviation + margin}')
def id_validate(cls, v):
        
        return str(v) if v else None
def __init__(self, _factory=message.Message, *, policy=compat32):
        

        
        self._factory = _factory
        self.policy = policy
        try:
            _factory(policy=self.policy)
            self._factory_kwds = lambda: {'policy': self.policy}
        except TypeError:
            # Assume this is an old-style factory
            self._factory_kwds = lambda: {}
        self._input = BufferedSubFile()
        self._msgstack = []
        self._parse = self._parsegen().__next__
        self._cur = None
        self._last = None
        self._headersonly = False
def write(self):
        
        if not self.raw_data.exists():
            raise FileNotFoundError
        doc = Document(self.raw_data, self.content_col, self.meta_col)
        docs, metadatas = doc.get_docs_and_metadatas()

        self.store = self._write(docs, metadatas)
        self.persist()
        return self.store
def call_base_info(power_wall: Powerwall, host: str) -> PowerwallBaseInfo:
    
    # Make sure the serial numbers always have the same order
    gateway_din = None
    with contextlib.suppress(AssertionError, PowerwallError):
        gateway_din = power_wall.get_gateway_din().upper()
    return PowerwallBaseInfo(
        gateway_din=gateway_din,
        site_info=power_wall.get_site_info(),
        status=power_wall.get_status(),
        device_type=power_wall.get_device_type(),
        serial_numbers=sorted(power_wall.get_serial_numbers()),
        url=f"https://{host}",
    )
def __init__(self, item_id: str):
        
        super().__init__(f"Tag with ID {item_id} already exists.")
        self.item_id = item_id
def inspect_step(self, task_id: TaskID) -> StepInspectResult:
        
        
        
        return self._inspect_step(task_id)
def load_step_args(self, step_id: StepID) -> ray.ObjectRef:
        
        
        with serialization_context.workflow_args_keeping_context():
            x = self._get(self._key_step_args(step_id))
        return ray.put(x)
def __call__(
            self,
            inputs: InputsType,
            batch_size: int = 1,
            return_vis: bool = False,
            show: bool = False,
            wait_time: int = 0,
            no_save_vis: bool = False,
            draw_pred: bool = True,
            pred_score_thr: float = 0.3,
            return_datasamples: bool = False,
            print_result: bool = False,
            no_save_pred: bool = True,
            out_dir: str = '',
            texts: Optional[Union[str, list]] = None,
            # by open panoptic task
            stuff_texts: Optional[Union[str, list]] = None,
            custom_entities: bool = False,  # by GLIP
            **kwargs) -> dict:
        
        
        assert batch_size == 1
        (
            preprocess_kwargs,
            forward_kwargs,
            visualize_kwargs,
            postprocess_kwargs,
        ) = self._dispatch_kwargs(**kwargs)

        ori_inputs = self._inputs_to_list(inputs)

        if isinstance(texts, str):
            texts = [texts] * len(ori_inputs)

        for i in range(len(texts)):
            if isinstance(ori_inputs[i], str):
                ori_inputs[i] = {
                    'text': texts[i],
                    'img_path': ori_inputs[i],
                    'custom_entities': custom_entities
                }
            else:
                ori_inputs[i] = {
                    'text': texts[i],
                    'img': ori_inputs[i],
                    'custom_entities': custom_entities
                }
        inputs = self.preprocess(
            ori_inputs, batch_size=batch_size, **preprocess_kwargs)

        results_dict = {'predictions': [], 'visualization': []}
        for ori_inputs, grounding_data, caption_data in track(
                inputs, description='Inference'):

            self.model.sem_seg_head.task = 'ref-seg'
            self.model.sem_seg_head.predictor.task = 'ref-seg'
            preds = self.forward(grounding_data, **forward_kwargs)

            for data_sample, pred_datasmaple in zip(
                    caption_data['data_samples'], preds):
                data_sample.pred_instances = pred_datasmaple.pred_instances
                data_sample.set_metainfo({
                    'grounding_img_shape':
                    pred_datasmaple.metainfo['img_shape']
                })

            self.model.sem_seg_head.task = 'caption'
            self.model.sem_seg_head.predictor.task = 'caption'

            preds = self.forward(caption_data, **forward_kwargs)

            if isinstance(ori_inputs, dict):
                ori_inputs = ori_inputs['img_path']

            visualization = self.visualize(
                ori_inputs,
                preds,
                return_vis=return_vis,
                show=show,
                wait_time=wait_time,
                draw_pred=draw_pred,
                pred_score_thr=pred_score_thr,
                no_save_vis=no_save_vis,
                img_out_dir=out_dir,
                **visualize_kwargs)
            results = self.postprocess(
                preds,
                visualization,
                return_datasamples=return_datasamples,
                print_result=print_result,
                no_save_pred=no_save_pred,
                pred_out_dir=out_dir,
                **postprocess_kwargs)
            results_dict['predictions'].extend(results['predictions'])
            if results['visualization'] is not None:
                results_dict['visualization'].extend(results['visualization'])
        return results_dict
def test_get_relations_alt_format(self):
        
        
        
        create_table_statements = [
            "CREATE TABLE track(id, art_id INTEGER, FOREIGN KEY(art_id) REFERENCES {}(id));",
            "CREATE TABLE track(id, art_id INTEGER, FOREIGN KEY (art_id) REFERENCES {}(id));"
        ]
        for statement in create_table_statements:
            with connection.cursor() as cursor:
                cursor.fetchone = mock.Mock(return_value=[statement.format(Article._meta.db_table)])
                relations = connection.introspection.get_relations(cursor, 'mocked_table')
            self.assertEqual(relations, {'art_id': ('id', Article._meta.db_table)})
def reduce_model_alter_delete(self, operation, other):
        
        
        
        if operation.name.lower() == other.name.lower():
            return [other]
def string_to_dict(string: str, pattern: str) -> Dict[str, str]:
    
    
    regex = re.sub(r"{(.+?)}", r"(?P<_\1>.+)", pattern)
    result = re.search(regex, string)
    if result is None:
        raise ValueError(f"String {string} doesn't match the pattern {pattern}")
    values = list(result.groups())
    keys = re.findall(r"{(.+?)}", pattern)
    _dict = dict(zip(keys, values))
    return _dict
def map_nested(
    function: Callable[[Any], Any],
    data_struct: Any,
    dict_only: bool = False,
    map_list: bool = True,
    map_tuple: bool = False,
    map_numpy: bool = False,
    num_proc: Optional[int] = None,
    parallel_min_length: int = 2,
    batched: bool = False,
    batch_size: Optional[int] = 1000,
    types: Optional[tuple] = None,
    disable_tqdm: bool = True,
    desc: Optional[str] = None,
) -> Any:
    
    
    if types is None:
        types = []
        if not dict_only:
            if map_list:
                types.append(list)
            if map_tuple:
                types.append(tuple)
            if map_numpy:
                types.append(np.ndarray)
        types = tuple(types)

    # Singleton
    if not isinstance(data_struct, dict) and not isinstance(data_struct, types):
        if batched:
            data_struct = [data_struct]
        mapped = function(data_struct)
        if batched:
            mapped = mapped[0]
        return mapped

    iterable = list(data_struct.values()) if isinstance(data_struct, dict) else data_struct

    if num_proc is None:
        num_proc = 1
    if any(isinstance(v, types) and len(v) > len(iterable) for v in iterable):
        mapped = [
            map_nested(
                function=function,
                data_struct=obj,
                num_proc=num_proc,
                parallel_min_length=parallel_min_length,
                batched=batched,
                batch_size=batch_size,
                types=types,
            )
            for obj in iterable
        ]
    elif num_proc != -1 and num_proc <= 1 or len(iterable) < parallel_min_length:
        if batched:
            if batch_size is None or batch_size <= 0:
                batch_size = max(len(iterable) // num_proc + int(len(iterable) % num_proc > 0), 1)
            iterable = list(iter_batched(iterable, batch_size))
        mapped = [
            _single_map_nested((function, obj, batched, batch_size, types, None, True, None))
            for obj in hf_tqdm(iterable, disable=disable_tqdm, desc=desc)
        ]
        if batched:
            mapped = [mapped_item for mapped_batch in mapped for mapped_item in mapped_batch]
    else:
        with warnings.catch_warnings():
            warnings.filterwarnings(
                "ignore",
                message=".* is experimental and might be subject to breaking changes in the future\\.$",
                category=UserWarning,
            )
            if batched:
                if batch_size is None or batch_size <= 0:
                    batch_size = len(iterable) // num_proc + int(len(iterable) % num_proc > 0)
                iterable = list(iter_batched(iterable, batch_size))
            mapped = parallel_map(
                function, iterable, num_proc, batched, batch_size, types, disable_tqdm, desc, _single_map_nested
            )
            if batched:
                mapped = [mapped_item for mapped_batch in mapped for mapped_item in mapped_batch]

    if isinstance(data_struct, dict):
        return dict(zip(data_struct.keys(), mapped))
    else:
        if isinstance(data_struct, list):
            return mapped
        elif isinstance(data_struct, tuple):
            return tuple(mapped)
        else:
            return np.array(mapped)
def from_path(
        cls,
        path: str,
        encoding: str = "utf-8",
        lexer: Optional[Union[Lexer, str]] = None,
        theme: Union[str, SyntaxTheme] = DEFAULT_THEME,
        dedent: bool = False,
        line_numbers: bool = False,
        line_range: Optional[Tuple[int, int]] = None,
        start_line: int = 1,
        highlight_lines: Optional[Set[int]] = None,
        code_width: Optional[int] = None,
        tab_size: int = 4,
        word_wrap: bool = False,
        background_color: Optional[str] = None,
        indent_guides: bool = False,
        padding: PaddingDimensions = 0,
    ) -> "Syntax":
        
        
        with open(path, "rt", encoding=encoding) as code_file:
            code = code_file.read()

        if not lexer:
            lexer = cls.guess_lexer(path, code=code)

        return cls(
            code,
            lexer,
            theme=theme,
            dedent=dedent,
            line_numbers=line_numbers,
            line_range=line_range,
            start_line=start_line,
            highlight_lines=highlight_lines,
            code_width=code_width,
            tab_size=tab_size,
            word_wrap=word_wrap,
            background_color=background_color,
            indent_guides=indent_guides,
            padding=padding,
        )
def _apply_stylized_ranges(self, text: Text) -> None:
        
        
        
        code = text.plain
        newlines_offsets = [
            # Let's add outer boundaries at each side of the list:
            -1,
            # N.B. using "\n" here is much faster than using metacharacters such as "^" or "\Z":
            *[match.start() for match in re.finditer("\n", code, flags=re.MULTILINE)],
            len(code),
        ]

        for stylized_range in self._stylized_ranges:
            start = _get_code_index_for_syntax_position(
                newlines_offsets, stylized_range.start
            )
            end = _get_code_index_for_syntax_position(
                newlines_offsets, stylized_range.end
            )
            if start is not None and end is not None:
                text.stylize(stylized_range.style, start, end)
def ask_collection_consent() -> bool:
    
    
    
    answer = input(
        "Is it ok if we store your prompts to help improve GPT Engineer? (y/n)"
    )
    while answer.lower() not in ("y", "n"):
        answer = input("Invalid input. Please enter y or n: ")

    if answer.lower() == "y":
        path = Path(".gpte_consent")
        path.write_text("true")
        print(colored("Thank you️", "light_green"))
        print()
        print(
            "(If you no longer wish to participate in data collection, delete the file .gpte_consent)"
        )
        return True
    else:
        print(
            colored(
                "No worries! GPT Engineer will not collect your prompts. ❤️",
                "light_green",
            )
        )
        return False
def extract_learning(
    prompt: str,
    model: str,
    temperature: float,
    config: Tuple[str, ...],
    memory: DiskMemory,
    review: Review,
) -> Learning:
    
    
    
    return Learning(
        prompt=prompt,
        model=model,
        temperature=temperature,
        config=json.dumps(config),
        session=get_session(),
        logs=memory.to_json(),
        review=review,
    )
def forward(
        self,
        query,
        key,
        value,
        attn_mask: Optional[torch.Tensor] = None,
        key_padding_mask: Optional[torch.Tensor] = None,
        return_attn_weights: Optional[torch.Tensor] = True,
        pos_embs: Optional[torch.Tensor] = None,
    ):
        
        
        
        # give tensors of shape (time, batch, fea)
        query = query.permute(1, 0, 2)
        key = key.permute(1, 0, 2)
        value = value.permute(1, 0, 2)

        # this will be legit because of https://github.com/pytorch/pytorch/blob/5288d05cfdda85c46c4df84617fa7f37c21b10b3/torch/nn/functional.py#L4946
        # we can inject relative learnable pos embeddings directly in MHA via the attn_mask
        if pos_embs is not None:
            if attn_mask is not None:
                attn_mask += pos_embs
            else:
                attn_mask = pos_embs

        output = self.att(
            query,
            key,
            value,
            attn_mask=attn_mask,
            key_padding_mask=key_padding_mask,
            need_weights=return_attn_weights,
        )

        if return_attn_weights:
            output, attention_weights = output
            # reshape the output back to (batch, time, fea)
            output = output.permute(1, 0, 2)
            return output, attention_weights
        else:
            output = output.permute(1, 0, 2)
            return output
def assert_nD(array, ndim, arg_name='image'):
    
    

    
    array = np.asanyarray(array)
    msg = "The parameter `%s` must be a %s-dimensional array"
    if isinstance(ndim, int):
        ndim = [ndim]
    if not array.ndim in ndim:
        raise ValueError(msg % (arg_name, '-or-'.join([str(n) for n in ndim])))
def _mode_deprecations(mode):
    
    if mode.lower() == 'nearest':
        warnings.warn(skimage_deprecation(
            "Mode 'nearest' has been renamed to 'edge'. Mode 'nearest' will be "
            "removed in a future release."))
        mode = 'edge'
    return mode
def convert_to_float(image, preserve_range):
    
    
    if preserve_range:
        if image.dtype.char not in 'df':
            image = image.astype(np.double)
    else:
        image = img_as_float(image)
    return image
def check_shape_equality(*images):
    
    image0 = images[0]
    if not all(image0.shape == image.shape for image in images[1:]):
        raise ValueError('Input images must have the same dimensions.')
    return
def _supported_float_type(input_dtype, allow_complex=False):
    
    
    if isinstance(input_dtype, tuple):
        return np.result_type(*(_supported_float_type(d) for d in input_dtype))
    input_dtype = np.dtype(input_dtype)
    if not allow_complex and input_dtype.kind == 'c':
        raise ValueError("complex valued input is not supported")
    return new_float_type.get(input_dtype.char, np.float64)
def test_basic_edit_GET_string_PK(self):
        
        
        
        response = self.client.get(reverse('admin:admin_views_section_change', args=('abc',)), follow=True)
        self.assertRedirects(response, reverse('admin:index'))
        self.assertEqual(
            [m.message for m in response.context['messages']],
            ["section with ID abc doesn't exist. Perhaps it was deleted?"]
        )
def test_change_list_sorting_callable_query_expression(self):
        
        tests = [
            ('order_by_expression', 9),
            ('order_by_f_expression', 12),
            ('order_by_orderby_expression', 13),
        ]
        for admin_order_field, index in tests:
            with self.subTest(admin_order_field):
                response = self.client.get(
                    reverse('admin:admin_views_article_changelist'),
                    {'o': index},
                )
                self.assertContentBefore(
                    response, 'Oldest content', 'Middle content',
                    'Results of sorting on callable are out of order.'
                )
                self.assertContentBefore(
                    response, 'Middle content', 'Newest content',
                    'Results of sorting on callable are out of order.'
                )
def to_bytes(obj, encoding=DEFAULT_ENCODING, errors='strict'):
     
    
    return obj.encode(encoding, errors) if isinstance(obj, six.text_type) else obj
def chown_r(path: str, user: str):
    
    # keep these imports here for Windows compatibility
    import grp
    import pwd

    uid = pwd.getpwnam(user).pw_uid
    gid = grp.getgrnam(user).gr_gid
    os.chown(path, uid, gid)
    for root, dirs, files in os.walk(path):
        for dirname in dirs:
            os.chown(os.path.join(root, dirname), uid, gid)
        for filename in files:
            os.chown(os.path.join(root, filename), uid, gid)
def rm_rf(path):
    
    
    
    if not path or not os.path.exists(path):
        return
    # Make sure all files are writeable and dirs executable to remove
    chmod_r(path, 0o777)
    if os.path.isfile(path):
        os.remove(path)
    else:
        shutil.rmtree(path)
def parse_request_data(method: str, path: str, data=None, headers=None):
    
    result = {}
    headers = headers or {}
    content_type = headers.get("Content-Type", "")

    # add query params to result
    parsed_path = urlparse(path)
    result.update(parse_qs(parsed_path.query))

    # add params from url-encoded payload
    if method in ["POST", "PUT", "PATCH"] and (not content_type or "form-" in content_type):
        # content-type could be either "application/x-www-form-urlencoded" or "multipart/form-data"
        try:
            params = parse_qs(to_str(data or ""))
            result.update(params)
        except Exception:
            pass  # probably binary / JSON / non-URL encoded payload - ignore

    # select first elements from result lists (this is assuming we are not using parameter lists!)
    result = dict([(k, v[0]) for k, v in result.items()])
    return result
def download(url: str, path: str, verify_ssl: bool = True, timeout: float = None):
    

    # make sure we're creating a new session here to enable parallel file downloads
    s = requests.Session()
    proxies = get_proxies()
    if proxies:
        s.proxies.update(proxies)

    # Use REQUESTS_CA_BUNDLE path. If it doesn't exist, use the method provided settings.
    # Note that a value that is not False, will result to True and will get the bundle file.
    _verify = os.getenv("REQUESTS_CA_BUNDLE", verify_ssl)

    r = None
    try:
        r = s.get(url, stream=True, verify=_verify, timeout=timeout)
        # check status code before attempting to read body
        if not r.ok:
            raise Exception("Failed to download %s, response code %s" % (url, r.status_code))

        total = 0
        if not os.path.exists(os.path.dirname(path)):
            os.makedirs(os.path.dirname(path))
        LOG.debug(
            "Starting download from %s to %s (%s bytes)"
            % (url, path, r.headers.get("Content-Length"))
        )
        with open(path, "wb") as f:
            iter_length = 0
            iter_limit = 1000000  # print a log line for every 1MB chunk
            for chunk in r.iter_content(DOWNLOAD_CHUNK_SIZE):
                total += len(chunk)
                iter_length += len(chunk)
                if chunk:  # filter out keep-alive new chunks
                    f.write(chunk)
                else:
                    LOG.debug("Empty chunk %s (total %s) from %s" % (chunk, total, url))
                if iter_length >= iter_limit:
                    LOG.debug("Written %s bytes (total %s) to %s" % (iter_length, total, path))
                    iter_length = 0
            f.flush()
            os.fsync(f)
        if os.path.getsize(path) == 0:
            LOG.warning("Zero bytes downloaded from %s, retrying" % url)
            download(url, path, verify_ssl)
            return
        LOG.debug(
            "Done downloading %s, response code %s, total bytes %d" % (url, r.status_code, total)
        )
    except requests.exceptions.ReadTimeout as e:
        raise TimeoutError(f"Timeout ({timeout}) reached on download: {url} - {e}")
    finally:
        if r is not None:
            r.close()
        s.close()
def _generate_auth_url(self, query_parameters_dict: Dict[str, str], force_browser_logout: bool) -> str:
        
        
        
        auth_url = "{}?{}".format(self._auth_url, urlencode(query_parameters_dict))
        if force_browser_logout:
            # The url after '?next=' should be urlencoded
            auth_url = "{}?next={}".format(MYCLOUD_LOGOFF_URL, quote_plus(auth_url))
        return auth_url
def checkCloseDo (self, event, byMenu):
        
        
        
                
        if (self.dirty):
            bits = os.path.splitext(self.saveDestination)
            title = '"' + os.path.basename(bits[0]) + '"'
            if title == '""': title = 'your story' 

            message = 'Do you want to save the changes to ' + title + ' before closing?'
            dialog = wx.MessageDialog(self, message, 'Unsaved Changes', \
                                      wx.ICON_WARNING | wx.YES_NO | wx.CANCEL | wx.YES_DEFAULT)
            result = dialog.ShowModal();
            if (result == wx.ID_CANCEL):
                event.Veto()
                return
            elif (result == wx.ID_NO):
                self.dirty = False
            else:
                self.save(None)
                if self.dirty:
                    event.Veto()
                    return
        
        # ask all our widgets to close any editor windows
        
        for w in list(self.storyPanel.widgets):
            if isinstance(w, PassageWidget):
                w.closeEditor()

        self.app.removeStory(self, byMenu)
        if event != None:
            event.Skip()
        self.Destroy()
def importHtml (self, path):
        
        try:
            # have a TiddlyWiki object parse it for us
            tw = TiddlyWiki()
            tw.addHtmlFromFilename(path)
            
            # add passages for each of the tiddlers the TiddlyWiki saw
            if len(tw.tiddlers):
                lastpos = [0, 0]
                for t in tw.tiddlers:
                    tiddler = tw.tiddlers[t]
                    new = self.storyPanel.newWidget(title = tiddler.title, text = tiddler.text, \
                                                    pos = tiddler.pos if tiddler.pos != None else lastpos, \
                                                    logicals = True, quietly = True)
                    lastpos = new.pos
                    new.passage.tags = tiddler.tags
                self.setDirty(True, 'Import')
            else:
                dialog = wx.MessageDialog(self, 'No passages were found in this file. Make sure ' + \
                                          'this is a Twine game file.', 'No Passages Found', \
                                          wx.ICON_INFORMATION | wx.OK)
                dialog.ShowModal()
        except:
            self.app.displayError('importing from HTML')
def importImage(self, file, showdialog = True):
        
        try:
            image64 = open(file, 'rb').read().encode('base64').replace('\n', '')
            title, mimeType = os.path.splitext(os.path.basename(file))
            # Remove the extension's dot
            mimeType = mimeType[1:]
            # Correct certain MIME types
            if mimeType == "jpg":
                mimeType == "jpeg"
            elif mimeType == "svg":
                mimeType += "+xml"
            self.storyPanel.newWidget(title = title, text = "data:image/" + mimeType + ";base64," + image64, tags = ['image'])
            if showdialog:
                dialog = wx.MessageDialog(self, 'Image file imported successfully.\n' + \
                                          'You can include the image in your passages with this syntax:\n\n' + \
                                          '[img[' + title + ']]', 'Image added', \
                                          wx.ICON_INFORMATION | wx.OK)
                dialog.ShowModal()
            return True
        except IOError:
            self.app.displayError('importing an image')
            return False
def importImage(self, file, showdialog = True):
        
        try:
            text, title = self.openImageAsBase64(file)
            self.storyPanel.newWidget(text = text, title = title, tags = ['Twine.image'])
            if showdialog:
                dialog = wx.MessageDialog(self, 'Image file imported successfully.\n' + \
                                          'You can include the image in your passages with this syntax:\n\n' + \
                                          '[img[' + title + ']]', 'Image added', \
                                          wx.ICON_INFORMATION | wx.OK)
                dialog.ShowModal()
            return True
        except IOError:
            self.app.displayError('importing an image')
            return False
def rebuild (self, event = None, temp = False, displayAfter = False):
        
        
                
        try:
            # assemble our tiddlywiki and write it out
            hasstartpassage = False
            tw = TiddlyWiki()
            for widget in self.storyPanel.widgets:
                if widget.passage.title != 'StoryIncludes' and \
                not any(t in TiddlyWiki.NOINCLUDE_TAGS for t in widget.passage.tags):
                    widget.passage.pos = widget.pos
                    tw.addTiddler(widget.passage)
                    if widget.passage.title == "Start":
                        hasstartpassage = True

            # is there a Start passage?
            if hasstartpassage == False:
                self.app.displayError('building your story because there is no "Start" passage. ' + "\n" 
                                      + 'Your story will build but the web browser will not be able to run the story. ' + "\n"
                                      + 'Please add a passage with the title "Start"')

            for widget in self.storyPanel.widgets:
                if widget.passage.title == 'StoryIncludes':
                    tw = self.buildIncludes(tw, widget.passage.text.splitlines())
                    break
            
            # Decode story settings
            for widget in self.storyPanel.widgets:
                if widget.passage.title == 'StorySettings':
                    lines = widget.passage.text.splitlines()
                    for line in lines: 
                        if ':' in line:
                            (skey,svalue) = line.split(':')
                            skey = skey.strip().lower()
                            svalue = svalue.strip().lower()
                            tw.storysettings[skey] = svalue or True
                    break
            
            # Write the output file
            if temp:
                dest = tempfile.NamedTemporaryFile(mode = 'w', suffix = ".html", delete = False);
                name = dest.name
                dest.write(tw.toHtml(self.app, self.target).encode('utf-8'))
                dest.close()
                if displayAfter: self.viewBuild(name = name)
            else:
                dest = open(self.buildDestination, 'w')
                dest.write(tw.toHtml(self.app, self.target).encode('utf-8'))
                dest.close()
                if displayAfter: self.viewBuild()
        except:
            self.app.displayError('building your story')
def override_properties(char, properties=None, valid_values=None):
    
    if properties:
        char.properties.update(properties)

    if valid_values:
        char.properties['ValidValues'].update(valid_values)
def convert_page_format(value: str) -> Tuple[float, float]:
    
    logging.warning(
        "!!! `vpype.convert_page_format()` is deprecated, use `vpype.convert_page_size()` "
        "instead."
    )
    return convert_page_size(value)
def convert_length(value: Union[str, float]) -> float:
    
    
    return _convert_unit(value, UNITS)
def __init__(self, **kwargs):
        
        
        
        
        self.models = __import__("models", globals={
            "__name__":__name__,
            "__package__": __package__,
        }, level=1)
        self.subword = __import__("subword", globals={
            "__name__":__name__,
            "__package__": __package__,
        }, level=1)
        self.torch = __import__("torch")

        self.config = DEFAULT_CONFIG.copy()
        self.config.update(kwargs)
        check_parameters(DEFAULT_CONFIG, self.config)
        
        if self.config["device"] is None:
            if self.torch.cuda.is_available():
                self.device = self.torch.device("cuda")
            else:
                self.device = self.torch.device("cpu")
        else:
            self.device = self.torch.device( self.config["device"] )
        
        self.processor = self.config["processor"]

        # Use DataManager Here
        model_path = DataManager.load("AttackAssist.SCPN")
        pp_model = self.torch.load(model_path["scpn.pt"], map_location=self.device)
        parse_model = self.torch.load(model_path["parse_generator.pt"], map_location=self.device)
        pp_vocab, rev_pp_vocab = pickle.load(open(model_path["parse_vocab.pkl"], 'rb'))
        bpe_codes = open(model_path["bpe.codes"], "r", encoding="utf-8")
        bpe_vocab = open(model_path["vocab.txt"], "r", encoding="utf-8")
        self.parse_gen_voc = pickle.load(open(model_path["ptb_tagset.pkl"], "rb"))


        self.pp_vocab = pp_vocab
        self.rev_pp_vocab = rev_pp_vocab
        self.rev_label_voc = dict((v,k) for (k,v) in self.parse_gen_voc.items())

        # load paraphrase network
        pp_args = pp_model['config_args']
        self.net = self.models.SCPN(pp_args.d_word, pp_args.d_hid, pp_args.d_nt, pp_args.d_trans, len(self.pp_vocab), len(self.parse_gen_voc) - 1, pp_args.use_input_parse)
        self.net.load_state_dict(pp_model['state_dict'])
        self.net = self.net.to(self.device).eval()

        # load parse generator network
        parse_args = parse_model['config_args']
        self.parse_net = self.models.ParseNet(parse_args.d_nt, parse_args.d_hid, len(self.parse_gen_voc))
        self.parse_net.load_state_dict(parse_model['state_dict'])
        self.parse_net = self.parse_net.to(self.device).eval()

        # instantiate BPE segmenter
        
        bpe_vocab = self.subword.read_vocabulary(bpe_vocab, 50)
        self.bpe = self.subword.BPE(bpe_codes, '@@', bpe_vocab, None)
def append(
        self, ids, predict, targets, ind2lab=None,
    ):
        
        
        self.ids.extend(ids)

        if ind2lab is not None:
            predict = ind2lab(predict)
            targets = [ind2lab(t) for t in targets]

        if self.merge_words:
            predict = merge_words(predict)
            targets = [merge_words(t) for t in targets]

        self.predicts.extend(predict)
        if self.targets is None:
            self.targets = targets
        else:
            assert len(self.targets) == len(targets)
            for i in range(len(self.targets)):
                self.targets[i].extend(targets[i])
def _make_disconnected(self, *_: core_Event) -> None:
        
        
        if self._attr_available:
            self._attr_available = False
            self.async_write_ha_state()
def predict(self, image_features, num_predictions_per_location,
              scope=None, **params):
    
    
    if len(image_features) != len(num_predictions_per_location):
      raise ValueError('image_feature and num_predictions_per_location must '
                       'be of same length, found: {} vs {}'.
                       format(len(image_features),
                              len(num_predictions_per_location)))
    if scope is not None:
      with tf.variable_scope(scope):
        return self._predict(image_features, num_predictions_per_location,
                             **params)
    else:
      return self._predict(image_features, num_predictions_per_location,
                           **params)
def get_serializer(self, *args, **kwargs):
        
        
        if len(args) == 1 and kwargs.get('many'):
            queryset = self.set_users_roles_for_cache(args[0])
            queryset = self.set_users_orgs_roles(args[0])
            args = (queryset,)
        return super().get_serializer(*args, **kwargs)
def check_recorded_metadata(obj, method, split_params=tuple(), **kwargs):
    

    
    records = getattr(obj, "_records", dict()).get(method, dict())
    assert set(kwargs.keys()) == set(records.keys())
    for key, value in kwargs.items():
        recorded_value = records[key]
        # The following condition is used to check for any specified parameters
        # being a subset of the original values
        if key in split_params and recorded_value is not None:
            assert np.isin(recorded_value, value).all()
        else:
            assert recorded_value is value
def translate_tariff(value, dsmr_version):
        
        # DSMR V5B: Note: In Belgium values are swapped:
        # Rate code 2 is used for low rate and rate code 1 is used for normal rate.
        if dsmr_version in ("5B"):
            if value == "0001":
                value = "0002"
            elif value == "0002":
                value = "0001"
        # DSMR V2.2: Note: Rate code 1 is used for low rate and rate code 2 is
        # used for normal rate.
        if value == "0002":
            return "normal"
        if value == "0001":
            return "low"

        return None
def __init__(
        self,
        coordinator: ScreenlogicDataUpdateCoordinator,
        entity_description: ScreenLogicPushEntityDescription,
    ) -> None:
        
        super().__init__(coordinator, entity_description)
        self._last_update_success = True
def _annotate_landmarks(self, image: np.ndarray, landmarks: np.ndarray) -> None:
         
        
        # Mesh
        for start, end, fill in LANDMARK_PARTS[LandmarkType.from_shape(landmarks.shape)].values():
            cv2.polylines(image, [landmarks[start:end]], fill, (255, 255, 0), 1)
        # Landmarks
        for (pos_x, pos_y) in landmarks:
            cv2.circle(image, (pos_x, pos_y), 1, (0, 255, 255), -1)
def tuple_variable_schema(self, schema: core_schema.TupleVariableSchema) -> JsonSchemaValue:
        
        
        
        # NOTE: The `items_schema` is always added, even when we explicitly create a
        # tuple variable schema without an `items_schema`.
        items = self.generate_inner(schema['items_schema'])  # type: ignore
        json_schema: JsonSchemaValue = {'type': 'array', 'items': items}
        self.update_with_validations(json_schema, schema, self.ValidationsMapping.array)
        return json_schema
def chain_schema(self, schema: core_schema.ChainSchema) -> JsonSchemaValue:
        
        
        step_index = 0 if self.mode == 'validation' else -1  # use first step for validation, last for serialization
        return self.generate_inner(schema['steps'][step_index])
def tuple_positional_schema(self, schema: core_schema.TupleSchema) -> JsonSchemaValue:
        
        warnings.warn(
            '`tuple_positional_schema` is deprecated. Use `tuple_schema` instead.',
            PydanticDeprecatedSince26,
            stacklevel=2,
        )
        return self.tuple_schema(schema)
def is_ready(
        ctrl_address: str,
        protocol: Optional[str] = 'grpc',
        timeout: float = 1.0,
        **kwargs,
    ) -> bool:
        
        
        

        if (
            protocol is None
            or protocol == GatewayProtocolType.GRPC
            or protocol == 'grpc'
        ):
            res = AsyncNewLoopRuntime.is_ready(ctrl_address)
        else:
            try:
                conn = urllib.request.urlopen(
                    url=f'http://{ctrl_address}', timeout=timeout
                )
                res = conn.code == HTTPStatus.OK
            except:
                res = False
        return res
def process(self, in_queue, out_queue):
         
        
        logger.debug("Starting convert process. (in_queue: %s, out_queue: %s)",
                     in_queue, out_queue)
        while True:
            items = in_queue.get()
            if items == "EOF":
                logger.debug("EOF Received")
                logger.debug("Patch queue finished")
                # Signal EOF to other processes in pool
                logger.debug("Putting EOF back to in_queue")
                in_queue.put(items)
                break

            if isinstance(items, dict):
                items = [items]
            for item in items:
                logger.trace("Patch queue got: '%s'", item["filename"])
                try:
                    image = self._patch_image(item)
                except Exception as err:  # pylint: disable=broad-except
                    # Log error and output original frame
                    logger.error("Failed to convert image: '%s'. Reason: %s",
                                 item["filename"], str(err))
                    image = item["image"]
                    # UNCOMMENT THIS CODE BLOCK TO PRINT TRACEBACK ERRORS
                    # import sys ; import traceback
                    # exc_info = sys.exc_info() ; traceback.print_exception(*exc_info)
                logger.trace("Out queue put: %s", item["filename"])
                out_queue.put((item["filename"], image))
        logger.debug("Completed convert process")
def _get_image_mask(self, new_face, detected_face, predicted_mask, reference_face):
         
        
        logger.trace("Getting mask. Image shape: %s", new_face.shape)
        if self._args.mask_type not in ("none", "predicted"):
            mask_centering = detected_face.mask[self._args.mask_type].stored_centering
        else:
            mask_centering = "face"  # Unused but requires a valid value
        crop_offset = (reference_face.pose.offset[self._centering] -
                       reference_face.pose.offset[mask_centering])
        mask, raw_mask = self._adjustments["mask"].run(detected_face, crop_offset, self._centering,
                                                       predicted_mask=predicted_mask)
        logger.trace("Adding mask to alpha channel")
        new_face = np.concatenate((new_face, mask), -1)
        logger.trace("Got mask. Image shape: %s", new_face.shape)
        return new_face, raw_mask
def add_queue(self, name, maxsize=0):
          

        logger.debug("QueueManager adding: (name: '%s', maxsize: %s)", name, maxsize)
        if name in self.queues.keys():
            raise ValueError("Queue '{}' already exists.".format(name))
        queue = self.manager.Queue(maxsize=maxsize)
        setattr(queue, "shutdown", self.shutdown)
        self.queues[name] = queue
        logger.debug("QueueManager added: (name: '%s')", name)
def test_create_without_credentials(self):
        
        
        
        conn_params = {'hostname': 'dummy.host.org',
                       'username': 'ubuntu'}
        mock = ParamikoSSHClient(**conn_params)

        self.assertEqual(mock.password, None)
        self.assertEqual(mock.key_material, None)
        self.assertEqual(mock.key_files, None)
def get_dataset_shard(
    dataset_name: Optional[str] = None,
) -> Optional["DataIterator"]:
    
    
    session = _get_session()
    if not isinstance(session, _TrainSessionImpl):
        raise RuntimeError(
            "`get_dataset_shard` can only be called for TrainSession! "
            "Make sure you only use that in `train_loop_per_worker` function"
            "that is passed into `DataParallelTrainer`."
        )
    return session.get_dataset_shard(dataset_name)
def __init__(self, root, engine, flist):
        
        
        super().__init__(root, engine)
        self.flist = flist
        self.globvar = StringVar(root)
        self.recvar = BooleanVar(root)
def _get_alignments(self, alignments_path, input_location):
         
        
        logger.debug("alignments_path: %s, input_location: %s", alignments_path, input_location)
        if alignments_path:
            folder, filename = os.path.split(alignments_path)
        else:
            filename = "alignments.fsa"
            if self._globals.is_video:
                folder, vid = os.path.split(os.path.splitext(input_location)[0])
                filename = "{}_{}".format(vid, filename)
            else:
                folder = input_location
        retval = Alignments(folder, filename)
        if retval.version == 1.0:
            logger.error("The Manual Tool is not compatible with legacy Alignments files.")
            logger.info("You can update legacy Alignments files by using the Extract job in the "
                        "Alignments tool to re-extract the faces in full-head format.")
            sys.exit(0)
        logger.debug("folder: %s, filename: %s, alignments: %s", folder, filename, retval)
        return retval
def post_edit_trigger(self, frame_index, face_index):
         
        
        face = self._frame_faces[frame_index][face_index]
        face.load_aligned(None, force=True)  # Update average distance
        face.mask = self._extractor.get_masks(frame_index, face_index)

        aligned = AlignedFace(face.landmarks_xy,
                              image=self._globals.current_frame["image"],
                              centering="head",
                              size=96)
        face.thumbnail = generate_thumbnail(aligned.face, size=96)
        if self._globals.filter_mode == "Misaligned Faces":
            self._detected_faces.tk_face_count_changed.set(True)
        self._tk_edited.set(True)
def available_masks(self) -> dict[str, int]:
          
        return self._alignments.mask_summary
def cancel_all_orders(self, symbol: Str = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' cancelAllOrders() requires a symbol argument')
        self.load_markets()
        request = {}
        market = None
        market = self.market(symbol)
        request['symbol'] = market['id']
        response = self.privateDeleteOrderAll(self.extend(request, params))
        #
        #     [
        #         {
        #             "title": "string",
        #             "symbol": "xht-usdt",
        #             "side": "sell",
        #             "size": 1,
        #             "type": "limit",
        #             "price": 0.1,
        #             "id": "string",
        #             "created_by": 34,
        #             "filled": 0
        #         }
        #     ]
        #
        return self.parse_orders(response, market)
def fetch_order_book(self, symbol: str, limit: Int = None, params={}) -> OrderBook:
        
        
        
        self.load_markets()
        market = self.market(symbol)
        request = {
            'symbol': market['id'],
        }
        response = self.publicGetOrderbook(self.extend(request, params))
        #
        #     {
        #         "btc-usdt": {
        #             "bids": [
        #                 [8836.4, 1.022],
        #                 [8800, 0.0668],
        #                 [8797.75, 0.2398],
        #             ],
        #             "asks": [
        #                 [8839.35, 1.5334],
        #                 [8852.6, 0.0579],
        #                 [8860.45, 0.1815],
        #             ],
        #             "timestamp": "2020-03-03T02:27:25.147Z"
        #         },
        #         "eth-usdt": {},
        #         # ...
        #     }
        #
        orderbook = self.safe_value(response, market['id'])
        timestamp = self.parse8601(self.safe_string(orderbook, 'timestamp'))
        return self.parse_order_book(orderbook, market['symbol'], timestamp)
def keypoint_rcnn_inference(pred_keypoint_logits, pred_instances):
    
    
    
    # flatten all bboxes from all images together (list[Boxes] -> Rx4 tensor)
    bboxes_flat = cat([b.pred_boxes.tensor for b in pred_instances], dim=0)

    pred_keypoint_logits = pred_keypoint_logits.detach()
    keypoint_results = heatmaps_to_keypoints(pred_keypoint_logits, bboxes_flat.detach())
    num_instances_per_image = [len(i) for i in pred_instances]
    keypoint_results = keypoint_results[:, :, [0, 1, 3]].split(num_instances_per_image, dim=0)
    heatmap_results = pred_keypoint_logits.split(num_instances_per_image, dim=0)

    for keypoint_results_per_image, heatmap_results_per_image, instances_per_image in zip(
        keypoint_results, heatmap_results, pred_instances
    ):
        # keypoint_results_per_image is (num instances)x(num keypoints)x(x, y, score)
        # heatmap_results_per_image is (num instances)x(num keypoints)x(side)x(side)
        instances_per_image.pred_keypoints = keypoint_results_per_image
        instances_per_image.pred_keypoint_heatmaps = heatmap_results_per_image
def forward(self, x, instances: List[Instances]):
        
        
        
        x = self.layers(x)
        if self.training:
            num_images = len(instances)
            normalizer = (
                None
                if self.normalize_by_visible_keypoints
                else num_images * self.normalizer_per_img
            )
            return {
                "loss_keypoint": keypoint_rcnn_loss(x, instances, normalizer=normalizer)
                * self.loss_weight
            }
        else:
            keypoint_rcnn_inference(x, instances)
            return instances
def async_get_options_flow(
        config_entry: ConfigEntry,
    ) -> SchemaOptionsFlowHandler:
        
        return SchemaOptionsFlowHandler(config_entry, OPTIONS_FLOW)
def __getitem__(self, idx: int) -> Dict[str, Any]:
        
        
        
        categories = [self.category_list[idx]]
        fpath = self.image_list[idx]

        try:
            image = torch.from_numpy(np.ascontiguousarray(read_image(fpath, format="BGR")))
            if self.transform is not None:
                # Transforms are done on batches
                image = self.transform(image.unsqueeze(0))[0]  # pyre-ignore[29]
            return {"images": image, "categories": categories}
        except (OSError, RuntimeError) as e:
            logger = logging.getLogger(__name__)
            logger.warning(f"Error opening image file container {fpath}: {e}")

        return {"images": self._EMPTY_IMAGE, "categories": []}
def __getitem__(self, idx: int) -> Dict[str, Any]:
        
        
        
        categories = [self.category_list[idx]]
        fpath = self.image_list[idx]
        transform = self.transform

        try:
            image = torch.from_numpy(np.ascontiguousarray(read_image(fpath, format="BGR")))
            image = image.permute(2, 0, 1).unsqueeze(0)
            if transform is not None:
                image = transform(image)
            return {"images": image, "categories": categories}
        except (OSError, RuntimeError) as e:
            logger = logging.getLogger(__name__)
            logger.warning(f"Error opening image file container {fpath}: {e}")

        return {"images": self._EMPTY_IMAGE, "categories": []}
def __call__(
        self,
        proposals_with_gt: List[Instances],
        densepose_predictor_outputs: Any,
        tensors_helper: Optional[SingleTensorsHelper] = None,
    ) -> torch.Tensor:
        
        
        if tensors_helper is None:
            tensors_helper = SingleTensorsHelper(proposals_with_gt)
        coarse_segm_est = densepose_predictor_outputs.coarse_segm[tensors_helper.index_with_dp]
        with torch.no_grad():
            coarse_segm_gt = resample_data(
                tensors_helper.coarse_segm_gt.unsqueeze(1),
                tensors_helper.bbox_xywh_gt,
                tensors_helper.bbox_xywh_est,
                self.heatmap_size,
                self.heatmap_size,
                mode="nearest",
                padding_mode="zeros",
            ).squeeze(1)
        if self.n_segm_chan == 2:
            coarse_segm_gt = coarse_segm_gt > 0
        return F.cross_entropy(coarse_segm_est, coarse_segm_gt.long())
def __call__(
        self,
        proposals_with_gt: List[Instances],
        densepose_predictor_outputs: Any,
        packed_annotations: Any,
    ) -> torch.Tensor:
        
        
        
        coarse_segm_est = densepose_predictor_outputs.coarse_segm[packed_annotations.bbox_indices]
        with torch.no_grad():
            coarse_segm_gt = resample_data(
                packed_annotations.coarse_segm_gt.unsqueeze(1),
                packed_annotations.bbox_xywh_gt,
                packed_annotations.bbox_xywh_est,
                self.heatmap_size,
                self.heatmap_size,
                mode="nearest",
                padding_mode="zeros",
            ).squeeze(1)
        if self.n_segm_chan == 2:
            coarse_segm_gt = coarse_segm_gt > 0
        return F.cross_entropy(coarse_segm_est, coarse_segm_gt.long())
def ddp_init_group(run_opts):
    
    
    if run_opts["distributed_launch"]:
        if "local_rank" not in run_opts:
            raise ValueError(
                "To use DDP backend, start your script with:\n\t"
                "python -m torch.distributed.launch [args]\n\t"
                "experiment.py hyperparams.yaml --distributed_launch "
                "--distributed_backend=nccl"
            )
        else:
            if run_opts["local_rank"] + 1 > torch.cuda.device_count():
                raise ValueError(
                    "Killing process " + str() + "\n"
                    "Not enough GPUs available!"
                )
        if "RANK" in os.environ is None or os.environ["RANK"] == "":
            raise ValueError(
                "To use DDP backend, start your script with:\n\t"
                "python -m torch.distributed.launch [args]\n\t"
                "experiment.py hyperparams.yaml --distributed_launch "
                "--distributed_backend=nccl"
            )
        rank = int(os.environ["RANK"])

        if run_opts["distributed_backend"] == "nccl":
            if not torch.distributed.is_nccl_available():
                raise ValueError("NCCL is not supported in your machine.")
        elif run_opts["distributed_backend"] == "gloo":
            if not torch.distributed.is_gloo_available():
                raise ValueError("GLOO is not supported in your machine.")
        elif run_opts["distributed_backend"] == "mpi":
            if not torch.distributed.is_mpi_available():
                raise ValueError("MPI is not supported in your machine.")
        else:
            logger.info(
                run_opts["distributed_backend"]
                + " communcation protocol doesn't exist."
            )
            raise ValueError(
                run_opts["distributed_backend"]
                + " communcation protocol doesn't exist."
            )
        # rank arg is used to set the right rank of the current process for ddp.
        # if you have 2 servers with 2 gpu:
        # server1:
        #   GPU0: local_rank=device=0, rank=0
        #   GPU1: local_rank=device=1, rank=1
        # server2:
        #   GPU0: local_rank=device=0, rank=2
        #   GPU1: local_rank=device=1, rank=3
        torch.distributed.init_process_group(
            backend=run_opts["distributed_backend"], rank=rank
        )
    else:
        logger.info(
            "distributed_launch flag is disabled, "
            "this experiment will be executed without DDP."
        )
        if "local_rank" in run_opts and run_opts["local_rank"] > 0:
            raise ValueError(
                "DDP is disabled, local_rank must not be set.\n"
                "For DDP training, please use --distributed_launch. "
                "For example:\n\tpython -m torch.distributed.launch "
                "experiment.py hyperparams.yaml "
                "--distributed_launch --distributed_backend=nccl"
            )
def if_main_process():
    
    
    if "RANK" in os.environ:
        if os.environ["RANK"] == "":
            return False
        else:
            if int(os.environ["RANK"]) == 0:
                return True
            return False
    return True
def convert_input(self, data: Any) -> Any:
        
        
        return data
def __init__(
        self,
        api_key: Optional[str] = None,
        output_type: Union[OutputType, dict] = "html",
        split: SplitType = "none",
        use_ocr: bool = False,
        exclude: list = [],
    ):
        
        
        
        self.api_key = get_from_param_or_env(
            "UPSTAGE_DOCUMENT_AI_API_KEY", api_key, "UPSTAGE_DOCUMENT_AI_API_KEY"
        )

        self.output_type = output_type
        self.split = split
        self.use_ocr = use_ocr
        self.exclude = exclude

        validate_api_key(self.api_key)
def from_llm(
        cls,
        llm: BaseLanguageModel,
        agent_tools: Optional[Sequence[BaseTool]] = None,
        output_parser: Optional[TrajectoryOutputParser] = None,
        **kwargs: Any,
    ) -> "TrajectoryEvalChain":
        
        
        if not isinstance(llm, BaseChatModel):
            raise NotImplementedError(
                "Only chat models supported by the current trajectory eval"
            )
        if agent_tools:
            prompt = EVAL_CHAT_PROMPT
        else:
            prompt = TOOL_FREE_EVAL_CHAT_PROMPT
        eval_chain = LLMChain(llm=llm, prompt=prompt)
        return cls(
            agent_tools=agent_tools,
            eval_chain=eval_chain,
            output_parser=output_parser or TrajectoryOutputParser(),
            **kwargs,
        )
def compile(self, context, schema="json", mode="children", template=SIMPLE_TEMPLATE) -> str:
        
        
        
        if schema == "raw":
            return context + "\n\n## Actions\n" + LANGUAGE_CONSTRAINT + "\n" + self.instruction

        # FIXME: json instruction会带来格式问题，如："Project name": "web_2048  # 项目名称使用下划线",
        # compile example暂时不支持markdown
        instruction = self.compile_instruction(schema="markdown", mode=mode)
        example = self.compile_example(schema=schema, tag=TAG, mode=mode)
        # nodes = ", ".join(self.to_dict(mode=mode).keys())
        constraints = [LANGUAGE_CONSTRAINT, FORMAT_CONSTRAINT]
        constraint = "\n".join(constraints)

        prompt = template.format(
            context=context,
            example=example,
            instruction=instruction,
            constraint=constraint,
        )
        return prompt
def _get_output_suffix(self, arguments):
         
        
        sfx = "{}_mask_preview_".format(self._mask_type)
        sfx += "face_" if not arguments.full_frame or self._input_is_faces else "frame_"
        sfx += "{}.png".format(arguments.output_type)
        return sfx
def _get_extractor(self) -> Optional[Extractor]:
         
        
        if self._args.processing == "output":
            logger.debug("Update type `output` selected. Not launching extractor")
            return None
        logger.debug("masker: %s", self._args.masker)
        extractor = Extractor(None, None, self._args.masker, exclude_gpus=self._args.exclude_gpus)
        logger.debug(extractor)
        return extractor
def _create_image(self, detected_face, mask_type):
         
        
        mask = detected_face.mask[mask_type]
        mask.set_blur_and_threshold(**self._output["opts"])
        if not self._output["full_frame"] or self._input_is_faces:
            if self._input_is_faces:
                face = AlignedFace(detected_face.landmarks_xy,
                                   image=detected_face.image,
                                   centering=mask.stored_centering,
                                   size=detected_face.image.shape[0],
                                   is_aligned=True).face
            else:
                centering = "legacy" if self._alignments.version == 1.0 else mask.stored_centering
                detected_face.load_aligned(detected_face.image, centering=centering, force=True)
                face = detected_face.aligned.face
            mask = cv2.resize(detected_face.mask[mask_type].mask,
                              (face.shape[1], face.shape[0]),
                              interpolation=cv2.INTER_CUBIC)[..., None]
        else:
            face = np.array(detected_face.image)  # cv2 fails if this comes as imageio.core.Array
            mask = mask.get_full_frame_mask(face.shape[1], face.shape[0])
            mask = np.expand_dims(mask, -1)

        height, width = face.shape[:2]
        if self._output["type"] == "combined":
            masked = (face.astype("float32") * mask.astype("float32") / 255.).astype("uint8")
            mask = np.tile(mask, 3)
            for img in (face, masked, mask):
                cv2.rectangle(img, (0, 0), (width - 1, height - 1), (255, 255, 255), 1)
                out_image = np.concatenate((face, masked, mask), axis=1)
        elif self._output["type"] == "mask":
            out_image = mask
        elif self._output["type"] == "masked":
            out_image = np.concatenate([face, mask], axis=-1)
        return out_image
def _update_faces(self, extractor_output: ExtractMedia) -> None:
         
        
        assert self._faces_saver is not None
        for face in extractor_output.detected_faces:
            frame_name = extractor_output.frame_metadata["source_filename"]
            face_index = extractor_output.frame_metadata["face_index"]
            logger.trace("Saving face: (frame: %s, face index: %s)",  # type: ignore
                         frame_name, face_index)

            self._alignments.update_face(frame_name, face_index, face.to_alignment())
            metadata = dict(alignments=face.to_png_meta(),
                            source=extractor_output.frame_metadata)
            self._faces_saver.save(extractor_output.filename,
                                   encode_image(extractor_output.image, ".png", metadata=metadata))

            if self._saver is not None:
                face.image = extractor_output.image
                self._save(frame_name, face_index, face)
def _get_alignments(self, alignments: str | None, input_location: str) -> Alignments | None:
         
        
        if alignments:
            logger.debug("Alignments location provided: %s", alignments)
            return Alignments(os.path.dirname(alignments),
                              filename=os.path.basename(alignments))
        if self._input_is_faces and self._update_type == "output":
            logger.debug("No alignments file provided for faces. Using PNG Header for output")
            return None
        if self._input_is_faces:
            logger.warning("Faces input selected without an alignments file. Masks wil only "
                           "be updated in the faces' PNG Header")
            return None

        folder = input_location
        if self._loader.is_video:
            logger.debug("Alignments from Video File: '%s'", folder)
            folder, filename = os.path.split(folder)
            filename = f"{os.path.splitext(filename)[0]}_alignments.fsa"
        else:
            logger.debug("Alignments from Input Folder: '%s'", folder)
            filename = "alignments"

        retval = Alignments(folder, filename=filename)
        self._loader.add_alignments(retval)
        return retval
def available(self) -> bool:
        
        return super().available and self._panel.online
def _fit(
        self,
        X,
        y,
        max_samples=None,
        max_depth=None,
        check_input=True,
        **fit_params,
    ):
        
        
        random_state = check_random_state(self.random_state)

        # Remap output
        n_samples = X.shape[0]
        self._n_samples = n_samples
        y = self._validate_y(y)

        # Check parameters
        self._validate_estimator(self._get_estimator())

        if _routing_enabled():
            routed_params = process_routing(self, "fit", **fit_params)
        else:
            routed_params = Bunch()
            routed_params.estimator = Bunch(fit=fit_params)
            if "sample_weight" in fit_params:
                routed_params.estimator.fit["sample_weight"] = fit_params[
                    "sample_weight"
                ]

        if max_depth is not None:
            self.estimator_.max_depth = max_depth

        # Validate max_samples
        if max_samples is None:
            max_samples = self.max_samples
        elif not isinstance(max_samples, numbers.Integral):
            max_samples = int(max_samples * X.shape[0])

        if max_samples > X.shape[0]:
            raise ValueError("max_samples must be <= n_samples")

        # Store validated integer row sampling value
        self._max_samples = max_samples

        # Validate max_features
        if isinstance(self.max_features, numbers.Integral):
            max_features = self.max_features
        elif isinstance(self.max_features, float):
            max_features = int(self.max_features * self.n_features_in_)

        if max_features > self.n_features_in_:
            raise ValueError("max_features must be <= n_features")

        max_features = max(1, int(max_features))

        # Store validated integer feature sampling value
        self._max_features = max_features

        # Other checks
        if not self.bootstrap and self.oob_score:
            raise ValueError("Out of bag estimation only available if bootstrap=True")

        if self.warm_start and self.oob_score:
            raise ValueError("Out of bag estimate only available if warm_start=False")

        if hasattr(self, "oob_score_") and self.warm_start:
            del self.oob_score_

        if not self.warm_start or not hasattr(self, "estimators_"):
            # Free allocated memory, if any
            self.estimators_ = []
            self.estimators_features_ = []

        n_more_estimators = self.n_estimators - len(self.estimators_)

        if n_more_estimators < 0:
            raise ValueError(
                "n_estimators=%d must be larger or equal to "
                "len(estimators_)=%d when warm_start==True"
                % (self.n_estimators, len(self.estimators_))
            )

        elif n_more_estimators == 0:
            warn(
                "Warm-start fitting without increasing n_estimators does not "
                "fit new trees."
            )
            return self

        # Parallel loop
        n_jobs, n_estimators, starts = _partition_estimators(
            n_more_estimators, self.n_jobs
        )
        total_n_estimators = sum(n_estimators)

        # Advance random state to state after training
        # the first n_estimators
        if self.warm_start and len(self.estimators_) > 0:
            random_state.randint(MAX_INT, size=len(self.estimators_))

        seeds = random_state.randint(MAX_INT, size=n_more_estimators)
        self._seeds = seeds

        all_results = Parallel(
            n_jobs=n_jobs, verbose=self.verbose, **self._parallel_args()
        )(
            delayed(_parallel_build_estimators)(
                n_estimators[i],
                self,
                X,
                y,
                seeds[starts[i] : starts[i + 1]],
                total_n_estimators,
                verbose=self.verbose,
                check_input=check_input,
                fit_params=routed_params.estimator.fit,
            )
            for i in range(n_jobs)
        )

        # Reduce
        self.estimators_ += list(
            itertools.chain.from_iterable(t[0] for t in all_results)
        )
        self.estimators_features_ += list(
            itertools.chain.from_iterable(t[1] for t in all_results)
        )

        if self.oob_score:
            self._set_oob_score(X, y)

        return self
def native_value(self) -> float:
        
        info: PriceInfo = self.coordinator.data[self._station_id]
        return getattr(info, self._fuel_type)
def __init__(self,
                 inplanes,
                 planes,
                 stride=1,
                 dilation=1,
                 downsample=None,
                 groups=1,
                 base_width=4,
                 style='pytorch',
                 with_cp=False):
        
        
        super(Bottleneck, self).__init__()
        assert style in ['pytorch', 'caffe']

        if groups == 1:
            width = planes
        else:
            width = math.floor(planes * (base_width / 64)) * groups

        if style == 'pytorch':
            conv1_stride = 1
            conv2_stride = stride
        else:
            conv1_stride = stride
            conv2_stride = 1

        self.conv1 = nn.Conv2d(
            inplanes, width, kernel_size=1, stride=conv1_stride, bias=False)
        self.bn1 = nn.BatchNorm2d(width)
        self.conv2 = nn.Conv2d(
            width,
            width,
            kernel_size=3,
            stride=conv2_stride,
            padding=dilation,
            dilation=dilation,
            groups=groups,
            bias=False)
        self.bn2 = nn.BatchNorm2d(width)
        self.conv3 = nn.Conv2d(
            width, planes * self.expansion, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride
        self.dilation = dilation
        self.with_cp = with_cp
def smart_split(text):
    
    
    text = force_unicode(text)
    for bit in smart_split_re.finditer(text):
        yield bit.group(0)
def truncate_html_words(s, num, end_text='...'):
    
    s = force_unicode(s)
    length = int(num)
    if length <= 0:
        return u''
    html4_singlets = ('br', 'col', 'link', 'base', 'img', 'param', 'area', 'hr', 'input')
    # Set up regular expressions
    re_words = re.compile(r'&.*?;|<.*?>|(\w[\w-]*)', re.U)
    re_tag = re.compile(r'<(/)?([^ ]+?)(?: (/)| .*?)?>')
    # Count non-HTML words and keep note of open tags
    pos = 0
    end_text_pos = 0
    words = 0
    open_tags = []
    while words <= length:
        m = re_words.search(s, pos)
        if not m:
            # Checked through whole string
            break
        pos = m.end(0)
        if m.group(1):
            # It's an actual non-HTML word
            words += 1
            if words == length:
                end_text_pos = pos
            continue
        # Check for tag
        tag = re_tag.match(m.group(0))
        if not tag or end_text_pos:
            # Don't worry about non tags or tags after our truncate point
            continue
        closing_tag, tagname, self_closing = tag.groups()
        tagname = tagname.lower()  # Element names are always case-insensitive
        if self_closing or tagname in html4_singlets:
            pass
        elif closing_tag:
            # Check for match in open tags list
            try:
                i = open_tags.index(tagname)
            except ValueError:
                pass
            else:
                # SGML: An end tag closes, back to the matching start tag, all unclosed intervening start tags with omitted end tags
                open_tags = open_tags[i+1:]
        else:
            # Add it to the start of the open tags list
            open_tags.insert(0, tagname)
    if words <= length:
        # Don't try to close tags if we don't need to truncate
        return s
    out = s[:end_text_pos]
    if end_text:
        out += ' ' + end_text
    # Close any tags still open
    for tag in open_tags:
        out += '</%s>' % tag
    # Return string
    return out
def get_valid_filename(s):
    
    
    
    s = force_unicode(s).strip().replace(' ', '_')
    return re.sub(r'(?u)[^-\w.]', '', s)
def get_text_list(list_, last_word=ugettext_lazy('or')):
    
    
    
    if len(list_) == 0: return ''
    if len(list_) == 1: return force_unicode(list_[0])
    return '%s %s %s' % (
        # Translators: This string is used as a separator between list elements
        _(', ').join([force_unicode(i) for i in list_][:-1]),
        force_unicode(last_word), force_unicode(list_[-1]))
def slugify(value, allow_unicode=False):
    
    
    
    value = str(value)
    if allow_unicode:
        value = unicodedata.normalize('NFKC', value)
    else:
        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')
    value = re.sub(r'[^\w\s-]', '', value.lower())
    return re.sub(r'[-\s]+', '-', value).strip('-_')
def configure(self, build_script_folder=None, args=None):
        
        
        
        script_folder = os.path.join(self._conanfile.source_folder, build_script_folder) \
            if build_script_folder else self._conanfile.source_folder

        configure_args = []
        configure_args.extend(args or [])

        self._configure_args = "{} {}".format(self._configure_args, args_to_string(configure_args))

        configure_cmd = "{}/configure".format(script_folder)
        subsystem = deduce_subsystem(self._conanfile, scope="build")
        configure_cmd = subsystem_path(subsystem, configure_cmd)
        cmd = '"{}" {}'.format(configure_cmd, self._configure_args)
        self._conanfile.output.info("Calling:\n > %s" % cmd)
        self._conanfile.run(cmd)
def autoreconf(self, build_script_folder=None, args=None):
        
        
        
        script_folder = os.path.join(self._conanfile.source_folder, build_script_folder) \
            if build_script_folder else self._conanfile.source_folder
        args = args or []
        command = join_arguments(["autoreconf", self._autoreconf_args, cmd_args_to_string(args)])
        with chdir(self, script_folder):
            self._conanfile.run(command)
def __call__(self, layer_output: torch.Tensor, moe_output: torch.Tensor, scores: torch.Tensor,
                 mapped_slots: torch.Tensor, expert_counts: torch.Tensor) -> torch.Tensor:
        
        
        
        self.kernel(layer_output, moe_output, scores, mapped_slots, expert_counts, self.normalize_scores)
        return layer_output
def discover(self, start_dir, pattern='test*.py', top_level_dir=None):
        
        
        set_implicit_top = False
        if top_level_dir is None and self._top_level_dir is not None:
            # make top_level_dir optional if called from load_tests in a package
            top_level_dir = self._top_level_dir
        elif top_level_dir is None:
            set_implicit_top = True
            top_level_dir = start_dir

        top_level_dir = os.path.abspath(top_level_dir)

        if not top_level_dir in sys.path:
            # all test modules must be importable from the top level directory
            # should we *unconditionally* put the start directory in first
            # in sys.path to minimise likelihood of conflicts between installed
            # modules and development versions?
            sys.path.insert(0, top_level_dir)
        self._top_level_dir = top_level_dir

        is_not_importable = False
        is_namespace = False
        tests = []
        if os.path.isdir(os.path.abspath(start_dir)):
            start_dir = os.path.abspath(start_dir)
            if start_dir != top_level_dir:
                is_not_importable = not os.path.isfile(os.path.join(start_dir, '__init__.py'))
        else:
            # support for discovery from dotted module names
            try:
                __import__(start_dir)
            except ImportError:
                is_not_importable = True
            else:
                the_module = sys.modules[start_dir]
                top_part = start_dir.split('.')[0]
                try:
                    start_dir = os.path.abspath(
                       os.path.dirname((the_module.__file__)))
                except AttributeError:
                    # look for namespace packages
                    try:
                        spec = the_module.__spec__
                    except AttributeError:
                        spec = None

                    if spec and spec.loader is None:
                        if spec.submodule_search_locations is not None:
                            is_namespace = True

                            for path in the_module.__path__:
                                if (not set_implicit_top and
                                    not path.startswith(top_level_dir)):
                                    continue
                                self._top_level_dir = \
                                    (path.split(the_module.__name__
                                         .replace(".", os.path.sep))[0])
                                tests.extend(self._find_tests(path,
                                                              pattern,
                                                              namespace=True))
                    elif the_module.__name__ in sys.builtin_module_names:
                        # builtin module
                        raise TypeError('Can not use builtin modules '
                                        'as dotted module names') from None
                    else:
                        raise TypeError(
                            'don\'t know how to discover from {!r}'
                            .format(the_module)) from None

                if set_implicit_top:
                    if not is_namespace:
                        self._top_level_dir = \
                           self._get_directory_containing_module(top_part)
                        sys.path.remove(top_level_dir)
                    else:
                        sys.path.remove(top_level_dir)

        if is_not_importable:
            raise ImportError('Start directory is not importable: %r' % start_dir)

        if not is_namespace:
            tests = list(self._find_tests(start_dir, pattern))
        return self.suiteClass(tests)
def send_error_email(subject, message, additional_recipients=None):
    
    
    
    recipients = _email_recipients(additional_recipients)
    if recipients:
        sender = email().sender
        logger.info("Sending warning email to %r", recipients)
        send_email(
            subject=subject,
            message=message,
            sender=sender,
            recipients=recipients
        )
def send_error_email(subject, message, additional_recipients=None):
    
    
    
    recipients = _email_recipients(additional_recipients)
    sender = email().sender
    send_email(
        subject=subject,
        message=message,
        sender=sender,
        recipients=recipients
    )
def send_email(subject, message, sender, recipients, image_png=None):
    
    
    
    notifiers = {
        'ses': send_email_ses,
        'sendgrid': send_email_sendgrid,
        'smtp': send_email_smtp,
        'sns': send_email_sns,
    }

    subject = _prefix(subject)
    if not recipients or recipients == (None,) or _email_disabled():
        return

    # Clean the recipients lists to allow multiple error-email addresses, comma
    # separated in luigi.cfg
    recipients_tmp = []
    for r in recipients:
        recipients_tmp.extend([a.strip() for a in r.split(',') if a.strip()])

    # Replace original recipients with the clean list
    recipients = recipients_tmp

    # Get appropriate sender and call it to send the notification
    email_sender = notifiers[email().method]
    email_sender(sender, subject, message, recipients, image_png)
def _convert_centering(self, image: np.ndarray) -> np.ndarray:
         
        
        logger.trace("image_size: %s, target_size: %s, coverage_ratio: %s",  # type: ignore
                     image.shape[0], self.size, self._coverage_ratio)

        img_size = image.shape[0]
        target_size = get_centered_size(self._source_centering,
                                        self._centering,
                                        img_size,
                                        self._coverage_ratio)
        out = np.zeros((target_size, target_size, image.shape[-1]), dtype=image.dtype)

        slices = self._get_cropped_slices(img_size, target_size)
        out[slices["out"][0], slices["out"][1], :] = image[slices["in"][0], slices["in"][1], :]
        logger.trace("Cropped from aligned extract: (centering: %s, in shape: %s, "  # type: ignore
                     "out shape: %s)", self._centering, image.shape, out.shape)
        return out
def get_centered_size(source_centering: CenteringType,
                      target_centering: CenteringType,
                      size: int,
                      coverage_ratio: float = 1.0) -> int:
     
    
    if source_centering == target_centering and coverage_ratio == 1.0:
        retval = size
    else:
        src_size = size - (size * _EXTRACT_RATIOS[source_centering])
        retval = 2 * int(np.rint((src_size / (1 - _EXTRACT_RATIOS[target_centering])
                                 * coverage_ratio) / 2))
    logger.trace("source_centering: %s, target_centering: %s, size: %s, "  # type: ignore
                 "coverage_ratio: %s, source_size: %s, crop_size: %s", source_centering,
                 target_centering, size, coverage_ratio, src_size, retval)
    return retval
def calculate_expectancy(trades: pd.DataFrame) -> float:
    
    
    
    if len(trades) == 0:
        return 0

    expectancy = 0

    winning_trades = trades.loc[trades['profit_abs'] > 0]
    losing_trades = trades.loc[trades['profit_abs'] < 0]
    profit_sum = winning_trades['profit_abs'].sum()
    loss_sum = abs(losing_trades['profit_abs'].sum())
    nb_win_trades = len(winning_trades)
    nb_loss_trades = len(losing_trades)

    average_win = profit_sum / nb_win_trades
    average_loss = loss_sum / nb_loss_trades
    winrate = nb_win_trades / len(trades)
    loserate = nb_loss_trades / len(trades)
    expectancy = (winrate * average_win) - (loserate * average_loss)

    return expectancy
def thin(image, max_num_iter=None):
    
    
    
    # check that image is 2d
    check_nD(image, 2)

    # convert image to uint8 with values in {0, 1}
    skel = np.asanyarray(image, dtype=bool).astype(np.uint8)

    # neighborhood mask
    mask = np.array([[ 8,  4,   2],
                     [16,  0,   1],
                     [32, 64, 128]], dtype=np.uint8)

    # iterate until convergence, up to the iteration limit
    max_num_iter = max_num_iter or np.inf
    num_iter = 0
    n_pts_old, n_pts_new = np.inf, np.sum(skel)
    while n_pts_old != n_pts_new and num_iter < max_num_iter:
        n_pts_old = n_pts_new

        # perform the two "subiterations" described in the paper
        for lut in [G123_LUT, G123P_LUT]:
            # correlate image with neighborhood mask
            N = ndi.correlate(skel, mask, mode='constant')
            # take deletion decision from this subiteration's LUT
            D = np.take(lut, N)
            # perform deletion
            skel[D] = 0

        n_pts_new = np.sum(skel)  # count points after thinning
        num_iter += 1

    return skel.astype(bool)
def skeletonize_3d(image):
    

    
    # make sure the image is 3D or 2D
    if image.ndim < 2 or image.ndim > 3:
        raise ValueError(
            "skeletonize_3d can only handle 2D or 3D images; "
            f"got image.ndim = {image.ndim} instead."
        )

    image_io = image.astype(bool, copy=False)

    # make a 2D input image 3D and pad it w/ zeros to simplify dealing w/ boundaries
    # NB: careful here to not clobber the original *and* minimize copying
    if image.ndim == 2:
        image_io = image_io[np.newaxis, ...]
    image_o = np.pad(image_io, pad_width=1, mode='constant')

    # do the computation
    image_o = _compute_thin_image(image_o)

    # crop it back and restore the original intensity range
    image_o = crop(image_o, crop_width=1)
    if image.ndim == 2:
        image_o = image_o[0]

    return image_o
def completed_prefetch(self, blocking_wait=False, max_yield=999):
        

        for worker, obj_ref in self.completed(blocking_wait=blocking_wait):
            self._fetching.append((worker, obj_ref))

        for _ in range(max_yield):
            if not self._fetching:
                break

            yield self._fetching.popleft()
def safe_message(value):
    
    if isinstance(value, str):
        value = value.encode(ENCODING, ERRORS)

    value = value.decode(ENCODING, ERRORS)
    value = value.strip().splitlines()[0].strip()

    return value
def post_process(self, paths, dry_run=False, **options):
        
        
        
        # don't even dare to process the files if we're in dry run mode
        if dry_run:
            return

        # delete cache of all handled paths
        self.cache.delete_many([self.cache_key(path) for path in paths])

        # build a list of adjustable files
        matches = lambda path: matches_patterns(path, self._patterns.keys())
        adjustable_paths = [path for path in paths if matches(path)]

        # then sort the files by the directory level
        path_level = lambda name: len(name.split(os.sep))
        for name in sorted(paths.keys(), key=path_level, reverse=True):

            # use the original, local file, not the copied-but-unprocessed
            # file, which might be somewhere far away, like S3
            with paths[name] as original_file:

                # generate the hash with the original content, even for
                # adjustable files.
                hashed_name = self.hashed_name(name, original_file)

                # then get the original's file content..
                if hasattr(original_file, 'seek'):
                    original_file.seek(0)

                hashed_file_exists = self.exists(hashed_name)
                processed = False

                # ..to apply each replacement pattern to the content
                if name in adjustable_paths:
                    content = original_file.read()
                    converter = self.url_converter(name)
                    for patterns in self._patterns.values():
                        for pattern in patterns:
                            content = pattern.sub(converter, content)
                    if hashed_file_exists:
                        self.delete(hashed_name)
                    # then save the processed result
                    content_file = ContentFile(smart_str(content))
                    saved_name = self._save(hashed_name, content_file)
                    hashed_name = force_unicode(saved_name.replace('\\', '/'))
                    processed = True
                else:
                    # or handle the case in which neither processing nor
                    # a change to the original file happened
                    if not hashed_file_exists:
                        processed = True
                        saved_name = self._save(hashed_name, original_file)
                        hashed_name = force_unicode(saved_name.replace('\\', '/'))

                # and then set the cache accordingly
                self.cache.set(self.cache_key(name), hashed_name)
                yield name, hashed_name, processed
def __init__(self,
               block3_strides=True,
               name='DELG',
               gem_power=3.0,
               embedding_layer_dim=2048,
               scale_factor_init=45.25,  # sqrt(2048)
               arcface_margin=0.1,
               use_dim_reduction=False,
               reduced_dimension=128,
               dim_expand_channels=1024):
    
    
    logging.info('Creating Delg model, gem_power %d, embedding_layer_dim %d',
                 gem_power, embedding_layer_dim)
    super(Delg, self).__init__(block3_strides=block3_strides,
                               name=name,
                               pooling='gem',
                               gem_power=gem_power,
                               embedding_layer=True,
                               embedding_layer_dim=embedding_layer_dim,
                               use_dim_reduction=use_dim_reduction,
                               reduced_dimension=reduced_dimension,
                               dim_expand_channels=dim_expand_channels)
    self._embedding_layer_dim = embedding_layer_dim
    self._scale_factor_init = scale_factor_init
    self._arcface_margin = arcface_margin
def _check_data_type(device_ids: object) -> None:
    
    
    msg = "Device IDs (GPU/TPU) must be an int, a string, a sequence of ints, but you passed"
    if device_ids is None:
        raise TypeError(f"{msg} None")
    if isinstance(device_ids, (MutableSequence, tuple)):
        for id_ in device_ids:
            if type(id_) is not int:
                raise TypeError(f"{msg} a sequence of {type(id_).__name__}.")
    elif type(device_ids) not in (int, str):
        raise TypeError(f"{msg} {device_ids!r}.")
def test_numpyndarrayInput_l1(self):
        
        ndarray_matrix = np.array([[1, 0, 2],
                                      [0, 0, 3],
                                      [4, 5, 6]])
        normalized = self.model_l1.normalize(ndarray_matrix)

        # Check if output is of same type
        self.assertTrue(isinstance(normalized, np.ndarray))

        # Check if output is correct
        expected = np.array([[0.04761905, 0., 0.0952381],
                                [0., 0., 0.14285714],
                                [0.19047619, 0.23809524, 0.28571429]])
        self.assertTrue(np.allclose(normalized, expected))

        # Test if error is raised on unsupported input type
        self.assertRaises(ValueError, lambda model, doc: model.normalize(doc), self.model_l1, [1, 2, 3])
def available(self):
        
        
        return self.poe_mode is None or self.client.sw_mac and (
            self.controller.available or
            self.client.sw_mac in self.controller.api.devices)
def _get_full_filename(root: str | Path, pathname: str | Path) -> Path | None:
        
        
        
        if re.match(r"^/.+", pathname):
            return pathname
        files = list_files(root=root)
        postfix = "/" + str(pathname)
        for i in files:
            if str(i).endswith(postfix):
                return i
        return None
def convert_form_errors(form):
    
    errors = []
    for field in form.errors:
        for message in form.errors[field]:
            errors.append(Error(field=field, message=message))
    return errors
def clean_instance(cls, instance, errors):
        
        
        try:
            instance.full_clean()
        except ValidationError as error:
            if hasattr(cls._meta, 'exclude'):
                # Ignore validation errors for fields that are specified as
                # excluded.
                new_error_dict = {}
                for field, errors in error.error_dict.items():
                    if field not in cls._meta.exclude:
                        new_error_dict[field] = errors
                error.error_dict = new_error_dict

            if error.error_dict:
                raise error
def clean_instance(cls, instance):
        
        
        try:
            instance.full_clean()
        except ValidationError as error:
            if hasattr(cls._meta, 'exclude'):
                # Ignore validation errors for fields that are specified as
                # excluded.
                new_error_dict = {}
                for field, errors in error.error_dict.items():
                    if field not in cls._meta.exclude:
                        new_error_dict[field] = errors
                error.error_dict = new_error_dict

            if error.error_dict:
                raise error
def user_is_allowed(cls, user):
        
        
        if cls._meta.permissions:
            return user.has_perms(cls._meta.permissions)
        return True
def check_permissions(cls, context, permissions=None):
        
        
        permissions = permissions or cls._meta.permissions
        if not permissions:
            return True
        if context.user.has_perms(permissions):
            return True
        app = getattr(context, "app", None)
        if app and app.has_perms(permissions):
            return True
        return False
def handle_typed_errors(cls, errors: list, **extra):
        
        if hasattr(cls, "ERROR_TYPE_CLASS") and hasattr(cls, "ERROR_TYPE_FIELD"):
            typed_errors = [
                cls.ERROR_TYPE_CLASS(field=e.field, message=e.message, code=code)
                for e, code in errors
            ]
            extra.update({cls.ERROR_TYPE_FIELD: typed_errors})
        return cls(errors=[e[0] for e in errors], **extra)
def task_map(self) -> dict:
        
        
        
        return {
            'detect': {
                'predictor': RTDETRPredictor,
                'validator': RTDETRValidator,
                'trainer': RTDETRTrainer,
                'model': RTDETRDetectionModel}}
def dynamic_bucketed_batch(
    data,
    len_key=None,
    len_fn=len,
    min_sample_len=None,
    max_sample_len=None,
    buffersize=1024,
    collate_fn=PaddedBatch,
    sampler_fn=indices_around_random_pivot,
    sampler_kwargs={},
    drop_end=False,
):
    
    
    databuffer = []
    if sampler_kwargs:
        sampler_fn = partial(sampler_fn, **sampler_kwargs)
    for sample in data:
        # Length fetching interface has multiple valid call signatures:
        if len_key is not None and len_fn is not None:
            length = len_fn(sample[len_key])
        elif len_key is not None:
            length = sample[len_key]
        elif len_fn is not None:
            length = len_fn(sample)
        else:
            raise ValueError("Must specify at least one of len_key or len_fn")
        # Possibly filter by length:
        if (min_sample_len is not None and length < min_sample_len) or (
            max_sample_len is not None and length > max_sample_len
        ):
            # Drop sample
            continue
        item = LengthItem(length, sample)
        # bisect.insort inserts in sorted order.
        # This should be a good way to maintain a sorted list,
        # but perhaps simply filling up the buffer and calling .sort()
        # could be good as well (Python's sort leverages already sorted segments)
        bisect.insort(databuffer, item)
        if len(databuffer) == buffersize:
            indices = sampler_fn(databuffer)
            batch_list = []
            # popping from highest to lowest is safe
            for i in sorted(indices, reverse=True):
                item = databuffer.pop(i)
                batch_list.append(item.data)
            yield collate_fn(batch_list)
    # Data stream was exhausted. Data buffer is relatively full at first,
    # but cannot be replenished, so batches might not be efficiently produced.
    # Either stop, or exhaust buffer.
    if not drop_end:
        while databuffer:
            indices = sampler_fn(databuffer)
            batch_list = []
            for i in sorted(indices, reverse=True):
                item = databuffer.pop(i)
                batch_list.append(item.data)
            yield collate_fn(batch_list)
def text(self, value):
        
        

        
        if not self.started:
            self.start()
        # 更多的输入用法请见 https://github.com/macacajs/android-unicode#use-in-adb-shell
        value = ensure_unicode(value)
        value = escape_special_char(value)
        self.adb.shell(f"am broadcast -a ADB_INPUT_TEXT --es msg {value}")
def to_serializable_dict(self, mask_secrets=False):
        
        
        
        serializable_dict = {}
        for k in sorted(six.iterkeys(self._fields)):
            v = getattr(self, k)
            v = str(v) if isinstance(v, JSON_UNFRIENDLY_TYPES) else v
            serializable_dict[k] = v

        if mask_secrets:
            serializable_dict = self.mask_secrets(value=serializable_dict)

        return serializable_dict
def __init__(
        self,
        coordinator: ScreenlogicDataUpdateCoordinator,
        entity_description: ScreenLogicNumberDescription,
    ) -> None:
        
        self._set_value_func = entity_description.set_value
        self._set_value_params = entity_description.set_value_params
        super().__init__(coordinator, entity_description)
def historical(
        self,
        symbol: Annotated[
            Union[str, List[str]],
            OpenBBCustomParameter(
                description="Symbol to get data for. Can use CURR1-CURR2 or CURR1CURR2 format. Multiple items allowed for provider(s): fmp, polygon, tiingo, yfinance."
            ),
        ],
        start_date: Annotated[
            Union[datetime.date, None, str],
            OpenBBCustomParameter(
                description="Start date of the data, in YYYY-MM-DD format."
            ),
        ] = None,
        end_date: Annotated[
            Union[datetime.date, None, str],
            OpenBBCustomParameter(
                description="End date of the data, in YYYY-MM-DD format."
            ),
        ] = None,
        provider: Optional[Literal["fmp", "polygon", "tiingo", "yfinance"]] = None,
        **kwargs
    ) -> OBBject:
        
          # noqa: E501

        return self._run(
            "/crypto/price/historical",
            **filter_inputs(
                provider_choices={
                    "provider": self._get_provider(
                        provider,
                        "/crypto/price/historical",
                        ("fmp", "polygon", "tiingo", "yfinance"),
                    )
                },
                standard_params={
                    "symbol": symbol,
                    "start_date": start_date,
                    "end_date": end_date,
                },
                extra_params=kwargs,
                extra_info={
                    "symbol": {
                        "multiple_items_allowed": [
                            "fmp",
                            "polygon",
                            "tiingo",
                            "yfinance",
                        ]
                    }
                },
            )
        )
def __init__(self, obj: T, **adapted_methods: Callable):
        
        self.obj = obj
        self.__dict__.update(adapted_methods)
def is_completed(grid):
    
    
    
    return all(all(cell != 0 for cell in row) for row in grid)
def sudoku(grid: Matrix) -> Optional[Matrix]:
    
    
    

    if is_completed(grid):
        return grid

    location = find_empty_location(grid)
    if location is not None:
        row, column = location
    else:
        # If the location is ``None``, then the grid is solved.
        return grid

    for digit in range(1, 10):
        if is_safe(grid, row, column, digit):
            grid[row][column] = digit

            if sudoku(grid) is not None:
                return grid

            grid[row][column] = 0

    return None
def lock_pair(pair: str, until: datetime, reason: str = None, *,
                  now: datetime = None, side: str) -> PairLock:
        
        
        
        lock = PairLock(
            pair=pair,
            lock_time=now or datetime.now(timezone.utc),
            lock_end_time=timeframe_to_next_date(PairLocks.timeframe, until),
            reason=reason,
            direction=side,
            active=True
        )
        if PairLocks.use_db:
            PairLock.query.session.add(lock)
            PairLock.query.session.commit()
        else:
            PairLocks.locks.append(lock)
        return lock
def __init__(self,
               num_attention_heads,
               intermediate_size,
               intermediate_activation,
               dropout_rate=0.0,
               attention_dropout_rate=0.0,
               kernel_initializer="glorot_uniform",
               bias_initializer="zeros",
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               use_bias=True,
               norm_first=False,
               norm_epsilon=1e-12,
               intermediate_dropout=0.0,
               attention_initializer=None,
               **kwargs):
    
    
    super().__init__(**kwargs)
    self.num_attention_heads = num_attention_heads
    self.intermediate_size = intermediate_size
    self.intermediate_activation = tf_keras.activations.get(
        intermediate_activation)
    self.dropout_rate = dropout_rate
    self.attention_dropout_rate = attention_dropout_rate
    self._kernel_initializer = tf_keras.initializers.get(kernel_initializer)
    self._bias_initializer = tf_keras.initializers.get(bias_initializer)
    self._kernel_regularizer = tf_keras.regularizers.get(kernel_regularizer)
    self._bias_regularizer = tf_keras.regularizers.get(bias_regularizer)
    self._activity_regularizer = tf_keras.regularizers.get(activity_regularizer)
    self._kernel_constraint = tf_keras.constraints.get(kernel_constraint)
    self._bias_constraint = tf_keras.constraints.get(bias_constraint)
    self._use_bias = use_bias
    self._norm_first = norm_first
    self._norm_epsilon = norm_epsilon
    self._intermediate_dropout = intermediate_dropout
    if attention_initializer:
      self._attention_initializer = tf_keras.initializers.get(
          attention_initializer)
    else:
      self._attention_initializer = tf_utils.clone_initializer(
          self._kernel_initializer)
    self._cross_attention_cls = layers.attention.MultiHeadAttention
def rotate(image, angle, resize=False, order=1, mode='constant', cval=0.,
           center=None):
    

    

    rows, cols = image.shape[0], image.shape[1]

    # rotation around center
    if center is None:
        center = np.array((cols, rows)) / 2. - 0.5
    else:
        center = np.asarray(center)
    tform1 = SimilarityTransform(translation=-center)
    tform2 = SimilarityTransform(rotation=np.deg2rad(angle))
    tform3 = SimilarityTransform(translation=center)
    tform = tform1 + tform2 + tform3

    output_shape = None
    if resize:
        # determine shape of output image
        corners = np.array([[1, 1], [1, rows], [cols, rows], [cols, 1]])
        corners = tform(corners - 1)
        minc = corners[:, 0].min()
        minr = corners[:, 1].min()
        maxc = corners[:, 0].max()
        maxr = corners[:, 1].max()
        out_rows = maxr - minr + 1
        out_cols = maxc - minc + 1
        output_shape = np.ceil((out_rows, out_cols))

        # fit output image in new shape
        translation = ((cols - out_cols) / 2., (rows - out_rows) / 2.)
        tform4 = SimilarityTransform(translation=translation)
        tform = tform4 + tform

    return warp(image, tform, output_shape=output_shape, order=order,
                mode=mode, cval=cval)
def rescale(image, scale, order=None, mode='reflect', cval=0, clip=True,
            preserve_range=False, multichannel=False,
            anti_aliasing=None, anti_aliasing_sigma=None):
    

    
    scale = np.atleast_1d(scale)
    if len(scale) > 1:
        if ((not multichannel and len(scale) != image.ndim) or
                (multichannel and len(scale) != image.ndim - 1)):
            raise ValueError("Supply a single scale, or one value per spatial "
                             "axis")
        if multichannel:
            scale = np.concatenate((scale, [1]))
    orig_shape = np.asarray(image.shape)
    output_shape = np.round(scale * orig_shape)
    if multichannel:  # don't scale channel dimension
        output_shape[-1] = orig_shape[-1]

    return resize(image, output_shape, order=order, mode=mode, cval=cval,
                  clip=clip, preserve_range=preserve_range,
                  anti_aliasing=anti_aliasing,
                  anti_aliasing_sigma=anti_aliasing_sigma)
def _downsample(array, factors, sum=True):
    

    

    pad_size = []
    if len(factors) != array.ndim:
        raise ValueError("'factors' must have the same length "
                         "as 'array.shape'")
    else:
        for i in range(len(factors)):
            if array.shape[i] % factors[i] != 0:
                pad_size.append(factors[i] - (array.shape[i] % factors[i]))
            else:
                pad_size.append(0)

    for i in range(len(pad_size)):
        array = _pad_asymmetric_zeros(array, pad_size[i], i)

    out = view_as_blocks(array, factors)
    block_shape = out.shape

    if sum:
        for i in range(len(block_shape) // 2):
            out = out.sum(-1)
    else:
        for i in range(len(block_shape) // 2):
            out = out.mean(-1)
    return out
def downscale_local_mean(image, factors, cval=0):
    

    
    return _local_func(image, factors, np.mean, cval)
def resize(image, output_shape, order=1, mode='reflect', cval=0, clip=True,
           preserve_range=False, anti_aliasing=True, anti_aliasing_sigma=None):
    

    
    output_shape = tuple(output_shape)
    output_ndim = len(output_shape)
    input_shape = image.shape
    if output_ndim > image.ndim:
        # append dimensions to input_shape
        input_shape = input_shape + (1, ) * (output_ndim - image.ndim)
        image = np.reshape(image, input_shape)
    elif output_ndim == image.ndim - 1:
        # multichannel case: append shape of last axis
        output_shape = output_shape + (image.shape[-1], )
    elif output_ndim < image.ndim - 1:
        raise ValueError("len(output_shape) cannot be smaller than the image "
                         "dimensions")

    factors = (np.asarray(input_shape, dtype=float) /
               np.asarray(output_shape, dtype=float))

    if anti_aliasing:
        if anti_aliasing_sigma is None:
            anti_aliasing_sigma = np.maximum(0, (factors - 1) / 2)
        else:
            anti_aliasing_sigma = \
                np.atleast_1d(anti_aliasing_sigma) * np.ones_like(factors)
            if np.any(anti_aliasing_sigma < 0):
                raise ValueError("Anti-aliasing standard deviation must be "
                                 "greater than or equal to zero")
            elif np.any((anti_aliasing_sigma > 0) & (factors <= 1)):
                warn("Anti-aliasing standard deviation greater than zero but "
                     "not down-sampling along all axes")

        # Translate modes used by np.pad to those used by ndi.gaussian_filter
        np_pad_to_ndimage = {
            'constant': 'constant',
            'edge': 'nearest',
            'symmetric': 'reflect',
            'reflect': 'mirror',
            'wrap': 'wrap'
        }
        try:
            ndi_mode = np_pad_to_ndimage[mode]
        except KeyError:
            raise ValueError("Unknown mode, or cannot translate mode. The "
                             "mode should be one of 'constant', 'edge', "
                             "'symmetric', 'reflect', or 'wrap'. See the "
                             "documentation of numpy.pad for more info.")

        image = ndi.gaussian_filter(image, anti_aliasing_sigma,
                                    cval=cval, mode=ndi_mode)

    # 2-dimensional interpolation
    if len(output_shape) == 2 or (len(output_shape) == 3 and
                                  output_shape[2] == input_shape[2]):
        rows = output_shape[0]
        cols = output_shape[1]
        input_rows = input_shape[0]
        input_cols = input_shape[1]
        if rows == 1 and cols == 1:
            tform = AffineTransform(translation=(input_cols / 2.0 - 0.5,
                                                 input_rows / 2.0 - 0.5))
        else:
            # 3 control points necessary to estimate exact AffineTransform
            src_corners = np.array([[1, 1], [1, rows], [cols, rows]]) - 1
            dst_corners = np.zeros(src_corners.shape, dtype=np.double)
            # take into account that 0th pixel is at position (0.5, 0.5)
            dst_corners[:, 0] = factors[1] * (src_corners[:, 0] + 0.5) - 0.5
            dst_corners[:, 1] = factors[0] * (src_corners[:, 1] + 0.5) - 0.5

            tform = AffineTransform()
            tform.estimate(src_corners, dst_corners)

        # Make sure the transform is exactly metric, to ensure fast warping.
        tform.params[2] = (0, 0, 1)
        tform.params[0, 1] = 0
        tform.params[1, 0] = 0

        out = warp(image, tform, output_shape=output_shape, order=order,
                   mode=mode, cval=cval, clip=clip,
                   preserve_range=preserve_range)

    else:  # n-dimensional interpolation
        coord_arrays = [factors[i] * (np.arange(d) + 0.5) - 0.5
                        for i, d in enumerate(output_shape)]

        coord_map = np.array(np.meshgrid(*coord_arrays,
                                         sparse=False,
                                         indexing='ij'))

        image = convert_to_float(image, preserve_range)

        ndi_mode = _to_ndimage_mode(mode)
        out = ndi.map_coordinates(image, coord_map, order=order,
                                  mode=ndi_mode, cval=cval)

        _clip_warp_output(image, out, order, mode, cval, clip)

    return out
def _linear_polar_mapping(output_coords, k_angle, k_radius, center):
    
    
    angle = output_coords[:, 1] / k_angle
    rr = ((output_coords[:, 0] / k_radius) * np.sin(angle)) + center[0]
    cc = ((output_coords[:, 0] / k_radius) * np.cos(angle)) + center[1]
    coords = np.column_stack((cc, rr))
    return coords
def warp_polar(image, center=None, *, radius=None, output_shape=None,
               scaling='linear', multichannel=False, channel_axis=None,
               **kwargs):
    
    
    multichannel = channel_axis is not None
    if image.ndim != 2 and not multichannel:
        raise ValueError("Input array must be 2-dimensional when "
                         f"`channel_axis=None`, got {image.ndim}")

    if image.ndim != 3 and multichannel:
        raise ValueError("Input array must be 3-dimensional when "
                         f"`channel_axis` is specified, got {image.ndim}")

    if center is None:
        center = (np.array(image.shape)[:2] / 2) - 0.5

    if radius is None:
        w, h = np.array(image.shape)[:2] / 2
        radius = np.sqrt(w ** 2 + h ** 2)

    if output_shape is None:
        height = 360
        width = int(np.ceil(radius))
        output_shape = (height, width)
    else:
        output_shape = safe_as_int(output_shape)
        height = output_shape[0]
        width = output_shape[1]

    if scaling == 'linear':
        k_radius = width / radius
        map_func = _linear_polar_mapping
    elif scaling == 'log':
        k_radius = width / np.log(radius)
        map_func = _log_polar_mapping
    else:
        raise ValueError("Scaling value must be in {'linear', 'log'}")

    k_angle = height / (2 * np.pi)
    warp_args = {'k_angle': k_angle, 'k_radius': k_radius, 'center': center}

    warped = warp(image, map_func, map_args=warp_args,
                  output_shape=output_shape, **kwargs)

    return warped
def resize_local_mean(image, output_shape, grid_mode=True,
                      preserve_range=False, *, channel_axis=None):
    

    
    if channel_axis is not None:
        if channel_axis < -image.ndim or channel_axis >= image.ndim:
            raise ValueError("invalid channel_axis")

        # move channels to last position
        image = np.moveaxis(image, channel_axis, -1)
        nc = image.shape[-1]

        output_ndim = len(output_shape)
        if output_ndim == image.ndim - 1:
            # insert channels dimension at the end
            output_shape = output_shape + (nc,)
        elif output_ndim == image.ndim:
            if output_shape[channel_axis] != nc:
                raise ValueError(
                    "Cannot reshape along the channel_axis. Use "
                    "channel_axis=None to reshape along all axes."
                )
            # move channels to last position in output_shape
            channel_axis = channel_axis % image.ndim
            output_shape = (
                output_shape[:channel_axis] + output_shape[channel_axis:] +
                (nc,)
            )
        else:
            raise ValueError(
                "len(output_shape) must be image.ndim or (image.ndim - 1) "
                "when a channel_axis is specified."
            )
        resized = image
    else:
        resized, output_shape = _preprocess_resize_output_shape(image,
                                                                output_shape)
    resized = convert_to_float(resized, preserve_range)
    dtype = resized.dtype

    for axis, (old_size, new_size) in enumerate(zip(image.shape,
                                                    output_shape)):
        if old_size == new_size:
            continue
        weights = _local_mean_weights(old_size, new_size, grid_mode, dtype)
        product = np.tensordot(resized, weights, [[axis], [-1]])
        resized = np.moveaxis(product, -1, axis)

    if channel_axis is not None:
        # restore channels to original axis
        resized = np.moveaxis(resized, -1, channel_axis)

    return resized
def _local_mean_weights(old_size, new_size, grid_mode, dtype):
    

    
    if grid_mode:
        old_breaks = np.linspace(0, old_size, num=old_size + 1, dtype=dtype)
        new_breaks = np.linspace(0, old_size, num=new_size + 1, dtype=dtype)
    else:
        old, new = old_size - 1, new_size - 1
        old_breaks = np.pad(np.linspace(0.5, old-0.5, old, dtype=dtype),
                            1, 'constant', constant_values=(0, old))
        val = 0.5 * old / new
        new_breaks = np.pad(np.linspace(val, old-val, new, dtype=dtype),
                            1, 'constant', constant_values=(0, old))

    upper = np.minimum(new_breaks[1:, np.newaxis], old_breaks[np.newaxis, 1:])
    lower = np.maximum(new_breaks[:-1, np.newaxis],
                       old_breaks[np.newaxis, :-1])

    weights = np.maximum(upper - lower, 0)
    weights /= weights.sum(axis=1, keepdims=True)

    return weights
def register_from_pack(self, pack_dir):
        
        
        
        pack_dir = pack_dir[:-1] if pack_dir.endswith("/") else pack_dir
        _, pack = os.path.split(pack_dir)
        rules_dir = self._pack_loader.get_content_from_pack(
            pack_dir=pack_dir, content_type="rules"
        )

        # Register pack first
        self.register_pack(pack_name=pack, pack_dir=pack_dir)

        registered_count = 0
        overridden_count = 0
        if not rules_dir:
            return registered_count, overridden_count

        LOG.debug("Registering rules from pack %s:, dir: %s", pack, rules_dir)

        try:
            rules = self._get_rules_from_pack(rules_dir=rules_dir)
            registered_count, overridden_count = self._register_rules_from_pack(pack=pack, rules=rules)
        except Exception as e:
            if self._fail_on_failure:
                raise e

            LOG.exception("Failed registering all rules from pack: %s", rules_dir)

        return registered_count, overridden_count
def normalize_insts(self, insts):
         
        
        insts = self.resolveAndRemoveLabels(insts)
        res = []
        for item in insts:
            assert isinstance(item, tuple)
            opcode, oparg, *loc = item
            opcode = dis.opmap.get(opcode, opcode)
            if isinstance(oparg, self.Label):
                arg = oparg.value
            else:
                arg = oparg if opcode in self.HAS_ARG else None
            opcode = dis.opname[opcode]
            res.append((opcode, arg, *loc))
        return res
def black_tophat(image, footprint=None, out=None):
    

    
    if out is image:
        original = image.copy()
    else:
        original = image
    out = closing(image, footprint, out=out)
    if np.issubdtype(out.dtype, np.bool_):
        np.logical_xor(out, original, out=out)
    else:
        out -= original
    return out
def dilation(image, footprint=None, out=None, shift_x=False, shift_y=False):
    

    
    if out is None:
        out = np.empty_like(image)

    if _footprint_is_sequence(footprint):
        # shift and invert (see comment below) each footprint
        footprints = tuple(
            (_invert_footprint(_shift_footprint(fp, shift_x, shift_y)), n)
            for fp, n in footprint
        )
        return _iterate_gray_func(ndi.grey_dilation, image, footprints, out)

    footprint = np.array(footprint)
    footprint = _shift_footprint(footprint, shift_x, shift_y)
    # Inside ndi.grey_dilation, the footprint is inverted,
    # e.g. `footprint = footprint[::-1, ::-1]` for 2D [1]_, for reasons unknown
    # to this author (@jni). To "patch" this behaviour, we invert our own
    # footprint before passing it to `ndi.grey_dilation`.
    # [1] https://github.com/scipy/scipy/blob/ec20ababa400e39ac3ffc9148c01ef86d5349332/scipy/ndimage/morphology.py#L1285  # noqa
    footprint = _invert_footprint(footprint)

    ndi.grey_dilation(image, footprint=footprint, output=out)
    return out
def _shift_footprint(footprint, shift_x, shift_y):
    
    
    footprint = np.asarray(footprint)
    if footprint.ndim != 2:
        # do nothing for 1D or 3D or higher footprints
        return footprint
    m, n = footprint.shape
    if m % 2 == 0:
        extra_row = np.zeros((1, n), footprint.dtype)
        if shift_x:
            footprint = np.vstack((footprint, extra_row))
        else:
            footprint = np.vstack((extra_row, footprint))
        m += 1
    if n % 2 == 0:
        extra_col = np.zeros((m, 1), footprint.dtype)
        if shift_y:
            footprint = np.hstack((footprint, extra_col))
        else:
            footprint = np.hstack((extra_col, footprint))
    return footprint
def writestr(self, zinfo_or_arcname, byts, permissions=0o600,
            compression=ZIP_DEFLATED, raw_bytes=False):
        
        assert not raw_bytes or (raw_bytes and
                isinstance(zinfo_or_arcname, ZipInfo))
        if not isinstance(byts, bytes):
            byts = byts.encode('utf-8')
        if not isinstance(zinfo_or_arcname, ZipInfo):
            if not isinstance(zinfo_or_arcname, unicode_type):
                zinfo_or_arcname = zinfo_or_arcname.decode(filesystem_encoding)
            zinfo = ZipInfo(filename=zinfo_or_arcname,
                            date_time=time.localtime(time.time())[:6])
            zinfo.compress_type = compression
            zinfo.external_attr = permissions << 16
        else:
            zinfo = zinfo_or_arcname

        if not self.fp:
            raise RuntimeError(
                  "Attempt to write to ZIP archive that was already closed")

        if not raw_bytes:
            zinfo.file_size = len(byts)            # Uncompressed size
        zinfo.header_offset = self.fp.tell()    # Start of header bytes
        self._writecheck(zinfo)
        self._didModify = True
        if not raw_bytes:
            zinfo.CRC = crc32(byts) & 0xffffffff       # CRC-32 checksum
            if zinfo.compress_type == ZIP_DEFLATED:
                co = zlib.compressobj(zlib.Z_DEFAULT_COMPRESSION,
                    zlib.DEFLATED, -15)
                byts = co.compress(byts) + co.flush()
                zinfo.compress_size = len(byts)    # Compressed size
            else:
                zinfo.compress_size = zinfo.file_size
        zinfo.header_offset = self.fp.tell()    # Start of header bytes
        self.fp.write(zinfo.FileHeader())
        self.fp.write(byts)
        self.fp.flush()
        if zinfo.flag_bits & 0x08:
            # Write CRC and file sizes after the file data
            self.fp.write(struct.pack("<LLL", zinfo.CRC, zinfo.compress_size,
                  zinfo.file_size))
        self.filelist.append(zinfo)
        self.NameToInfo[zinfo.filename] = zinfo
def is_zipfile(filename):
    
    
    result = False
    try:
        if hasattr(filename, "read"):
            result = _check_zipfile(filename)
        else:
            with open(filename, "rb") as fp:
                result = _check_zipfile(fp)
    except IOError:
        pass
    return result
def __call__(self, c):
        
        k = self.key2 | 2
        c = c ^ (((k * (k^1)) >> 8) & 255)
        self._UpdateKeys(c)
        return c
def _extract_cfn_gateway_v2_api(
        self,
        stack_path: str,
        logical_id: str,
        api_resource: Dict,
        collector: ApiCollector,
        cwd: Optional[str] = None,
        disable_authorizer: Optional[bool] = False,
    ) -> None:
        
        
        
        properties = api_resource.get("Properties", {})
        body = properties.get("Body")
        body_s3_location = properties.get("BodyS3Location")
        cors = self.extract_cors_http(properties.get("CorsConfiguration"))
        target = properties.get("Target")
        route_key = properties.get("RouteKey")
        protocol_type = properties.get("ProtocolType")

        if not body and not body_s3_location:
            LOG.debug("Swagger document not found in Body and BodyS3Location for resource '%s'.", logical_id)
            if cors:
                collector.cors = cors
            if target and protocol_type == CfnApiProvider.HTTP_API_PROTOCOL_TYPE:
                method, path = self._parse_route_key(route_key)
                routes = Route(
                    methods=[method],
                    path=path,
                    function_name=LambdaUri.get_function_name(target),
                    event_type=Route.HTTP,
                    stack_path=stack_path,
                )
                collector.add_routes(logical_id, [routes])
            return

        CfnBaseApiProvider.extract_swagger_route(
            stack_path,
            logical_id,
            body,
            body_s3_location,
            None,
            collector,
            cwd,
            Route.HTTP,
            disable_authorizer=disable_authorizer,
        )
def import_fresh_module(name, fresh=(), blocked=(), *,
                        deprecated=False,
                        usefrozen=False,
                        ):
    
    
    # NOTE: test_heapq, test_json and test_warnings include extra sanity checks
    # to make sure that this utility function is working as expected
    with _ignore_deprecated_imports(deprecated):
        # Keep track of modules saved for later restoration as well
        # as those which just need a blocking entry removed
        fresh = list(fresh)
        blocked = list(blocked)
        names = {name, *fresh, *blocked}
        orig_modules = _save_and_remove_modules(names)
        for modname in blocked:
            sys.modules[modname] = None

        try:
            with frozen_modules(usefrozen):
                # Return None when one of the "fresh" modules can not be imported.
                try:
                    for modname in fresh:
                        __import__(modname)
                except ImportError:
                    return None
                return importlib.import_module(name)
        finally:
            _save_and_remove_modules(names)
            sys.modules.update(orig_modules)
def iterate(space: Space, items) -> Iterator:
    
    
    raise ValueError(
        f"Space of type `{type(space)}` is not a valid `gym.Space` instance."
    )
def test_freqmismatch(self):
        
        
        
        with self.assertWarns(RuntimeWarning):
            rf.OpenShort(self.open, self.short[0:len(self.short)//2])
        
        with self.assertWarns(RuntimeWarning):
            self.dm.deembed(self.raw[0:len(self.raw)//2])
def test_open(self):
        
        
        
        dut = self.dm_o.deembed(self.raw7_1f)
        ind_calc = 1e9*np.imag(1/dut.y[0,0,0])/2/np.pi/dut.f
        self.assertTrue(np.isclose(ind_calc, 1, rtol=self.rtol))
def test_IEEEP370_SE_ZC_2xThru_with_dc(self):
        
         
        
        s2xthru = rf.Network(os.path.join(self.test_dir, 's2xthru.s2p'))
        fdf = rf.Network(os.path.join(self.test_dir, 'fdf.s2p'))
        s2xthru_dc = s2xthru.extrapolate_to_dc(kind='linear')
        fdf_dc = fdf.extrapolate_to_dc(kind='linear')
        dm_zc  = rf.IEEEP370_SE_ZC_2xThru(dummy_2xthru = s2xthru_dc, 
                                       dummy_fix_dut_fix = fdf_dc, 
                                       bandwidth_limit = 10e9, 
                                       pullback1 = 0, pullback2 = 0,
                                       leadin = 0,
                                       NRP_enable = False,
                                       name = 'zc2xthru')
        print(s2xthru_dc.frequency.f[0])
        print(fdf_dc.frequency.f[0])
        print(dm_zc.s_side1.frequency.f[0])
        residuals = dm_zc.deembed(s2xthru_dc)
        # insertion loss magnitude deviate from 1.0 from less than 0.2 dB
        il_mag = 20.*np.log10(np.abs(residuals.s[:, 1, 0] + 1e-12))
        self.assertTrue(np.max(np.abs(il_mag)) <= 0.2, 'residual IL magnitude')
        # insertion loss phase deviate from 0 degree from less than 45 degree
        # too much tolerance here allowed as for now
        il_phase = np.angle(residuals.s[:, 1, 0]) * 180/np.pi
        self.assertTrue(np.max(np.abs(il_phase)) <= 2.0, 'residual IL Phase')
def __init__(
        self,
        max_batch_size: int,
        batch_wait_timeout_s: float,
        handle_batch_func: Optional[Callable] = None,
    ) -> None:
        
        
        self.queue: asyncio.Queue[_SingleRequest] = asyncio.Queue()
        self.max_batch_size = max_batch_size
        self.batch_wait_timeout_s = batch_wait_timeout_s
        self.queue_put_event = asyncio.Event()

        self._handle_batch_task = None
        if handle_batch_func is not None:
            self._handle_batch_task = get_or_create_event_loop().create_task(
                self._process_batches(handle_batch_func)
            )
def logout():
    
    
    
    SETTINGS['api_key'] = ''
    yaml_save(USER_CONFIG_DIR / 'settings.yaml', SETTINGS)
    LOGGER.info(f"{PREFIX}logged out ✅. To log in again, use 'yolo hub login'.")
def non_negative_factorization(
    X,
    W=None,
    H=None,
    n_components="warn",
    *,
    init=None,
    update_H=True,
    solver="cd",
    beta_loss="frobenius",
    tol=1e-4,
    max_iter=200,
    alpha_W=0.0,
    alpha_H="same",
    l1_ratio=0.0,
    random_state=None,
    verbose=0,
    shuffle=False,
):
    
    
    est = NMF(
        n_components=n_components,
        init=init,
        solver=solver,
        beta_loss=beta_loss,
        tol=tol,
        max_iter=max_iter,
        random_state=random_state,
        alpha_W=alpha_W,
        alpha_H=alpha_H,
        l1_ratio=l1_ratio,
        verbose=verbose,
        shuffle=shuffle,
    )
    est._validate_params()

    X = check_array(X, accept_sparse=("csr", "csc"), dtype=[np.float64, np.float32])

    with config_context(assume_finite=True):
        W, H, n_iter = est._fit_transform(X, W=W, H=H, update_H=update_H)

    return W, H, n_iter
def _initialize_nmf(X, n_components, init=None, eps=1e-6, random_state=None):
    
    
    check_non_negative(X, "NMF initialization")
    n_samples, n_features = X.shape

    if (
        init is not None
        and init != "random"
        and n_components > min(n_samples, n_features)
    ):
        raise ValueError(
            "init = '{}' can only be used when "
            "n_components <= min(n_samples, n_features)".format(init)
        )

    if init is None:
        if n_components <= min(n_samples, n_features):
            init = "nndsvda"
        else:
            init = "random"

    # Random initialization
    if init == "random":
        avg = np.sqrt(X.mean() / n_components)
        rng = check_random_state(random_state)
        H = avg * rng.randn(n_components, n_features).astype(X.dtype, copy=False)
        W = avg * rng.randn(n_samples, n_components).astype(X.dtype, copy=False)
        np.abs(H, out=H)
        np.abs(W, out=W)
        return W, H

    # NNDSVD initialization
    U, S, V = randomized_svd(X, n_components, random_state=random_state)
    W = np.zeros_like(U)
    H = np.zeros_like(V)

    # The leading singular triplet is non-negative
    # so it can be used as is for initialization.
    W[:, 0] = np.sqrt(S[0]) * np.abs(U[:, 0])
    H[0, :] = np.sqrt(S[0]) * np.abs(V[0, :])

    for j in range(1, n_components):
        x, y = U[:, j], V[j, :]

        # extract positive and negative parts of column vectors
        x_p, y_p = np.maximum(x, 0), np.maximum(y, 0)
        x_n, y_n = np.abs(np.minimum(x, 0)), np.abs(np.minimum(y, 0))

        # and their norms
        x_p_nrm, y_p_nrm = norm(x_p), norm(y_p)
        x_n_nrm, y_n_nrm = norm(x_n), norm(y_n)

        m_p, m_n = x_p_nrm * y_p_nrm, x_n_nrm * y_n_nrm

        # choose update
        if m_p > m_n:
            u = x_p / x_p_nrm
            v = y_p / y_p_nrm
            sigma = m_p
        else:
            u = x_n / x_n_nrm
            v = y_n / y_n_nrm
            sigma = m_n

        lbd = np.sqrt(S[j] * sigma)
        W[:, j] = lbd * u
        H[j, :] = lbd * v

    W[W < eps] = 0
    H[H < eps] = 0

    if init == "nndsvd":
        pass
    elif init == "nndsvda":
        avg = X.mean()
        W[W == 0] = avg
        H[H == 0] = avg
    elif init == "nndsvdar":
        rng = check_random_state(random_state)
        avg = X.mean()
        W[W == 0] = abs(avg * rng.randn(len(W[W == 0])) / 100)
        H[H == 0] = abs(avg * rng.randn(len(H[H == 0])) / 100)
    else:
        raise ValueError(
            "Invalid init parameter: got %r instead of one of %r"
            % (init, (None, "random", "nndsvd", "nndsvda", "nndsvdar"))
        )

    return W, H
def _fit_multiplicative_update(
    X,
    W,
    H,
    beta_loss="frobenius",
    max_iter=200,
    tol=1e-4,
    l1_reg_W=0,
    l1_reg_H=0,
    l2_reg_W=0,
    l2_reg_H=0,
    update_H=True,
    verbose=0,
):
    
    
    start_time = time.time()

    beta_loss = _beta_loss_to_float(beta_loss)

    # gamma for Maximization-Minimization (MM) algorithm [Fevotte 2011]
    if beta_loss < 1:
        gamma = 1.0 / (2.0 - beta_loss)
    elif beta_loss > 2:
        gamma = 1.0 / (beta_loss - 1.0)
    else:
        gamma = 1.0

    # used for the convergence criterion
    error_at_init = _beta_divergence(X, W, H, beta_loss, square_root=True)
    previous_error = error_at_init

    H_sum, HHt, XHt = None, None, None
    for n_iter in range(1, max_iter + 1):
        # update W
        # H_sum, HHt and XHt are saved and reused if not update_H
        W, H_sum, HHt, XHt = _multiplicative_update_w(
            X,
            W,
            H,
            beta_loss=beta_loss,
            l1_reg_W=l1_reg_W,
            l2_reg_W=l2_reg_W,
            gamma=gamma,
            H_sum=H_sum,
            HHt=HHt,
            XHt=XHt,
            update_H=update_H,
        )

        # necessary for stability with beta_loss < 1
        if beta_loss < 1:
            W[W < np.finfo(np.float64).eps] = 0.0

        # update H (only at fit or fit_transform)
        if update_H:
            H = _multiplicative_update_h(
                X,
                W,
                H,
                beta_loss=beta_loss,
                l1_reg_H=l1_reg_H,
                l2_reg_H=l2_reg_H,
                gamma=gamma,
            )

            # These values will be recomputed since H changed
            H_sum, HHt, XHt = None, None, None

            # necessary for stability with beta_loss < 1
            if beta_loss <= 1:
                H[H < np.finfo(np.float64).eps] = 0.0

        # test convergence criterion every 10 iterations
        if tol > 0 and n_iter % 10 == 0:
            error = _beta_divergence(X, W, H, beta_loss, square_root=True)

            if verbose:
                iter_time = time.time()
                print(
                    "Epoch %02d reached after %.3f seconds, error: %f"
                    % (n_iter, iter_time - start_time, error)
                )

            if (previous_error - error) / error_at_init < tol:
                break
            previous_error = error

    # do not print if we have already printed in the convergence test
    if verbose and (tol == 0 or n_iter % 10 != 0):
        end_time = time.time()
        print(
            "Epoch %02d reached after %.3f seconds." % (n_iter, end_time - start_time)
        )

    return W, H, n_iter
def inverse_transform(self, X=None, *, Xt=None):
        
        

        X = _deprecate_Xt_in_inverse_transform(X, Xt)

        check_is_fitted(self)
        return X @ self.components_
def try_import_tfp(error=False):
    
    
    
    if "RLLIB_TEST_NO_TF_IMPORT" in os.environ:
        logger.warning("Not importing TensorFlow Probability for test "
                       "purposes.")
        return None

    try:
        import tensorflow_probability as tfp
        return tfp
    except ImportError as e:
        if error:
            raise e
        return None
def get_variable(value: Any,
                 framework: str = "tf",
                 trainable: bool = False,
                 tf_name: str = "unnamed-variable",
                 torch_tensor: bool = False,
                 device: Optional[str] = None,
                 shape: Optional[TensorShape] = None,
                 dtype: Optional[TensorType] = None) -> Any:
    
    
    if framework in ["tf2", "tf", "tfe"]:
        import tensorflow as tf
        dtype = dtype or getattr(
            value, "dtype", tf.float32
            if isinstance(value, float) else tf.int32
            if isinstance(value, int) else None)
        return tf.compat.v1.get_variable(
            tf_name,
            initializer=value,
            dtype=dtype,
            trainable=trainable,
            **({} if shape is None else {
                "shape": shape
            }))
    elif framework == "torch" and torch_tensor is True:
        torch, _ = try_import_torch()
        var_ = torch.from_numpy(value)
        if dtype in [torch.float32, np.float32]:
            var_ = var_.float()
        elif dtype in [torch.int32, np.int32]:
            var_ = var_.int()
        elif dtype in [torch.float64, np.float64]:
            var_ = var_.double()

        if device:
            var_ = var_.to(device)
        var_.requires_grad = trainable
        return var_
    # torch or None: Return python primitive.
    return value
def get_activation_fn(name: Optional[str] = None, framework: str = "tf"):
    
    
    if framework == "torch":
        if name in ["linear", None]:
            return None
        if name == "swish":
            from ray.rllib.utils.torch_ops import Swish
            return Swish
        _, nn = try_import_torch()
        if name == "relu":
            return nn.ReLU
        elif name == "tanh":
            return nn.Tanh
    else:
        if name in ["linear", None]:
            return None
        tf1, tf, tfv = try_import_tf()
        fn = getattr(tf.nn, name, None)
        if fn is not None:
            return fn

    raise ValueError("Unknown activation ({}) for framework={}!".format(
        name, framework))
def test_legacy_variables_ignored_if_given(self):
        
        environment = {
            "EDGE_BIND_HOST": "192.168.0.1",
            "EDGE_PORT": "10101",
            "EDGE_PORT_HTTP": "20202",
        }
        (
            localstack_host,
            gateway_listen,
        ) = config.populate_edge_configuration(environment)

        assert localstack_host == "localhost.localstack.cloud:4566"
        assert gateway_listen == [
            HostAndPort(host=ip(), port=4566),
        ]
def runtest(test, verbose, quiet,
            huntrleaks=False, debug=False, use_resources=None,
            output_on_failure=False, failfast=False, match_tests=None,
            timeout=None):
    
    

    if use_resources is not None:
        support.use_resources = use_resources
    use_timeout = (timeout is not None)
    if use_timeout:
        faulthandler.dump_tracebacks_later(timeout, exit=True)
    try:
        support.match_tests = match_tests
        if failfast:
            support.failfast = True
        if output_on_failure:
            support.verbose = True

            # Reuse the same instance to all calls to runtest(). Some
            # tests keep a reference to sys.stdout or sys.stderr
            # (eg. test_argparse).
            if runtest.stringio is None:
                stream = io.StringIO()
                runtest.stringio = stream
            else:
                stream = runtest.stringio
                stream.seek(0)
                stream.truncate()

            orig_stdout = sys.stdout
            orig_stderr = sys.stderr
            try:
                sys.stdout = stream
                sys.stderr = stream
                result = runtest_inner(test, verbose, quiet, huntrleaks,
                                       debug, display_failure=False)
                if result[0] == FAILED:
                    output = stream.getvalue()
                    orig_stderr.write(output)
                    orig_stderr.flush()
            finally:
                sys.stdout = orig_stdout
                sys.stderr = orig_stderr
        else:
            support.verbose = verbose  # Tell tests to be moderately quiet
            result = runtest_inner(test, verbose, quiet, huntrleaks, debug,
                                   display_failure=not verbose)
        return result
    finally:
        if use_timeout:
            faulthandler.cancel_dump_tracebacks_later()
        cleanup_test_droppings(test, verbose)
def _convert_namespace_to_getopt(ns):
    
    
    opts = []
    args_dict = vars(ns)
    for key in sorted(args_dict.keys()):
        if key == 'args':
            continue
        val = args_dict[key]
        # Don't continue if val equals '' because this means an option
        # accepting a value was provided the empty string.  Such values should
        # show up in the returned opts list.
        if val is None or val is False:
            continue
        if val is True:
            # Then an option with action store_true was passed. getopt
            # includes these with value '' in the opts list.
            val = ''
        opts.append(('--' + key, val))
    return opts
def about():
    
    return redirect(url_for('help_page', pagename='about'))
def from_keys(
        cls,
        required_keys: Set[str],
    ) -> "OutputKeys":
        
        return cls(required_keys=required_keys)
def _mosaic(self, *patch_samples):
    
    if self._mosaic_frequency >= 1.0:
      mosaic_prob = 1.0
    else:
      mosaic_prob = preprocessing_ops.random_uniform_strong(
          0.0, 1.0, dtype=tf.float32, seed=self._seed
      )
      sample = patch_samples[0].copy()

    if mosaic_prob >= (1 - self._mosaic_frequency):
      mosaic9_prob = preprocessing_ops.random_uniform_strong(
          0.0, 1.0, dtype=tf.float32, seed=self._seed + 1
      )
      if self._mosaic9_frequency > 0 and mosaic9_prob >= (
          1 - self._mosaic9_frequency
      ):
        return self._mosaic9(*patch_samples)
      else:
        return self._mosaic4(*patch_samples)
    else:
      return self._add_param(sample)
def get_rag_embedding(self, key: EmbeddingType = None) -> BaseEmbedding:
        
        return super().get_instance(key or self._resolve_embedding_type())
def polygon_area(py, px):
    
    
    py = np.asarray(py)
    px = np.asarray(px)
    return 0.5 * np.abs(np.sum((px[:-1] * py[1:]) - (px[1:] * py[:-1])))
def polygon_area(pr, pc):
    
    
    pr = np.asarray(pr)
    pc = np.asarray(pc)
    return 0.5 * np.abs(np.sum((pc[:-1] * pr[1:]) - (pc[1:] * pr[:-1])))
def _process_data(self, data, timestamps, loss, is_live):
         
        
        loss = list(loss)
        timestamps = list(timestamps)

        if len(loss[-1]) != len(self._loss_labels):
            logger.debug("Truncated loss found. loss count: %s", len(loss))
            idx = sorted(data)[-1]
            if is_live:
                logger.debug("Setting carried over data: %s", data[idx])
                self._carry_over[idx] = data[idx]
            logger.debug("Removing truncated loss: (timestamp: %s, loss: %s)",
                         timestamps[-1], loss[-1])
            del loss[-1]
            del timestamps[-1]

        return timestamps, loss
def _parse_outputs(self, event):
         
        
        serializer = get_serializer("json")
        struct = event.summary.value[0].tensor.string_val[0]

        config = serializer.unmarshal(struct)["config"]
        model_outputs = self._get_outputs(config)
        split_output = len(np.unique(model_outputs[..., 1])) == 1

        for side_outputs, side in zip(model_outputs, ("a", "b")):
            logger.debug("side: '%s', outputs: '%s'", side, side_outputs)
            layer_name = side_outputs[0][0]

            output_config = next(layer for layer in config["layers"]
                                 if layer["name"] == layer_name)["config"]
            layer_outputs = self._get_outputs(output_config)
            for output in layer_outputs:  # Drill into sub-model to get the actual output names
                loss_name = output[0][0]
                if not split_output:  # Rename losses to reflect the side's output
                    loss_name = f"{loss_name.replace('_both', '')}_{side}"
                if loss_name not in self._loss_labels:
                    logger.debug("Adding loss name: '%s'", loss_name)
                    self._loss_labels.append(loss_name)
        logger.debug("Collated loss labels: %s", self._loss_labels)
def session_ids(self) -> List[int]:
          
        return self._log_files.session_ids
def authenticate(self, user, password):
        
        
        auth = HTTPBasicAuth(user, password)
        url = self.router.common_authenticate()
        logger.debug("REST: Authenticate to get access_token: %s" % url)
        ret = self.requester.get(url, auth=auth, headers=self.custom_headers,
                                 verify=self.verify_ssl)

        self._check_error_response(ret)
        return decode_text(ret.content)
def get_permissions_user_has_not(user: "User", permissions: List[str]):
    
    indexes = []
    for index, perm in enumerate(permissions):
        if not user.has_perm(perm):
            indexes.append(index)
    return indexes
def can_manage_service_account(user: "User", service_account: "ServiceAccount") -> bool:
    
    permissions = service_account.get_permissions()
    return user.has_perms(permissions)
def can_manage_service_account(
    requestor: Union["User", "ServiceAccount"], service_account: "ServiceAccount"
) -> bool:
    
    permissions = service_account.get_permissions()
    return requestor.has_perms(permissions)
def can_user_manage_group(info, user: "User", group: Group) -> bool:
    
    return can_user_manage_group_permissions(
        user, group
    ) and can_user_manage_group_channels(info, user, group)
def get_users_and_look_for_permissions_in_groups_with_manage_staff(
    groups_data: dict, permissions_to_find: Set[str],
):
    

    
    users_with_manage_staff: Set[int] = set()
    for data in groups_data.values():
        permissions = data["permissions"]
        users = data["users"]
        has_manage_staff = AccountPermissions.MANAGE_STAFF.value in permissions
        has_users = bool(users)
        # only consider groups with active users and manage_staff permission
        if has_users and has_manage_staff:
            common_permissions = permissions_to_find & permissions
            # remove found permission from set
            permissions_to_find.difference_update(common_permissions)
            users_with_manage_staff.update(users)

    return users_with_manage_staff
def requestor_has_access(
    requestor: Union["User", "App"], owner: Optional["User"], *perms
) -> bool:
    
    
    return requestor == owner or has_one_of_permissions(requestor, perms)
def check_if_removing_left_not_manageable_permissions(cls, user):
        
        
        permissions = get_not_manageable_permissions_when_deactivate_or_remove_users(
            [user]
        )
        if permissions:
            # add error
            msg = "Users cannot be removed, some of permissions will not be manageable."
            code = AccountErrorCode.LEFT_NOT_MANAGEABLE_PERMISSION.value
            params = {"permissions": permissions}
            raise ValidationError(
                {"id": ValidationError(msg, code=code, params=params)}
            )
def check_if_removing_left_not_manageable_permissions(cls, users, field, errors):
        
        
        permissions = get_not_manageable_permissions_when_deactivate_or_remove_users(
            users
        )
        if permissions:
            # add error
            msg = "Users cannot be removed, some of permissions will not be manageable."
            code = AccountErrorCode.LEFT_NOT_MANAGEABLE_PERMISSION.value
            params = {"permissions": permissions}
            error = ValidationError(msg, code=code, params=params)
            errors[field] = error
def __init__(
            self,
            params_descriptor: str,
            MWF: bool = False,
            stft_backend: STFTBackend = STFTBackend.AUTO,
            multiprocess: bool = True) -> None:
        
            
        
        self._params = load_configuration(params_descriptor)
        self._sample_rate = self._params['sample_rate']
        self._MWF = MWF
        self._tf_graph = tf.Graph()
        self._prediction_generator = None
        self._input_provider = None
        self._builder = None
        self._features = None
        self._session = None
        if multiprocess:
            self._pool = Pool()
            atexit.register(self._pool.close)
        else:
            self._pool = None
        self._tasks = []
        self._params['stft_backend'] = get_backend(stft_backend)
        self._data_generator = DataGenerator()
def next_greatest_element(arr: list) -> list:
    
    
    
    stack = []
    result = [-1] * len(arr)

    for index in reversed(range(len(arr))):
        if len(stack):
            while stack[-1] <= arr[index]:
                stack.pop()
                if len(stack) == 0:
                    break

        if len(stack) != 0:
            result[index] = stack[-1]

        stack.append(arr[index])

    return result
def progress_bar_callback(self) -> Optional[ProgressBar]:
        
        for c in self.callbacks:
            if isinstance(c, ProgressBar):
                return c
        return None
def init_module(self, empty_init: Optional[bool] = None) -> Generator:
        

        
        if is_overridden("model_sharded_context", self.strategy, parent=Strategy):
            # warning instead of error so that code changes are not required when changing strategies
            # this is a limitation because processes are not expected to have been launched when this is called
            rank_zero_warn(
                f"`trainer.init_module` cannot fully support proper instantiation of your model with the"
                f" `{type(self.strategy).__name__}` strategy. Please instantiate your model inside the"
                f"`LightningModule.configure_model` hook instead",
                # ideally we would check if `configure_model` is already overridden, but we don't have a reliable
                # reference to the model yet
                category=PossibleUserWarning,
            )
        with self.strategy.tensor_init_context(empty_init=empty_init):
            yield
def extract_features(self, preprocessed_inputs):
    
    
    image_shape = preprocessed_inputs.get_shape()
    image_shape.assert_has_rank(4)
    image_height = image_shape[1].value
    image_width = image_shape[2].value

    if image_height is None or image_width is None:
      shape_assert = tf.Assert(
          tf.logical_and(tf.equal(tf.shape(preprocessed_inputs)[1], 256),
                         tf.equal(tf.shape(preprocessed_inputs)[2], 256)),
          ['image size must be 256 in both height and width.'])
      with tf.control_dependencies([shape_assert]):
        preprocessed_inputs = tf.identity(preprocessed_inputs)
    elif image_height != 256 or image_width != 256:
      raise ValueError('image size must be = 256 in both height and width;'
                       ' image dim = %d,%d' % (image_height, image_width))

    feature_map_layout = {
        'from_layer': [
            'Conv2d_11_pointwise', 'Conv2d_13_pointwise', '', '', ''
        ],
        'layer_depth': [-1, -1, 512, 256, 256],
        'conv_kernel_size': [-1, -1, 3, 3, 2],
        'use_explicit_padding': self._use_explicit_padding,
    }

    with slim.arg_scope(self._conv_hyperparams):
      with slim.arg_scope([slim.batch_norm], fused=False):
        with tf.variable_scope('MobilenetV1',
                               reuse=self._reuse_weights) as scope:
          _, image_features = mobilenet_v1.mobilenet_v1_base(
              ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
              final_endpoint='Conv2d_13_pointwise',
              min_depth=self._min_depth,
              depth_multiplier=self._depth_multiplier,
              scope=scope)
          feature_maps = feature_map_generators.multi_resolution_feature_maps(
              feature_map_layout=feature_map_layout,
              depth_multiplier=self._depth_multiplier,
              min_depth=self._min_depth,
              insert_1x1_conv=True,
              image_features=image_features)

    return feature_maps.values()
def load_module_state_dict(self, state_dict, strict=True):
        
        
        if (state_dict is not None) and (not isinstance(state_dict, str)):
            super().load_module_state_dict(state_dict, strict)
            return

        self.module.load_state_dir(load_dir=self._curr_ckpt_path, strict=strict)
def _allocate_zeros(self, shape, **kwargs):
         
        
        if "dtype" not in kwargs and self.fp16_enabled():
            kwargs["dtype"] = torch.half

        return torch.zeros(shape, device=self.device, **kwargs)
def set_description(self, desc=None, **_):
        
        
        
        self.sp(desc=desc)
def find_docs(self) -> t.Optional[str]:
        
        if self.command != 'sanity':
            return None  # only sanity tests have docs links

        filename = f'{self.test}.html' if self.test else ''
        url = get_docs_url(f'https://docs.ansible.com/ansible-core/devel/dev_guide/testing/{self.command}/{filename}')

        return url
def start(self, refresh=False) -> None:
        
        
        with self._lock:
            if self._started:
                return
            self.console.set_live(self)
            self._started = True
            self.console.show_cursor(False)
            self._enable_redirect_io()
            self.console.push_render_hook(self)
            if refresh:
                self.refresh()
            if self.auto_refresh:
                self._refresh_thread = _RefreshThread(self, self.refresh_per_second)
                self._refresh_thread.start()
def value(self):
        
        
        
        return self.used_parameters.get(self.parameter_name, None)
def _compile_metrics(metrics_output_directory) -> Dict:
    
        
    
    songs = glob(join(metrics_output_directory, 'test/*.json'))
    index = pd.MultiIndex.from_tuples(
        product(EVALUATION_INSTRUMENTS, EVALUATION_METRICS),
        names=['instrument', 'metric'])
    pd.DataFrame([], index=['config1', 'config2'], columns=index)
    metrics = {
        instrument: {k: [] for k in EVALUATION_METRICS}
        for instrument in EVALUATION_INSTRUMENTS}
    for song in songs:
        with open(song, 'r') as stream:
            data = json.load(stream)
        for target in data['targets']:
            instrument = target['name']
            for metric in EVALUATION_METRICS:
                sdr_med = np.median([
                    frame['metrics'][metric]
                    for frame in target['frames']
                    if not np.isnan(frame['metrics'][metric])])
                metrics[instrument][metric].append(sdr_med)
    return metrics
def new_span(
        self,
        id_: str,
        bound_args: inspect.BoundArguments,
        instance: Optional[Any] = None,
        parent_span_id: Optional[str] = None,
        **kwargs: Any,
    ) -> Optional[T]:
        
        
        ...
def update(self, **kwargs):
        
        
        self.status = status.parse(status.get(host=self._host, port=self._port))
def __init__(self, make_env: Callable[[int], EnvType],
                 existing_envs: List["MultiAgentEnv"], num_envs: int):
        
        
        self.make_env = make_env
        self.envs = existing_envs
        self.num_envs = num_envs
        self.dones = set()
        while len(self.envs) < self.num_envs:
            self.envs.append(self.make_env(len(self.envs)))
        for env in self.envs:
            assert isinstance(env, MultiAgentEnv)
        self.env_states = [_MultiAgentEnvState(env) for env in self.envs]
def convert_to_base_env(
    env: EnvType,
    make_env: Callable[[int], EnvType] = None,
    num_envs: int = 1,
    remote_envs: bool = False,
    remote_env_batch_wait_ms: int = 0,
    worker: Optional["RolloutWorker"] = None,
    restart_failed_sub_environments: bool = False,
) -> "BaseEnv":
    
    

    from ray.rllib.env.remote_base_env import RemoteBaseEnv
    from ray.rllib.env.external_env import ExternalEnv
    from ray.rllib.env.multi_agent_env import MultiAgentEnv
    from ray.rllib.env.vector_env import VectorEnv, VectorEnvWrapper

    if remote_envs and num_envs == 1:
        raise ValueError(
            "Remote envs only make sense to use if num_envs > 1 "
            "(i.e. environment vectorization is enabled)."
        )

    # Given `env` has a `to_base_env` method -> Call that to convert to a BaseEnv type.
    if isinstance(env, (BaseEnv, MultiAgentEnv, VectorEnv, ExternalEnv)):
        return env.to_base_env(
            make_env=make_env,
            num_envs=num_envs,
            remote_envs=remote_envs,
            remote_env_batch_wait_ms=remote_env_batch_wait_ms,
        )
    # `env` is not a BaseEnv yet -> Need to convert/vectorize.
    else:
        # Sub-environments are ray.remote actors:
        if remote_envs:
            # Determine, whether the already existing sub-env (could
            # be a ray.actor) is multi-agent or not.
            multiagent = (
                ray.get(env._is_multi_agent.remote())
                if hasattr(env, "_is_multi_agent")
                else False
            )
            env = RemoteBaseEnv(
                make_env,
                num_envs,
                multiagent=multiagent,
                remote_env_batch_wait_ms=remote_env_batch_wait_ms,
                existing_envs=[env],
                worker=worker,
                restart_failed_sub_environments=restart_failed_sub_environments,
            )
        # Sub-environments are not ray.remote actors.
        else:
            # Convert gym.Env to VectorEnv ...
            env = VectorEnv.vectorize_gym_envs(
                make_env=make_env,
                existing_envs=[env],
                num_envs=num_envs,
                action_space=env.action_space,
                observation_space=env.observation_space,
                restart_failed_sub_environments=restart_failed_sub_environments,
            )
            # ... then the resulting VectorEnv to a BaseEnv.
            env = VectorEnvWrapper(env)

    # Make sure conversion went well.
    assert isinstance(env, BaseEnv), env

    return env
def process(self):
         
        
        logger.debug("Starting Training Process")
        logger.info("Training data directory: %s", self._args.model_dir)

        # TODO Move these args to config and remove these deprecation warnings
        if hasattr(self._args, "warp_to_landmarks") and self._args.warp_to_landmarks:
            deprecation_warning("`-wl`, ``--warp-to-landmarks``",
                                additional_info="This option will be available within training "
                                                "config settings (/config/train.ini).")
        if hasattr(self._args, "no_augment_color") and self._args.no_flip:
            deprecation_warning("`-nac`, ``--no-augment-color``",
                                additional_info="This option will be available within training "
                                                "config settings (/config/train.ini).")
        set_system_verbosity(self._args.loglevel)
        thread = self._start_thread()
        # from lib.queue_manager import queue_manager; queue_manager.debug_monitor(1)

        err = self._monitor(thread)

        self._end_thread(thread, err)
        logger.debug("Completed Training Process")
def monitor_console(self, thread):
          
        logger.debug("Launching Console Monitor")
        logger.info("R|===============================================")
        logger.info("R|- Starting                                    -")
        logger.info("R|- Press 'ENTER' to save and quit              -")
        logger.info("R|- Press 'S' to save model weights immediately -")
        logger.info("R|===============================================")
        keypress = KBHit(is_gui=self.args.redirect_gui)
        err = False
        while True:
            try:
                if thread.has_error:
                    logger.debug("Thread error detected")
                    err = True
                    break
                if self.stop:
                    logger.debug("Stop received")
                    break
                if keypress.kbhit():
                    key = keypress.getch()
                    if key in ("\n", "\r"):
                        logger.debug("Exit requested")
                        break
                    if key in ("s", "S"):
                        logger.info("Save requested")
                        self.save_now = True
            except KeyboardInterrupt:
                logger.debug("Keyboard Interrupt received")
                break
        keypress.set_normal_term()
        logger.debug("Closed Console Monitor")
        return err
def alignments_paths(self):
          
        alignments_paths = dict()
        for side in ("a", "b"):
            alignments_path = getattr(self.args, "alignments_path_{}".format(side))
            if not alignments_path:
                image_path = getattr(self.args, "input_{}".format(side))
                alignments_path = os.path.join(image_path, "alignments.fsa")
            alignments_paths[side] = alignments_path
        logger.debug("Alignments paths: %s", alignments_paths)
        return alignments_paths
def _get_images(self):
         
        
        logger.debug("Getting image paths")
        images = dict()
        for side in ("a", "b"):
            image_dir = getattr(self._args, "input_{}".format(side))
            if not os.path.isdir(image_dir):
                logger.error("Error: '%s' does not exist", image_dir)
                sys.exit(1)

            images[side] = get_image_paths(image_dir, ".png")
            if not images[side]:
                logger.error("Error: '%s' contains no images", image_dir)
                sys.exit(1)
            # Validate the first image is a detected face
            test_image = next(img for img in images[side])
            meta = read_image_meta(test_image)
            logger.debug("Test file: (filename: %s, metadata: %s)", test_image, meta)
            if "itxt" not in meta or "alignments" not in meta["itxt"]:
                logger.error("The input folder '%s' contains images that are not extracted faces.",
                             image_dir)
                logger.error("You can only train a model on faces generated from Faceswap's "
                             "extract process. Please check your sources and try again.")
                sys.exit(1)

            logger.info("Model %s Directory: '%s' (%s images)",
                        side.upper(), image_dir, len(images[side]))
        logger.debug("Got image paths: %s", [(key, str(len(val)) + " images")
                                             for key, val in images.items()])
        self._validate_image_counts(images)
        return images
def _monitor(self, thread: MultiThread) -> bool:
         
        
        self._output_startup_info()
        keypress = KBHit(is_gui=self._args.redirect_gui)
        window_created = False
        err = False
        while True:
            try:
                if self._args.preview:
                    self._do_preview(window_created)
                    cv_key = cv2.waitKey(1000)  # pylint: disable=no-member
                else:
                    cv_key = None

                if thread.has_error:
                    logger.debug("Thread error detected")
                    err = True
                    break
                if self._stop:
                    logger.debug("Stop received")
                    break

                # Preview Monitor
                if not self._preview_monitor(cv_key):
                    break

                # Console Monitor
                if self._check_keypress(keypress):
                    break  # Exit requested

                # GUI Preview trigger update monitor
                self._process_gui_triggers()

                sleep(1)
            except KeyboardInterrupt:
                logger.debug("Keyboard Interrupt received")
                break
        keypress.set_normal_term()
        logger.debug("Closed Monitor")
        return err
def _process_gui_triggers(self) -> Dict[Literal["mask", "refresh"], bool]:
         
        
        retval: Dict[Literal["mask", "refresh"], bool] = {key: False for key in self._gui_triggers}
        if not self._args.redirect_gui:
            return retval

        for trigger, filename in self._gui_triggers.items():
            if os.path.isfile(filename):
                logger.debug("GUI Trigger received for: '%s'", trigger)
                retval[trigger] = True
                logger.debug("Removing gui trigger file: %s", filename)
                os.remove(filename)
                if trigger == "refresh":
                    print("")  # Let log print on different line from loss output
                    logger.info("Refresh preview requested...")
        return retval
def _show(self, image: np.ndarray, name: str = "") -> None:
         
        
        logger.debug("Updating preview: (name: %s)", name)
        try:
            scriptpath = os.path.realpath(os.path.dirname(sys.argv[0]))
            if self._args.write_image:
                logger.debug("Saving preview to disk")
                img = "training_preview.png"
                imgfile = os.path.join(scriptpath, img)
                cv2.imwrite(imgfile, image)  # pylint: disable=no-member
                logger.debug("Saved preview to: '%s'", img)
            if self._args.redirect_gui:
                logger.debug("Generating preview for GUI")
                img = ".gui_training_preview.png"
                imgfile = os.path.join(scriptpath, "lib", "gui", ".cache", "preview", img)
                cv2.imwrite(imgfile, image)  # pylint: disable=no-member
                logger.debug("Generated preview for GUI: '%s'", imgfile)
            if self._args.preview:
                logger.debug("Generating preview for display: '%s'", name)
                self._preview.buffer.add_image(name, image)
                logger.debug("Generated preview for display: '%s'", name)
        except Exception as err:
            logging.error("could not preview sample")
            raise err
        logger.debug("Updated preview: (name: %s)", name)
def test_qucs_network(self):
        
        
        
        if self.verbose:
            fig, axs = plt.subplots(2, 2, figsize = (8,6))
            fig.suptitle('qucs/skrf')
            fig2, axs2 = plt.subplots(2, 2, figsize = (8,6))
            fig2.suptitle('qucs/skrf residuals')
            
        limit_db = 0.1
        limit_deg = 1.
        
        for ref in self.ref_qucs:
            cpw = CPW(frequency = ref['n'].frequency, z0 = 50.,
                            w = ref['w'], s = ref['s'], t = ref['t'],
                            h = ref['h'],
                            has_metal_backside = ref['has_metal_backside'],
                            ep_r = self.ep_r, rho = self.rho,
                            tand = self.tand)
            line = cpw.line(d=self.l, unit='m', embed = True, z0=cpw.Z0)
            line.name = '`Media.CPW` skrf,qucs'
            
            # residuals
            res = line / ref['n']
            res.name = 'residuals ' + ref['n'].name

            # test if within limit lines
            # fixme : fail against all qucs networks
            #self.assertTrue(npy.all(npy.abs(res.s_db) < limit_db))
            #self.assertTrue(npy.all(npy.abs(res.s_deg) < limit_deg))
            
            if self.verbose:
                line.plot_s_db(0, 0, ax = axs[0, 0], color = ref['color'],
                               linestyle = 'none', marker = 'x')
                ref['n'].plot_s_db(0, 0, ax = axs[0, 0], color = ref['color'])
                res.plot_s_db(0, 0, ax = axs2[0, 0], linestyle = 'dashed',
                              color = ref['color'])
                
                line.plot_s_deg(0, 0, ax = axs[0, 1], color = ref['color'],
                               linestyle = 'none', marker = 'x')
                ref['n'].plot_s_deg(0, 0, ax = axs[0, 1], color = ref['color'])
                res.plot_s_deg(0, 0, ax = axs2[0, 1], linestyle = 'dashed',
                              color = ref['color'])
                
                line.plot_s_db(1, 0, ax = axs[1, 0], color = ref['color'],
                               linestyle = 'none', marker = 'x')
                ref['n'].plot_s_db(1, 0, ax = axs[1, 0], color = ref['color'])
                res.plot_s_db(1, 0, ax = axs2[1, 0], linestyle = 'dashed',
                              color = ref['color'])
                
                line.plot_s_deg(1, 0, ax = axs[1, 1], color = ref['color'],
                               linestyle = 'none', marker = 'x')
                ref['n'].plot_s_deg(1, 0, ax = axs[1, 1], color = ref['color'])
                res.plot_s_deg(1, 0, ax = axs2[1, 1], linestyle = 'dashed',
                              color = ref['color'])
                
        
        if self.verbose:
            axs[1, 0].legend(prop={'size': 6})
            axs[0, 0].get_legend().remove()
            axs[0, 1].get_legend().remove()
            axs[1, 1].get_legend().remove()
            fig.tight_layout()
            
            axs2[1, 0].legend(prop={'size': 6})
            axs2[0, 0].get_legend().remove()
            axs2[0, 1].get_legend().remove()
            axs2[1, 1].get_legend().remove()
            fig2.tight_layout()
def test_Z0(self):
        
        
        
        # values from http://wcalc.sourceforge.net/cgi-bin/coplanar.cgi
        assert_array_almost_equal(self.cpw1.Z0, 77.93, decimal=2)
def match_template(image, template, pad_input=False, mode='constant',
                   constant_values=0):
    
    
    assert_nD(image, (2, 3))

    if image.ndim < template.ndim:
        raise ValueError("Dimensionality of template must be less than or "
                         "equal to the dimensionality of image.")
    if np.any(np.less(image.shape, template.shape)):
        raise ValueError("Image must be larger than template.")

    image_shape = image.shape

    image = np.array(image, dtype=np.float64, copy=False)

    pad_width = tuple((width, width) for width in template.shape)
    if mode == 'constant':
        image = pad(image, pad_width=pad_width, mode=mode,
                    constant_values=constant_values)
    else:
        image = pad(image, pad_width=pad_width, mode=mode)

    # Use special case for 2-D images for much better performance in
    # computation of integral images
    if image.ndim == 2:
        image_window_sum = _window_sum_2d(image, template.shape)
        image_window_sum2 = _window_sum_2d(image ** 2, template.shape)
    elif image.ndim == 3:
        image_window_sum = _window_sum_3d(image, template.shape)
        image_window_sum2 = _window_sum_3d(image ** 2, template.shape)

    template_mean = template.mean()
    template_volume = np.prod(template.shape)
    template_ssd = np.sum((template - template_mean) ** 2)

    if image.ndim == 2:
        xcorr = fftconvolve(image, template[::-1, ::-1],
                            mode="valid")[1:-1, 1:-1]
    elif image.ndim == 3:
        xcorr = fftconvolve(image, template[::-1, ::-1, ::-1],
                            mode="valid")[1:-1, 1:-1, 1:-1]

    numerator = xcorr - image_window_sum * template_mean

    denominator = image_window_sum2
    np.multiply(image_window_sum, image_window_sum, out=image_window_sum)
    np.divide(image_window_sum, template_volume, out=image_window_sum)
    denominator -= image_window_sum
    denominator *= template_ssd
    np.maximum(denominator, 0, out=denominator)  # sqrt of negative number not allowed
    np.sqrt(denominator, out=denominator)

    response = np.zeros_like(xcorr, dtype=np.float64)

    # avoid zero-division
    mask = denominator > np.finfo(np.float64).eps

    response[mask] = numerator[mask] / denominator[mask]

    slices = []
    for i in range(template.ndim):
        if pad_input:
            d0 = (template.shape[i] - 1) // 2
            d1 = d0 + image_shape[i]
        else:
            d0 = template.shape[i] - 1
            d1 = d0 + image_shape[i] - template.shape[i] + 1
        slices.append(slice(d0, d1))

    return response[slices]
def __init__(self, *, max_history: int = 3):
        
        
        self.max_history: int = max_history
        self.history_buffer: List[Tuple[int, DatasetStats]] = []
        self.count = 0
        self.wait_time_s = []

        # Iteration stats, filled out if the user iterates over the pipeline.
        self._iter_stats = {
            "iter_ds_wait_s": Timer(),
            "iter_wait_s": Timer(),
            "iter_get_s": Timer(),
            "iter_next_batch_s": Timer(),
            "iter_format_batch_s": Timer(),
            "iter_collate_batch_s": Timer(),
            "iter_user_s": Timer(),
            "iter_total_s": Timer(),
        }
def summary_string(
        self, already_printed: Set[str] = None, include_parent: bool = True
    ) -> str:
        
        
        if already_printed is None:
            already_printed = set()

        if self.needs_stats_actor:
            ac = self.stats_actor
            # TODO(chengsu): this is a super hack, clean it up.
            stats_map, self.time_total_s = ray.get(ac.get.remote(self.stats_uuid))
            if DatasetContext.get_current().block_splitting_enabled:
                self.stages["read"] = []
                for _, blocks_metadata in sorted(stats_map.items()):
                    self.stages["read"] += blocks_metadata
            else:
                for i, metadata in stats_map.items():
                    self.stages["read"][i] = metadata[0]
        out = ""
        if self.parents and include_parent:
            for p in self.parents:
                parent_sum = p.summary_string(already_printed)
                if parent_sum:
                    out += parent_sum
                    out += "\n"
        if len(self.stages) == 1:
            stage_name, metadata = next(iter(self.stages.items()))
            stage_uuid = self.dataset_uuid + stage_name
            out += "Stage {} {}: ".format(self.number, stage_name)
            if stage_uuid in already_printed:
                out += "[execution cached]\n"
            else:
                already_printed.add(stage_uuid)
                out += self._summarize_blocks(metadata, is_substage=False)
        elif len(self.stages) > 1:
            rounded_total = round(self.time_total_s, 2)
            if rounded_total <= 0:
                # Handle -0.0 case.
                rounded_total = 0
            out += "Stage {} {}: executed in {}s\n".format(
                self.number, self.base_name, rounded_total
            )
            for n, (stage_name, metadata) in enumerate(self.stages.items()):
                stage_uuid = self.dataset_uuid + stage_name
                out += "\n"
                out += "\tSubstage {} {}: ".format(n, stage_name)
                if stage_uuid in already_printed:
                    out += "\t[execution cached]\n"
                else:
                    already_printed.add(stage_uuid)
                    out += self._summarize_blocks(metadata, is_substage=True)
        out += self._summarize_iter()
        return out
def __init__(
        self,
        *,
        metadata: StatsDict,
        parent: Union[Optional["DatasetStats"], List["DatasetStats"]],
        needs_stats_actor: bool = False,
        stats_uuid: str = None,
        base_name: str = None,
    ):
        
        

        self.metadata: StatsDict = metadata
        if parent is not None and not isinstance(parent, list):
            parent = [parent]
        self.parents: List["DatasetStats"] = parent or []
        self.number: int = (
            0 if not self.parents else max(p.number for p in self.parents) + 1
        )
        self.base_name = base_name
        # TODO(ekl) deprecate and remove the notion of dataset UUID once we move
        # fully to streaming execution.
        self.dataset_uuid: str = "unknown_uuid"
        self.time_total_s: float = 0
        self.needs_stats_actor = needs_stats_actor
        self.stats_uuid = stats_uuid

        # Iteration stats, filled out if the user iterates over the dataset.
        self.iter_wait_s: Timer = Timer()
        self.iter_get_s: Timer = Timer()
        self.iter_next_batch_s: Timer = Timer()
        self.iter_format_batch_s: Timer = Timer()
        self.iter_collate_batch_s: Timer = Timer()
        self.iter_finalize_batch_s: Timer = Timer()
        self.iter_total_blocked_s: Timer = Timer()
        self.iter_user_s: Timer = Timer()
        self.iter_total_s: Timer = Timer()
        self.extra_metrics = {}

        # Block fetch stats during iteration.
        # These are stats about locations of blocks when the iterator is trying to
        # consume them. The iteration performance will be affected depending on
        # whether the block is in the local object store of the node where the
        # iterator is running.
        # This serves as an indicator of block prefetching effectiveness.
        self.iter_blocks_local: int = 0
        self.iter_blocks_remote: int = 0
        self.iter_unknown_location: int = 0

        # Memory usage stats
        self.global_bytes_spilled: int = 0
        self.global_bytes_restored: int = 0
        self.dataset_bytes_spilled: int = 0
def fetch_orders(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}) -> List[Order]:
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' fetchOrders() requires a symbol argument')
        self.load_markets()
        market = self.market(symbol)
        request = {
            'currencyPair': market['id'],
        }
        # offset param that appears in other parts of the API doesn't appear to be supported here
        if limit is not None:
            request['limit'] = limit
        response = self.privatePostOrderHistory(self.extend(request, params))
        return self.parse_orders(response['data'], market, since, limit)
def fetch_ticker(self, symbol: str, params={}) -> Ticker:
        
        
        
        self.load_markets()
        market = self.market(symbol)
        request = {
            'currencyPair': market['id'],
        }
        response = self.publicGetTicker(self.extend(request, params))
        #
        #     {
        #         "error": False,
        #         "errorMessage": null,
        #         "data": {
        #             "last": 0.55105,
        #             "high": 0.56439,
        #             "low": 0.54358,
        #             "amount": 37038.993381,
        #             "bid": 0.54595,
        #             "ask": 0.55324,
        #             "change": 3.03659243,
        #             "open": 0.53481,
        #             "timestamp": 1708074779
        #         }
        #     }
        #
        data = self.safe_value(response, 'data')
        return self.parse_ticker(data, market)
def test_correct_labelsize():
    
    dataset = datasets.load_iris()
    X = dataset.data

    # n_labels = n_samples
    y = np.arange(X.shape[0])
    assert_raises_regexp(ValueError,
                         'Number of labels is %d\. Valid values are 2 '
                         'to n_samples - 1 \(inclusive\)' % len(np.unique(y)),
                         silhouette_score, X, y)

    # n_labels = 1
    y = np.zeros(X.shape[0])
    assert_raises_regexp(ValueError,
                         'Number of labels is %d\. Valid values are 2 '
                         'to n_samples - 1 \(inclusive\)' % len(np.unique(y)),
                         silhouette_score, X, y)
def __init__(self, bottleneck_dimension, attention_temperature,
               output_dimension=None, is_training=False,
               name='AttentionBlock', max_num_proposals=100,
               **kwargs):
    
    

    self._key_proj = ContextProjection(bottleneck_dimension)
    self._val_proj = ContextProjection(bottleneck_dimension)
    self._query_proj = ContextProjection(bottleneck_dimension)
    self._feature_proj = None
    self._attention_temperature = attention_temperature
    self._bottleneck_dimension = bottleneck_dimension
    self._is_training = is_training
    self._output_dimension = output_dimension
    self._max_num_proposals = max_num_proposals
    if self._output_dimension:
      self._feature_proj = ContextProjection(self._output_dimension)
    super(AttentionBlock, self).__init__(name=name, **kwargs)
def _Segmentation(self, input, K):
        
        
        B, N, L = input.shape
        P = K // 2
        input, gap = self._padding(input, K)
        # [B, N, K, S]
        input1 = input[:, :, :-P].contiguous().view(B, N, -1, K)
        input2 = input[:, :, P:].contiguous().view(B, N, -1, K)
        input = (
            torch.cat([input1, input2], dim=3).view(B, N, -1, K).transpose(2, 3)
        )

        return input.contiguous(), gap
def merge_ssegs_same_speaker(lol):
    
    
    

    new_lol = []

    # Start from the first sub-seg
    sseg = lol[0]
    flag = False
    for i in range(1, len(lol)):
        next_sseg = lol[i]

        # IF sub-segments overlap AND has same speaker THEN merge
        if is_overlapped(sseg[2], next_sseg[1]) and sseg[3] == next_sseg[3]:
            sseg[2] = next_sseg[2]  # just update the end time
            # This is important. For the last sseg, if it is the same speaker the merge
            # Make sure we don't append the last segment once more. Hence, set FLAG=True
            if i == len(lol) - 1:
                flag = True
                new_lol.append(sseg)
        else:
            new_lol.append(sseg)
            sseg = next_sseg

    # Add last segment only when it was skipped earlier.
    if flag is False:
        new_lol.append(lol[-1])

    return new_lol
def check_random_state(seed):
    
    
    

    if seed is None or seed is np.random:
        return np.random.mtrand._rand
    if isinstance(seed, numbers.Integral):
        return np.random.RandomState(seed)
    if isinstance(seed, np.random.RandomState):
        return seed
    raise ValueError(
        "%r cannot be used to seed a np.random.RandomState" " instance" % seed
    )
def perform_sc(self, X, n_neighbors=10):
        
        
        

        # Computation of affinity matrix
        connectivity = kneighbors_graph(
            X, n_neighbors=n_neighbors, include_self=True,
        )
        self.affinity_matrix_ = 0.5 * (connectivity + connectivity.T)

        # Perform spectral clustering on affinity matrix
        self.labels_ = spectral_clustering_sb(
            self.affinity_matrix_, n_clusters=self.n_clusters,
        )
        return self
def spectral_clustering_sb(
    affinity, n_clusters=8, n_components=None, random_state=None, n_init=10,
):
    
    
    

    random_state = _check_random_state(random_state)
    n_components = n_clusters if n_components is None else n_components

    maps = spectral_embedding_sb(
        affinity, n_components=n_components, drop_first=False,
    )

    _, labels, _ = k_means(
        maps, n_clusters, random_state=random_state, n_init=n_init
    )

    return labels
def do_AHC(diary_obj, out_rttm_file, rec_id, k_oracle=4, p_val=0.3):
    
    

    from sklearn.cluster import AgglomerativeClustering

    # p_val is the threshold_val (for AHC)

    diary_obj.norm_stat1()

    # processing
    if k_oracle is not None:
        num_of_spk = k_oracle

        clustering = AgglomerativeClustering(
            n_clusters=num_of_spk, affinity="cosine", linkage="ward",
        ).fit(diary_obj.stat1)
        labels = clustering.labels_

    else:
        # Estimate num of using max eigen gap with `cos` affinity matrix.
        # This is just for experimentation.
        clustering = AgglomerativeClustering(
            n_clusters=None,
            affinity="cosine",
            linkage="ward",
            distance_threshold=p_val,
        ).fit(diary_obj.stat1)
        labels = clustering.labels_

    # Convert labels to speaker boundaries
    subseg_ids = diary_obj.segset
    lol = []

    for i in range(labels.shape[0]):
        spkr_id = rec_id + "_" + str(labels[i])

        sub_seg = subseg_ids[i]

        splitted = sub_seg.rsplit("_", 2)
        rec_id = str(splitted[0])
        sseg_start = float(splitted[1])
        sseg_end = float(splitted[2])

        a = [rec_id, sseg_start, sseg_end, spkr_id]
        lol.append(a)

    # Sorting based on start time of sub-segment
    lol.sort(key=lambda x: float(x[1]))

    # Merge and split in 2 simple steps: (i) Merge sseg of same speakers then (ii) split different speakers
    # Step 1: Merge adjacent sub-segments that belong to same speaker (or cluster)
    lol = merge_ssegs_same_speaker(lol)

    # Step 2: Distribute duration of adjacent overlapping sub-segments belonging to different speakers (or cluster)
    # Taking mid-point as the splitting time location.
    lol = distribute_overlap(lol)

    # logger.info("Completed diarizing " + rec_id)
    write_rttm(lol, out_rttm_file)
def complex_semver_match(version, version_specifier):
    
    
    
    if version_specifier == 'all':
        return True

    split_version_specifier = version_specifier.split(',')

    if len(split_version_specifier) == 1:
        # No comma, we can do a simple comparision
        return semver.match(version, version_specifier)
    else:
        # Compare part by part
        for version_specifier_part in split_version_specifier:
            version_specifier_part = version_specifier_part.strip()

            if not version_specifier_part:
                continue

            if not semver.match(version, version_specifier_part):
                return False

        return True
def add(self, exception, handler, route_names: Optional[List[str]] = None):
        

          # noqa: E501
        if route_names:
            for route in route_names:
                self._add((exception, route), handler)
        else:
            self._add((exception, None), handler)
def paragraph(self, nb_sentences=3, variable_nb_sentences=True, ext_word_list=None):
        
        
        
        if nb_sentences <= 0:
            return ''

        if variable_nb_sentences:
            nb_sentences = self.randomize_nb_elements(nb_sentences)

        para = self.word_connector.join(self.sentences(
            nb_sentences, ext_word_list=ext_word_list
        ))

        return para
def paragraphs(cls, nb=3, ext_word_list=None):
        
        
        

        return [cls.paragraph(ext_word_list=ext_word_list) for _ in range(0, nb)]
def visit_call(self, node: astroid.Call) -> None:
        
        if not isinstance(node.func, astroid.Attribute) or not isinstance(
            node.func.expr, astroid.Name
        ):
            return

        if not node.func.expr.name in LOGGER_NAMES:
            return

        if not node.args:
            return

        first_arg = node.args[0]

        if not isinstance(first_arg, astroid.Const) or not first_arg.value:
            return

        log_message = first_arg.value

        if len(log_message) < 1:
            return

        if log_message[-1] == ".":
            self.add_message("hass-logger-period", node=node)

        if (
            isinstance(node.func.attrname, str)
            and node.func.attrname not in LOG_LEVEL_ALLOWED_LOWER_START
            and log_message[0].upper() != log_message[0]
        ):
            self.add_message("hass-logger-capital", node=node)
def test_sequence_creation(self):
        
        
        
        out = StringIO()
        management.call_command("dumpdata", "m2m_through_regress", format="json", stdout=out)
        self.assertJSONEqual(
            out.getvalue().strip(),
            '[{"pk": 1, "model": "m2m_through_regress.usermembership", "fields": {"price": 100, "group": 1, "user"'
            ': 1}}, {"pk": 1, "model": "m2m_through_regress.person", "fields": {"name": "Guido"}}, {"pk": 1, '
            '"model": "m2m_through_regress.group", "fields": {"name": "Python Core Group"}}]'
        )
def test_join_trimming_forwards(self):
        
        
        
        self.assertQuerysetEqual(
            self.rock.members.filter(membership__price=50), [
                "<Person: Jim>",
            ]
        )
def __init__(self, unique_id, zha_device, listeners, **kwargs):
        
        super().__init__(unique_id, zha_device, listeners, **kwargs)
        sensor_type = kwargs.get(SENSOR_TYPE, GENERIC)
        self._unit = UNIT_REGISTRY.get(sensor_type)
        self._formatter_function = FORMATTER_FUNC_REGISTRY.get(
            sensor_type,
            pass_through_formatter
        )
        self._force_update = FORCE_UPDATE_REGISTRY.get(
            sensor_type,
            False
        )
        self._should_poll = POLLING_REGISTRY.get(
            sensor_type,
            False
        )
        self._listener = self.cluster_listeners.get(
            LISTENER_REGISTRY.get(sensor_type, LISTENER_ATTRIBUTE)
        )
def set_env(self, env: 'Environment'):
        
        self._rc.env = env
def _watch(self, actions: Iterable[Type[Action]] | Iterable[Action]):
        
        
        self.rc.watch = {any_to_str(t) for t in actions}
        # check RoleContext after adding watch actions
        self.rc.check(self.role_id)
def async_api_activate(hass, config, request, entity):
    
    if entity.domain == group.DOMAIN:
        domain = ha.DOMAIN
    else:
        domain = entity.domain

    yield from hass.services.async_call(domain, SERVICE_TURN_ON, {
        ATTR_ENTITY_ID: entity.entity_id
    }, blocking=False)

    payload = {
        'cause': {'type': _Cause.VOICE_INTERACTION},
        'timestamp': '%sZ' % (datetime.utcnow().isoformat(),)
    }

    return api_message(
        request,
        name='ActivationStarted',
        namespace='Alexa.SceneController',
        payload=payload,
    )
def __init__(self,
               optimizer: tf_keras.optimizers.Optimizer,
               trainable_weights_only: bool = True,
               average_decay: float = 0.99,
               start_step: int = 0,
               dynamic_decay: bool = True,
               name: str = 'ExponentialMovingAverage',
               **kwargs):
    
    
    super().__init__(name, **kwargs)
    self._average_decay = average_decay
    self._trainable_weights_only = trainable_weights_only
    self._start_step = tf.constant(start_step, tf.float32)
    self._dynamic_decay = dynamic_decay
    self._optimizer = optimizer
    self._track_trackable(self._optimizer, 'ema_base_optimizer')
    self._average_weights = None
    self._model_weights = None
def get_output(script):
    

    
    if six.PY2:
        warn('Experimental instant mode is Python 3+ only')
        return None

    if 'THEFUCK_OUTPUT_LOG' not in os.environ:
        warn("Output log isn't specified")
        return None

    if const.USER_COMMAND_MARK not in os.environ.get('PS1', ''):
        warn("PS1 doesn't contain user command mark, please ensure "
             "that PS1 is not changed after The Fuck alias initialization")
        return None

    try:
        with open(os.environ['THEFUCK_OUTPUT_LOG'], 'rb') as log_file:
            lines = _get_output_lines(script, log_file)
            output = '\n'.join(lines).strip()
            debug(u'Received output: {}'.format(output))
            return output
    except OSError:
        warn("Can't read output log")
        return None
    except ScriptNotInLog:
        warn("Script not found in output log")
        return None
def test_unzip_local_file(mocker, tmpdir):
    
    
    mock_prompt_and_delete = mocker.patch(
        'cookiecutter.zipfile.prompt_and_delete',
        return_value=True,
        autospec=True
    )

    clone_to_dir = tmpdir.mkdir('clone')

    output_dir = zipfile.unzip(
        'tests/files/fake-repo-tmpl.zip',
        is_url=False,
        clone_to_dir=str(clone_to_dir)
    )

    assert output_dir.startswith(tempfile.gettempdir())
    assert not mock_prompt_and_delete.called
def __init__(self):
        
        self.repo_zip_url = "https://github.com/danielmiessler/fabric/archive/refs/heads/main.zip"
        self.config_directory = os.path.expanduser("~/.config/fabric")
        self.pattern_directory = os.path.join(
            self.config_directory, "patterns")
        os.makedirs(self.pattern_directory, exist_ok=True)
        self.update_patterns()
def inpaint_biharmonic(image, mask, *,
                       split_into_regions=False, channel_axis=None):
    
    

    if image.ndim < 1:
        raise ValueError('Input array has to be at least 1D')

    multichannel = channel_axis is not None
    img_baseshape = image.shape[:-1] if multichannel else image.shape
    if img_baseshape != mask.shape:
        raise ValueError('Input arrays have to be the same shape')

    if np.ma.isMaskedArray(image):
        raise TypeError('Masked arrays are not supported')

    image = skimage.img_as_float(image)

    # float16->float32 and float128->float64
    float_dtype = utils._supported_float_type(image.dtype)
    image = image.astype(float_dtype, copy=False)

    mask = mask.astype(bool, copy=False)
    if not multichannel:
        image = image[..., np.newaxis]
    out = np.copy(image, order='C')

    # Create biharmonic coefficients ndarray
    radius = 2
    coef_shape = (2 * radius + 1,) * mask.ndim
    coef_center = (radius,) * mask.ndim
    neigh_coef_full, coef_idx, coef_vals = _get_neigh_coef(coef_shape,
                                                           coef_center,
                                                           dtype=out.dtype)

    # stride for the last spatial dimension
    channel_stride_bytes = out.strides[-2]

    # offsets to all neighboring non-zero elements in the footprint
    offsets = coef_idx - radius

    # determine per-channel intensity limits
    known_points = image[~mask]
    limits = (known_points.min(axis=0), known_points.max(axis=0))

    if split_into_regions:
        # Split inpainting mask into independent regions
        kernel = ndi.generate_binary_structure(mask.ndim, 1)
        mask_dilated = ndi.binary_dilation(mask, structure=kernel)
        mask_labeled = label(mask_dilated)
        mask_labeled *= mask

        bbox_slices = ndi.find_objects(mask_labeled)

        for idx_region, bb_slice in enumerate(bbox_slices, 1):
            # expand object bounding boxes by the biharmonic kernel radius
            roi_sl = tuple(slice(max(sl.start - radius, 0),
                                 min(sl.stop + radius, size))
                           for sl, size in zip(bb_slice, mask_labeled.shape))
            # extract only the region surrounding the label of interest
            mask_region = mask_labeled[roi_sl] == idx_region
            # add slice for axes
            roi_sl += (slice(None), )
            # copy for contiguity and to account for possible ROI overlap
            otmp = out[roi_sl].copy()

            # compute raveled offsets for the ROI
            ostrides = np.array([s // channel_stride_bytes
                                 for s in otmp[..., 0].strides])
            raveled_offsets = np.sum(offsets * ostrides[..., np.newaxis],
                                     axis=0)

            _inpaint_biharmonic_single_region(
                image[roi_sl], mask_region, otmp,
                neigh_coef_full, coef_vals, raveled_offsets
            )
            # assign output to the
            out[roi_sl] = otmp
    else:
        # compute raveled offsets for output image
        ostrides = np.array([s // channel_stride_bytes
                             for s in out[..., 0].strides])
        raveled_offsets = np.sum(offsets * ostrides[..., np.newaxis], axis=0)

        _inpaint_biharmonic_single_region(
            image, mask, out, neigh_coef_full, coef_vals, raveled_offsets
        )

    # Handle enormous values on a per-channel basis
    np.clip(out, a_min=limits[0], a_max=limits[1], out=out)

    if not multichannel:
        out = out[..., 0]

    return out
def get_file_paths(self) -> List[Dict]:
        base_url = (
            f"{self.github_api_url}/repos/{self.repo}/git/trees/"
            f"{self.branch}?recursive=1"
        )
        response = requests.get(base_url, headers=self.headers)
        response.raise_for_status()
        all_files = response.json()["tree"]
         
        
        return [
            f
            for f in all_files
            if not (self.file_filter and not self.file_filter(f["path"]))
        ]
def call(self, h):
        
        
        # Send internal state through MLP.
        out = self.mlp(h)
        # Generate a z vector (stochastic, discrete sample).
        return self.representation_layer(out)
def extract(self, path_or_paths, num_proc=None):
        
        
        download_config = self._download_config.copy()
        download_config.extract_compressed_file = True
        download_config.force_extract = False
        return map_nested(
            partial(cached_path, download_config=download_config),
            path_or_paths,
            num_proc=num_proc,
        )
def iter_files(self, paths: Union[str, List[str]]):
        
        
        if not isinstance(paths, list):
            paths = [paths]
        for path in paths:
            if os.path.isfile(path):
                yield path
            else:
                for dirpath, _, filenames in os.walk(path):
                    for filename in filenames:
                        yield os.path.join(dirpath, filename)
def __init__(
        self,
        dataset_name: Optional[str] = None,
        data_dir: Optional[str] = None,
        download_config: Optional[DownloadConfig] = None,
        base_path: Optional[str] = None,
        record_checksums=True,
    ):
        
        
        self._dataset_name = dataset_name
        self._data_dir = data_dir
        self._base_path = base_path or os.path.abspath(".")
        # To record what is being used: {url: {num_bytes: int, checksum: str}}
        self._recorded_sizes_checksums: Dict[str, Dict[str, Optional[Union[int, str]]]] = {}
        self.record_checksums = record_checksums
        self.download_config = download_config or DownloadConfig()
        self.downloaded_paths = {}
        self.extracted_paths = {}
def added(self, function):
        
        
        pass
def _make_estimator(self, append=True, random_state=None):
        
        
        estimator = clone(self.estimator_)
        estimator.set_params(**{p: getattr(self, p) for p in self.estimator_params})

        # TODO(1.3): Remove
        # max_features = 'auto' would cause warnings in every call to
        # Tree.fit(..)
        if isinstance(estimator, BaseDecisionTree):
            if getattr(estimator, "max_features", None) == "auto":
                if isinstance(estimator, DecisionTreeClassifier):
                    estimator.set_params(max_features="sqrt")
                elif isinstance(estimator, DecisionTreeRegressor):
                    estimator.set_params(max_features=1.0)

        if random_state is not None:
            _set_random_states(estimator, random_state)

        if append:
            self.estimators_.append(estimator)

        return estimator
def load_collected(self):
        
        logger.info(
            f"Loading pretrained files for: {', '.join(self.loadables)}"
        )
        paramfiles = {}
        for name in self.loadables:
            if not self.is_loadable(name):
                continue
            filename = name + PARAMFILE_EXT
            paramfiles[name] = self.collect_in / filename

            if name in self.is_local:
                logger.info(
                    f"Redirecting (loading from local path): {paramfiles[name]} -> {self.paths[name]}"
                )
                paramfiles[name] = self.paths[name]
        self._call_load_hooks(paramfiles)
def test_increase_target_capacity(self, mock_deployment_state):
        
        
        
        deployment_state, timer, cluster_node_info_cache = mock_deployment_state

        b_info_1, b_version_1 = deployment_info(num_replicas=10)
        updating = deployment_state.deploy(b_info_1)
        assert updating

        # Start with target_capacity set to 0, should have no replicas start up.
        deployment_state.update(target_capacity=0)
        check_counts(deployment_state, total=0)
        assert deployment_state.curr_status_info.status == DeploymentStatus.HEALTHY

        # Increase target_capacity to 0, should have 1 replica start up.
        deployment_state_update_result = deployment_state.update(target_capacity=1)
        deployment_state._deployment_scheduler.schedule(
            {deployment_state._id: deployment_state_update_result.upscale}, {}
        )
        check_counts(deployment_state, total=1, by_state=[(ReplicaState.STARTING, 1)])
        # TODO(edoakes): when we update the state machine to include
        # upscaling/downscaling, this should be upscaling.
        assert deployment_state.curr_status_info.status == DeploymentStatus.HEALTHY

        for replica in deployment_state._replicas.get():
            replica._actor.set_ready()

        deployment_state.update(target_capacity=0)
        check_counts(deployment_state, total=1, by_state=[(ReplicaState.RUNNING, 1)])
        assert deployment_state.curr_status_info.status == DeploymentStatus.HEALTHY

        # Set target_capacity to 50, should have 4 more replicas start up.
        deployment_state_update_result = deployment_state.update(target_capacity=50)
        deployment_state._deployment_scheduler.schedule(
            {deployment_state._id: deployment_state_update_result.upscale}, {}
        )
        check_counts(
            deployment_state,
            total=5,
            by_state=[(ReplicaState.RUNNING, 1), (ReplicaState.STARTING, 4)],
        )
        # TODO(edoakes): when we update the state machine to include
        # upscaling/downscaling, this should be upscaling.
        assert deployment_state.curr_status_info.status == DeploymentStatus.HEALTHY

        for replica in deployment_state._replicas.get():
            replica._actor.set_ready()

        deployment_state.update(target_capacity=50)
        check_counts(deployment_state, total=5, by_state=[(ReplicaState.RUNNING, 5)])
        assert deployment_state.curr_status_info.status == DeploymentStatus.HEALTHY

        # Set target_capacity to 100, should have 5 more replicas start up.
        deployment_state_update_result = deployment_state.update(target_capacity=100)
        deployment_state._deployment_scheduler.schedule(
            {deployment_state._id: deployment_state_update_result.upscale}, {}
        )
        check_counts(
            deployment_state,
            total=10,
            by_state=[(ReplicaState.RUNNING, 5), (ReplicaState.STARTING, 5)],
        )
        # TODO(edoakes): when we update the state machine to include
        # upscaling/downscaling, this should be upscaling.
        assert deployment_state.curr_status_info.status == DeploymentStatus.HEALTHY

        for replica in deployment_state._replicas.get():
            replica._actor.set_ready()

        deployment_state.update(target_capacity=100)
        check_counts(deployment_state, total=10, by_state=[(ReplicaState.RUNNING, 10)])
        assert deployment_state.curr_status_info.status == DeploymentStatus.HEALTHY
def test_reconfigure_throttling(mock_deployment_state_manager_full):
    
    

    create_dsm, _, _ = mock_deployment_state_manager_full
    dsm: DeploymentStateManager = create_dsm()

    b_info_1, v1 = deployment_info(num_replicas=2, version="1", user_config="1")
    assert dsm.deploy(TEST_DEPLOYMENT_ID, b_info_1)
    ds = dsm._deployment_states[TEST_DEPLOYMENT_ID]

    dsm.update()
    check_counts(ds, total=2, by_state=[(ReplicaState.STARTING, 2, v1)])
    assert ds.curr_status_info.status == DeploymentStatus.UPDATING
    assert (
        ds.curr_status_info.status_trigger
        == DeploymentStatusTrigger.CONFIG_UPDATE_STARTED
    )

    for replica in ds._replicas.get():
        replica._actor.set_ready()

    # Check that the new replicas have started.
    dsm.update()
    check_counts(ds, total=2, by_state=[(ReplicaState.RUNNING, 2, v1)])
    assert ds.curr_status_info.status == DeploymentStatus.HEALTHY
    assert (
        ds.curr_status_info.status_trigger
        == DeploymentStatusTrigger.CONFIG_UPDATE_COMPLETED
    )

    # Now deploy a new user_config. One replica should be updated.
    b_info_2, v2 = deployment_info(num_replicas=2, version="1", user_config="2")
    assert dsm.deploy(TEST_DEPLOYMENT_ID, b_info_2)
    assert ds.curr_status_info.status == DeploymentStatus.UPDATING
    assert (
        ds.curr_status_info.status_trigger
        == DeploymentStatusTrigger.CONFIG_UPDATE_STARTED
    )

    dsm.update()
    check_counts(
        ds,
        total=2,
        by_state=[(ReplicaState.RUNNING, 1, v1), (ReplicaState.UPDATING, 1, v2)],
    )

    # Mark the updating replica as ready.
    ds._replicas.get(states=[ReplicaState.UPDATING])[0]._actor.set_ready()

    # The updated replica should now be RUNNING.
    # The second replica should now be updated.
    dsm.update()
    check_counts(
        ds,
        total=2,
        by_state=[(ReplicaState.RUNNING, 1, v2), (ReplicaState.UPDATING, 1, v2)],
    )
    assert ds.curr_status_info.status == DeploymentStatus.UPDATING
    assert (
        ds.curr_status_info.status_trigger
        == DeploymentStatusTrigger.CONFIG_UPDATE_STARTED
    )

    # Mark the updating replica as ready.
    ds._replicas.get(states=[ReplicaState.UPDATING])[0]._actor.set_ready()

    # Both replicas should now be RUNNING.
    dsm.update()
    check_counts(ds, total=2, by_state=[(ReplicaState.RUNNING, 2, v2)])
    assert ds.curr_status_info.status == DeploymentStatus.HEALTHY
    assert (
        ds.curr_status_info.status_trigger
        == DeploymentStatusTrigger.CONFIG_UPDATE_COMPLETED
    )
def save(self, is_exit: bool = False, force_save_optimizer: bool = False) -> None:
         
        
        logger.debug("Backing up and saving models")
        print("")  # Insert a new line to avoid spamming the same row as loss output
        save_averages = self._get_save_averages()
        if save_averages and self._should_backup(save_averages):
            self._backup.backup_model(self._filename)
            # pylint:disable=protected-access
            self._backup.backup_model(self._plugin.state._filename)

        include_optimizer = (force_save_optimizer or
                             self._save_optimizer == "always" or
                             (self._save_optimizer == "exit" and is_exit))

        self._plugin.model.save(self._filename, include_optimizer=include_optimizer)
        self._plugin.state.save()

        msg = "[Saved optimizer state for Snapshot]" if force_save_optimizer else "[Saved models]"
        if save_averages:
            lossmsg = [f"face_{side}: {avg:.5f}"
                       for side, avg in zip(("a", "b"), save_averages)]
            msg += f" - Average loss since last save: {', '.join(lossmsg)}"
        logger.info(msg)
def _load(self) -> tf.keras.models.Model:
         
        
        logger.debug("Loading model: %s", self._filename)
        if self._is_predict and not self.model_exists:
            logger.error("Model could not be found in folder '%s'. Exiting", self._model_dir)
            sys.exit(1)

        try:
            model = kmodels.load_model(self._filename, compile=False)
        except RuntimeError as err:
            if "unable to get link info" in str(err).lower():
                msg = (f"Unable to load the model from '{self._filename}'. This may be a "
                       "temporary error but most likely means that your model has corrupted.\n"
                       "You can try to load the model again but if the problem persists you "
                       "should use the Restore Tool to restore your model from backup.\n"
                       f"Original error: {str(err)}")
                raise FaceswapError(msg) from err
            raise err
        except KeyError as err:
            if "unable to open object" in str(err).lower():
                msg = (f"Unable to load the model from '{self._filename}'. This may be a "
                       "temporary error but most likely means that your model has corrupted.\n"
                       "You can try to load the model again but if the problem persists you "
                       "should use the Restore Tool to restore your model from backup.\n"
                       f"Original error: {str(err)}")
                raise FaceswapError(msg) from err
            raise err

        logger.info("Loaded model from disk: '%s'", self._filename)
        return model
def sample_boundaries(
        blocks: List[ObjectRef[Block]], sort_key: SortKey, num_reducers: int
    ) -> List[T]:
        
        
        
        columns = sort_key.get_columns()
        n_samples = int(num_reducers * 10 / len(blocks))

        sample_block = cached_remote_fn(_sample_block)

        sample_results = [
            sample_block.remote(block, n_samples, sort_key) for block in blocks
        ]
        sample_bar = ProgressBar(
            SortTaskSpec.SORT_SAMPLE_SUB_PROGRESS_BAR_NAME, len(sample_results)
        )
        samples = sample_bar.fetch_until_complete(sample_results)
        sample_bar.close()
        del sample_results
        samples = [s for s in samples if len(s) > 0]
        # The dataset is empty
        if len(samples) == 0:
            return [None] * (num_reducers - 1)
        builder = DelegatingBlockBuilder()
        for sample in samples:
            builder.add_block(sample)
        samples = builder.build()

        sample_dict = BlockAccessor.for_block(samples).to_numpy(columns=columns)
        # Compute sorted indices of the samples. In np.lexsort last key is the
        # primary key hence have to reverse the order.
        indices = np.lexsort(list(reversed(list(sample_dict.values()))))
        # Sort each column by indices, and calculate q-ths quantile items.
        # Ignore the 1st item as it's not required for the boundary
        for k, v in sample_dict.items():
            sorted_v = v[indices]
            sample_dict[k] = [
                np.quantile(sorted_v, q, interpolation="nearest")
                for q in np.linspace(0, 1, num_reducers)
            ][1:]
        # Return the list of boundaries as tuples
        # of a form (col1_value, col2_value, ...)
        return [
            tuple(sample_dict[k][i] for k in sample_dict)
            for i in range(num_reducers - 1)
        ]
def _register_fallback(cls, handler, wrapper, priority):
        
        
        while priority in cls.fallback_handlers:
            priority += 1

        cls.fallback_handlers[priority] = wrapper
        cls.wrapper_map.append((handler, wrapper))
def bbox2roi(bbox_list: List[Union[Tensor, BaseBoxes]]) -> Tensor:
    
    
    rois_list = []
    for img_id, bboxes in enumerate(bbox_list):
        bboxes = get_box_tensor(bboxes)
        img_inds = bboxes.new_full((bboxes.size(0), 1), img_id)
        rois = torch.cat([img_inds, bboxes], dim=-1)
        rois_list.append(rois)
    rois = torch.cat(rois_list, 0)
    return rois
def extract(self, image, keypoints):
        

        

        np.random.seed(self.sample_seed)

        image = _prepare_grayscale_input_2D(image)

        # Gaussian low-pass filtering to alleviate noise sensitivity
        image = np.ascontiguousarray(gaussian_filter(image, self.sigma))

        # Sampling pairs of decision pixels in patch_size x patch_size window
        desc_size = self.descriptor_size
        patch_size = self.patch_size
        if self.mode == 'normal':
            samples = (patch_size / 5.0) * np.random.randn(desc_size * 8)
            samples = np.array(samples, dtype=np.int32)
            samples = samples[(samples < (patch_size // 2))
                              & (samples > - (patch_size - 2) // 2)]

            pos1 = samples[:desc_size * 2].reshape(desc_size, 2)
            pos2 = samples[desc_size * 2:desc_size * 4].reshape(desc_size, 2)
        elif self.mode == 'uniform':
            samples = np.random.randint(-(patch_size - 2) // 2,
                                        (patch_size // 2) + 1,
                                        (desc_size * 2, 2))
            samples = np.array(samples, dtype=np.int32)
            pos1, pos2 = np.split(samples, 2)

        pos1 = np.ascontiguousarray(pos1)
        pos2 = np.ascontiguousarray(pos2)

        # Removing keypoints that are within (patch_size / 2) distance from the
        # image border
        self.mask = _mask_border_keypoints(image.shape, keypoints,
                                            patch_size // 2)

        keypoints = np.array(keypoints[self.mask, :], dtype=np.intp,
                             order='C', copy=False)

        self.descriptors = np.zeros((keypoints.shape[0], desc_size),
                                     dtype=bool, order='C')

        _brief_loop(image, self.descriptors.view(np.uint8), keypoints,
                    pos1, pos2)
def update_cache(self, soco: SoCo, update_id: int | None = None) -> bool:
        
        try:
            self.alarms.update(soco)
        except (OSError, SoCoException) as err:
            _LOGGER.error("Could not update alarms using %s: %s", soco, err)
            return False

        if update_id and self.alarms.last_id < update_id:
            # Skip updates if latest query result is outdated or lagging
            return False

        if (
            self.last_processed_event_id
            and self.alarms.last_id <= self.last_processed_event_id
        ):
            # Skip updates already processed
            return False

        _LOGGER.debug(
            "Updating processed event %s from %s (was %s)",
            self.alarms.last_id,
            soco,
            self.last_processed_event_id,
        )
        self.last_processed_event_id = self.alarms.last_id
        return True
def run(
    weights='yolov8n.pt',
    source=None,
    device='cpu',
    view_img=False,
    save_img=False,
    exist_ok=False,
    line_thickness=2,
    track_thickness=2,
    region_thickness=2,
):
    
    
    
    vid_frame_count = 0

    # Check source path
    if not Path(source).exists():
        raise FileNotFoundError(f"Source path '{source}' does not exist.")

    # Setup Model
    model = YOLO(f'{weights}')
    model.to('cuda') if device == '0' else model.to('cpu')

    # Video setup
    videocapture = cv2.VideoCapture(source)
    frame_width, frame_height = int(videocapture.get(3)), int(videocapture.get(4))
    fps, fourcc = int(videocapture.get(5)), cv2.VideoWriter_fourcc(*'mp4v')

    # Output setup
    save_dir = increment_path(Path('ultralytics_rc_output') / 'exp', exist_ok)
    save_dir.mkdir(parents=True, exist_ok=True)
    video_writer = cv2.VideoWriter(str(save_dir / f'{Path(source).stem}.mp4'), fourcc, fps, (frame_width, frame_height))

    # Iterate over video frames
    while videocapture.isOpened():
        success, frame = videocapture.read()
        if not success:
            break
        vid_frame_count += 1

        # Extract the results
        results = model.track(frame, persist=True)
        boxes = results[0].boxes.xywh.cpu()
        track_ids = results[0].boxes.id.int().cpu().tolist()
        clss = results[0].boxes.cls.cpu().tolist()
        names = results[0].names

        annotator = Annotator(frame, line_width=line_thickness, example=str(names))

        for box, track_id, cls in zip(boxes, track_ids, clss):
            x, y, w, h = box
            label = str(names[cls])
            xyxy = (x - w / 2), (y - h / 2), (x + w / 2), (y + h / 2)

            # Bounding box plot
            bbox_color = colors(cls, True)
            annotator.box_label(xyxy, label, color=bbox_color)

            # Tracking Lines plot
            track = track_history[track_id]
            track.append((float(x), float(y)))
            if len(track) > 30:
                track.pop(0)
            points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))
            cv2.polylines(frame, [points], isClosed=False, color=bbox_color, thickness=track_thickness)

            # Check if detection inside region
            for region in counting_regions:
                if is_inside_polygon((x, y), region['polygon']):
                    region['counts'] += 1

        # Draw regions (Polygons/Rectangles)
        for region in counting_regions:
            region_label = str(region['counts'])
            region_color = region['region_color']
            region_text_color = region['text_color']

            polygon_coords = np.array(region['polygon'].exterior.coords, dtype=np.int32)
            centroid_x, centroid_y = int(region['polygon'].centroid.x), int(region['polygon'].centroid.y)

            text_size, _ = cv2.getTextSize(region_label,
                                           cv2.FONT_HERSHEY_SIMPLEX,
                                           fontScale=0.7,
                                           thickness=line_thickness)
            text_x = centroid_x - text_size[0] // 2
            text_y = centroid_y + text_size[1] // 2
            cv2.rectangle(frame, (text_x - 5, text_y - text_size[1] - 5), (text_x + text_size[0] + 5, text_y + 5),
                          region_color, -1)
            cv2.putText(frame, region_label, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, region_text_color,
                        line_thickness)
            cv2.polylines(frame, [polygon_coords], isClosed=True, color=region_color, thickness=region_thickness)

        if view_img:
            if vid_frame_count == 1:
                cv2.namedWindow('Ultralytics YOLOv8 Region Counter Movable')
                cv2.setMouseCallback('Ultralytics YOLOv8 Region Counter Movable', mouse_callback)
            cv2.imshow('Ultralytics YOLOv8 Region Counter Movable', frame)

        if save_img:
            video_writer.write(frame)

        for region in counting_regions:  # Reinitialize count for each region
            region['counts'] = 0

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    del vid_frame_count
    video_writer.release()
    videocapture.release()
    cv2.destroyAllWindows()
def get_model_learning_rate(learning_policy,
                            base_learning_rate,
                            learning_rate_decay_step,
                            learning_rate_decay_factor,
                            training_number_of_steps,
                            learning_power,
                            slow_start_step,
                            slow_start_learning_rate,
                            slow_start_burnin_type='none'):
  
  
  global_step = tf.train.get_or_create_global_step()
  adjusted_global_step = global_step

  if slow_start_burnin_type != 'none':
    adjusted_global_step -= slow_start_step

  if learning_policy == 'step':
    learning_rate = tf.train.exponential_decay(
        base_learning_rate,
        adjusted_global_step,
        learning_rate_decay_step,
        learning_rate_decay_factor,
        staircase=True)
  elif learning_policy == 'poly':
    learning_rate = tf.train.polynomial_decay(
        base_learning_rate,
        adjusted_global_step,
        training_number_of_steps,
        end_learning_rate=0,
        power=learning_power)
  else:
    raise ValueError('Unknown learning policy.')

  adjusted_slow_start_learning_rate = slow_start_learning_rate
  if slow_start_burnin_type == 'linear':
    # Do linear burnin. Increase linearly from slow_start_learning_rate and
    # reach base_learning_rate after (global_step >= slow_start_steps).
    adjusted_slow_start_learning_rate = (
        slow_start_learning_rate +
        (base_learning_rate - slow_start_learning_rate) *
        tf.to_float(global_step) / slow_start_step)
  elif slow_start_burnin_type != 'none':
    raise ValueError('Unknown burnin type.')

  # Employ small learning rate at the first few steps for warm start.
  return tf.where(global_step < slow_start_step,
                  adjusted_slow_start_learning_rate, learning_rate)
def get_entity(self, name):
        ""
        return eval(name, {**sys.modules, **globals()})
def graph_laplacian(csgraph, normed=False, return_diag=False):
     
    
    if csgraph.ndim != 2 or csgraph.shape[0] != csgraph.shape[1]:
        raise ValueError('csgraph must be a square matrix or array')

    if normed and (np.issubdtype(csgraph.dtype, np.int)
                   or np.issubdtype(csgraph.dtype, np.uint)):
        csgraph = csgraph.astype(np.float)

    if sparse.isspmatrix(csgraph):
        return _laplacian_sparse(csgraph, normed=normed,
                                 return_diag=return_diag)
    else:
        return _laplacian_dense(csgraph, normed=normed,
                                return_diag=return_diag)
def flip_horizontal(keypoints, flip_point, flip_permutation=None, scope=None):
  
  
  with tf.name_scope(scope, 'FlipHorizontal'):
    keypoints = tf.transpose(keypoints, [1, 0, 2])
    if flip_permutation:
      keypoints = tf.gather(keypoints, flip_permutation)
    v, u = tf.split(value=keypoints, num_or_size_splits=2, axis=2)
    u = flip_point * 2.0 - u
    new_keypoints = tf.concat([v, u], 2)
    new_keypoints = tf.transpose(new_keypoints, [1, 0, 2])
    return new_keypoints
def print_alignments(
    details_by_utterance, file=sys.stdout, empty_symbol="<eps>", separator=" ; ",
    print_header=True
):
    
    
    if print_header:
        _print_alignments_global_header(
            file=file, empty_symbol=empty_symbol, separator=separator
        )
    for dets in details_by_utterance:
        if dets["scored"]:
            if print_header:
                _print_alignment_header(dets, file=file)
            _print_alignment(
                dets["alignment"],
                dets["ref_tokens"],
                dets["hyp_tokens"],
                file=file,
                empty_symbol=empty_symbol,
                separator=separator,
            )
def print_alignments(
    details_by_utterance, file=sys.stdout, empty_symbol="<eps>", separator=" ; ",
    print_header=True,
    sample_separator=None
):
    
    
    if print_header:
        _print_alignments_global_header(
            file=file, empty_symbol=empty_symbol, separator=separator
        )
    for dets in details_by_utterance:
        if dets["scored"]:
            if print_header:
                _print_alignment_header(dets, file=file)
            _print_alignment(
                dets["alignment"],
                dets["ref_tokens"],
                dets["hyp_tokens"],
                file=file,
                empty_symbol=empty_symbol,
                separator=separator,
            )
            if sample_separator:
                print(sample_separator, file=file)
def decode(string):
    
    
    if not string:
        return bytes()
    elif isinstance(string, str):
        return a2b_base64(string.encode('raw-unicode-escape'))
    else:
        return a2b_base64(string)
def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool:
    
    
    
    result = tf.constant(True)
    for i, target_length in enumerate(target_shape):
        if target_length:
            result = tf.logical_and(
                result, tf.equal(tf.constant(target_length), tf.shape(tensor_tf)[i])
            )
    return result
def parse(*args, **kwargs):
    
    from xml.dom import pulldom
    return _doparse(pulldom.parse, args, kwargs)
def test_count_lines(self):
        
        editwin = self.make_mock_editor_window()
        squeezer = self.make_squeezer_instance(editwin)

        for text_code, line_width, expected in [
            (r"'\n'", 80, 1),
            (r"'\n' * 3", 80, 3),
            (r"'a' * 40 + '\n'", 80, 1),
            (r"'a' * 80 + '\n'", 80, 1),
            (r"'a' * 200 + '\n'", 80, 3),
            (r"'aa\t' * 20", 80, 2),
            (r"'aa\t' * 21", 80, 3),
            (r"'aa\t' * 20", 40, 4),
        ]:
            with self.subTest(text_code=text_code,
                              line_width=line_width,
                              expected=expected):
                text = eval(text_code)
                squeezer.get_line_width.return_value = line_width
                self.assertEqual(squeezer.count_lines(text), expected)
def get_config(locations=None):
        
        
        
        if not ConfigurationManager.__config:
            ConfigurationManager.__config = ConfigurationLoader.load()
            RemoteConfiguration().update()

        if locations:
            ConfigurationManager.load(locations)

        return ConfigurationManager.__config
def init(ws):
        
        # TODO: deprecate this method in 19.08
        Configuration.set_config_update_handlers(ws)
def set_config_update_handlers(bus):
        
        
        bus.on("configuration.updated", Configuration.updated)
        bus.on("configuration.patch", Configuration.patch)
        bus.on("configuration.patch.clear", Configuration.patch_clear)
def get(configs=None, cache=True, remote=True):
        
        
        if Configuration.__config:
            return Configuration.__config
        else:
            return Configuration.load_config_stack(configs, cache, remote)
def store(self, path=None, force=False):
        
        
        result = False
        with self._lock:
            path = path or self.path
            config_dir = dirname(path)
            if not exists(config_dir):
                os.makedirs(config_dir)

            if self.is_valid or force:
                with open(path, 'w') as f:
                    json.dump(self, f, indent=2)
                result = True
            else:
                LOG.warning((f'"{path}" was not a valid config file when '
                             'loaded, will not save config. Please correct '
                             'the json or remove it to allow updates.'))
                result = False
        return result
def map(
        self,
        function: Callable,
        with_indices: bool = False,
        input_columns: Optional[Union[str, List[str]]] = None,
        batched: bool = False,
        batch_size: int = 1000,
        remove_columns: Optional[Union[str, List[str]]] = None,
    ):
        
        
        
        if isinstance(input_columns, str):
            input_columns = [input_columns]
        if isinstance(remove_columns, str):
            remove_columns = [remove_columns]
        info = copy.deepcopy(self._info)
        info.features = None
        ex_iterable = MappedExamplesIterable(
            TypedExamplesIterable(self._ex_iterable, self._info.features)
            if self._info.features is not None
            else self._ex_iterable,
            function=function,
            with_indices=with_indices,
            input_columns=input_columns,
            batched=batched,
            batch_size=batch_size,
            remove_columns=remove_columns,
        )
        return iterable_dataset(
            ex_iterable=ex_iterable,
            info=info,
            split=self._split,
            format_type=self._format_type,
            shuffling=copy.deepcopy(self._shuffling),
        )
def add_task(
        self,
        description: str,
        start: bool = True,
        total: Optional[float] = 100.0,
        completed: int = 0,
        visible: bool = True,
        **fields: Any,
    ) -> TaskID:
        
        
        with self._lock:
            task = Task(
                self._task_index,
                description,
                total,
                completed,
                visible=visible,
                fields=fields,
                _get_time=self.get_time,
                _lock=self._lock,
            )
            self._tasks[self._task_index] = task
            if start:
                self.start_task(self._task_index)
            new_task_index = self._task_index
            self._task_index = TaskID(int(self._task_index) + 1)
        self.refresh()
        return new_task_index
def read(
        self,
        file: Union[str, PathLike[str], BinaryIO],
        total: Optional[int] = None,
        task_id: Optional[TaskID] = None,
        description: str = "Reading...",
    ) -> BinaryIO:
        
        

        if total is None:
            if isinstance(file, (str, PathLike)):
                task_total = stat(file).st_size
            else:
                raise ValueError(
                    f"unable to get size of {file!r}, please specify 'total'"
                )
        else:
            task_total = total

        if task_id is None:
            task_id = self.add_task(description, total=task_total)
        else:
            self.update(task_id, total=task_total)

        if isinstance(file, (str, PathLike)):
            handle = open(file, "rb")
            close_handle = True
        else:
            if not isinstance(file.read(0), bytes):
                raise ValueError("expected file open in binary mode")
            handle = file
            close_handle = False

        return _Reader(handle, self, task_id, close_handle=close_handle)
def track(
        self,
        sequence: Union[Iterable[ProgressType], Sequence[ProgressType]],
        total: int = None,
        task_id: Optional[TaskID] = None,
        description="Working...",
        update_period: float = 0.1,
    ) -> Iterable[ProgressType]:
        
        
        if total is None:
            if isinstance(sequence, Sized):
                task_total = len(sequence)
            else:
                raise ValueError(
                    f"unable to get size of {sequence!r}, please specify 'total'"
                )
        else:
            task_total = total

        if task_id is None:
            task_id = self.add_task(description, total=task_total)
        else:
            self.update(task_id, total=task_total)
        with self:
            if self.auto_refresh:
                with _TrackThread(self, task_id, update_period) as track_thread:
                    for value in sequence:
                        yield value
                        track_thread.completed += 1
            else:
                advance = self.advance
                refresh = self.refresh
                for value in sequence:
                    yield value
                    advance(task_id, 1)
                    refresh()
def update(
        self,
        task_id: TaskID,
        *,
        total: float = None,
        completed: float = None,
        advance: float = None,
        description: str = None,
        visible: bool = None,
        refresh: bool = False,
        **fields: Any,
    ) -> None:
        
        
        current_time = self.get_time()
        with self._lock:
            task = self._tasks[task_id]
            completed_start = task.completed

            if total is not None:
                task.total = total
            if advance is not None:
                task.completed += advance
            if completed is not None:
                task.completed = completed
            if description is not None:
                task.description = description
            if visible is not None:
                task.visible = visible
            task.fields.update(fields)

            update_completed = task.completed - completed_start
            old_sample_time = current_time - self.speed_estimate_period
            _progress = task._progress

            while _progress and _progress[0].timestamp < old_sample_time:
                _progress.popleft()
            task._progress.append(ProgressSample(current_time, update_completed))
            if refresh:
                self.refresh()
def read(
    file: Union[str, PathLike[str], BinaryIO],
    description: str = "Reading...",
    total: Optional[int] = None,
    auto_refresh: bool = True,
    console: Optional[Console] = None,
    transient: bool = False,
    get_time: Optional[Callable[[], float]] = None,
    refresh_per_second: float = 10,
    style: StyleType = "bar.back",
    complete_style: StyleType = "bar.complete",
    finished_style: StyleType = "bar.finished",
    pulse_style: StyleType = "bar.pulse",
    update_period: float = 0.1,
    disable: bool = False,
) -> ContextManager[BinaryIO]:
    

    

    columns: List["ProgressColumn"] = (
        [TextColumn("[progress.description]{task.description}")] if description else []
    )
    columns.extend(
        (
            BarColumn(
                style=style,
                complete_style=complete_style,
                finished_style=finished_style,
                pulse_style=pulse_style,
            ),
            DownloadColumn(),
            TimeRemainingColumn(),
        )
    )
    progress = Progress(
        *columns,
        auto_refresh=auto_refresh,
        console=console,
        transient=transient,
        get_time=get_time,
        refresh_per_second=refresh_per_second or 10,
        disable=disable,
    )

    reader = progress.read(file, total=total, description=description)
    return _ReadContext(progress, reader)
def read(
    file: Union[str, "PathLike[str]", BinaryIO],
    description: str = "Reading...",
    total: Optional[int] = None,
    auto_refresh: bool = True,
    console: Optional[Console] = None,
    transient: bool = False,
    get_time: Optional[Callable[[], float]] = None,
    refresh_per_second: float = 10,
    style: StyleType = "bar.back",
    complete_style: StyleType = "bar.complete",
    finished_style: StyleType = "bar.finished",
    pulse_style: StyleType = "bar.pulse",
    update_period: float = 0.1,
    disable: bool = False,
) -> ContextManager[BinaryIO]:
    

    

    columns: List["ProgressColumn"] = (
        [TextColumn("[progress.description]{task.description}")] if description else []
    )
    columns.extend(
        (
            BarColumn(
                style=style,
                complete_style=complete_style,
                finished_style=finished_style,
                pulse_style=pulse_style,
            ),
            DownloadColumn(),
            TimeRemainingColumn(),
        )
    )
    progress = Progress(
        *columns,
        auto_refresh=auto_refresh,
        console=console,
        transient=transient,
        get_time=get_time,
        refresh_per_second=refresh_per_second or 10,
        disable=disable,
    )

    reader = progress.read(file, total=total, description=description)
    return _ReadContext(progress, reader)
def open(
    file: Union[str, "PathLike[str]", bytes],
    mode: Union[Literal["rb"], Literal["rt"], Literal["r"]] = "r",
    buffering: int = -1,
    encoding: Optional[str] = None,
    errors: Optional[str] = None,
    newline: Optional[str] = None,
    total: Optional[int] = None,
    description: str = "Reading...",
    auto_refresh: bool = True,
    console: Optional[Console] = None,
    transient: bool = False,
    get_time: Optional[Callable[[], float]] = None,
    refresh_per_second: float = 10,
    style: StyleType = "bar.back",
    complete_style: StyleType = "bar.complete",
    finished_style: StyleType = "bar.finished",
    pulse_style: StyleType = "bar.pulse",
    disable: bool = False,
) -> Union[ContextManager[BinaryIO], ContextManager[TextIO]]:
    

    

    columns: List["ProgressColumn"] = (
        [TextColumn("[progress.description]{task.description}")] if description else []
    )
    columns.extend(
        (
            BarColumn(
                style=style,
                complete_style=complete_style,
                finished_style=finished_style,
                pulse_style=pulse_style,
            ),
            DownloadColumn(),
            TimeRemainingColumn(),
        )
    )
    progress = Progress(
        *columns,
        auto_refresh=auto_refresh,
        console=console,
        transient=transient,
        get_time=get_time,
        refresh_per_second=refresh_per_second or 10,
        disable=disable,
    )

    reader = progress.open(
        file,
        mode=mode,
        buffering=buffering,
        encoding=encoding,
        errors=errors,
        newline=newline,
        total=total,
        description=description,
    )
    return _ReadContext(progress, reader)
def render_speed(cls, speed: float | None) -> Text:
        
        
        if speed is None:
            return Text("", style="progress.percentage")
        unit, suffix = filesize.pick_unit_and_suffix(
            int(speed),
            ["", "×10³", "×10⁶", "×10⁹", "×10¹²"],
            1000,
        )
        data_speed = speed / unit
        return Text(f"{data_speed:.1f}{suffix} it/s", style="progress.percentage")
def main(investment: float = 3.0, n_round: int = 100):
    
    
    
    asyncio.run(start_game(investment, n_round))
def snapshot(self, filename="tmp.png", quality=None):
        
        

        
        w, h = self.get_current_resolution()
        dsp = display.Display()
        root = dsp.screen().root
        raw = root.get_image(0, 0, w, h, X.ZPixmap, 0xffffffff)
        image = Image.frombytes("RGB", (w, h), raw.data, "raw", "BGRX")
        from airtest.aircv.utils import pil_2_cv2
        image = pil_2_cv2(image)
        return image
def getCurrentThreadData():
    
    
    

    global ThreadData

    return ThreadData
def books(self, oncard=None, end_session=True):
         
         
            
        raise NotImplementedError()
def add_books_to_metadata(cls, locations, metadata, booklists):
        '''
        

        '''
        raise NotImplementedError
def is_usb_connected(cls, devices_on_system, debug=False):
        '''
        
        '''
        if iswindows:
            return cls.is_usb_connected_windows(devices_on_system, debug=debug), None

        vendors_on_system = set([x[0] for x in devices_on_system])
        vendors = cls.VENDOR_ID if hasattr(cls.VENDOR_ID, '__len__') else [cls.VENDOR_ID]
        if hasattr(cls.VENDOR_ID, 'keys'):
            products = []
            for ven in cls.VENDOR_ID:
                products.extend(cls.VENDOR_ID[ven].keys())
        else:
            products = cls.PRODUCT_ID if hasattr(cls.PRODUCT_ID, '__len__') else [cls.PRODUCT_ID]

        for vid in vendors:
            if vid in vendors_on_system:
                for dev in devices_on_system:
                    cvid, pid, bcd = dev[:3]
                    if cvid == vid:
                        if pid in products:
                            if hasattr(cls.VENDOR_ID, 'keys'):
                                cbcd = cls.VENDOR_ID[vid][pid]
                            else:
                                cbcd = cls.BCD
                            if cls.test_bcd(bcd, cbcd):
                                if debug:
                                    cls.print_usb_device_info(dev)
                                if cls.can_handle(dev, debug=debug):
                                    return True, dev
        return False, None
def __init__(self, maxsize=MAX_JOBS_QUEUE, ns_conf=None):
        

        
        self.maxsize = maxsize
        self.callback = None
        self.ns_conf = ns_conf if ns_conf is not None else {}
def __init__(self,
                 http_method=None,
                 body=None,
                 resource=None,
                 request_context=None,
                 query_string_params=None,
                 multi_value_query_string_params=None,
                 headers=None,
                 multi_value_headers=None,
                 path_parameters=None,
                 stage_variables=None,
                 path=None,
                 is_base_64_encoded=False):
        
        
        

        if not isinstance(query_string_params, dict) and \
                query_string_params is not None:
            raise TypeError("'query_string_params' must be of type dict or None")

        if not isinstance(multi_value_query_string_params, dict) and \
                multi_value_query_string_params is not None:
            raise TypeError("'multi_value_query_string_params' must be of type dict or None")

        if not isinstance(headers, dict) and headers is not None:
            raise TypeError("'headers' must be of type dict or None")

        if not isinstance(multi_value_headers, dict) and multi_value_headers is not None:
            raise TypeError("'multi_value_headers' must be of type dict or None")

        if not isinstance(path_parameters, dict) and path_parameters is not None:
            raise TypeError("'path_parameters' must be of type dict or None")

        if not isinstance(stage_variables, dict) and stage_variables is not None:
            raise TypeError("'stage_variables' must be of type dict or None")

        self.http_method = http_method
        self.body = body
        self.resource = resource
        self.request_context = request_context
        self.query_string_params = query_string_params
        self.multi_value_query_string_params = multi_value_query_string_params
        self.headers = headers
        self.multi_value_headers = multi_value_headers
        self.path_parameters = path_parameters
        self.stage_variables = stage_variables
        self.path = path
        self.is_base_64_encoded = is_base_64_encoded
def to_dict(self) -> Dict[str, Any]:
        
        
        
        request_context_dict = {}
        if self.request_context:
            request_context_dict = self.request_context.to_dict()

        json_dict = {
            "version": self.version,
            "httpMethod": self.http_method,
            "body": self.body if self.body else None,
            "resource": self.resource,
            "requestContext": request_context_dict,
            "queryStringParameters": dict(self.query_string_params) if self.query_string_params else None,
            "multiValueQueryStringParameters": dict(self.multi_value_query_string_params)
            if self.multi_value_query_string_params
            else None,
            "headers": dict(self.headers) if self.headers else None,
            "multiValueHeaders": dict(self.multi_value_headers) if self.multi_value_headers else None,
            "pathParameters": dict(self.path_parameters) if self.path_parameters else None,
            "stageVariables": dict(self.stage_variables) if self.stage_variables else None,
            "path": self.path,
            "isBase64Encoded": self.is_base_64_encoded,
        }

        return json_dict
def __sum_stats(self, key, sub_key=None, mmm=None):
        
        
        # Compute stats summary
        ret = 0
        for p in self.stats:
            if key not in p:
                # Correct issue #1188
                continue
            if p[key] is None:
                # Correct https://github.com/nicolargo/glances/issues/1105#issuecomment-363553788
                continue
            if sub_key is None:
                ret += p[key]
            else:
                ret += p[key][sub_key]

        # Manage Min/Max/Mean
        mmm_key = self.__mmm_key(key, sub_key)
        if mmm == 'min':
            try:
                if self.mmm_min[mmm_key] > ret:
                    self.mmm_min[mmm_key] = ret
            except AttributeError:
                self.mmm_min = {}
                return 0
            except KeyError:
                self.mmm_min[mmm_key] = ret
            ret = self.mmm_min[mmm_key]
        elif mmm == 'max':
            try:
                if self.mmm_max[mmm_key] < ret:
                    self.mmm_max[mmm_key] = ret
            except AttributeError:
                self.mmm_max = {}
                return 0
            except KeyError:
                self.mmm_max[mmm_key] = ret
            ret = self.mmm_max[mmm_key]

        return ret
def stop(self):
        
        
        
        if not self._running:
            raise RuntimeError("screenrecord is not started")
        self._stop_event.set()
        ret = self._done_event.wait(10.0)

        # reset
        self._stop_event.clear()
        self._done_event.clear()
        self._running = False
        return ret
def status_tag(pod: Dict[str, Any]) -> NodeStatus:
    
    
    if (
        "containerStatuses" not in pod["status"]
        or not pod["status"]["containerStatuses"]
    ):
        return "pending"

    state = pod["status"]["containerStatuses"][0]["state"]

    if "pending" in state:
        return "pending"
    if "running" in state:
        return STATUS_UP_TO_DATE
    if "waiting" in state:
        return "waiting"
    if "terminated" in state:
        return STATUS_UPDATE_FAILED
    raise ValueError("Unexpected container state.")
def validate(self):
        
        self._config['formats'] = self.validate_formats()
        self._config['conda'] = self.validate_conda()
        # This should be called before validate_python
        self._config['build'] = self.validate_build()
        self._config['python'] = self.validate_python()
        # Call this before validate sphinx and mkdocs
        self.validate_doc_types()
        self._config['mkdocs'] = self.validate_mkdocs()
        self._config['sphinx'] = self.validate_sphinx()
        self._config['submodules'] = self.validate_submodules()
        self._config['search'] = self.validate_search()
        self.validate_keys()
def validate_keys(self):
        
        
        
        msg = (
            'Unsupported configuration option: {}. '
            'Make sure the key name is correct.'
        )
        self.pop_config('version', None)
        wrong_key = '.'.join(self._get_extra_key(self.raw_config))
        if wrong_key:
            self.error(
                wrong_key,
                msg.format(wrong_key),
                code=INVALID_KEY,
            )
def validate_keys(self):
        
        
        
        msg = (
            'Invalid configuration option: {}. '
            'Make sure the key name is correct.'
        )
        # The version key isn't popped, but it's
        # validated in `load`.
        self.pop_config('version', None)
        wrong_key = '.'.join(self._get_extra_key(self.raw_config))
        if wrong_key:
            self.error(
                wrong_key,
                msg.format(wrong_key),
                code=INVALID_KEY,
            )
def validate_python_install(self, index):
        
        python_install = {}
        key = 'python.install.{}'.format(index)
        raw_install = self.raw_config['python']['install'][str(index)]
        with self.catch_validation_error(key):
            validate_dict(raw_install)

        if 'requirements' in raw_install:
            requirements_key = key + '.requirements'
            with self.catch_validation_error(requirements_key):
                requirements = validate_file(
                    self.pop_config(requirements_key),
                    self.base_path
                )
                python_install['requirements'] = requirements
        elif 'path' in raw_install:
            path_key = key + '.path'
            with self.catch_validation_error(path_key):
                path = validate_directory(
                    self.pop_config(path_key),
                    self.base_path
                )
                python_install['path'] = path

            method_key = key + '.method'
            with self.catch_validation_error(method_key):
                method = validate_choice(
                    self.pop_config(method_key, PIP),
                    self.valid_install_method
                )
                python_install['method'] = method

            extrareq_key = key + '.extra_requirements'
            with self.catch_validation_error(extrareq_key):
                extra_requirements = validate_list(
                    self.pop_config(extrareq_key, [])
                )
                if extra_requirements and python_install['method'] != PIP:
                    self.error(
                        extrareq_key,
                        'You need to install your project with pip '
                        'to use extra_requirements',
                        code=PYTHON_INVALID,
                    )
                python_install['extra_requirements'] = extra_requirements
        else:
            self.error(
                key,
                '"path" or "requirements" key is required',
                code=CONFIG_REQUIRED,
            )
        return python_install
def validate_python_install(self, index):
        
        python_install = {}
        key = 'python.install.{}'.format(index)
        raw_install = self.raw_config['python']['install'][str(index)]
        with self.catch_validation_error(key):
            validate_dict(raw_install)

        if 'requirements' in raw_install:
            requirements_key = key + '.requirements'
            with self.catch_validation_error(requirements_key):
                requirements = validate_file(
                    self.pop_config(requirements_key),
                    self.base_path
                )
                python_install['requirements'] = requirements
        elif 'path' in raw_install:
            path_key = key + '.path'
            with self.catch_validation_error(path_key):
                path = validate_directory(
                    self.pop_config(path_key),
                    self.base_path
                )
                python_install['path'] = path

            method_key = key + '.method'
            with self.catch_validation_error(method_key):
                method = validate_choice(
                    self.pop_config(method_key, PIP),
                    self.valid_install_method
                )
                python_install['method'] = method

            extra_req_key = key + '.extra_requirements'
            with self.catch_validation_error(extra_req_key):
                extra_requirements = validate_list(
                    self.pop_config(extra_req_key, [])
                )
                if extra_requirements and python_install['method'] != PIP:
                    self.error(
                        extra_req_key,
                        'You need to install your project with pip '
                        'to use extra_requirements',
                        code=PYTHON_INVALID,
                    )
                python_install['extra_requirements'] = extra_requirements
        else:
            self.error(
                key,
                '"path" or "requirements" key is required',
                code=CONFIG_REQUIRED,
            )
        return python_install
def valid_build_images(self):
        
        
        
        images = {'stable', 'latest', 'testing'}
        for k in settings.DOCKER_IMAGE_SETTINGS:
            _, version = k.split(':')
            if re.fullmatch(r'^[\d\.]+$', version):
                images.add(version)
        return images
def catch_validation_error(self, key):
        
        # NOTE: I don't like too much this pattern of re-raising an exception via a context manager.
        # I think we should raise the exception where it happens, instead of encapsulating all of them.
        # The only small limitation that I found is the requirement of passing ``key`` down to where
        # the exception happens.
        # I'm keeping this pattern for now until we decide to refactor it.
        try:
            yield
        except ConfigValidationError as error:
            raise ConfigError(
                message_id=error.message_id,
                format_values={
                    "key": key,
                    "value": error.format_values.get("value"),
                    "source_file": os.path.relpath(self.source_file, self.base_path),
                },
            ) from error
def complex_filter(self, filter_obj):
        
        
        
        if isinstance(filter_obj, Q):
            clone = self._clone()
            clone.query.add_q(filter_obj)
            return clone
        else:
            return self._filter_or_exclude(None, **filter_obj)
def select_related(self, *fields, **kwargs):
        
        
        
        if 'depth' in kwargs:
            warnings.warn('The "depth" keyword argument has been deprecated.\n'
                    'Use related field names instead.', DeprecationWarning, stacklevel=2)
        depth = kwargs.pop('depth', 0)
        if kwargs:
            raise TypeError('Unexpected keyword arguments to select_related: %s'
                    % (list(kwargs),))
        obj = self._clone()
        if fields == (None,):
            obj.query.select_related = False
        elif fields:
            if depth:
                raise TypeError('Cannot pass both "depth" and fields to select_related()')
            obj.query.add_select_related(fields)
        else:
            obj.query.select_related = True
        if depth:
            obj.query.max_depth = depth
        return obj
def _next_is_sticky(self):
        
        
        
        self._sticky_filter = True
        return self
def get_cached_row(row, index_start, using,  klass_info, offset=0,
                   parent_data=()):
    
    
    
    if klass_info is None:
        return None
    klass, field_names, field_count, related_fields, reverse_related_fields, pk_idx = klass_info


    fields = row[index_start : index_start + field_count]
    # If the pk column is None (or the Oracle equivalent ''), then the related
    # object must be non-existent - set the relation to None.
    if fields[pk_idx] == None or fields[pk_idx] == '':
        obj = None
    elif field_names:
        obj = klass(**dict(zip(field_names, fields)))
    else:
        obj = klass(*fields)
    # If an object was retrieved, set the database state.
    if obj:
        obj._state.db = using
        obj._state.adding = False

    # Instantiate related fields
    index_end = index_start + field_count + offset
    # Iterate over each related object, populating any
    # select_related() fields
    for f, klass_info in related_fields:
        # Recursively retrieve the data for the related object
        cached_row = get_cached_row(row, index_end, using, klass_info)
        # If the recursive descent found an object, populate the
        # descriptor caches relevant to the object
        if cached_row:
            rel_obj, index_end = cached_row
            if obj is not None:
                # If the base object exists, populate the
                # descriptor cache
                setattr(obj, f.get_cache_name(), rel_obj)
            if f.unique and rel_obj is not None:
                # If the field is unique, populate the
                # reverse descriptor cache on the related object
                setattr(rel_obj, f.related.get_cache_name(), obj)

    # Now do the same, but for reverse related objects.
    # Only handle the restricted case - i.e., don't do a depth
    # descent into reverse relations unless explicitly requested
    for f, klass_info in reverse_related_fields:
        # Transfer data from this object to childs.
        parent_data = []
        for rel_field, rel_model in klass_info[0]._meta.get_fields_with_model():
            if rel_model is not None and isinstance(obj, rel_model):
                parent_data.append((rel_field, getattr(obj, rel_field.attname)))
        # Recursively retrieve the data for the related object
        cached_row = get_cached_row(row, index_end, using, klass_info,
                                   parent_data=parent_data)
        # If the recursive descent found an object, populate the
        # descriptor caches relevant to the object
        if cached_row:
            rel_obj, index_end = cached_row
            if obj is not None:
                # populate the reverse descriptor cache
                setattr(obj, f.related.get_cache_name(), rel_obj)
            if rel_obj is not None:
                # If the related object exists, populate
                # the descriptor cache.
                setattr(rel_obj, f.get_cache_name(), obj)
                # Populate related object caches using parent data.
                for rel_field, _ in parent_data:
                    if rel_field.rel:
                        setattr(rel_obj, rel_field.attname, getattr(obj, rel_field.attname))
                        try:
                            cached_obj = getattr(obj, rel_field.get_cache_name())
                            setattr(rel_obj, rel_field.get_cache_name(), cached_obj)
                        except AttributeError:
                            # Related object hasn't been cached yet
                            pass
    return obj, index_end
def contains(self, obj):
        
        
        
        self._not_support_combined_queries("contains")
        if self._fields is not None:
            raise TypeError(
                "Cannot call QuerySet.contains() after .values() or .values_list()."
            )
        try:
            if obj._meta.concrete_model != self.model._meta.concrete_model:
                return False
        except AttributeError:
            raise TypeError("'obj' must be a model instance.")
        if obj.pk is None:
            raise ValueError("QuerySet.contains() cannot be used on unsaved objects.")
        if self._result_cache is not None:
            return obj in self._result_cache
        return self.filter(pk=obj.pk).exists()
def dates(self, field_name, kind, order='ASC'):
        
        
        
        assert kind in ("year", "month", "day"), \
                "'kind' must be one of 'year', 'month' or 'day'."
        assert order in ('ASC', 'DESC'), \
                "'order' must be either 'ASC' or 'DESC'."
        return self._clone(klass=DateQuerySet, setup=True,
                _field_name=field_name, _kind=kind, _order=order)
def bulk_create(self, objs, batch_size=None):
        
        
        
        # When you bulk insert you don't get the primary keys back (if it's an
        # autoincrement, except if can_return_ids_from_bulk_insert=True), so
        # you can't insert into the child tables which references this. There
        # are two workarounds:
        # 1) This could be implemented if you didn't have an autoincrement pk
        # 2) You could do it by doing O(n) normal inserts into the parent
        #    tables to get the primary keys back and then doing a single bulk
        #    insert into the childmost table.
        # We currently set the primary keys on the objects when using
        # PostgreSQL via the RETURNING ID clause. It should be possible for
        # Oracle as well, but the semantics for  extracting the primary keys is
        # trickier so it's not done yet.
        assert batch_size is None or batch_size > 0
        # Check that the parents share the same concrete model with the our
        # model to detect the inheritance pattern ConcreteGrandParent ->
        # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
        # would not identify that case as involving multiple tables.
        for parent in self.model._meta.get_parent_list():
            if parent._meta.concrete_model is not self.model._meta.concrete_model:
                raise ValueError("Can't bulk create a multi-table inherited model")
        if not objs:
            return objs
        self._for_write = True
        connection = connections[self.db]
        fields = self.model._meta.concrete_fields
        objs = list(objs)
        self._populate_pk_values(objs)
        with transaction.atomic(using=self.db, savepoint=False):
            if (connection.features.can_combine_inserts_with_and_without_auto_increment_pk
                    and self.model._meta.has_auto_field):
                self._batched_insert(objs, fields, batch_size)
            else:
                objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
                if objs_with_pk:
                    self._batched_insert(objs_with_pk, fields, batch_size)
                if objs_without_pk:
                    fields = [f for f in fields if not isinstance(f, AutoField)]
                    ids = self._batched_insert(objs_without_pk, fields, batch_size)
                    if connection.features.can_return_ids_from_bulk_insert:
                        assert len(ids) == len(objs_without_pk)
                    for i in range(len(ids)):
                        objs_without_pk[i].pk = ids[i]

        return objs
def in_bulk(self, id_list=None):
        
        
        
        assert self.query.can_filter(), \
            "Cannot use 'limit' or 'offset' with in_bulk"
        if id_list is not None:
            if not id_list:
                return {}
            qs = self.filter(pk__in=id_list).order_by()
        else:
            qs = self._clone()
        return {obj._get_pk_val(): obj for obj in qs}
def _batched_insert(self, objs, fields, batch_size):
        
        
        
        ops = connections[self.db].ops
        batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))
        inserted_ids = []
        for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
            if connections[self.db].features.can_return_ids_from_bulk_insert:
                inserted_id = self._insert(item, fields=fields, using=self.db, return_id=True)
                if isinstance(inserted_id, list):
                    inserted_ids.extend(inserted_id)
                else:
                    inserted_ids.append(inserted_id)
            else:
                self._insert(item, fields=fields, using=self.db)
        return inserted_ids
def get_error_message(subscriber, num=1e6, error_type=None, timeout=20):
    
    
    deadline = time.time() + timeout
    msgs = []
    while time.time() < deadline and len(msgs) < num:
        _, error_data = subscriber.poll(timeout=deadline - time.time())
        if not error_data:
            # Timed out before any data is received.
            break
        if error_type is None or error_type == error_data.type:
            msgs.append(error_data)
        else:
            time.sleep(0.01)

    return msgs
def run(self, blocking=True):
        

        
        # localize variable access to minimize overhead
        # and to improve thread safety
        with self._lock:
            q = self._queue
            delayfunc = self.delayfunc
            timefunc = self.timefunc
            pop = heapq.heappop
            while q:
                time, priority, action, argument, kwargs = checked_event = q[0]
                now = timefunc()
                if now < time:
                    if not blocking:
                        return time - now
                    delayfunc(time - now)
                else:
                    event = pop(q)
                    # Verify that the event was not removed or altered
                    # by another thread after we last looked at q[0].
                    if event is checked_event:
                        action(*argument, **kwargs)
                        delayfunc(0)   # Let other threads run
                    else:
                        heapq.heappush(q, event)
def matches(self, user):
        

        # TODO: remove this method and refactor the API response in ``/api/v2/repos/``
        # (or v3) to just return the linked Project (slug, url) if the ``RemoteRepository``
        # connection exists. Note the frontend needs to be simplified as well in
        # ``import.js`` and ``project_import.html``.

        projects = Project.objects.public(user).filter(
            remote_repository=self,
        ).values('slug')

        return [{
            'id': project['slug'],
            'url': reverse(
                'projects_detail',
                kwargs={
                    'project_slug': project['slug'],
                },
            ),
        } for project in projects]
def do_export(self, resource_id, resource_dict, parent_dir):
        
        
        
        # code signer only accepts files which has '.zip' extension in it
        # so package artifact with '.zip' if it is required to be signed
        should_sign_package = self.code_signer.should_sign_package(resource_id)
        artifact_extension = "zip" if should_sign_package else None
        uploaded_url = upload_local_artifacts(
            resource_id, resource_dict, self.PROPERTY_NAME, parent_dir, self.uploader, artifact_extension
        )
        if should_sign_package:
            uploaded_url = self.code_signer.sign_package(
                resource_id, uploaded_url, self.uploader.get_version_of_artifact(uploaded_url)
            )
        set_value_from_jmespath(resource_dict, self.PROPERTY_NAME, uploaded_url)
def decoder(self, side):
         
        
        input_ = Input(shape=(8, 8, 512))
        var_x = input_
        var_x = UpscaleBlock(256)(var_x)
        var_x = UpscaleBlock(128)(var_x)
        var_x = UpscaleBlock(64)(var_x)
        var_x = Conv2DOutput(3, 5, name="face_out_{}".format(side))(var_x)
        outputs = [var_x]

        if self.learn_mask:
            var_y = input_
            var_y = UpscaleBlock(256)(var_y)
            var_y = UpscaleBlock(128)(var_y)
            var_y = UpscaleBlock(64)(var_y)
            var_y = Conv2DOutput(1, 5, name="mask_out_{}".format(side))(var_y)
            outputs.append(var_y)
        return KerasModel(input_, outputs=outputs, name="decoder_{}".format(side))
def build_model(self, inputs):
         
        
        input_a = inputs[0]
        input_b = inputs[1]

        encoder = self.encoder()
        encoder_a = [encoder(input_a)]
        encoder_b = [encoder(input_b)]

        outputs = [self.decoder("a")(encoder_a), self.decoder("b")(encoder_b)]

        autoencoder = KModel(inputs, outputs, name=self.model_name)
        return autoencoder
def _find_valid_signature_through_ports(context: RequestContext) -> FindSigV4Result:
    
    
    
    request_sig = context.request.args["X-Amz-Signature"]

    sigv4_context = S3SigV4SignatureContext(context=context)
    # get the port of the request
    match = re.match(HOST_COMBINATION_REGEX, sigv4_context.host)
    request_port = match.group(2) if match else None

    # get the signature from the request
    signature, canonical_request, string_to_sign = sigv4_context.get_signature_data()
    if signature == request_sig:
        return signature, None

    # if the signature does not match, save the data for the exception
    exception_context = NotValidSigV4SignatureContext(
        signature_provided=request_sig,
        string_to_sign=string_to_sign,
        canonical_request=canonical_request,
    )

    # we try to iterate through possible ports, to match the signature
    for port in PORT_REPLACEMENT:
        if request_port:
            # the request has already been tested before the loop, skip
            if request_port == port:
                continue
            sigv4_context.update_host_port(port, request_port)

        else:
            sigv4_context.update_host_port(port)

        # we ignore the additional data because we want the exception raised to match the original request
        signature, _, _ = sigv4_context.get_signature_data()
        if signature == request_sig:
            return signature, None

    # Return the exception data from the original request after trying to loop through ports
    return None, exception_context
def _create_new_request(request: Request, headers: Dict[str, str], query_string: str) -> Request:
    
    
    
    return Request(
        method=request.method,
        headers=headers,
        path=request.path,
        query_string=query_string,
        scheme=request.scheme,
        root_path=request.root_path,
        server=request.server,
        remote_addr=request.remote_addr,
    )
def _reverse_inject_signature_hmac_v1_query(
    request: Request,
) -> tuple[urlparse.SplitResult, HTTPHeaders]:
    
    
    
    new_headers = {}
    new_query_string_dict = {}

    for header, value in request.args.items():
        header_low = header.lower()
        if header_low not in HmacV1QueryAuthValidation.post_signature_headers:
            new_headers[header] = value
        elif header_low in HmacV1QueryAuthValidation.QSAOfInterest_low:
            new_query_string_dict[header] = value

    # there should not be any headers here. If there are, it means they have been added by the client
    # We should verify them, they will fail the signature except if they were part of the original request
    for header, value in request.headers.items():
        header_low = header.lower()
        if header_low.startswith("x-amz-") or header_low in ["content-type", "date", "content-md5"]:
            new_headers[header_low] = value

    # rebuild the query string
    new_query_string = percent_encode_sequence(new_query_string_dict)

    # we need to URL encode the path, as the key needs to be urlencoded for the signature to match
    encoded_path = urlparse.quote(request.path)

    reversed_url = f"{request.scheme}://{request.host}{encoded_path}?{new_query_string}"

    reversed_headers = HTTPHeaders()
    for key, value in new_headers.items():
        reversed_headers[key] = value

    return urlsplit(reversed_url), reversed_headers
def validate_presigned_url_s3v4(context: RequestContext) -> None:
    
    
    
    sigv4_context, exception = _find_valid_signature_through_ports(context)
    if exception:
        if config.S3_SKIP_SIGNATURE_VALIDATION:
            LOG.warning(
                "Signatures do not match, but not raising an error, as S3_SKIP_SIGNATURE_VALIDATION=1"
            )
        else:
            ex: SignatureDoesNotMatch = create_signature_does_not_match_sig_v4(exception)
            raise ex

    # Checking whether the url is expired or not
    query_parameters = context.request.args
    # TODO: should maybe try/except here -> create auth params validation before checking signature, above!!
    x_amz_date = datetime.datetime.strptime(query_parameters["X-Amz-Date"], "%Y%m%dT%H%M%SZ")
    x_amz_expires = int(query_parameters["X-Amz-Expires"])
    x_amz_expires_dt = datetime.timedelta(seconds=x_amz_expires)
    expiration_time = x_amz_date + x_amz_expires_dt
    expiration_time = expiration_time.replace(tzinfo=datetime.timezone.utc)

    if is_expired(expiration_time):
        if config.S3_SKIP_SIGNATURE_VALIDATION:
            LOG.warning(
                "Signature is expired, but not raising an error, as S3_SKIP_SIGNATURE_VALIDATION=1"
            )
        else:
            raise AccessDenied(
                "Request has expired",
                HostId=FAKE_HOST_ID,
                Expires=expiration_time.timestamp(),
                ServerTime=time.time(),
                X_Amz_Expires=x_amz_expires,
            )

    add_headers_to_original_request(context, sigv4_context.signed_headers)
def _get_signed_headers_and_filtered_query_string(
        self,
    ) -> tuple[dict[str, str], str, dict[str, str]]:
        
        
        
        headers = copy.copy(self._headers)
        # set automatically by the handler chain, we don't want that
        headers.pop("Authorization", None)
        signed_headers = self._query_parameters.get("X-Amz-SignedHeaders")

        new_query_args = {}
        query_args_to_headers = {}
        for qs_parameter, qs_value in self._query_parameters.items():
            # skip the signature
            if qs_parameter == "X-Amz-Signature":
                continue

            qs_param_low = qs_parameter.lower()
            if (
                qs_parameter not in SIGNATURE_V4_PARAMS
                and qs_param_low.startswith("x-amz-")
                and qs_param_low not in headers
            ):
                if qs_param_low in signed_headers:
                    # AWS JS SDK does not behave like boto, and will add some parameters as query string when signing
                    # when boto would not. this difference in behaviour would lead to pre-signed URLs generated by the
                    # JS SDK to be invalid for the boto signer.
                    # This fixes the behaviour by manually adding the parameter to the headers like boto would, if the
                    # SDK put them in the SignedHeaders
                    # this is especially valid for headers starting with x-amz-server-side-encryption, treated
                    # specially in the old JS SDK v2
                    headers.add(qs_param_low, qs_value)
                else:
                    query_args_to_headers[qs_param_low] = qs_value

            new_query_args[qs_parameter] = qs_value

        signature_headers = {}
        for header, value in headers.items():
            header_low = header.lower()
            if header_low.startswith("x-amz-") and header_low not in signed_headers.lower():
                if header_low in IGNORED_SIGV4_HEADERS:
                    continue
                self.missing_signed_headers.append(header_low)
            if header_low in signed_headers:
                signature_headers[header_low] = value

        new_query_string = percent_encode_sequence(new_query_args)
        return signature_headers, new_query_string, query_args_to_headers
def _get_next_execution(self):
        
        
        
        query = {
            'scheduled_start_timestamp__lte': date.get_datetime_utc_now(),
            'handling': False,
            'limit': 1,
            'order_by': [
                '+scheduled_start_timestamp',
            ]
        }

        execution_queue_item_db = ActionExecutionSchedulingQueue.query(**query).first()

        if not execution_queue_item_db:
            return None

        # Mark that this scheduler process is currently handling (processing) that request
        # NOTE: This operation is atomic (CAS)
        execution_queue_item_db.handling = True

        try:
            ActionExecutionSchedulingQueue.add_or_update(execution_queue_item_db, publish=False)
            return execution_queue_item_db
        except db_exc.StackStormDBObjectWriteConflictError:
            LOG.info('Execution queue item handled by another scheduler: %s',
                     execution_queue_item_db.id)

        return None
def detected_faces(self):
         
        
        logger.debug("Running Detection. Phase: '%s'", self.phase)
        # If not multiprocessing, intercept the align in queue for
        # detection phase
        out_queue = self._output_queue
        while True:
            try:
                if self._check_and_raise_error():
                    break
                faces = out_queue.get(True, 1)
                if faces == "EOF":
                    break
            except QueueEmpty:
                continue

            yield faces
        self._join_threads()
        if self.final_pass:
            # Cleanup queues
            for q_name in self._queues.keys():
                queue_manager.del_queue(q_name)
            logger.debug("Detection Complete")
        else:
            logger.debug("Switching to align phase")
            self.phase = "align"
def _load_align(self,
                    aligner: Optional[str],
                    configfile: Optional[str],
                    normalize_method: Optional[Literal["none", "clahe", "hist", "mean"]],
                    re_feed: int) -> Optional["Aligner"]:
         
        
        if aligner is None or aligner.lower() == "none":
            logger.debug("No aligner selected. Returning None")
            return None
        aligner_name = aligner.replace("-", "_").lower()
        logger.debug("Loading Aligner: '%s'", aligner_name)
        plugin = PluginLoader.get_aligner(aligner_name)(exclude_gpus=self._exclude_gpus,
                                                        configfile=configfile,
                                                        normalize_method=normalize_method,
                                                        re_feed=re_feed,
                                                        instance=self._instance)
        return plugin
def _set_flow(detector: Optional[str],
                  aligner: Optional[str],
                  masker: List[Optional[str]],
                  recognition: Optional[str]) -> List[str]:
         
        
        logger.debug("detector: %s, aligner: %s, masker: %s recognition: %s",
                     detector, aligner, masker, recognition)
        retval = []
        if detector is not None and detector.lower() != "none":
            retval.append("detect")
        if aligner is not None and aligner.lower() != "none":
            retval.append("align")
        if recognition is not None and recognition.lower() != "none":
            retval.append("recognition")
        retval.extend([f"mask_{idx}"
                       for idx, mask in enumerate(masker)
                       if mask is not None and mask.lower() != "none"])
        logger.debug("flow: %s", retval)
        return retval
def _load_mask(self,
                   masker: Optional[str],
                   configfile: Optional[str]) -> Optional["Masker"]:
         
        
        if masker is None or masker.lower() == "none":
            logger.debug("No masker selected. Returning None")
            return None
        masker_name = masker.replace("-", "_").lower()
        logger.debug("Loading Masker: '%s'", masker_name)
        plugin = PluginLoader.get_masker(masker_name)(exclude_gpus=self._exclude_gpus,
                                                      configfile=configfile,
                                                      instance=self._instance)
        return plugin
def _set_extractor_batchsize(self):
        
        
        
        if get_backend() != "nvidia":
            logger.debug("Backend is not Nvidia. Not updating batchsize requirements")
            return
        if sum([plugin.vram for plugin in self._active_plugins]) == 0:
            logger.debug("No plugins use VRAM. Not updating batchsize requirements.")
            return

        batch_required = sum([plugin.vram_per_batch * plugin.batchsize
                              for plugin in self._active_plugins])
        gpu_plugins = [p for p in self._current_phase if self._vram_per_phase[p] > 0]
        plugins_required = sum([self._vram_per_phase[p]
                                for p in gpu_plugins]) * self._parallel_scaling[len(gpu_plugins)]
        if plugins_required + batch_required <= self._vram_stats["vram_free"]:
            logger.debug("Plugin requirements within threshold: (plugins_required: %sMB, "
                         "vram_free: %sMB)", plugins_required, self._vram_stats["vram_free"])
            return
        # Hacky split across plugins that use vram
        available_vram = (self._vram_stats["vram_free"] - plugins_required) // len(gpu_plugins)
        self._set_plugin_batchsize(gpu_plugins, available_vram)
def set_batchsize(self,
                      plugin_type: Literal["align", "detect"],
                      batchsize: int) -> None:
         
        
        logger.debug("Overriding batchsize for plugin_type: %s to: %s", plugin_type, batchsize)
        plugin = getattr(self, f"_{plugin_type}")
        plugin.batchsize = batchsize
def _load_detect(self,
                     detector: str | None,
                     aligner: str | None,
                     rotation: str | None,
                     min_size: int,
                     configfile: str | None) -> Detector | None:
         
        
        if detector is None or detector.lower() == "none":
            logger.debug("No detector selected. Returning None")
            return None
        detector_name = detector.replace("-", "_").lower()

        if aligner == "external" and detector_name != "external":
            logger.warning("Unsupported '%s' detector selected for 'External' aligner. Switching "
                           "detector to 'External'", detector_name)
            detector_name = aligner

        logger.debug("Loading Detector: '%s'", detector_name)
        plugin = PluginLoader.get_detector(detector_name)(exclude_gpus=self._exclude_gpus,
                                                          rotation=rotation,
                                                          min_size=min_size,
                                                          configfile=configfile,
                                                          instance=self._instance)
        return plugin
def input_queue(self):
         
        
        qname = "extract_{}_in".format(self.phase)
        retval = self._queues[qname]
        logger.trace("%s: %s", qname, retval)
        return retval
def detected_faces(self) -> List["DetectedFace"]:
         
        return self._detected_faces
def _set_plugin_batchsize(plugin, available_vram):
         
        
        if plugin.vram_per_batch != 0:
            plugin.batchsize = int(max(1, available_vram // plugin.vram_per_batch))
            logger.verbose("Reset batchsize for %s to %s", plugin.name, plugin.batchsize)
def get_image_copy(self, color_format):
         
        
        logger.trace("Requested color format '%s' for frame '%s'", color_format, self._filename)
        image = getattr(self, "_image_as_{}".format(color_format.lower()))()
        return image
def test_fields():
    
    fields = Metadata.model_fields.keys()
    assert "arguments" in fields
    assert "duration" in fields
    assert "route" in fields
    assert "timestamp" in fields
def plot_complex_polar(z: NumberLike,
                       x_label: Union[str, None] = None, y_label: Union[str, None] = None,
                       title: Union[str, None] = None, show_legend: bool = True,
                       axis_equal: bool = False, ax: Union[plt.Axes, None] = None,
                       *args, **kwargs):
    
    
    
    theta = npy.angle(z)
    r = npy.abs(z)
    plot_polar(theta=theta, r=r, x_label=x_label, y_label=y_label,
        title=title, show_legend=show_legend, axis_equal=axis_equal,
        ax=ax, *args, **kwargs)
def add_markers_to_lines(ax: Union[plt.Axes, None] = None,
                         marker_list: List = ['o', 'D', 's', '+', 'x'],
                         markevery: int = 10):
    
    

    
    if ax is None:
        ax=plt.gca()
    lines = ax.get_lines()
    if len(lines) > len (marker_list ):
        marker_list *= 3
    [k[0].set_marker(k[1]) for k in zip(lines, marker_list)]
    [line.set_markevery(markevery) for line in lines]
def plot_smith(s: NumberLike, smith_r: float = 1, chart_type: str = 'z',
               x_label: str = 'Real', y_label: str = 'Imaginary', title: str = 'Complex Plane',
               show_legend: bool = True, axis: str = 'equal', ax: Union[plt.Axes, None] = None,
               force_chart: bool = False, draw_vswr: Union[List, bool, None] = None, draw_labels: bool = False,
               *args, **kwargs):
    
    

    if ax is None:
        ax = plt.gca()

    # test if smith chart is already drawn
    if not force_chart:
        if len(ax.patches) == 0:
            smith(ax=ax, smithR = smith_r, chart_type=chart_type, draw_vswr=draw_vswr, draw_labels=draw_labels)

    plot_complex_rectangular(s, x_label=x_label, y_label=y_label,
        title=title, show_legend=show_legend, axis=axis,
        ax=ax, *args, **kwargs)

    ax.axis(smith_r*npy.array([-1.1, 1.1, -1.1, 1.1]))
    if plt.isinteractive():
        plt.draw()
def atomic_write_in_dir(path: str, mode: str = 'w', buffering: int = -1, encoding: Optional[str] = None, newline: Optional[str] = None, 
                        overwrite: bool = False):
  
  dir_name = os.path.dirname(path)

  if not overwrite and os.path.exists(path):
    raise FileExistsError(f"File '{path}' already exists. To overwrite it, set 'overwrite' to True.")

  with tempfile.NamedTemporaryFile(mode=mode, buffering=buffering, encoding=encoding, newline=newline, dir=dir_name, delete=False) as tmp_file:
    yield tmp_file
    tmp_file_name = tmp_file.name
  os.replace(tmp_file_name, path)
def async_render(
        self,
        variables: TemplateVarsType = None,
        parse_result: bool = True,
        limited: bool = False,
        **kwargs: Any,
    ) -> Any:
        
        
        if self.is_static:
            if self.hass.config.legacy_templates or not parse_result:
                return self.template
            return self._parse_result(self.template)

        compiled = self._compiled or self._ensure_compiled(limited)

        if variables is not None:
            kwargs.update(variables)

        try:
            render_result = compiled.render(kwargs)
        except Exception as err:  # pylint: disable=broad-except
            raise TemplateError(err) from err

        render_result = render_result.strip()

        if self.hass.config.legacy_templates or not parse_result:
            return render_result

        return self._parse_result(render_result)
def device_id(hass: HomeAssistant, entity_id_or_device_name: str) -> str | None:
    
    entity_reg = entity_registry.async_get(hass)
    entity = entity_reg.async_get(entity_id_or_device_name)
    if entity is not None:
        return entity.device_id

    dev_reg = device_registry.async_get(hass)
    return next(
        (
            id
            for id, device in dev_reg.devices.items()
            if (name := device.name_by_user or device.name)
            and (str(entity_id_or_device_name) == name)
        ),
        None,
    )
def expand(hass: HomeAssistant, *args: Any) -> Iterable[State]:
    
    # circular import.
    from . import entity as entity_helper  # pylint: disable=import-outside-toplevel

    search = list(args)
    found = {}
    while search:
        entity = search.pop()
        if isinstance(entity, str):
            entity_id = entity
            if (entity := _get_state(hass, entity)) is None:
                continue
        elif isinstance(entity, State):
            entity_id = entity.entity_id
        elif isinstance(entity, collections.abc.Iterable):
            search += entity
            continue
        else:
            # ignore other types
            continue

        if entity_id.startswith(_GROUP_DOMAIN_PREFIX) or (
            (source := entity_helper.entity_sources(hass).get(entity_id))
            and source["domain"] == "group"
        ):
            # Collect state will be called in here since it's wrapped
            if group_entities := entity.attributes.get(ATTR_ENTITY_ID):
                search += group_entities
        elif entity_id.startswith(_ZONE_DOMAIN_PREFIX):
            if zone_entities := entity.attributes.get(ATTR_PERSONS):
                search += zone_entities
        else:
            _collect_state(hass, entity_id)
            found[entity_id] = entity

    return sorted(found.values(), key=lambda a: a.entity_id)
def gen_code(ai: AI, prompt: str, memory: BaseRepository) -> Code:
    
    
    
    preprompts = OnDiskRepository(PREPROMPTS_PATH)
    messages = ai.start(setup_sys_prompt(preprompts), prompt, step_name=curr_fn())
    chat = messages[-1].content.strip()
    memory[CODE_GEN_LOG_FILE] = chat
    files = parse_chat(chat)
    code = Code({key: val for key, val in files})
    return code
def execute_entrypoint(ai: AI, execution_env: BaseExecutionEnv, code: Code) -> None:
    
    
    


    if not ENTRYPOINT_FILE in code:
        raise FileNotFoundError(
            "The required entrypoint " + ENTRYPOINT_FILE + " does not exist in the code."
        )

    command = code[ENTRYPOINT_FILE]

    print()
    print(
        colored(
            "Do you want to execute this code? (Y/n)",
            "red",
        )
    )
    print()
    print(command)
    print()
    if input().lower() not in ["", "y", "yes"]:
        print("Ok, not executing the code.")
        return []
    print("Executing the code...")
    print()
    print(
        colored(
            "Note: If it does not work as expected, consider running the code"
            + " in another way than above.",
            "green",
        )
    )
    print()
    print("You can press ctrl+c *once* to stop the execution.")
    print()

    execution_env.execute_program(code)
def run(self,
            workflow_id: Optional[str] = None,
            metadata: Optional[Dict[str, Any]] = None) -> Any:
        
        
        return ray.get(self.run_async(workflow_id, metadata))
def _save_subimports(self, code, top_level_dependencies):
        
        
        

        # check if any known dependency is an imported package
        for x in top_level_dependencies:
            if isinstance(x, types.ModuleType) and hasattr(x, '__package__') and x.__package__:
                # check if the package has any currently loaded sub-imports
                prefix = x.__name__ + '.'
                # A concurrent thread could mutate sys.modules,
                # make sure we iterate over a copy to avoid exceptions
                for name in list(sys.modules):
                    # Older versions of pytest will add a "None" module to sys.modules.
                    if name is not None and name.startswith(prefix):
                        # check whether the function can address the sub-module
                        tokens = set(name[len(prefix):].split('.'))
                        if not tokens - set(code.co_names):
                            # ensure unpickler executes this import
                            self.save(sys.modules[name])
                            # then discards the reference to it
                            self.write(pickle.POP)
def cell_set(cell, value):
    
    

    if sys.version_info[:2] >= (3, 7):  # pragma: no branch
        cell.cell_contents = value
    else:
        _cell_set = types.FunctionType(
            _cell_set_template_code, {}, '_cell_set', (), (cell,),)
        _cell_set(value)
def _make_skel_func(code, cell_count, base_globals=None):
    
    
    # This function is deprecated and should be removed in cloudpickle 1.7
    warnings.warn(
        "A pickle file created using an old (<=1.4.1) version of cloudpickle "
        "is currently being loaded. This is not supported by cloudpickle and "
        "will break in cloudpickle 1.7",
        category=UserWarning,
    )
    # This is backward-compatibility code: for cloudpickle versions between
    # 0.5.4 and 0.7, base_globals could be a string or None. base_globals
    # should now always be a dictionary.
    if base_globals is None or isinstance(base_globals, str):
        base_globals = {}

    base_globals["__builtins__"] = __builtins__

    closure = (
        tuple(_make_empty_cell() for _ in range(cell_count))
        if cell_count >= 0
        else None
    )
    return types.FunctionType(code, base_globals, None, None, closure)
def _walk_global_ops(code):
    
    
    
    for instr in dis.get_instructions(code):
        op = instr.opcode
        if op in GLOBAL_OPS:
            yield instr.argval
def cyclegan_generator_resnet(images,
                              arg_scope_fn=cyclegan_arg_scope,
                              num_resnet_blocks=6,
                              num_filters=64,
                              upsample_fn=cyclegan_upsample,
                              kernel_size=3,
                              tanh_linear_slope=0.0,
                              is_training=False):
  
  
  # Neither dropout nor batch norm -> dont need is_training
  del is_training

  end_points = {}

  input_size = images.shape.as_list()
  height, width = input_size[1], input_size[2]
  if height and height % 4 != 0:
    raise ValueError('The input height must be a multiple of 4.')
  if width and width % 4 != 0:
    raise ValueError('The input width must be a multiple of 4.')
  num_outputs = input_size[3]

  if not isinstance(kernel_size, (list, tuple)):
    kernel_size = [kernel_size, kernel_size]

  kernel_height = kernel_size[0]
  kernel_width = kernel_size[1]
  pad_top = (kernel_height - 1) // 2
  pad_bottom = kernel_height // 2
  pad_left = (kernel_width - 1) // 2
  pad_right = kernel_width // 2
  paddings = np.array(
      [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]],
      dtype=np.int32)
  spatial_pad_3 = np.array([[0, 0], [3, 3], [3, 3], [0, 0]])

  with tf.contrib.framework.arg_scope(arg_scope_fn()):

    ###########
    # Encoder #
    ###########
    with tf.variable_scope('input'):
      # 7x7 input stage
      net = tf.pad(images, spatial_pad_3, 'REFLECT')
      net = layers.conv2d(net, num_filters, kernel_size=[7, 7], padding='VALID')
      end_points['encoder_0'] = net

    with tf.variable_scope('encoder'):
      with tf.contrib.framework.arg_scope(
          [layers.conv2d],
          kernel_size=kernel_size,
          stride=2,
          activation_fn=tf.nn.relu,
          padding='VALID'):

        net = tf.pad(net, paddings, 'REFLECT')
        net = layers.conv2d(net, num_filters * 2)
        end_points['encoder_1'] = net
        net = tf.pad(net, paddings, 'REFLECT')
        net = layers.conv2d(net, num_filters * 4)
        end_points['encoder_2'] = net

    ###################
    # Residual Blocks #
    ###################
    with tf.variable_scope('residual_blocks'):
      with tf.contrib.framework.arg_scope(
          [layers.conv2d],
          kernel_size=kernel_size,
          stride=1,
          activation_fn=tf.nn.relu,
          padding='VALID'):
        for block_id in xrange(num_resnet_blocks):
          with tf.variable_scope('block_{}'.format(block_id)):
            res_net = tf.pad(net, paddings, 'REFLECT')
            res_net = layers.conv2d(res_net, num_filters * 4)
            res_net = tf.pad(res_net, paddings, 'REFLECT')
            res_net = layers.conv2d(res_net, num_filters * 4,
                                    activation_fn=None)
            net += res_net

            end_points['resnet_block_%d' % block_id] = net

    ###########
    # Decoder #
    ###########
    with tf.variable_scope('decoder'):

      with tf.contrib.framework.arg_scope(
          [layers.conv2d],
          kernel_size=kernel_size,
          stride=1,
          activation_fn=tf.nn.relu):

        with tf.variable_scope('decoder1'):
          net = upsample_fn(net, num_outputs=num_filters * 2, stride=[2, 2])
        end_points['decoder1'] = net

        with tf.variable_scope('decoder2'):
          net = upsample_fn(net, num_outputs=num_filters, stride=[2, 2])
        end_points['decoder2'] = net

    with tf.variable_scope('output'):
      net = tf.pad(net, spatial_pad_3, 'REFLECT')
      logits = layers.conv2d(
          net,
          num_outputs, [7, 7],
          activation_fn=None,
          normalizer_fn=None,
          padding='valid')
      logits = tf.reshape(logits, _dynamic_or_static_shape(images))

      end_points['logits'] = logits
      end_points['predictions'] = tf.tanh(logits) + logits * tanh_linear_slope

  return end_points['predictions'], end_points
def format_target_temperature(target_temperature):
    
    return str(round(float(target_temperature) * 2, 0) / 2).rstrip("0").rstrip(".")
def iradon(radon_image, theta=None, output_size=None,
           filter_name="ramp", interpolation="linear", circle=True,
           preserve_range=True):
    

    
    if radon_image.ndim != 2:
        raise ValueError('The input image must be 2-D')

    if theta is None:
        theta = np.linspace(0, 180, radon_image.shape[1], endpoint=False)

    angles_count = len(theta)
    if angles_count != radon_image.shape[1]:
        raise ValueError("The given ``theta`` does not match the number of "
                         "projections in ``radon_image``.")

    interpolation_types = ('linear', 'nearest', 'cubic')
    if interpolation not in interpolation_types:
        raise ValueError("Unknown interpolation: %s" % interpolation)

    filter_types = ('ramp', 'shepp-logan', 'cosine', 'hamming', 'hann', None)
    if filter_name not in filter_types:
        raise ValueError("Unknown filter: %s" % filter_name)

    radon_image = convert_to_float(radon_image, preserve_range)
    dtype = radon_image.dtype

    img_shape = radon_image.shape[0]
    if output_size is None:
        # If output size not specified, estimate from input radon image
        if circle:
            output_size = img_shape
        else:
            output_size = int(np.floor(np.sqrt((img_shape) ** 2 / 2.0)))

    if circle:
        radon_image = _sinogram_circle_to_square(radon_image)
        img_shape = radon_image.shape[0]

    # Resize image to next power of two (but no less than 64) for
    # Fourier analysis; speeds up Fourier and lessens artifacts
    projection_size_padded = max(64, int(2 ** np.ceil(np.log2(2 * img_shape))))
    pad_width = ((0, projection_size_padded - img_shape), (0, 0))
    img = np.pad(radon_image, pad_width, mode='constant', constant_values=0)

    # Apply filter in Fourier domain
    fourier_filter = _get_fourier_filter(projection_size_padded, filter_name)
    projection = fft(img, axis=0) * fourier_filter
    radon_filtered = np.real(ifft(projection, axis=0)[:img_shape, :])

    # Reconstruct image by interpolation
    reconstructed = np.zeros((output_size, output_size),
                             dtype=dtype)
    radius = output_size // 2
    xpr, ypr = np.mgrid[:output_size, :output_size] - radius
    x = np.arange(img_shape) - img_shape // 2

    for col, angle in zip(radon_filtered.T, np.deg2rad(theta)):
        t = ypr * np.cos(angle) - xpr * np.sin(angle)
        if interpolation == 'linear':
            interpolant = partial(np.interp, xp=x, fp=col, left=0, right=0)
        else:
            interpolant = interp1d(x, col, kind=interpolation,
                                   bounds_error=False, fill_value=0)
        reconstructed += interpolant(t)

    if circle:
        out_reconstruction_circle = (xpr ** 2 + ypr ** 2) > radius ** 2
        reconstructed[out_reconstruction_circle] = 0.

    return reconstructed * np.pi / (2 * angles_count)
def iradon_sart(radon_image, theta=None, image=None, projection_shifts=None,
                clip=None, relaxation=0.15, dtype=None):
    

    
    if radon_image.ndim != 2:
        raise ValueError('radon_image must be two dimensional')

    if dtype is None:
        if radon_image.dtype.char in 'fd':
            dtype = radon_image.dtype
        else:
            warn("Only floating point data type are valid for SART inverse "
                 "radon transform. Input data is cast to float. To disable "
                 "this warning, please cast image_radon to float.")
            dtype = np.dtype(float)
    elif np.dtype(dtype).char not in 'fd':
        raise ValueError("Only floating point data type are valid for inverse "
                         "radon transform.")

    dtype = np.dtype(dtype)
    radon_image = radon_image.astype(dtype, copy=False)

    reconstructed_shape = (radon_image.shape[0], radon_image.shape[0])

    if theta is None:
        theta = np.linspace(0, 180, radon_image.shape[1],
                            endpoint=False, dtype=dtype)
    elif len(theta) != radon_image.shape[1]:
        raise ValueError('Shape of theta (%s) does not match the '
                         'number of projections (%d)'
                         % (len(theta), radon_image.shape[1]))
    else:
        theta = np.asarray(theta, dtype=dtype)

    if image is None:
        image = np.zeros(reconstructed_shape, dtype=dtype)
    elif image.shape != reconstructed_shape:
        raise ValueError('Shape of image (%s) does not match first dimension '
                         'of radon_image (%s)'
                         % (image.shape, reconstructed_shape))
    elif image.dtype != dtype:
        warn("image dtype does not match output dtype: "
             "image is cast to {}".format(dtype))

    image = np.asarray(image, dtype=dtype)

    if projection_shifts is None:
        projection_shifts = np.zeros((radon_image.shape[1],), dtype=dtype)
    elif len(projection_shifts) != radon_image.shape[1]:
        raise ValueError('Shape of projection_shifts (%s) does not match the '
                         'number of projections (%d)'
                         % (len(projection_shifts), radon_image.shape[1]))
    else:
        projection_shifts = np.asarray(projection_shifts, dtype=dtype)
    if clip is not None:
        if len(clip) != 2:
            raise ValueError('clip must be a length-2 sequence')
        clip = np.asarray(clip, dtype=dtype)

    for angle_index in order_angles_golden_ratio(theta):
        image_update = sart_projection_update(image, theta[angle_index],
                                              radon_image[:, angle_index],
                                              projection_shifts[angle_index])
        image += relaxation * image_update
        if clip is not None:
            image = np.clip(image, clip[0], clip[1])
    return image
def _dispatch_flow_creation(cls, flow_path, raise_error=True, **kwargs):
        
        if is_prompty_flow(file_path=flow_path, raise_error=raise_error):
            from .prompty import Prompty

            return Prompty._load(path=flow_path, raise_error=True, **kwargs)

        with open(flow_path, "r", encoding=DEFAULT_ENCODING) as f:
            flow_content = f.read()
            data = load_yaml_string(flow_content)
            content_hash = hash(flow_content)
        if is_flex_flow(yaml_dict=data):
            from .flex import FlexFlow

            return FlexFlow._load(path=flow_path, data=data, raise_error=raise_error, **kwargs)
        else:
            from .dag import Flow

            # TODO: schema validation and warning on unknown fields
            return Flow._load(path=flow_path, dag=data, content_hash=content_hash, **kwargs)
def calc_curvature(self, sa: float, u: float, roll: float) -> float:
    
    
    return (self.curvature_factor(u) * sa / self.sR) + self.roll_compensation(roll, u)
def fixnewlines(self):
        
        

        if hasattr(self.editwin, "interp"):  # Saving shell.
            text = self.editwin.get_prompt_text('1.0', self.text.index('end-1c'))
        else:
            if self.text.get("end-2c") != '\n':
                self.text.insert("end-1c", "\n")  # Changes 'end-1c' value.
            text = self.text.get('1.0', "end-1c")
        if self.eol_convention != "\n":
            text = text.replace("\n", self.eol_convention)
        return text
def test_dqn_compilation(self):
        
        num_iterations = 1
        config = dqn.dqn.DQNConfig().rollouts(num_rollout_workers=2)

        for _ in framework_iterator(config, with_eager_tracing=True):
            # Double-dueling DQN.
            print("Double-dueling")
            plain_config = deepcopy(config)
            trainer = dqn.DQN(config=plain_config, env="CartPole-v0")
            for i in range(num_iterations):
                results = trainer.train()
                check_train_results(results)
                print(results)

            check_compute_single_action(trainer)
            trainer.stop()

            # Rainbow.
            print("Rainbow")
            rainbow_config = deepcopy(config).training(
                num_atoms=10, noisy=True, double_q=True, dueling=True, n_step=5
            )
            trainer = dqn.DQN(config=rainbow_config, env="CartPole-v0")
            for i in range(num_iterations):
                results = trainer.train()
                check_train_results(results)
                print(results)

            check_compute_single_action(trainer)

            trainer.stop()
def create_order(self, symbol, type, side, amount, price=None, params={}):
        
        
        
        # if type == 'market':
        #     raise ExchangeError(self.id + ' createOrder() does not accept market orders')
        # }
        self.load_markets()
        market = self.market(symbol)
        if not market['spot']:
            raise NotSupported(self.id + ' createOrder() does not support ' + market['type'] + ' orders, only spot orders are accepted')
        request = {
            'symbol': market['id'],
            'side': side,
            # 'timeInForce': timeInForce,
            # 'accountType': 'SPOT',
            # 'amount': amount,
        }
        orderRequest = self.order_request(symbol, type, side, amount, request, price, params)
        response = self.privatePostOrders(self.extend(orderRequest[0], orderRequest[1]))
        #
        #     {
        #         "id" : "78923648051920896",
        #         "clientOrderId" : ""
        #     }
        #
        response = self.extend(response, {
            'type': side,
        })
        return self.parse_order(response, market)
def fetch_my_trades(self, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):
        
        
        
        self.load_markets()
        paginate = False
        paginate, params = self.handle_option_and_params(params, 'fetchMyTrades', 'paginate')
        if paginate:
            return self.fetch_paginated_call_dynamic('fetchMyTrades', symbol, since, limit, params)
        market = None
        if symbol is not None:
            market = self.market(symbol)
        request = {
            # 'from': 12345678,  # A 'trade Id'. The query begins at ‘from'.
            # 'direction': 'PRE',  # PRE, NEXT The direction before or after ‘from'.
        }
        if since is not None:
            request['startTime'] = since
        if limit is not None:
            request['limit'] = limit
        request, params = self.handle_until_option('endTime', request, params)
        response = self.privateGetTrades(self.extend(request, params))
        #
        #     [
        #         {
        #             "id": "32164924331503616",
        #             "symbol": "LINK_USDT",
        #             "accountType": "SPOT",
        #             "orderId": "32164923987566592",
        #             "side": "SELL",
        #             "type": "MARKET",
        #             "matchRole": "TAKER",
        #             "createTime": 1648635115525,
        #             "price": "11",
        #             "quantity": "0.5",
        #             "amount": "5.5",
        #             "feeCurrency": "USDT",
        #             "feeAmount": "0.007975",
        #             "pageId": "32164924331503616",
        #             "clientOrderId": "myOwnId-321"
        #         }
        #     ]
        #
        result = self.parse_trades(response, market)
        return self.filter_by_since_limit(result, since, limit)
def create_sql_query_chain(
    llm: BaseLanguageModel,
    db: SQLDatabase,
    prompt: Optional[BasePromptTemplate] = None,
    k: int = 5,
) -> Runnable[Union[SQLInput, SQLInputWithTables, Dict[str, Any]], str]:
    
      # noqa: E501
    if prompt is not None:
        prompt_to_use = prompt
    elif db.dialect in SQL_PROMPTS:
        prompt_to_use = SQL_PROMPTS[db.dialect]
    else:
        prompt_to_use = PROMPT
    if {"input", "top_k", "table_info"}.difference(prompt_to_use.input_variables):
        raise ValueError(
            f"Prompt must have input variables: 'input', 'top_k', "
            f"'table_info'. Received prompt with input variables: "
            f"{prompt_to_use.input_variables}. Full prompt:\n\n{prompt_to_use}"
        )
    if "dialect" in prompt_to_use.input_variables:
        prompt_to_use = prompt_to_use.partial(dialect=db.dialect)

    inputs = {
        "input": lambda x: x["question"] + "\nSQLQuery: ",
        "table_info": lambda x: db.get_table_info(
            table_names=x.get("table_names_to_use")
        ),
    }
    return (
        RunnablePassthrough.assign(**inputs)  # type: ignore
        | (
            lambda x: {
                k: v
                for k, v in x.items()
                if k not in ("question", "table_names_to_use")
            }
        )
        | prompt_to_use.partial(top_k=str(k))
        | llm.bind(stop=["\nSQLResult:"])
        | StrOutputParser()
        | _strip
    )
def robust_scale(X, *, axis=0, with_centering=True, with_scaling=True,
                 quantile_range=(25.0, 75.0), copy=True, unit_variance=False):
    
    
    X = check_array(X, accept_sparse=('csr', 'csc'), copy=False,
                    ensure_2d=False, dtype=FLOAT_DTYPES,
                    force_all_finite='allow-nan')
    original_ndim = X.ndim

    if original_ndim == 1:
        X = X.reshape(X.shape[0], 1)

    s = RobustScaler(with_centering=with_centering, with_scaling=with_scaling,
                     quantile_range=quantile_range,
                     unit_variance=unit_variance, copy=copy)
    if axis == 0:
        X = s.fit_transform(X)
    else:
        X = s.fit_transform(X.T).T

    if original_ndim == 1:
        X = X.ravel()

    return X
def _check_input(self, X, in_fit, check_positive=False, check_shape=False):
        
        
        X = self._validate_data(
            X,
            ensure_2d=True,
            dtype=FLOAT_DTYPES,
            copy=self.copy,
            force_all_finite="allow-nan",
            reset=in_fit,
        )

        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
            if check_positive and self.method == "box-cox" and np.nanmin(X) <= 0:
                raise ValueError(
                    "The Box-Cox transformation can only be "
                    "applied to strictly positive data"
                )

        if check_shape and not X.shape[1] == len(self.lambdas_):
            raise ValueError(
                "Input data has a different number of features "
                "than fitting data. Should have {n}, data has {m}".format(
                    n=len(self.lambdas_), m=X.shape[1]
                )
            )

        return X
def _create_http_error(response):  # type: (HttpResponse) -> ApplicationError
        
        response_json = response.json()
        stack_trace = ''

        if 'message' in response_json:
            message = response_json['message']
        elif 'errorMessage' in response_json:
            message = response_json['errorMessage'].strip()
            if 'stackTrace' in response_json:
                traceback_lines = response_json['stackTrace']

                # AWS Lambda on Python 2.7 returns a list of tuples
                # AWS Lambda on Python 3.7 returns a list of strings
                if traceback_lines and isinstance(traceback_lines[0], list):
                    traceback_lines = traceback.format_list(traceback_lines)

                trace = '\n'.join([x.rstrip() for x in traceback_lines])
                stack_trace = ('\nTraceback (from remote server):\n%s' % trace)
        else:
            message = str(response_json)

        return CoreHttpError(response.status_code, message, stack_trace)
def core_requirements():
    
    with open("pyproject.toml", "rb") as fp:
        data = tomllib.load(fp)
    return data["project"]["dependencies"]
def _GetMSBuildExternalBuilderTargets(spec):
  
  
  build_cmd = _BuildCommandLineForRuleRaw(
      spec, spec['msvs_external_builder_build_cmd'],
      False, False, False, False)
  build_target = ['Target', {'Name': 'Build'}]
  build_target.append(['Exec', {'Command': build_cmd}])

  clean_cmd = _BuildCommandLineForRuleRaw(
      spec, spec['msvs_external_builder_clean_cmd'],
      False, False, False, False)
  clean_target = ['Target', {'Name': 'Clean'}]
  clean_target.append(['Exec', {'Command': clean_cmd}])

  targets = [build_target, clean_target]

  if spec.get('msvs_external_builder_clcompile_cmd'):
    clcompile_cmd = _BuildCommandLineForRuleRaw(
        spec, spec['msvs_external_builder_clcompile_cmd'],
        False, False, False, False)
    clcompile_target = ['Target', {'Name': 'ClCompile'}]
    clcompile_target.append(['Exec', {'Command': clcompile_cmd}])
    targets.append(clcompile_target)

  return targets
def _IsWindowsAbsPath(path):
    
  
  
    return path.startswith("c:") or path.startswith("C:")
def _AdjustSourcesAndConvertToFilterHierarchy(
    spec, options, gyp_dir, sources, excluded_sources, list_excluded, version):
  
  
  # Exclude excluded sources coming into the generator.
  excluded_sources.update(OrderedSet(spec.get('sources_excluded', [])))
  # Add excluded sources into sources for good measure.
  sources.update(excluded_sources)
  # Convert to proper windows form.
  # NOTE: sources goes from being a set to a list here.
  # NOTE: excluded_sources goes from being a set to a list here.
  sources = _FixPaths(sources)
  # Convert to proper windows form.
  excluded_sources = _FixPaths(excluded_sources)

  excluded_idl = _IdlFilesHandledNonNatively(spec, sources)

  precompiled_related = _GetPrecompileRelatedFiles(spec)
  # Find the excluded ones, minus the precompiled header related ones.
  fully_excluded = [i for i in excluded_sources if i not in precompiled_related]

  # Convert to folders and the right slashes.
  sources = [i.split('\\') for i in sources]
  sources = _ConvertSourcesToFilterHierarchy(sources, excluded=fully_excluded,
                                             list_excluded=list_excluded,
                                             msvs_version=version)

  # Prune filters with a single child to flatten ugly directory structures
  # such as ../../src/modules/module1 etc.
  while len(sources) == 1 and isinstance(sources[0], MSVSProject.Filter):
    sources = sources[0].contents

  return sources, excluded_sources, excluded_idl
def realize(self, *lst:Tensor) -> Tensor:
    
    run_schedule(*self.schedule_with_vars(*lst))
    return self
def calculate_heuristic(self) -> float:
        
        
        
        dy = self.pos_x - self.goal_x
        dx = self.pos_y - self.goal_y
        if HEURISTIC == 1:
            return abs(dx) + abs(dy)
        else:
            return sqrt(dy ** 2 + dx ** 2)
def is_locked(self):
        
        return not (
            self._nuki_device.is_rto_activated
            or self._nuki_device.mode == MODE_OPENER_CONTINUOUS
        )
def __init__(
        self,
        include_lsb=True,
        os_release_file="",
        distro_release_file="",
        include_uname=True,
        root_dir=None,
    ):
        # type: (bool, str, str, bool, Optional[str]) -> None
        
        
        
        self.root_dir = root_dir
        self.etc_dir = os.path.join(root_dir, "etc") if root_dir else _UNIXCONFDIR
        self.usr_lib_dir = (
            os.path.join(root_dir, "usr/lib") if root_dir else _UNIXUSRLIBDIR
        )

        if os_release_file:
            self.os_release_file = os_release_file
        else:
            etc_dir_os_release_file = os.path.join(self.etc_dir, _OS_RELEASE_BASENAME)
            usr_lib_os_release_file = os.path.join(
                self.usr_lib_dir, _OS_RELEASE_BASENAME
            )

            # NOTE: The idea is to respect order **and** have it set
            #       at all times for API backwards compatibility.
            if os.path.isfile(etc_dir_os_release_file) or not os.path.isfile(
                usr_lib_os_release_file
            ):
                self.os_release_file = etc_dir_os_release_file
            else:
                self.os_release_file = usr_lib_os_release_file

        self.distro_release_file = distro_release_file or ""  # updated later
        self.include_lsb = include_lsb
        self.include_uname = include_uname
def from_global_id_or_error(
    global_id: str, only_type: Union[ObjectType, str] = None, raise_error: bool = False
):
    
    
    try:
        type_, id_ = graphene.Node.from_global_id(global_id)
    except (binascii.Error, UnicodeDecodeError, ValueError):
        raise GraphQLError(f"Couldn't resolve id: {global_id}.")
    if type_ == APP_ID_PREFIX:
        id_ = global_id
    else:
        if not validate_if_int_or_uuid(id_):
            raise GraphQLError(f"Error occurred during ID - {global_id} validation.")

    if only_type and str(type_) != str(only_type):
        if not raise_error:
            return type_, None
        raise GraphQLError(f"Must receive a {only_type} id.")
    return type_, id_
def validate_credit_card_number(credit_card_number: str) -> bool:
    
    
    
    error_message = f"{credit_card_number} is an invalid credit card number because"
    if not credit_card_number.isdigit():
        print(f"{error_message} it has nonnumerical characters.")
        return False

    if not 13 <= len(credit_card_number) <= 16:
        print(f"{error_message} of its length.")
        return False

    if not validate_initial_digits(credit_card_number):
        print(f"{error_message} of its first two digits.")
        return False

    if not luhn_validation(credit_card_number):
        print(f"{error_message} it fails the Luhn check.")
        return False

    print(f"{credit_card_number} is a valid credit card number.")
    return True
def __init__(self,
               input_specs,
               use_fpn=False,
               use_spatial_attention=False,
               csp_stack=False,
               fpn_depth=4,
               max_fpn_depth=None,
               max_csp_stack=None,
               fpn_filter_scale=1,
               path_process_len=6,
               max_level_process_len=None,
               embed_spp=False,
               activation='leaky',
               use_sync_bn=False,
               use_separable_conv=False,
               norm_momentum=0.99,
               norm_epsilon=0.001,
               kernel_initializer='VarianceScaling',
               kernel_regularizer=None,
               bias_regularizer=None,
               **kwargs):
    
    

    self._input_specs = input_specs
    self._use_fpn = use_fpn
    self._fpn_depth = fpn_depth
    self._max_fpn_depth = max_fpn_depth
    self._max_csp_stack = max_csp_stack
    self._path_process_len = path_process_len
    self._max_level_process_len = max_level_process_len
    self._embed_spp = embed_spp

    self._activation = activation
    self._use_sync_bn = use_sync_bn
    self._use_separable_conv = use_separable_conv
    self._norm_momentum = norm_momentum
    self._norm_epsilon = norm_epsilon
    self._kernel_initializer = kernel_initializer
    self._kernel_regularizer = kernel_regularizer
    self._bias_regularizer = bias_regularizer

    self._base_config = dict(
        use_spatial_attention=use_spatial_attention,
        csp_stack=csp_stack,
        activation=self._activation,
        use_sync_bn=self._use_sync_bn,
        use_separable_conv=self._use_separable_conv,
        fpn_filter_scale=fpn_filter_scale,
        norm_momentum=self._norm_momentum,
        norm_epsilon=self._norm_epsilon,
        kernel_initializer=self._kernel_initializer,
        kernel_regularizer=self._kernel_regularizer,
        bias_regularizer=self._bias_regularizer)

    self._decoder_config = dict(
        path_process_len=self._path_process_len,
        max_level_process_len=self._max_level_process_len,
        embed_spp=self._embed_spp,
        fpn_input=self._use_fpn,
        **self._base_config)

    inputs = {
        key: tf_keras.layers.Input(shape=value[1:])
        for key, value in input_specs.items()
    }
    if self._use_fpn:
      inter_outs = YoloFPN(
          fpn_depth=self._fpn_depth,
          max_fpn_depth=self._max_fpn_depth,
          max_csp_stack=self._max_csp_stack,
          **self._base_config)(inputs)
      outputs = YoloPAN(**self._decoder_config)(inter_outs)
    else:
      inter_outs = None
      outputs = YoloPAN(**self._decoder_config)(inputs)

    self._output_specs = {key: value.shape for key, value in outputs.items()}
    super().__init__(inputs=inputs, outputs=outputs, name='YoloDecoder')
def estimate(self, src, dst):
        

        

        ndim = src.shape[1]
        # forward piecewise affine
        # triangulate input positions into mesh
        self._tesselation = spatial.Delaunay(src)
        # find affine mapping from source positions to destination
        self.affines = []
        for tri in self._tesselation.vertices:
            affine = AffineTransform(dimensionality=ndim)
            affine.estimate(src[tri, :], dst[tri, :])
            self.affines.append(affine)

        # inverse piecewise affine
        # triangulate input positions into mesh
        self._inverse_tesselation = spatial.Delaunay(dst)
        # find affine mapping from source positions to destination
        self.inverse_affines = []
        for tri in self._inverse_tesselation.vertices:
            affine = AffineTransform(dimensionality=ndim)
            affine.estimate(dst[tri, :], src[tri, :])
            self.inverse_affines.append(affine)

        return True
def estimate(self, src, dst):
        

        
        self.params = _umeyama(src, dst, False)

        # _umeyama will return nan if the problem is not well-conditioned.
        return not np.any(np.isnan(self.params))
def estimate(self, src, dst, order=2, weights=None):
        

        
        xs = src[:, 0]
        ys = src[:, 1]
        xd = dst[:, 0]
        yd = dst[:, 1]
        rows = src.shape[0]

        # number of unknown polynomial coefficients
        order = safe_as_int(order)
        u = (order + 1) * (order + 2)

        A = np.zeros((rows * 2, u + 1))
        pidx = 0
        for j in range(order + 1):
            for i in range(j + 1):
                A[:rows, pidx] = xs ** (j - i) * ys ** i
                A[rows:, pidx + u // 2] = xs ** (j - i) * ys ** i
                pidx += 1

        A[:rows, -1] = xd
        A[rows:, -1] = yd

        # Get the vectors that correspond to singular values, also applying
        # the weighting if provided
        if weights is None:
            _, _, V = np.linalg.svd(A)
        else:
            W = np.diag(np.tile(np.sqrt(weights / np.max(weights)), 2))
            _, _, V = np.linalg.svd(W @ A)

        # solution is right singular vector that corresponds to smallest
        # singular value
        params = - V[-1, :-1] / V[-1, -1]

        self.params = params.reshape((2, u // 2))

        return True
def __call__(self, coords):
        

        
        x = coords[:, 0]
        y = coords[:, 1]
        u = len(self._params.ravel())
        # number of coefficients -> u = (order + 1) * (order + 2)
        order = int((- 3 + math.sqrt(9 - 4 * (2 - u))) / 2)
        dst = np.zeros(coords.shape)

        pidx = 0
        for j in range(order + 1):
            for i in range(j + 1):
                dst[:, 0] += self._params[0, pidx] * x ** (j - i) * y ** i
                dst[:, 1] += self._params[1, pidx] * x ** (j - i) * y ** i
                pidx += 1

        return dst
def warp(image, inverse_map=None, map_args={}, output_shape=None, order=1,
         mode='constant', cval=0., reverse_map=None):
    

    
    # Backward API compatibility
    if reverse_map is not None:
        inverse_map = reverse_map

    if image.ndim < 2:
        raise ValueError("Input must have more than 1 dimension.")

    image = np.atleast_3d(img_as_float(image))
    ishape = np.array(image.shape)
    bands = ishape[2]

    if output_shape is None:
        output_shape = ishape

    coords = np.empty(np.r_[3, output_shape], dtype=float)

    ## Construct transformed coordinates

    rows, cols = output_shape[:2]

    # Reshape grid coordinates into a (P, 2) array of (x, y) pairs
    tf_coords = np.indices((cols, rows), dtype=float).reshape(2, -1).T

    # Map each (x, y) pair to the source image according to
    # the user-provided mapping
    tf_coords = inverse_map(tf_coords, **map_args)

    # Reshape back to a (2, M, N) coordinate grid
    tf_coords = tf_coords.T.reshape((-1, cols, rows)).swapaxes(1, 2)

    # Place the y-coordinate mapping
    _stackcopy(coords[1, ...], tf_coords[0, ...])

    # Place the x-coordinate mapping
    _stackcopy(coords[0, ...], tf_coords[1, ...])

    # colour-coordinate mapping
    coords[2, ...] = range(bands)

    # Prefilter not necessary for order 1 interpolation
    prefilter = order > 1
    mapped = ndimage.map_coordinates(image, coords, prefilter=prefilter,
                                     mode=mode, order=order, cval=cval)

    # The spline filters sometimes return results outside [0, 1],
    # so clip to ensure valid data
    return np.clip(mapped.squeeze(), 0, 1)
def warp_coords(coord_map, shape, dtype=np.float64):
    

    
    shape = safe_as_int(shape)
    rows, cols = shape[0], shape[1]
    coords_shape = [len(shape), rows, cols]
    if len(shape) == 3:
        coords_shape.append(shape[2])
    coords = np.empty(coords_shape, dtype=dtype)

    # Reshape grid coordinates into a (P, 2) array of (row, col) pairs
    tf_coords = np.indices((cols, rows), dtype=dtype).reshape(2, -1).T

    # Map each (row, col) pair to the source image according to
    # the user-provided mapping
    tf_coords = coord_map(tf_coords)

    # Reshape back to a (2, M, N) coordinate grid
    tf_coords = tf_coords.T.reshape((-1, cols, rows)).swapaxes(1, 2)

    # Place the y-coordinate mapping
    _stackcopy(coords[1, ...], tf_coords[0, ...])

    # Place the x-coordinate mapping
    _stackcopy(coords[0, ...], tf_coords[1, ...])

    if len(shape) == 3:
        coords[2, ...] = range(shape[2])

    return coords
def _clip_warp_output(input_image, output_image, order, mode, cval, clip):
    

    
    mode = _mode_deprecations(mode)
    if clip and order != 0:
        min_val = input_image.min()
        max_val = input_image.max()

        preserve_cval = mode == 'constant' and not \
                        (min_val <= cval <= max_val)

        if preserve_cval:
            cval_mask = output_image == cval

        np.clip(output_image, min_val, max_val, out=output_image)

        if preserve_cval:
            output_image[cval_mask] = cval
def rayleigh_quotient(A: np.array, v: np.array) -> float:
    
    
    
    v_star = v.conjugate().T
    return (v_star.dot(A).dot(v)) / (v_star.dot(v))
def get_enabled_apis() -> Set[str]:
    
    
    
    from localstack.services.plugins import SERVICE_PLUGINS

    services_env = os.environ.get("SERVICES", "").strip()
    services = SERVICE_PLUGINS.list_available()

    if services_env and is_env_true("STRICT_SERVICE_LOADING"):
        # SERVICES and STRICT_SERVICE_LOADING are set
        # we filter the result of SERVICE_PLUGINS.list_available() to cross the user-provided list with
        # the available ones
        enabled_services = []
        for service_port in re.split(r"\s*,\s*", services_env):
            # Only extract the service name, discard the port
            parts = re.split(r"[:=]", service_port)
            service = parts[0]
            enabled_services.append(service)

        services = [service for service in enabled_services if service in services]
        # TODO: log a message if a service was not supported? see with pro loading

    return resolve_apis(services)
def test_box_actions_out_of_bound(env: gym.Env):
    
    
    env.reset(seed=42)

    oob_env = gym.make(env.spec.id, disable_env_checker=True)
    oob_env.reset(seed=42)

    assert isinstance(env.action_space, spaces.Box)
    dtype = env.action_space.dtype
    upper_bounds = env.action_space.high
    lower_bounds = env.action_space.low

    for i, (is_upper_bound, is_lower_bound) in enumerate(
        zip(env.action_space.bounded_above, env.action_space.bounded_below)
    ):
        if is_upper_bound:
            obs, _, _, _ = env.step(upper_bounds)
            oob_action = upper_bounds.copy()
            oob_action[i] += np.cast[dtype](OOB_VALUE)

            assert oob_action[i] > upper_bounds[i]
            oob_obs, _, _, _ = oob_env.step(oob_action)

            assert np.alltrue(obs == oob_obs)

        if is_lower_bound:
            obs, _, _, _ = env.step(lower_bounds)
            oob_action = lower_bounds.copy()
            oob_action[i] -= np.cast[dtype](OOB_VALUE)

            assert oob_action[i] < lower_bounds[i]
            oob_obs, _, _, _ = oob_env.step(oob_action)

            assert np.alltrue(obs == oob_obs)

    env.close()
def force_update(self):
        
        return not self.should_poll
def track_event(
        event_name: str,
        event_value: str,
        session_id: Optional[str] = None,
        thread_id: Optional[UUID] = None,
        exception_name: Optional[str] = None,
    ):
        
        

        if session_id:
            EventTracker._session_id = session_id

        try:
            should_send: bool = False
            # Validate the thread ID
            if not thread_id:  # Passed value is not a UUID or None
                thread_id = uuid4()
            with EventTracker._event_lock:
                EventTracker._events.append(
                    Event(event_name, event_value, thread_id=thread_id, exception_name=exception_name)
                )

                # Get the session ID (needed for multithreading sending)
                EventTracker._set_session_id()

                if len(EventTracker._events) >= EventTracker.MAX_EVENTS:
                    should_send = True
            if should_send:
                EventTracker.send_events()
        except EventCreationError as e:
            LOG.debug("Error occurred while trying to track an event: %s", e)
def inverse_transform(self, X=None, Xt=None):
        
        
        X = _deprecate_Xt_in_inverse_transform(X, Xt)
        check_is_fitted(self)
        return self.best_estimator_.inverse_transform(X)
def fit(self, X, y=None, groups=None):
        
        
        return self._fit(X, y, groups, ParameterGrid(self.param_grid))
def _get_scorers(self):
        
        
        refit_metric = "score"

        if callable(self.scoring):
            scorers = self.scoring
        elif self.scoring is None or isinstance(self.scoring, str):
            scorers = check_scoring(self.estimator, self.scoring)
        else:
            scorers = _check_multimetric_scoring(self.estimator, self.scoring)
            self._check_refit_for_multimetric(scorers)
            refit_metric = self.refit
            scorers = _MultimetricScorer(
                scorers=scorers, raise_exc=(self.error_score == "raise")
            )

        return scorers, refit_metric
def parse_config_overrides(
    args: List[str], env_var: Optional[str] = OVERRIDES_ENV_VAR
) -> Dict[str, Any]:
    
    
    env_string = os.environ.get(env_var, "") if env_var else ""
    env_overrides = _parse_overrides(split_arg_string(env_string))
    cli_overrides = _parse_overrides(args, is_cli=True)
    if cli_overrides:
        keys = [k for k in cli_overrides if k not in env_overrides]
        logger.debug(f"Config overrides from CLI: {keys}")
    if env_overrides:
        logger.debug(f"Config overrides from env variables: {list(env_overrides)}")
    return {**cli_overrides, **env_overrides}
def string_to_list(value: str, intify: bool = False) -> Union[List[str], List[int]]:
    
    
    if not value:
        return []
    if value.startswith("[") and value.endswith("]"):
        value = value[1:-1]
    result = []
    for p in value.split(","):
        p = p.strip()
        if p.startswith("'") and p.endswith("'"):
            p = p[1:-1]
        if p.startswith('"') and p.endswith('"'):
            p = p[1:-1]
        p = p.strip()
        if intify:
            p = int(p)
        result.append(p)
    return result
def get_hash(data, exclude: Iterable[str] = tuple()) -> str:
    
    
    if isinstance(data, dict):
        data = {k: v for k, v in data.items() if k not in exclude}
    data_str = srsly.json_dumps(data, sort_keys=True).encode("utf8")
    return hashlib.md5(data_str).hexdigest()
def _is_mac_os_version(versions):
    
    
    
    import platform
    mac_version, _, _ = platform.mac_ver()
    return '.'.join(mac_version.split('.')[:2]) in versions
def test_compress_streaming_response_unicode(self):
        
        
        
        r = GZipMiddleware().process_response(self.req, self.stream_resp_unicode)
        self.assertEqual(
            self.decompress(b''.join(r)),
            b''.join(x.encode('utf-8') for x in self.sequence_unicode)
        )
        self.assertEqual(r.get('Content-Encoding'), 'gzip')
        self.assertFalse(r.has_header('Content-Length'))
def getuser():
    
    

    for name in ('LOGNAME', 'USER', 'LNAME', 'USERNAME'):
        user = os.environ.get(name)
        if user:
            return user

    try:
        import pwd
        return pwd.getpwuid(os.getuid())[0]
    except (ImportError, KeyError) as e:
        raise OSError('No username set in the environment') from e
def build_autoencoders(self, inputs):
          
        logger.debug("Initializing model")
        for side in "a", "b":
            logger.debug("Adding Autoencoder. Side: %s", side)
            decoder = self.networks["decoder_{}".format(side)].network
            output = decoder(self.networks["encoder"].network(inputs[0]))
            autoencoder = KerasModel(inputs, output)
            self.add_predictor(side, autoencoder)
        logger.debug("Initialized model")
def upload(self, image, resource_name):
        
        
        
        if not self.login_session_active:
            self.login()
            self.login_session_active = True
        try:
            docker_img = self.docker_client.images.get(image)

            _tag = tag_translation(image, docker_image_id=docker_img.id, gen_tag=self.tag)
            repository = (
                self.ecr_repo if not isinstance(self.ecr_repo_multi, dict) else self.ecr_repo_multi.get(resource_name)
            )

            docker_img.tag(repository=repository, tag=_tag)
            push_logs = self.docker_client.api.push(
                repository=repository, tag=_tag, auth_config=self.auth_config, stream=True, decode=True
            )
            self._stream_progress(push_logs)

        except (BuildError, APIError) as ex:
            raise DockerPushFailedError(msg=str(ex)) from ex

        return f"{repository}:{_tag}"
def __init__(
        self,
        coordinator: WeatherKitDataUpdateCoordinator,
    ) -> None:
        
        super().__init__(coordinator)
        WeatherKitEntity.__init__(self, coordinator, unique_id_suffix=None)
def __init__(
        self,
        file_path: str,
        processed_file_format: str = "md",
        max_wait_time_seconds: int = 500,
        should_clean_pdf: bool = False,
        extra_request_data: Optional[Dict[str, Any]] = None,
        **kwargs: Any,
    ) -> None:
        
        
        self.mathpix_api_key = get_from_dict_or_env(
            kwargs, "mathpix_api_key", "MATHPIX_API_KEY"
        )
        self.mathpix_api_id = get_from_dict_or_env(
            kwargs, "mathpix_api_id", "MATHPIX_API_ID"
        )
        super().__init__(file_path, **kwargs)
        self.processed_file_format = processed_file_format
        self.extra_request_data = (
            extra_request_data if extra_request_data is not None else {}
        )
        self.max_wait_time_seconds = max_wait_time_seconds
        self.should_clean_pdf = should_clean_pdf
def __init__(
        self,
        file_path: str,
        *,
        headers: Optional[Dict] = None,
        extract_images: bool = False,
        concatenate_pages: bool = True,
    ) -> None:
        
        
        try:
            from pdfminer.high_level import extract_text  # noqa:F401
        except ImportError:
            raise ImportError(
                "`pdfminer` package not found, please install it with "
                "`pip install pdfminer.six`"
            )

        super().__init__(file_path, headers=headers)
        self.parser = PDFMinerParser(
            extract_images=extract_images, concatenate_pages=concatenate_pages
        )
def forward(self, query: Tensor, value: Tensor, key_padding_mask: Tensor,
                self_attn_mask: Tensor, reference_points: Tensor,
                spatial_shapes: Tensor, level_start_index: Tensor,
                valid_ratios: Tensor, reg_branches: nn.ModuleList,
                **kwargs) -> Tuple[Tensor]:
        
        
        intermediate = []
        intermediate_reference_points = [reference_points]
        for lid, layer in enumerate(self.layers):
            if reference_points.shape[-1] == 4:
                reference_points_input = \
                    reference_points[:, :, None] * torch.cat(
                        [valid_ratios, valid_ratios], -1)[:, None]
            else:
                assert reference_points.shape[-1] == 2
                reference_points_input = \
                    reference_points[:, :, None] * valid_ratios[:, None]

            query_sine_embed = coordinate_to_encoding(
                reference_points_input[:, :, 0, :])
            query_pos = self.ref_point_head(query_sine_embed)

            query = layer(
                query,
                query_pos=query_pos,
                value=value,
                key_padding_mask=key_padding_mask,
                self_attn_mask=self_attn_mask,
                spatial_shapes=spatial_shapes,
                level_start_index=level_start_index,
                valid_ratios=valid_ratios,
                reference_points=reference_points_input,
                **kwargs)

            if reg_branches is not None:
                tmp = reg_branches[lid](query)
                assert reference_points.shape[-1] == 4
                new_reference_points = tmp + inverse_sigmoid(
                    reference_points, eps=1e-3)
                new_reference_points = new_reference_points.sigmoid()
                reference_points = new_reference_points.detach()

            if self.return_intermediate:
                intermediate.append(self.norm(query))
                intermediate_reference_points.append(new_reference_points)
                # NOTE this is for the "Look Forward Twice" module,
                # in the DeformDETR, reference_points was appended.

        if self.return_intermediate:
            return torch.stack(intermediate), torch.stack(
                intermediate_reference_points)

        return query, reference_points
def icon(self) -> str | None:
        
        return ZODIAC_ICONS.get(self._state)
def available(self) -> bool:
        
        if STATE_MAP.get(self.coordinator.data["alarm"]) is None:
            return False
        return super().available
def initialize(app: flask.Flask, settings):
    
    global _INSTALLED  # pylint: disable=global-statement
    if not settings['server']['limiter'] and not settings['server']['public_instance']:
        return
    redis_client = redisdb.client()
    if not redis_client:
        logger.error(
            "The limiter requires Redis, please consult the documentation: "
            + "https://docs.searxng.org/admin/searx.botdetection.html#limiter"
        )
        if settings['server']['public_instance']:
            sys.exit(1)
        return
    botdetection.init(get_cfg(), redis_client)
    app.before_request(pre_request)
    _INSTALLED = True
def initialize(app: flask.Flask, settings):
    
    global _INSTALLED  # pylint: disable=global-statement

    if not (settings['server']['limiter'] or settings['server']['public_instance']):
        return

    redis_client = redisdb.client()
    if not redis_client:
        logger.error(
            "The limiter requires Redis, please consult the documentation: "
            "https://docs.searxng.org/admin/searx.limiter.html"
        )
        if settings['server']['public_instance']:
            sys.exit(1)
        return

    _INSTALLED = True

    cfg = get_cfg()
    if settings['server']['public_instance']:
        # overwrite limiter.toml setting
        cfg.set('botdetection.ip_limit.link_token', True)

    botdetection.init(cfg, redis_client)
    app.before_request(pre_request)
def format_permissions_for_display(permissions):
    

    
    permissions_data = permissions.annotate(
        formated_codename=Concat("content_type__app_label", Value("."), "codename")
    ).values("name", "formated_codename")

    formatted_permissions = [
        Permission(
            code=PermissionEnum.get(data["formated_codename"]), name=data["name"]
        )
        for data in permissions_data
    ]
    return formatted_permissions
def get_nodes(
    ids,
    graphene_type: Union[graphene.ObjectType, str] = None,
    model=None,
    qs=None,
    schema=None,
):
    
    
    nodes_type, pks = resolve_global_ids_to_primary_keys(
        ids, graphene_type, raise_error=True
    )

    # If `graphene_type` was not provided, check if all resolved types are
    # the same. This prevents from accidentally mismatching IDs of different
    # types.
    if nodes_type and not graphene_type:
        if schema:
            graphene_type = _resolve_graphene_type(schema, nodes_type)
        else:
            raise GraphQLError("GraphQL schema was not provided")

    if qs is None and graphene_type and not isinstance(graphene_type, str):
        qs = graphene_type._meta.model.objects
    elif model is not None:
        qs = model.objects

    nodes = list(qs.filter(pk__in=pks))
    nodes.sort(key=lambda e: pks.index(str(e.pk)))  # preserve order in pks

    if not nodes:
        raise GraphQLError(ERROR_COULD_NO_RESOLVE_GLOBAL_ID % ids)

    nodes_pk_list = [str(node.pk) for node in nodes]
    for pk in pks:
        assert pk in nodes_pk_list, "There is no node of type {} with pk {}".format(
            graphene_type, pk
        )
    return nodes
def forward(
        self,
        input_ids: torch.Tensor,
        token_type_ids: torch.Tensor,
        attention_mask: torch.Tensor,
    ):
        
        
        with torch.set_grad_enabled(not self.freeze):
            output = self.model.forward(
                input_ids,
                token_type_ids=token_type_ids,
                attention_mask=attention_mask,
            )
        return output
def all_exceptions(
        self, handler: Callable[..., Any]
    ) -> Callable[..., Any]:
        
          # noqa: E501
        return self.exception(Exception)(handler)
def stop(self):
        
        self.shouldStop = True
def _offsets_to_raveled_neighbors(image_shape, footprint, center, order='C'):
    
    
    if not footprint.ndim == len(image_shape) == len(center):
        raise ValueError(
            "number of dimensions in image shape, footprint and its"
            "center index does not match"
        )

    footprint_indices = np.stack(np.nonzero(footprint), axis=-1)
    offsets = footprint_indices - center

    if order == 'F':
        offsets = offsets[:, ::-1]
        image_shape = image_shape[::-1]
    elif order != 'C':
        raise ValueError("order must be 'C' or 'F'")

    # Scale offsets in each dimension and sum
    ravel_factors = image_shape[1:] + (1,)
    ravel_factors = np.cumprod(ravel_factors[::-1])[::-1]
    raveled_offsets = (offsets * ravel_factors).sum(axis=1)

    # Sort by distance
    distances = np.abs(offsets).sum(axis=1)
    raveled_offsets = raveled_offsets[np.argsort(distances)]

    # If any dimension in image_shape is smaller than footprint.shape
    # duplicates might occur, remove them
    if any(x < y for x, y in zip(image_shape, footprint.shape)):
        # np.unique reorders, which we don't want
        _, indices = np.unique(raveled_offsets, return_index=True)
        raveled_offsets = raveled_offsets[np.sort(indices)]

    # Remove "offset to center"
    raveled_offsets = raveled_offsets[1:]

    return raveled_offsets
def _resolve_neighborhood(footprint, connectivity, ndim,
                          enforce_adjacency=True):
    
    
    if footprint is None:
        if connectivity is None:
            connectivity = ndim
        footprint = ndi.generate_binary_structure(ndim, connectivity)
    else:
        # Validate custom structured element
        footprint = np.asarray(footprint, dtype=bool)
        # Must specify neighbors for all dimensions
        if footprint.ndim != ndim:
            raise ValueError(
                "number of dimensions in image and footprint do not"
                "match"
            )
        # Must only specify direct neighbors
        if enforce_adjacency and any(s != 3 for s in footprint.shape):
            raise ValueError("dimension size in footprint is not 3")
        elif any((s % 2 != 1) for s in footprint.shape):
            raise ValueError("footprint size must be odd along all dimensions")

    return footprint
def get_wrapped_command(self):
        
        
        
        command = (
            ' '.join(
                self._escape_command(part) if self.escape_command else part
                for part in self.command
            )
        )
        return f"/bin/bash -c '{command}'"
def get_wrapped_command(self):
        
        
        
        prefix = ''
        if self.bin_path:
            bin_path = self._escape_command(self.bin_path)
            prefix += f'PATH={bin_path}:$PATH '

        command = (
            ' '.join(
                self._escape_command(part) if self.escape_command else part
                for part in self.command
            )
        )
        return (
            "/bin/sh -c '{prefix}{cmd}'".format(
                prefix=prefix,
                cmd=command,
            )
        )
def run(self):
        
        
        
        log.info("Running: '%s' [%s]", self.get_command(), self.cwd)

        self.start_time = datetime.utcnow()
        stdout = subprocess.PIPE
        stderr = subprocess.PIPE
        stdin = None
        if self.input_data is not None:
            stdin = subprocess.PIPE
        if self.combine_output:
            stderr = subprocess.STDOUT

        environment = {}
        environment.update(self.environment)
        environment['READTHEDOCS'] = 'True'
        if self.build_env is not None:
            environment['READTHEDOCS_VERSION'] = self.build_env.version.slug
            environment['READTHEDOCS_PROJECT'] = self.build_env.project.slug
        if 'DJANGO_SETTINGS_MODULE' in environment:
            del environment['DJANGO_SETTINGS_MODULE']
        if 'PYTHONPATH' in environment:
            del environment['PYTHONPATH']
        if self.bin_path is not None:
            env_paths = environment.get('PATH', '').split(':')
            env_paths.insert(0, self.bin_path)
            environment['PATH'] = ':'.join(env_paths)

        try:
            proc = subprocess.Popen(
                self.command,
                shell=self.shell,
                cwd=self.cwd,
                stdin=stdin,
                stdout=stdout,
                stderr=stderr,
                env=environment,
            )
            cmd_input = None
            if self.input_data is not None:
                cmd_input = self.input_data

            if isinstance(cmd_input, six.string_types):
                cmd_input_bytes = cmd_input.encode('utf-8')
            else:
                cmd_input_bytes = cmd_input
            cmd_output = proc.communicate(input=cmd_input_bytes)
            (cmd_stdout, cmd_stderr) = cmd_output
            try:
                self.output = cmd_stdout.decode('utf-8', 'replace')
            except (TypeError, AttributeError):
                self.output = None
            try:
                self.error = cmd_stderr.decode('utf-8', 'replace')
            except (TypeError, AttributeError):
                self.error = None
            self.exit_code = proc.returncode
        except OSError:
            self.error = traceback.format_exc()
            self.output = self.error
            self.exit_code = -1
        finally:
            self.end_time = datetime.utcnow()
def update_build(self, state=None):
        
        
        if not self.record or (state == BUILD_STATE_FINISHED and
                               not self.report_build_success):
            return None

        self.build['project'] = self.project.pk
        self.build['version'] = self.version.pk
        self.build['builder'] = socket.gethostname()
        self.build['state'] = state
        if self.done:
            self.build['success'] = self.successful

            # TODO drop exit_code and provide a more meaningful UX for error
            # reporting
            if self.failure and isinstance(self.failure,
                                           BuildEnvironmentException):
                self.build['exit_code'] = self.failure.status_code
            elif len(self.commands) > 0:
                self.build['exit_code'] = max([cmd.exit_code
                                               for cmd in self.commands])

        self.build['setup'] = self.build['setup_error'] = ""
        self.build['output'] = self.build['error'] = ""

        if self.start_time:
            build_length = (datetime.utcnow() - self.start_time)
            self.build['length'] = build_length.total_seconds()

        if self.failure is not None:
            self.build['error'] = str(self.failure)

        # Attempt to stop unicode errors on build reporting
        for key, val in self.build.items():
            if isinstance(val, basestring):
                self.build[key] = val.decode('utf-8', 'ignore')

        try:
            resp = api_v2.build(self.build['id']).put(self.build)
        except Exception:
            log.error("Unable to post a new build", exc_info=True)
def successful(self):
        
        return (
            self.done and self.failure is None and
            all(cmd.successful for cmd in self.commands)
        )
def sanitize_output(self, output: str) -> str:
        
        
        sanitized = ""
        try:
            # Replace NULL (\x00) character to avoid PostgreSQL db to fail
            # https://code.djangoproject.com/ticket/28201
            sanitized = output.replace("\x00", "")
        except (TypeError, AttributeError):
            pass

        # Chunk the output data to be less than ``DATA_UPLOAD_MAX_MEMORY_SIZE``
        # The length is calculated in bytes, so we need to encode the string first.
        # TODO: we are calculating the length in bytes, but truncating the string
        # in characters. We should use bytes or characters, but not both.
        output_length = len(sanitized.encode("utf-8"))
        # Left some extra space for the rest of the request data
        threshold = 512 * 1024  # 512Kb
        allowed_length = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - threshold
        if output_length > allowed_length:
            log.info(
                'Command output is too big.',
                command=self.get_command(),
            )
            truncated_output = sanitized[-allowed_length:]
            sanitized = (
                '.. (truncated) ...\n'
                f'Output is too big. Truncated at {allowed_length} bytes.\n\n\n'
                f'{truncated_output}'
            )

        return sanitized
def _get_binds(self):
        
        
        
        if getattr(settings, 'RTD_DOCKER_COMPOSE', False):
            from pathlib import Path
            binds = {
                settings.RTD_DOCKER_COMPOSE_VOLUME: {
                    'bind': str(Path(self.project.doc_path).parent),
                    'mode': 'rw',
                },
            }
        else:
            binds = {
                self.project.doc_path: {
                    'bind': self.project.doc_path,
                    'mode': 'rw',
                },
            }

        return binds
def raise_container_error(self, state):
        
        
        
        if state is not None and state.get('Running') is False:
            # TODO: make sure that `state.ExitCode` grabs the exit code from
            # the `sleep ; exit` command. We've noticed that we are not
            # reporting this correctly in different opportunities
            if state.get('ExitCode') == DOCKER_TIMEOUT_EXIT_CODE:
                raise BuildUserError(
                    _('Build exited due to time out'),
                )
            elif state.get('OOMKilled', False):
                raise BuildUserError(
                    _('Build exited due to excessive memory consumption'),
                )
            elif state.get('Error'):
                raise BuildAppError(
                    (
                        _('Build exited due to unknown error: {0}')
                        .format(state.get('Error')),
                    )
                )
def install(self, arg1, arg2=None):
        
        
        
        if arg2 is None:
            device_ip, apk_url = None, arg1
        else:
            device_ip, apk_url = arg1, arg2
        u = u2.connect(device_ip)
        pkg_name = u.app_install(apk_url)
        print("Installed", pkg_name)
def export_onnx_model(cfg, model, inputs):
    
    
    
    return Caffe2Tracer(cfg, model, inputs).export_onnx()
def _convert_arff_data(arff, col_slice_x, col_slice_y, shape=None):
    
    
    
    arff_data = arff['data']
    if isinstance(arff_data, Generator):
        if shape[0] == -1:
            count = -1
        else:
            count = shape[0] * shape[1]
        data = np.fromiter(itertools.chain.from_iterable(arff_data),
                           dtype='float64', count=count)
        data = data.reshape(*shape)
        X = data[:, col_slice_x]
        y = data[:, col_slice_y]
        return X, y
    elif isinstance(arff_data, tuple):
        arff_data_X = _split_sparse_columns(arff_data, col_slice_x)
        num_obs = max(arff_data[1]) + 1
        X_shape = (num_obs, len(col_slice_x))
        X = scipy.sparse.coo_matrix(
            (arff_data_X[0], (arff_data_X[1], arff_data_X[2])),
            shape=X_shape, dtype=np.float64)
        X = X.tocsr()
        y = _sparse_data_to_array(arff_data, col_slice_y)
        return X, y
    else:
        # This should never happen
        raise ValueError('Unexpected Data Type obtained from arff.')
def _download_data_to_bunch(
    url: str,
    sparse: bool,
    data_home: Optional[str],
    *,
    as_frame: bool,
    openml_columns_info: List[dict],
    data_columns: List[str],
    target_columns: List[str],
    shape: Optional[Tuple[int, int]],
    md5_checksum: str,
    n_retries: int = 3,
    delay: float = 1.0,
    parser: str,
    read_csv_kwargs: Optional[Dict] = None,
):
    
    
    # Prepare which columns and data types should be returned for the X and y
    features_dict = {feature["name"]: feature for feature in openml_columns_info}

    if sparse:
        output_type = "sparse"
    elif as_frame:
        output_type = "pandas"
    else:
        output_type = "numpy"

    # XXX: target columns should all be categorical or all numeric
    _verify_target_data_type(features_dict, target_columns)
    for name in target_columns:
        column_info = features_dict[name]
        n_missing_values = int(column_info["number_of_missing_values"])
        if n_missing_values > 0:
            raise ValueError(
                f"Target column '{column_info['name']}' has {n_missing_values} missing "
                "values. Missing values are not supported for target columns."
            )

    no_retry_exception = None
    if parser == "pandas":
        # If we get a ParserError with pandas, then we don't want to retry and we raise
        # early.
        from pandas.errors import ParserError

        no_retry_exception = ParserError

    X, y, frame, categories = _retry_with_clean_cache(
        url, data_home, no_retry_exception
    )(_load_arff_response)(
        url,
        data_home,
        parser=parser,
        output_type=output_type,
        openml_columns_info=features_dict,
        feature_names_to_select=data_columns,
        target_names_to_select=target_columns,
        shape=shape,
        md5_checksum=md5_checksum,
        n_retries=n_retries,
        delay=delay,
        read_csv_kwargs=read_csv_kwargs,
    )

    return Bunch(
        data=X,
        target=y,
        frame=frame,
        categories=categories,
        feature_names=data_columns,
        target_names=target_columns,
    )
def _get_data_info_by_name(
    name: str,
    version: Union[int, str],
    data_home: Optional[str],
    n_retries: int = 3,
    delay: float = 1.0,
):
    
    

    
    if version == "active":
        # situation in which we return the oldest active version
        url = _SEARCH_NAME.format(name) + "/status/active/"
        error_msg = "No active dataset {} found.".format(name)
        json_data = _get_json_content_from_openml_api(
            url,
            error_msg,
            data_home=data_home,
            n_retries=n_retries,
            delay=delay,
        )
        res = json_data["data"]["dataset"]
        if len(res) > 1:
            warn(
                "Multiple active versions of the dataset matching the name"
                " {name} exist. Versions may be fundamentally different, "
                "returning version"
                " {version}.".format(name=name, version=res[0]["version"])
            )
        return res[0]

    # an integer version has been provided
    url = (_SEARCH_NAME + "/data_version/{}").format(name, version)
    try:
        json_data = _get_json_content_from_openml_api(
            url,
            error_message=None,
            data_home=data_home,
            n_retries=n_retries,
            delay=delay,
        )
    except OpenMLError:
        # we can do this in 1 function call if OpenML does not require the
        # specification of the dataset status (i.e., return datasets with a
        # given name / version regardless of active, deactivated, etc. )
        # TODO: feature request OpenML.
        url += "/status/deactivated"
        error_msg = "Dataset {} with version {} not found.".format(name, version)
        json_data = _get_json_content_from_openml_api(
            url,
            error_msg,
            data_home=data_home,
            n_retries=n_retries,
            delay=delay,
        )

    return json_data["data"]["dataset"][0]
def _create_file(
        self, blueprint: Blueprint, blueprint_path: str, allow_override: bool
    ) -> bool:
        
        

        path = pathlib.Path(
            self.hass.config.path(BLUEPRINT_FOLDER, self.domain, blueprint_path)
        )
        exists = path.exists()

        if not allow_override and exists:
            raise FileAlreadyExists(self.domain, blueprint_path)

        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(blueprint.yaml(), encoding="utf-8")
        return exists
def timedelta_isoformat(td: datetime.timedelta) -> str:
    
    
    
    minutes, seconds = divmod(td.seconds, 60)
    hours, minutes = divmod(minutes, 60)
    return f'{"-" if td.days < 0 else ""}P{abs(td.days)}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'
def _handle_m2m_field_node(self, node, field):
        
        
        
        return [field.rel.to._meta.pk.to_python(
                    c.getAttribute("pk"))
                    for c in node.getElementsByTagName("object")]
def get_distribution_strategy(distribution_strategy="mirrored",
                              num_gpus=0,
                              all_reduce_alg=None,
                              num_packs=1,
                              tpu_address=None,
                              **kwargs):
  
  
  del kwargs
  if num_gpus < 0:
    raise ValueError("`num_gpus` can not be negative.")

  distribution_strategy = distribution_strategy.lower()
  if distribution_strategy == "off":
    if num_gpus > 1:
      raise ValueError("When {} GPUs are specified, distribution_strategy "
                       "flag cannot be set to `off`.".format(num_gpus))
    return None

  if distribution_strategy == "tpu":
    # When tpu_address is an empty string, we communicate with local TPUs.
    cluster_resolver = tpu_initialize(tpu_address)
    return tf.distribute.experimental.TPUStrategy(cluster_resolver)

  if distribution_strategy == "multi_worker_mirrored":
    return tf.distribute.experimental.MultiWorkerMirroredStrategy(
        communication=_collective_communication(all_reduce_alg))

  if distribution_strategy == "one_device":
    if num_gpus == 0:
      return tf.distribute.OneDeviceStrategy("device:CPU:0")
    if num_gpus > 1:
      raise ValueError("`OneDeviceStrategy` can not be used for more than "
                       "one device.")
    return tf.distribute.OneDeviceStrategy("device:GPU:0")

  if distribution_strategy == "mirrored":
    if num_gpus == 0:
      devices = ["device:CPU:0"]
    else:
      devices = ["device:GPU:%d" % i for i in range(num_gpus)]
    return tf.distribute.MirroredStrategy(
        devices=devices,
        cross_device_ops=_mirrored_cross_device_ops(all_reduce_alg, num_packs))

  if distribution_strategy == "parameter_server":
    return tf.distribute.experimental.ParameterServerStrategy()

  raise ValueError("Unrecognized Distribution Strategy: %r" %
                   distribution_strategy)
def __init__(self, params, modifier_rank=None, fwd_module=None, enabled=True):
        
        

        self.enabled = enabled
        if not enabled:
            return

        if not (isinstance(params, list) or isinstance(params, tuple)):
            params = [params]

        # enable if at least one is zero-param, otherwise a noop
        if not any(is_zero_param(p) for p in params):
            self.enabled = False
            return

        self.params = [p for p in params if hasattr(p, "ds_id")]
        self.src_rank = None
        if modifier_rank is not None:
            if self.params[0].ds_process_group == dist.get_world_group():
                self.src_rank = modifier_rank
            else:
                # A group was specified; convert DP rank to global rank
                self.src_rank = dist.get_global_rank(self.params[0].ds_process_group,
                                                     modifier_rank)
        self.fwd_module = fwd_module
        if self.fwd_module is not None:
            # is a no-op if already registered
            for p in self.params:
                register_external_parameter(self.fwd_module, p)
def __init__(self,
                 module=None,
                 data_parallel_group=None,
                 mem_efficient_linear=True,
                 remote_device=None,
                 pin_memory=False,
                 config_dict_or_path=None,
                 config=None,
                 enabled=True,
                 dtype=None,
                 mpu=None):
        
        
        if config is not None:
            config_dict_or_path = config
            logger.warning(
                f'zero.Init: the `config` argument is deprecated. Please use `config_dict_or_path` instead.'
            )

        _ds_config = DeepSpeedConfig(config_dict_or_path,
                                     mpu) if config_dict_or_path is not None else None
        super().__init__(enabled=enabled,
                         mem_efficient_linear=mem_efficient_linear,
                         ds_config=_ds_config,
                         dtype=dtype)
        if not torch.distributed.is_initialized():
            init_distributed()
            assert torch.distributed.is_initialized(), "Parameters cannot be scattered without initializing torch.distributed"
        if data_parallel_group is None:
            self.ds_process_group = torch.distributed.group.WORLD
        else:
            self.ds_process_group = data_parallel_group

        self.rank = torch.distributed.get_rank(group=self.ds_process_group)
        self.world_size = torch.distributed.get_world_size(group=self.ds_process_group)

        # Local device is the device where the parameters are consumed
        # It is the device where parameters are fully instantiated using allgather
        self.local_device = torch.device('cuda:{}'.format(os.environ["LOCAL_RANK"]))

        if _ds_config is not None and _ds_config.zero_config.offload_param is not None:
            remote_device = _ds_config.zero_config.offload_param[OFFLOAD_PARAM_DEVICE]
            pin_memory = _ds_config.zero_config.offload_param[OFFLOAD_PARAM_PIN_MEMORY]

        self._validate_remote_device(remote_device, _ds_config)

        # Remote device is the device where parameter partiitons are stored
        # It can be same as local_device or it could be CPU or NVMe.
        self.remote_device = self.local_device if remote_device is None else remote_device
        self.pin_memory = pin_memory if (
            self.remote_device == OFFLOAD_CPU_DEVICE) else False

        # Enable fp16 param swapping to NVMe
        if self.remote_device == OFFLOAD_NVME_DEVICE:
            self.param_swapper = AsyncPartitionedParameterSwapper(_ds_config)
        else:
            self.param_swapper = None

        # If we are provided an already-allocated module to prepare.
        if module is not None:
            assert isinstance(module, torch.nn.Module)
            self._convert_to_zero_parameters(module.parameters(recurse=True))

        self.use_all_gather_base = False
        try:
            from torch.distributed.distributed_c10d import _all_gather_base as all_gather
            self.use_all_gather_base = True
        except:
            logger.info(
                f"_all_gather_base API is not available in torch {torch.__version__}")
def shutdown_init_context():
    
    
    
    if top_level_context:
        top_level_context.unpatch_init_and_builtins()
def stderr(self):
        
        
        
        stream = self._log_file_handle if self._log_file_handle else osutils.stderr()
        return StreamWriter(stream, self._is_debugging)
def _get_debug_context(debug_ports, debug_args, debugger_path, container_env_vars):
        
        
        
        if debug_ports and debugger_path:
            try:
                debugger = Path(debugger_path).resolve(strict=True)
            except OSError as error:
                if error.errno == errno.ENOENT:
                    raise DebugContextException("'{}' could not be found.".format(debugger_path)) from error

                raise error

            if not debugger.is_dir():
                raise DebugContextException("'{}' should be a directory with the debugger in it.".format(debugger_path))
            debugger_path = str(debugger)

        return DebugContext(
            debug_ports=debug_ports,
            debug_args=debug_args,
            debugger_path=debugger_path,
            container_env_vars=container_env_vars,
        )
def __exit__(self, *args):
        
        
        

        if self._log_file_handle:
            self._log_file_handle.close()
            self._log_file_handle = None

        if self._containers_mode == ContainersMode.WARM:
            self._clean_running_containers_and_related_resources()
def _get_container_manager(docker_network, skip_pull_image, shutdown):
        
        
        

        return ContainerManager(
            docker_network_id=docker_network, skip_pull_image=skip_pull_image, do_shutdown_event=shutdown
        )
def _clean_running_containers_and_related_resources(self) -> None:
        
        
        
        cast(WarmLambdaRuntime, self.lambda_runtime).clean_running_containers_and_related_resources()
def node_name(self) -> str:
        
        
        return f"{self.sketch_name} {self.node_id}"
def test_wavelet_denoising_scaling(dtype, convert2ycbcr, estimate_sigma):
    
    rstate = np.random.RandomState(1234)
    # clean signal in range [0, 255]
    x = np.linspace(0, 255, 1024, dtype=dtype)

    # add noise and clip to original signal range
    sigma = 25.
    noisy = x + sigma * rstate.randn(x.size)
    noisy = np.clip(noisy, x.min(), x.max())
    noisy = noisy.astype(x.dtype)

    if estimate_sigma:
        if convert2ycbcr:
            # YCbCr expects a sigma appropriate to data in the range [0, 1]
            sigma_est = restoration.estimate_sigma(noisy/x.max())
        else:
            sigma_est = restoration.estimate_sigma(noisy)
    else:
        sigma_est = None
    denoised = restoration.denoise_wavelet(noisy,
                                           sigma=sigma_est,
                                           wavelet='sym4',
                                           multichannel=False,
                                           convert2ycbcr=convert2ycbcr)

    data_range = x.max() - x.min()
    psnr_noisy = compare_psnr(x, noisy, data_range=data_range)
    if np.dtype(dtype).kind == 'f':
        psnr_denoised = compare_psnr(x, denoised, data_range=data_range)

        # output's max value is not substantially smaller than signal's
        assert_(denoised.max() > 0.9 * x.max())
    else:
        # have to compare to x_as_float in integer input cases
        x_as_float = img_as_float(x)
        f_data_range = x_as_float.max() - x_as_float.min()
        psnr_denoised = compare_psnr(x_as_float, denoised,
                                     data_range=f_data_range)

        # output has been clipped to expected range
        assert_(denoised.max() <= 1.0)
        if np.dtype(dtype).kind == 'u':
            assert_(denoised.min() >= 0)
        else:
            assert_(denoised.min() >= -1)

    assert_(psnr_denoised > psnr_noisy)
def test_wavelet_denoising_scaling(case, dtype, convert2ycbcr,
                                   estimate_sigma):
    
    rstate = np.random.RandomState(1234)

    if case == '1d':
        # 1D single-channel in range [0, 255]
        x = np.linspace(0, 255, 1024)
    elif case == '2d multichannel':
        # 2D multichannel in range [0, 255]
        x = data.astronaut()[:64, :64]
    x = x.astype(dtype)

    # add noise and clip to original signal range
    sigma = 25.
    noisy = x + sigma * rstate.randn(*x.shape)
    noisy = np.clip(noisy, x.min(), x.max())
    noisy = noisy.astype(x.dtype)

    multichannel = x.shape[-1] == 3

    if estimate_sigma:
        if convert2ycbcr:
            # YCbCr expects a sigma appropriate to data in the range [0, 1]
            sigma_est = restoration.estimate_sigma(noisy/x.max(),
                                                   multichannel=multichannel)
        else:
            sigma_est = restoration.estimate_sigma(noisy,
                                                   multichannel=multichannel)
    else:
        sigma_est = None

    if convert2ycbcr and not multichannel:
        # YCbCr requires multichannel == True
        with testing.raises(ValueError):
            denoised = restoration.denoise_wavelet(noisy,
                                                   sigma=sigma_est,
                                                   wavelet='sym4',
                                                   multichannel=multichannel,
                                                   convert2ycbcr=convert2ycbcr)
        return

    denoised = restoration.denoise_wavelet(noisy,
                                           sigma=sigma_est,
                                           wavelet='sym4',
                                           multichannel=multichannel,
                                           convert2ycbcr=convert2ycbcr)

    data_range = x.max() - x.min()
    psnr_noisy = compare_psnr(x, noisy, data_range=data_range)
    clipped = np.dtype(dtype).kind != 'f'
    if not clipped:
        psnr_denoised = compare_psnr(x, denoised, data_range=data_range)

        # output's max value is not substantially smaller than x's
        assert_(denoised.max() > 0.9 * x.max())
    else:
        # have to compare to x_as_float in integer input cases
        x_as_float = img_as_float(x)
        f_data_range = x_as_float.max() - x_as_float.min()
        psnr_denoised = compare_psnr(x_as_float, denoised,
                                     data_range=f_data_range)

        # output has been clipped to expected range
        assert_(denoised.max() <= 1.0)
        if np.dtype(dtype).kind == 'u':
            assert_(denoised.min() >= 0)
        else:
            assert_(denoised.min() >= -1)

    assert_(psnr_denoised > psnr_noisy)
def tcm(
        self,
        start_date: Annotated[
            Union[datetime.date, None, str],
            OpenBBCustomParameter(
                description="Start date of the data, in YYYY-MM-DD format."
            ),
        ] = None,
        end_date: Annotated[
            Union[datetime.date, None, str],
            OpenBBCustomParameter(
                description="End date of the data, in YYYY-MM-DD format."
            ),
        ] = None,
        maturity: Annotated[
            Optional[Literal["3m", "2y"]],
            OpenBBCustomParameter(description="The maturity"),
        ] = "3m",
        provider: Optional[Literal["fred"]] = None,
        **kwargs
    ) -> OBBject:
        
          # noqa: E501

        return self._run(
            "/fixedincome/spreads/tcm",
            **filter_inputs(
                provider_choices={
                    "provider": self._get_provider(
                        provider,
                        "/fixedincome/spreads/tcm",
                        ("fred",),
                    )
                },
                standard_params={
                    "start_date": start_date,
                    "end_date": end_date,
                    "maturity": maturity,
                },
                extra_params=kwargs,
            )
        )
def query(self, left: int, right: int) -> T | None:
        
        
        
        left, right = left + self.N, right + self.N

        res: T | None = None
        while left <= right:
            if left % 2 == 1:
                res = self.st[left] if res is None else self.fn(res, self.st[left])
            if right % 2 == 0:
                res = self.st[right] if res is None else self.fn(res, self.st[right])
            left, right = (left + 1) // 2, (right - 1) // 2
        return res
def _calculate_transition_prob(self, current, delta):
        
        
        new_position = np.array(current) + np.array(delta)
        new_position = self._limit_coordinates(new_position).astype(int)
        new_state = np.ravel_multi_index(tuple(new_position), self.shape)
        if self._cliff[tuple(new_position)]:
            return [(1.0, self.start_state_index, -100, False)]

        terminal_state = (self.shape[0] - 1, self.shape[1] - 1)
        is_terminated = tuple(new_position) == terminal_state
        return [(1.0, new_state, -1, is_terminated)]
def _wordwise_detokenize(tokenizer, sequence, output_separator, token_separator):
    

    
    if isinstance(sequence, str) and sequence == "":
        return ""
    if token_separator not in sequence:
        sequence_list = (
            sequence if isinstance(sequence, list) else sequence.tolist()
        )
        return tokenizer.sp.decode_ids(sequence_list)
    words = list(_split_list(sequence, token_separator))
    encoded_words = [
        tokenizer.sp.decode_ids(word_tokens) for word_tokens in words
    ]
    return output_separator.join(encoded_words)
def handle_audio_end(event):
    
        
    
    if config.get("listener").get("mute_during_output"):
        loop.unmute()
def server_context():
    
    
    context = ssl.SSLContext(ssl.PROTOCOL_TLS)  # pylint: disable=no-member

    context.options |= (
        ssl.OP_NO_SSLv2 | ssl.OP_NO_SSLv3 |
        ssl.OP_NO_TLSv1 | ssl.OP_NO_TLSv1_1 |
        ssl.OP_CIPHER_SERVER_PREFERENCE
    )
    if hasattr(ssl, 'OP_NO_COMPRESSION'):
        context.options |= ssl.OP_NO_COMPRESSION

    context.set_ciphers(
        "ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:"
        "ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:"
        "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:"
        "ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:"
        "ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256"
    )
    return context
def initialize(self, version: str, build_data: dict[str, Any]) -> None:
        
        
        
        self._process_all_built_in_extras(version)
        self._process_all_provider_extras(version)

        # Adds all-core extras for the extras that are built-in and not devel
        self.optional_dependencies["all-core"] = sorted(
            set([f"apache-airflow[{extra}]" for extra in CORE_EXTRAS.keys()])
        )
        # Adds "apache-airflow[extra]" for all extras that are not devel extras for wheel and editable builds
        self.optional_dependencies["all"] = [
            f"apache-airflow[{extra}]" for extra in sorted(self.all_non_devel_extras)
        ]
        # Adds all devel extras for the extras that are built-in only for editable builds
        if version != "standard":
            self.optional_dependencies["devel-all"] = [
                f"apache-airflow[{extra}]" for extra in sorted(self.all_devel_extras)
            ]
        # This is special dependency that is used to install all possible
        # 3rd-party dependencies for airflow for the CI image. It is exposed in the wheel package
        # because we want to use for building the image cache from GitHub URL.
        self.optional_dependencies["devel-ci"] = sorted(self.all_devel_ci_dependencies)
        self._dependencies = DEPENDENCIES

        if version == "standard":
            # Inject preinstalled providers into the dependencies for standard packages
            self._dependencies.extend(PREINSTALLED_PROVIDER_REQUIREMENTS)
            self._dependencies.extend(PREINSTALLED_NOT_READY_PROVIDER_DEPS)
        else:
            self._dependencies.extend(ALL_PREINSTALLED_PROVIDER_DEPS)

        # with hatchling, we can modify dependencies dynamically by modifying the build_data
        build_data["dependencies"] = self._dependencies

        # unfortunately hatchling currently does not have a way to override optional_dependencies
        # via build_data (or so it seem) so we need to modify internal _optional_dependencies
        # field in core.metadata until this is possible
        self.metadata.core._optional_dependencies = self.optional_dependencies
def __init__(self, sensor_type, name, serial_number, unit, coordinator):
        
        self._type = sensor_type
        self._name = name
        self._serial_number = serial_number
        self._unit_of_measurement = unit

        super().__init__(coordinator)
def __init__(
        self,
        vocab: Vocab,
        model: Model[Tuple[List[Doc], Ragged], Floats2d],
        suggester: Suggester,
        name: str = "spancat",
        *,
        add_negative_label: bool = False,
        spans_key: str = "spans",
        negative_weight: Optional[float] = 1.0,
        allow_overlap: Optional[bool] = True,
        max_positive: Optional[int] = None,
        threshold: Optional[float] = 0.5,
        scorer: Optional[Callable] = spancat_score,
    ) -> None:
        
        
        self.cfg = {
            "labels": [],
            "spans_key": spans_key,
            "threshold": threshold,
            "max_positive": max_positive,
            "negative_weight": negative_weight,
            "allow_overlap": allow_overlap,
        }
        self.vocab = vocab
        self.suggester = suggester
        self.model = model
        self.name = name
        self.scorer = scorer
        self.add_negative_label = add_negative_label
        if not allow_overlap and max_positive is not None and max_positive > 1:
            raise ValueError(Errors.E1051.format(max_positive=max_positive))
def initialize(
        self,
        get_examples: Callable[[], Iterable[Example]],
        *,
        nlp: Optional[Language] = None,
        labels: Optional[List[str]] = None,
    ) -> None:
        
        
        subbatch: List[Example] = []
        if labels is not None:
            for label in labels:
                self.add_label(label)
        for eg in get_examples():
            if labels is None:
                for span in eg.reference.spans.get(self.key, []):
                    self.add_label(span.label_)
            if len(subbatch) < 10:
                subbatch.append(eg)
        self._require_labels()
        if subbatch:
            docs = [eg.x for eg in subbatch]
            spans = self.suggester(docs)
            Y = self.model.ops.alloc2f(spans.dataXd.shape[0], len(self.labels))
            self.model.initialize(X=(docs, spans), Y=Y)
        else:
            self.model.initialize()
def gating_distance(self,
                        mean: np.ndarray,
                        covariance: np.ndarray,
                        measurements: np.ndarray,
                        only_position: bool = False,
                        metric: str = 'maha') -> np.ndarray:
        
        
        
        mean, covariance = self.project(mean, covariance)
        if only_position:
            mean, covariance = mean[:2], covariance[:2, :2]
            measurements = measurements[:, :2]

        d = measurements - mean
        if metric == 'gaussian':
            return np.sum(d * d, axis=1)
        elif metric == 'maha':
            cholesky_factor = np.linalg.cholesky(covariance)
            z = scipy.linalg.solve_triangular(cholesky_factor, d.T, lower=True, check_finite=False, overwrite_b=True)
            return np.sum(z * z, axis=0)  # square maha
        else:
            raise ValueError('Invalid distance metric')
def quote(
        self,
        symbol: Annotated[
            Union[str, List[str]],
            OpenBBCustomParameter(
                description="Symbol to get data for. This endpoint will accept multiple symbols separated by commas."
            ),
        ],
        provider: Optional[Literal["fmp", "intrinio", "yfinance"]] = None,
        **kwargs
    ) -> OBBject:
        
          # noqa: E501

        return self._run(
            "/equity/price/quote",
            **filter_inputs(
                provider_choices={
                    "provider": provider,
                },
                standard_params={
                    "symbol": ",".join(symbol) if isinstance(symbol, list) else symbol,
                },
                extra_params=kwargs,
            )
        )
def historical(
        self,
        symbol: Annotated[
            Union[str, List[str]],
            OpenBBCustomParameter(
                description="Symbol to get data for. Multiple items allowed for provider(s): fmp, polygon, tiingo, yfinance."
            ),
        ],
        interval: Annotated[
            Optional[str],
            OpenBBCustomParameter(description="Time interval of the data to return."),
        ] = "1d",
        start_date: Annotated[
            Union[datetime.date, None, str],
            OpenBBCustomParameter(
                description="Start date of the data, in YYYY-MM-DD format."
            ),
        ] = None,
        end_date: Annotated[
            Union[datetime.date, None, str],
            OpenBBCustomParameter(
                description="End date of the data, in YYYY-MM-DD format."
            ),
        ] = None,
        provider: Optional[
            Literal["fmp", "intrinio", "polygon", "tiingo", "yfinance"]
        ] = None,
        **kwargs
    ) -> OBBject:
        
          # noqa: E501

        return self._run(
            "/equity/price/historical",
            **filter_inputs(
                provider_choices={
                    "provider": self._get_provider(
                        provider,
                        "/equity/price/historical",
                        ("fmp", "intrinio", "polygon", "tiingo", "yfinance"),
                    )
                },
                standard_params={
                    "symbol": symbol,
                    "interval": interval,
                    "start_date": start_date,
                    "end_date": end_date,
                },
                extra_params=kwargs,
                extra_info={
                    "symbol": {
                        "multiple_items_allowed": [
                            "fmp",
                            "polygon",
                            "tiingo",
                            "yfinance",
                        ]
                    }
                },
            )
        )
def forward(self, x: Tensor) -> Tensor:
        

        identity = x

        out = self.conv1(x)
        if self.with_norm:
            out = self.norm1(out)
        out = self.relu(out)

        out = self.conv2(out)
        if self.with_norm:
            out = self.norm2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity

        return out
def is_on(self) -> bool | None:
        
        
        props = self.fibaro_device.properties
        if self.current_binary_state:
            return True
        if "brightness" in props and props.brightness != "0":
            return True
        if "currentProgram" in props and props.currentProgram != "0":
            return True
        if "currentProgramID" in props and props.currentProgramID != "0":
            return True

        return False
def __call__(self, x, t, reduce='sum'):
        

        
        return negative_sampling.negative_sampling(
            x, t, self.W, self.sampler.sample, self.sample_size,
            reduce=reduce)
def forward(self, x, t, reduce='sum', **kwargs):
        

        
        return_samples = False
        if kwargs:
            return_samples, = argument.parse_kwargs(
                kwargs, ('return_samples', return_samples))

        ret = negative_sampling.negative_sampling(
            x, t, self.W, self.sampler.sample, self.sample_size,
            reduce=reduce, return_samples=return_samples)
        return ret
def __init__(
        self,
        dataset: data.Dataset,
        sampler: Sampler,
        shard_sampler: bool = True,
        shard_chunk_size: int = 1,
    ):
        
        
        
        assert not isinstance(dataset, data.IterableDataset), dataset
        assert isinstance(sampler, Sampler), sampler
        self.dataset = dataset
        self.sampler = sampler
        self.shard_sampler = shard_sampler
        self.shard_chunk_size = shard_chunk_size
def century(self, thai_digit: bool = False, buddhist_era: bool = True) -> str:
        
        
        
        end_century = 22
        if buddhist_era:
            end_century = 26
        text = str(self.random_element(range(1, end_century)))
        if thai_digit:
            text = text.translate(_HA_TH_DIGITS)
        return text
def _save_image_gts_results(self,
                                dataset,
                                results,
                                performances,
                                out_dir=None,
                                task='det'):
        
        
        mkdir_or_exist(out_dir)

        for performance_info in performances:
            index, performance = performance_info
            data_info = dataset[index]
            data_info['gt_instances'] = data_info['instances']

            # calc save file path
            filename = data_info['img_path']
            fname, name = osp.splitext(osp.basename(filename))
            save_filename = fname + '_' + str(round(performance, 3)) + name
            out_file = osp.join(out_dir, save_filename)

            if task == 'det':
                gt_instances = InstanceData()
                gt_instances.bboxes = results[index]['gt_instances']['bboxes']
                gt_instances.labels = results[index]['gt_instances']['labels']

                pred_instances = InstanceData()
                pred_instances.bboxes = results[index]['pred_instances'][
                    'bboxes']
                pred_instances.labels = results[index]['pred_instances'][
                    'labels']
                pred_instances.scores = results[index]['pred_instances'][
                    'scores']

                data_samples = DetDataSample()
                data_samples.pred_instances = pred_instances
                data_samples.gt_instances = gt_instances

            elif task == 'seg':
                gt_panoptic_seg = PixelData()
                gt_panoptic_seg.sem_seg = results[index]['gt_seg_map']

                pred_panoptic_seg = PixelData()
                pred_panoptic_seg.sem_seg = results[index][
                    'pred_panoptic_seg']['sem_seg']

                data_samples = DetDataSample()
                data_samples.pred_panoptic_seg = pred_panoptic_seg
                data_samples.gt_panoptic_seg = gt_panoptic_seg

            self.visualizer.add_datasample(
                'image',
                results[index]['img'],
                data_samples,
                show=self.show,
                draw_gt=False,
                pred_score_thr=self.score_thr,
                out_file=out_file)
def _is_cache_enabled(self, project):
        
        plan_has_cdn = PlanFeature.objects.get_feature(
            obj=project, type=PlanFeature.TYPE_CDN
        )
        return settings.ALLOW_PRIVATE_REPOS and (
            plan_has_cdn or project.has_feature(Feature.CDN_ENABLED)
        )
def mute_and_speak(utterance):
    
        
    
    global tts_hash

    # update TTS object if configuration has changed
    if tts_hash != hash(str(config.get('tts', ''))):
        global tts
        # Stop tts playback thread
        tts.playback.stop()
        tts.playback.join()
        # Create new tts instance
        tts = TTSFactory.create()
        tts.init(ws)
        tts_hash = hash(str(config.get('tts', '')))

    LOG.info("Speak: " + utterance)
    tts.execute(utterance)
def mute_and_speak(utterance, ident):
    
        
    
    global tts_hash

    # update TTS object if configuration has changed
    if tts_hash != hash(str(config.get('tts', ''))):
        global tts
        # Stop tts playback thread
        tts.playback.stop()
        tts.playback.join()
        # Create new tts instance
        tts = TTSFactory.create()
        tts.init(ws)
        tts_hash = hash(str(config.get('tts', '')))

    LOG.info("Speak: " + utterance)
    tts.execute(utterance, ident)
def init(messagebus):
     
    

    global bus
    global tts
    global tts_hash
    global config

    bus = messagebus
    Configuration.init(bus)
    config = Configuration.get()
    bus.on('mycroft.stop', handle_stop)
    bus.on('mycroft.audio.speech.stop', handle_stop)
    bus.on('speak', handle_speak)
    bus.on('mycroft.mic.listen', _start_listener)

    tts = TTSFactory.create()
    tts.init(bus)
    tts_hash = config.get('tts')
def fetch_balance(self, params={}):
        
        
        
        self.load_markets()
        response = self.v3PrivateGetBalances(params)
        #
        #     {
        #         "success": True,
        #         "data": {
        #             "holding": [
        #                 {
        #                     "token": "0_token",
        #                     "holding": 1,
        #                     "frozen": 0,
        #                     "staked": 0,
        #                     "unbonding": 0,
        #                     "vault": 0,
        #                     "interest": 0,
        #                     "pendingShortQty": 0,
        #                     "pendingLongQty": 0,
        #                     "availableBalance": 0,
        #                     "updatedTime": 312321.121
        #                 }
        #             ]
        #         },
        #         "timestamp": 1673323746259
        #     }
        #
        data = self.safe_value(response, 'data')
        return self.parse_balance(data)
def cancel_all_orders(self, symbol: Optional[str] = None, params={}):
        
        
        
        self.load_markets()
        stop = self.safe_value(params, 'stop')
        params = self.omit(params, 'stop')
        if stop:
            return self.v3PrivateDeleteAlgoOrdersPending(params)
        self.check_required_symbol('cancelOrders', symbol)
        market = self.market(symbol)
        request = {
            'symbol': market['id'],
        }
        response = self.v1PrivateDeleteOrders(self.extend(request, params))
        #
        #     {
        #         "success":true,
        #         "status":"CANCEL_ALL_SENT"
        #     }
        #
        return response
def create_order(self, symbol: str, type: OrderType, side: OrderSide, amount, price=None, params={}):
        
        
        
        reduceOnly = self.safe_value_2(params, 'reduceOnly', 'reduce_only')
        params = self.omit(params, ['reduceOnly', 'reduce_only'])
        orderType = type.upper()
        self.load_markets()
        market = self.market(symbol)
        orderSide = side.upper()
        request = {
            'symbol': market['id'],
            'side': orderSide,
        }
        stopPrice = self.safe_number_2(params, 'triggerPrice', 'stopPrice')
        stopLoss = self.safe_value(params, 'stopLoss')
        takeProfit = self.safe_value(params, 'takeProfit')
        algoType = self.safe_string(params, 'algoType')
        trailingTriggerPrice = self.safe_string_2(params, 'trailingTriggerPrice', 'activatedPrice', price)
        trailingAmount = self.safe_string_2(params, 'trailingAmount', 'callbackValue')
        trailingPercent = self.safe_string_2(params, 'trailingPercent', 'callbackRate')
        isTrailingAmountOrder = trailingAmount is not None
        isTrailingPercentOrder = trailingPercent is not None
        isTrailing = isTrailingAmountOrder or isTrailingPercentOrder
        isStop = isTrailing or stopPrice is not None or stopLoss is not None or takeProfit is not None or (self.safe_value(params, 'childOrders') is not None)
        isMarket = orderType == 'MARKET'
        timeInForce = self.safe_string_lower(params, 'timeInForce')
        postOnly = self.is_post_only(isMarket, None, params)
        reduceOnlyKey = 'reduceOnly' if isStop else 'reduce_only'
        clientOrderIdKey = 'clientOrderId' if isStop else 'client_order_id'
        orderQtyKey = 'quantity' if isStop else 'order_quantity'
        priceKey = 'price' if isStop else 'order_price'
        typeKey = 'type' if isStop else 'order_type'
        request[typeKey] = orderType  # LIMIT/MARKET/IOC/FOK/POST_ONLY/ASK/BID
        if not isStop:
            if postOnly:
                request['order_type'] = 'POST_ONLY'
            elif timeInForce == 'fok':
                request['order_type'] = 'FOK'
            elif timeInForce == 'ioc':
                request['order_type'] = 'IOC'
        if reduceOnly:
            request[reduceOnlyKey] = reduceOnly
        if price is not None:
            request[priceKey] = self.price_to_precision(symbol, price)
        if isMarket and not isStop:
            # for market buy it requires the amount of quote currency to spend
            if market['spot'] and orderSide == 'BUY':
                quoteAmount = None
                createMarketBuyOrderRequiresPrice = True
                createMarketBuyOrderRequiresPrice, params = self.handle_option_and_params(params, 'createOrder', 'createMarketBuyOrderRequiresPrice', True)
                cost = self.safe_number_2(params, 'cost', 'order_amount')
                params = self.omit(params, ['cost', 'order_amount'])
                if cost is not None:
                    quoteAmount = self.cost_to_precision(symbol, cost)
                elif createMarketBuyOrderRequiresPrice:
                    if price is None:
                        raise InvalidOrder(self.id + ' createOrder() requires the price argument for market buy orders to calculate the total cost to spend(amount * price), alternatively set the createMarketBuyOrderRequiresPrice option or param to False and pass the cost to spend(quote quantity) in the amount argument')
                    else:
                        amountString = self.number_to_string(amount)
                        priceString = self.number_to_string(price)
                        costRequest = Precise.string_mul(amountString, priceString)
                        quoteAmount = self.cost_to_precision(symbol, costRequest)
                else:
                    quoteAmount = self.cost_to_precision(symbol, amount)
                request['order_amount'] = quoteAmount
            else:
                request['order_quantity'] = self.amount_to_precision(symbol, amount)
        elif algoType != 'POSITIONAL_TP_SL':
            request[orderQtyKey] = self.amount_to_precision(symbol, amount)
        clientOrderId = self.safe_string_n(params, ['clOrdID', 'clientOrderId', 'client_order_id'])
        if clientOrderId is not None:
            request[clientOrderIdKey] = clientOrderId
        if isTrailing:
            if trailingTriggerPrice is None:
                raise ArgumentsRequired(self.id + ' createOrder() requires a trailingTriggerPrice parameter for trailing orders')
            request['activatedPrice'] = self.price_to_precision(symbol, trailingTriggerPrice)
            request['algoType'] = 'TRAILING_STOP'
            if isTrailingAmountOrder:
                request['callbackValue'] = trailingAmount
            elif isTrailingPercentOrder:
                convertedTrailingPercent = Precise.string_div(trailingPercent, '100')
                request['callbackRate'] = convertedTrailingPercent
        elif stopPrice is not None:
            if algoType != 'TRAILING_STOP':
                request['triggerPrice'] = self.price_to_precision(symbol, stopPrice)
                request['algoType'] = 'STOP'
        elif (stopLoss is not None) or (takeProfit is not None):
            request['algoType'] = 'BRACKET'
            outterOrder = {
                'symbol': market['id'],
                'reduceOnly': False,
                'algoType': 'POSITIONAL_TP_SL',
                'childOrders': [],
            }
            closeSide = 'SELL' if (orderSide == 'BUY') else 'BUY'
            if stopLoss is not None:
                stopLossPrice = self.safe_number_2(stopLoss, 'triggerPrice', 'price', stopLoss)
                stopLossOrder = {
                    'side': closeSide,
                    'algoType': 'STOP_LOSS',
                    'triggerPrice': self.price_to_precision(symbol, stopLossPrice),
                    'type': 'CLOSE_POSITION',
                    'reduceOnly': True,
                }
                outterOrder['childOrders'].append(stopLossOrder)
            if takeProfit is not None:
                takeProfitPrice = self.safe_number_2(takeProfit, 'triggerPrice', 'price', takeProfit)
                takeProfitOrder = {
                    'side': closeSide,
                    'algoType': 'TAKE_PROFIT',
                    'triggerPrice': self.price_to_precision(symbol, takeProfitPrice),
                    'type': 'CLOSE_POSITION',
                    'reduceOnly': True,
                }
                outterOrder['childOrders'].append(takeProfitOrder)
            request['childOrders'] = [outterOrder]
        params = self.omit(params, ['clOrdID', 'clientOrderId', 'client_order_id', 'postOnly', 'timeInForce', 'stopPrice', 'triggerPrice', 'stopLoss', 'takeProfit', 'trailingPercent', 'trailingAmount', 'trailingTriggerPrice'])
        response = None
        if isStop:
            response = self.v3PrivatePostAlgoOrder(self.extend(request, params))
        else:
            response = self.v1PrivatePostOrder(self.extend(request, params))
        # {
        #     "success": True,
        #     "timestamp": "1641383206.489",
        #     "order_id": "86980774",
        #     "order_type": "LIMIT",
        #     "order_price": "1",  # null for "MARKET" order
        #     "order_quantity": "12",  # null for "MARKET" order
        #     "order_amount": null,  # NOT-null for "MARKET" order
        #     "client_order_id": "0"
        # }
        # stop orders
        # {
        #     "success": True,
        #     "data": {
        #       "rows": [
        #         {
        #           "orderId": "1578938",
        #           "clientOrderId": "0",
        #           "algoType": "STOP_LOSS",
        #           "quantity": "0.1"
        #         }
        #       ]
        #     },
        #     "timestamp": "1686149372216"
        # }
        data = self.safe_value(response, 'data')
        if data is not None:
            rows = self.safe_value(data, 'rows', [])
            return self.parse_order(rows[0], market)
        order = self.parse_order(response, market)
        order['type'] = type
        return order
def fetch_ohlcv(self, symbol: str, timeframe='1m', since: Int = None, limit: Int = None, params={}) -> List[list]:
        
        
        
        self.load_markets()
        market = self.market(symbol)
        request = {
            'symbol': market['id'],
            'type': self.safe_string(self.timeframes, timeframe, timeframe),
        }
        useHistEndpoint = since is not None
        if (limit is not None) and (since is not None):
            oneThousandCandles = self.parse_timeframe(timeframe) * 1000 * 999  # 999 because there will be delay between self and the request, causing the latest candle to be excluded sometimes
            startWithLimit = self.milliseconds() - oneThousandCandles
            useHistEndpoint = since < startWithLimit
        if useHistEndpoint:
            request['start_time'] = since
        elif limit is not None:  # the hist endpoint does not accept limit
            request['limit'] = min(limit, 1000)
        response = None
        if not useHistEndpoint:
            response = self.v1PublicGetKline(self.extend(request, params))
            #
            #    {
            #        "success": True,
            #        "rows": [
            #            {
            #                "open": "0.94238",
            #                "close": "0.94271",
            #                "low": "0.94238",
            #                "high": "0.94296",
            #                "volume": "73.55",
            #                "amount": "69.32040520",
            #                "symbol": "SPOT_WOO_USDT",
            #                "type": "1m",
            #                "start_timestamp": "1641584700000",
            #                "end_timestamp": "1641584760000"
            #            },
            #            ...
            #        ]
            #    }
            #
        else:
            response = self.v1PubGetHistKline(self.extend(request, params))
            response = self.safe_dict(response, 'data')
            #
            #    {
            #        "success": True,
            #        "data": {
            #            "rows": [
            #                {
            #                    "symbol": "SPOT_BTC_USDT",
            #                    "open": 44181.40000000,
            #                    "close": 44174.29000000,
            #                    "high": 44193.44000000,
            #                    "low": 44148.34000000,
            #                    "volume": 110.11930100,
            #                    "amount": 4863796.24318878,
            #                    "type": "1m",
            #                    "start_timestamp": 1704153600000,
            #                    "end_timestamp": 1704153660000
            #                },
            #                ...
            #            ]
            #        }
            #    }
            #
        rows = self.safe_value(response, 'rows', [])
        return self.parse_ohlcvs(rows, market, timeframe, since, limit)
def fetch_transfers(self, code: Str = None, since: Int = None, limit: Int = None, params={}):
        
        
        
        request = {}
        if limit is not None:
            request['size'] = limit
        if since is not None:
            request['start_t'] = since
        until = self.safe_integer_2(params, 'until', 'till')  # unified in milliseconds
        params = self.omit(params, ['until', 'till'])
        if until is not None:
            request['end_t'] = until
        response = self.v1PrivateGetAssetMainSubTransferHistory(self.extend(request, params))
        #
        #     {
        #         "rows": [
        #             {
        #                 "id": 46704,
        #                 "token": "USDT",
        #                 "amount": 30000.00000000,
        #                 "status": "COMPLETED",
        #                 "from_application_id": "0f1bd3cd-dba2-4563-b8bb-0adb1bfb83a3",
        #                 "to_application_id": "c01e6940-a735-4022-9b6c-9d3971cdfdfa",
        #                 "from_user": "LeverageLow",
        #                 "to_user": "dev",
        #                 "created_time": "1709022325.427",
        #                 "updated_time": "1709022325.542"
        #             }
        #         ],
        #         "meta": {
        #             "total": 50,
        #             "records_per_page": 25,
        #             "current_page": 1
        #         },
        #         "success": True
        #     }
        #
        data = self.safe_list(response, 'rows', [])
        return self.parse_transfers(data, None, since, limit, params)
def close(self, timeout: Optional[float] = None):
        
        
        
        # Cause a call to connection_lost where further cleanup occurs
        if self.transport:
            self.transport.close()
            if timeout is None:
                timeout = self.app.config.GRACEFUL_SHUTDOWN_TIMEOUT
            self.loop.call_later(timeout, self.abort)
def test_mlflow_logger_with_long_param_value(client, _, param, tmpdir):
    
    logger = MLFlowLogger("test", save_dir=tmpdir)

    params = {"test": "test_param" * 50}
    logger.log_hyperparams(params)

    # assert_called_once_with() won't properly check the parameter value.
    logger.experiment.log_batch.assert_called_once()
def test_mlflow_logger_with_many_params(_, mlflow_mock, tmp_path):
    
    logger = MLFlowLogger("test", save_dir=str(tmp_path))

    params = {f"test_{idx}": f"test_param_{idx}" for idx in range(150)}
    logger.log_hyperparams(params)

    assert logger.experiment.log_batch.call_count == 2
def has_same_digits(num1: int, num2: int) -> bool:
    
    
    
    return sorted(str(num1)) == sorted(str(num2))
def call(self, inputs: tf.Tensor, *args, **kwargs) -> tf.Tensor:
        
        
        if self.data_format == 'channels_last':
            pooled = K.min(inputs, axis=[1, 2])
        else:
            pooled = K.min(inputs, axis=[2, 3])
        return pooled
def call(self, inputs, **kwargs):  # pylint:disable=unused-argument
        
        
        input_shape = K.int_shape(inputs)
        if len(input_shape) != 4:
            raise ValueError('Inputs should have rank ' +
                             str(4) +
                             '; Received input shape:', str(input_shape))

        if self.data_format == 'channels_first':
            batch_size, channels, height, width = input_shape
            if batch_size is None:
                batch_size = -1
            r_height, r_width = self.size
            o_height, o_width = height * r_height, width * r_width
            o_channels = channels // (r_height * r_width)

            out = K.reshape(inputs, (batch_size, r_height, r_width, o_channels, height, width))
            out = K.permute_dimensions(out, (0, 3, 4, 1, 5, 2))
            out = K.reshape(out, (batch_size, o_channels, o_height, o_width))
        elif self.data_format == 'channels_last':
            batch_size, height, width, channels = input_shape
            if batch_size is None:
                batch_size = -1
            r_height, r_width = self.size
            o_height, o_width = height * r_height, width * r_width
            o_channels = channels // (r_height * r_width)

            out = K.reshape(inputs, (batch_size, height, width, r_height, r_width, o_channels))
            out = K.permute_dimensions(out, (0, 1, 3, 2, 4, 5))
            out = K.reshape(out, (batch_size, o_height, o_width, o_channels))
        return out
def _get_runtime_from_image(image: str) -> Optional[str]:
    
    
    
    match = re.fullmatch(r"amazon/([a-z0-9.]*)-?([a-z0-9.]*)-base", image)
    if match is None:
        return None
    runtime, base = match.groups()
    if base != "":
        return f"{runtime} ({base})"
    return runtime
def img_as_float(image, force_copy=False):
    

    
    return convert(image, np.float64, force_copy)
def add_items(self, arr_list):
         
        np_arrays = [arr.astype(self._dtype) for arr in arr_list]
        self._data_file.writelines([arr.tobytes(order='C') for arr in np_arrays])
        for arr in np_arrays:
            self._sizes.append(arr.size)
def _request_with_retry(
    method: str,
    url: str,
    max_retries: int = 0,
    base_wait_time: float = 0.5,
    max_wait_time: float = 2,
    timeout: float = 10.0,
    **params,
) -> requests.Response:
    
    
    _raise_if_offline_mode_is_enabled(f"Tried to reach {url}")
    tries, success = 0, False
    while not success:
        tries += 1
        try:
            response = requests.request(method=method.upper(), url=url, timeout=timeout, **params)
            success = True
        except requests.exceptions.ConnectTimeout as err:
            if tries > max_retries:
                raise err
            else:
                logger.info(f"{method} request to {url} timed out, retrying... [{tries/max_retries}]")
                sleep_time = max(max_wait_time, base_wait_time * 2 ** (tries - 1))  # Exponential backoff
                time.sleep(sleep_time)
    return response
def generate_consumer(
    filter_pattern: Optional[str] = None, output: OutputOption = OutputOption.text, resource_name: Optional[str] = None
):
    
    
    
    if output == OutputOption.json:
        return generate_json_consumer()

    return generate_text_consumer(filter_pattern)
def current_cover_position(self):
        
        
        # In KNX 0 is open, 100 is closed.
        try:
            return 100 - self.device.current_position()
        except TypeError:
            return None
def copy(src, dst):
    

    
    if os.path.isdir(dst):
        dst = os.path.join(dst, os.path.basename(src))
    copyfile(src, dst)
    copymode(src, dst)
def copyfile(src, dst, *, follow_symlinks=True):
    

    
    if _samefile(src, dst):
        raise SameFileError("{!r} and {!r} are the same file".format(src, dst))

    file_size = 0
    for i, fn in enumerate([src, dst]):
        try:
            st = os.stat(fn)
        except OSError:
            # File most likely does not exist
            pass
        else:
            # XXX What about other special files? (sockets, devices...)
            if stat.S_ISFIFO(st.st_mode):
                raise SpecialFileError("`%s` is a named pipe" % fn)
            if _WINDOWS and i == 0:
                file_size = st.st_size

    if not follow_symlinks and os.path.islink(src):
        os.symlink(os.readlink(src), dst)
    else:
        with open(src, 'rb') as fsrc, open(dst, 'wb') as fdst:
            # macOS
            if _HAS_FCOPYFILE:
                try:
                    _fastcopy_fcopyfile(fsrc, fdst, posix._COPYFILE_DATA)
                    return dst
                except _GiveupOnFastCopy:
                    pass
            # Linux / Solaris
            elif _HAS_SENDFILE:
                try:
                    _fastcopy_sendfile(fsrc, fdst)
                    return dst
                except _GiveupOnFastCopy:
                    pass
            # Windows, see:
            # https://github.com/python/cpython/pull/7160#discussion_r195405230
            elif _WINDOWS and file_size > 0:
                _copyfileobj_readinto(fsrc, fdst, min(file_size, COPY_BUFSIZE))
                return dst

            copyfileobj(fsrc, fdst)

    return dst
def copystat(src, dst):
    
    st = os.stat(src)
    mode = stat.S_IMODE(st.st_mode)
    if hasattr(os, 'utime'):
        os.utime(dst, (st.st_atime, st.st_mtime))
    if hasattr(os, 'chmod'):
        os.chmod(dst, mode)
    if hasattr(os, 'chflags') and hasattr(st, 'st_flags'):
        os.chflags(dst, st.st_flags)
def copytree(src, dst, symlinks=False, ignore=None, copy_function=copy2,
             ignore_dangling_symlinks=False, dirs_exist_ok=False):
    

    
    with os.scandir(src) as entries:
        return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,
                         ignore=ignore, copy_function=copy_function,
                         ignore_dangling_symlinks=ignore_dangling_symlinks,
                         dirs_exist_ok=dirs_exist_ok)
def move(src, dst, copy_function=copy2):
    

    
    real_dst = dst
    if os.path.isdir(dst):
        if _samefile(src, dst):
            # We might be on a case insensitive filesystem,
            # perform the rename anyway.
            os.rename(src, dst)
            return

        real_dst = os.path.join(dst, _basename(src))
        if os.path.exists(real_dst):
            raise Error("Destination path '%s' already exists" % real_dst)
    try:
        os.rename(src, real_dst)
    except OSError:
        if os.path.islink(src):
            linkto = os.readlink(src)
            os.symlink(linkto, real_dst)
            os.unlink(src)
        elif os.path.isdir(src):
            if _destinsrc(src, dst):
                raise Error("Cannot move a directory '%s' into itself"
                            " '%s'." % (src, dst))
            copytree(src, real_dst, copy_function=copy_function,
                     symlinks=True)
            rmtree(src)
        else:
            copy_function(src, real_dst)
            os.unlink(src)
    return real_dst
def _make_zipfile(base_name, base_dir, verbose=0, dry_run=0, logger=None):
    
    
    import zipfile  # late import for breaking circular dependency

    zip_filename = base_name + ".zip"
    archive_dir = os.path.dirname(base_name)

    if archive_dir and not os.path.exists(archive_dir):
        if logger is not None:
            logger.info("creating %s", archive_dir)
        if not dry_run:
            os.makedirs(archive_dir)

    if logger is not None:
        logger.info("creating '%s' and adding '%s' to it",
                    zip_filename, base_dir)

    if not dry_run:
        with zipfile.ZipFile(zip_filename, "w",
                             compression=zipfile.ZIP_DEFLATED) as zf:
            path = os.path.normpath(base_dir)
            if path != os.curdir:
                zf.write(path, path)
                if logger is not None:
                    logger.info("adding '%s'", path)
            for dirpath, dirnames, filenames in os.walk(base_dir):
                for name in sorted(dirnames):
                    path = os.path.normpath(os.path.join(dirpath, name))
                    zf.write(path, path)
                    if logger is not None:
                        logger.info("adding '%s'", path)
                for name in filenames:
                    path = os.path.normpath(os.path.join(dirpath, name))
                    if os.path.isfile(path):
                        zf.write(path, path)
                        if logger is not None:
                            logger.info("adding '%s'", path)

    return zip_filename
def _fastcopy_sendfile(fsrc, fdst):
    
    
    # Note: copyfileobj() is left alone in order to not introduce any
    # unexpected breakage. Possible risks by using zero-copy calls
    # in copyfileobj() are:
    # - fdst cannot be open in "a"(ppend) mode
    # - fsrc and fdst may be open in "t"(ext) mode
    # - fsrc may be a BufferedReader (which hides unread data in a buffer),
    #   GzipFile (which decompresses data), HTTPResponse (which decodes
    #   chunks).
    # - possibly others (e.g. encrypted fs/partition?)
    global _USE_CP_SENDFILE
    try:
        infd = fsrc.fileno()
        outfd = fdst.fileno()
    except Exception as err:
        raise _GiveupOnFastCopy(err)  # not a regular file

    # Hopefully the whole file will be copied in a single call.
    # sendfile() is called in a loop 'till EOF is reached (0 return)
    # so a bufsize smaller or bigger than the actual file size
    # should not make any difference, also in case the file content
    # changes while being copied.
    try:
        blocksize = max(os.fstat(infd).st_size, 2 ** 23)  # min 8MB
    except Exception:
        blocksize = 2 ** 27  # 128MB

    offset = 0
    while True:
        try:
            sent = os.sendfile(outfd, infd, offset, blocksize)
        except OSError as err:
            # ...in oder to have a more informative exception.
            err.filename = fsrc.name
            err.filename2 = fdst.name

            if err.errno == errno.ENOTSOCK:
                # sendfile() on this platform (probably Linux < 2.6.33)
                # does not support copies between regular files (only
                # sockets).
                _USE_CP_SENDFILE = False
                raise _GiveupOnFastCopy(err)

            if err.errno == errno.ENOSPC:  # filesystem is full
                raise err from None

            # Give up on first call and if no data was copied.
            if offset == 0 and os.lseek(outfd, 0, os.SEEK_CUR) == 0:
                raise _GiveupOnFastCopy(err)

            raise err
        else:
            if sent == 0:
                break  # EOF
            offset += sent
def unpack_archive(filename, extract_dir=None, format=None, *, filter=None):
    
    
    sys.audit("shutil.unpack_archive", filename, extract_dir, format)

    if extract_dir is None:
        extract_dir = os.getcwd()

    extract_dir = os.fspath(extract_dir)
    filename = os.fspath(filename)

    if filter is None:
        filter_kwargs = {}
    else:
        filter_kwargs = {'filter': filter}
    if format is not None:
        try:
            format_info = _UNPACK_FORMATS[format]
        except KeyError:
            raise ValueError("Unknown unpack format '{0}'".format(format)) from None

        func = format_info[1]
        func(filename, extract_dir, **dict(format_info[2]), **filter_kwargs)
    else:
        # we need to look at the registered unpackers supported extensions
        format = _find_unpack_format(filename)
        if format is None:
            raise ReadError("Unknown archive format '{0}'".format(filename))

        func = _UNPACK_FORMATS[format][1]
        kwargs = dict(_UNPACK_FORMATS[format][2]) | filter_kwargs
        func(filename, extract_dir, **kwargs)
def chown(path, user=None, group=None, *, dir_fd=None, follow_symlinks=True):
    
    
    sys.audit('shutil.chown', path, user, group)

    if user is None and group is None:
        raise ValueError("user and/or group must be set")

    _user = user
    _group = group

    # -1 means don't change it
    if user is None:
        _user = -1
    # user can either be an int (the uid) or a string (the system username)
    elif isinstance(user, str):
        _user = _get_uid(user)
        if _user is None:
            raise LookupError("no such user: {!r}".format(user))

    if group is None:
        _group = -1
    elif not isinstance(group, int):
        _group = _get_gid(group)
        if _group is None:
            raise LookupError("no such group: {!r}".format(group))

    os.chown(path, _user, _group, dir_fd=dir_fd,
             follow_symlinks=follow_symlinks)
def __str__(self):
        
        return '\n'.join(['%s: %s' % (key, value)
            for key, value in self._headers.values()]) \
            + '\n\n' + self.content
def set_cookie(self, key, value='', max_age=None, expires=None, path='/',
                   domain=None, secure=False, httponly=False):
        
        

        
        self.cookies[key] = value
        if expires is not None:
            if isinstance(expires, datetime.datetime):
                if timezone.is_aware(expires):
                    expires = timezone.make_naive(expires, timezone.utc)
                delta = expires - expires.utcnow()
                # Add one second so the date matches exactly (a fraction of
                # time gets lost between converting to a timedelta and
                # then the date string).
                delta = delta + datetime.timedelta(seconds=1)
                # Just set max_age - the max_age logic will set expires.
                expires = None
                max_age = max(0, delta.days * 86400 + delta.seconds)
            else:
                self.cookies[key]['expires'] = expires
        if max_age is not None:
            self.cookies[key]['max-age'] = max_age
            # IE requires expires, so set it if hasn't been already.
            if not expires:
                self.cookies[key]['expires'] = cookie_date(time.time() +
                                                           max_age)
        if path is not None:
            self.cookies[key]['path'] = path
        if domain is not None:
            self.cookies[key]['domain'] = domain
        if secure:
            self.cookies[key]['secure'] = True
        if httponly:
            self.cookies[key]['httponly'] = True
def _get_encoder_model(self) -> keras.models.Model:
         
        
        model, kwargs = self._selected_model
        if model.keras_name:
            kwargs["input_shape"] = self._input_shape
            kwargs["include_top"] = False
            kwargs["weights"] = "imagenet" if self._config["enc_load_weights"] else None
            retval = getattr(kapp, model.keras_name)(**kwargs)
        else:
            retval = _EncoderFaceswap(self._config)
        return retval
def _get_normalized_url(self, trigger):
        
        
        
        return trigger['parameters']['url'].strip('/')
def loss(self, prediction_dict, true_image_shapes):
    
    
    pass
def predict(self, preprocessed_inputs, true_image_shapes, **side_inputs):
    
    
    pass
def groundtruth_lists(self, field):
    
    
    if field not in self._groundtruth_lists:
      raise RuntimeError('Groundtruth tensor {} has not been provided'.format(
          field))
    return self._groundtruth_lists[field]
def restore_map(self,
                  fine_tune_checkpoint_type='detection',
                  load_all_detection_checkpoint_vars=False):
    
    
    pass
def normalize_angles(self) -> None:
        
        
        
        self.tensor[:, 4] = (self.tensor[:, 4] + 180.0) % 360.0 - 180.0
def get_unsupported_parameters(argument_spec, parameters, legal_inputs=None):
    
    

    if legal_inputs is None:
        aliases, legal_inputs = handle_aliases(argument_spec, parameters)

    unsupported_parameters = set()
    for k in parameters.keys():
        if k not in legal_inputs:
            unsupported_parameters.add(k)

    return unsupported_parameters
def run(self):
        
        self._remove_git_locks()
        self._connected_event.wait()
        self._load_on_startup()

        # Scan the file folder that contains Skills.  If a Skill is updated,
        # unload the existing version from memory and reload from the disk.
        while not self._stop_event.is_set():
            self._reload_modified_skills()
            self._load_new_skills()
            self._unload_removed_skills()
            self._update_skills()
            sleep(2)
def _camera_image(self):
        
        return self._doorbell_detail.get_doorbell_image(timeout=self._timeout)
def __init__(self, data, device, session, timeout):
        
        super().__init__(data, device)
        self._timeout = timeout
        self._session = session
        self._image_url = None
        self._image_content = None
        self._attr_unique_id = f"{self._device_id:s}_camera"
def test_auto_revision_even_without_scm_git(self):
        
        
        
        ref = RecipeReference.loads("lib/1.0@conan/testing")
        client = TurboTestClient()
        conanfile = GenConanfile().with_revision_mode("scm")
        commit = client.init_git_repo(files={"file.txt": "hey", "conanfile.py": str(conanfile)},
                                      origin_url="http://myrepo.git")
        client.create(ref, conanfile=conanfile)
        self.assertEqual(client.recipe_revision(ref), commit)

        # Change the conanfile and make another create, the revision should be the same
        client.save({"conanfile.py": str(conanfile.with_build_msg("New changes!"))})
        client.create(ref, conanfile=conanfile, assert_error=True)
        self.assertIn("Can't have a dirty repository using revision_mode='scm' and doing", client.out)
def set(self, item, column=None, value=None):
        
        res = self.tk.call(self._w, "set", item, column, value)
        if column is None and value is None:
            return _splitdict(self.tk, res,
                              cut_minus=False, conv=_tclobj_to_py)
        else:
            return res
def selection(self, selop=_sentinel, items=None):
        
        if selop is _sentinel:
            selop = None
        elif selop is None:
            import warnings
            warnings.warn(
                "The selop=None argument of selection() is deprecated "
                "and will be removed in Python 3.7",
                DeprecationWarning, 3)
        elif selop in ('set', 'add', 'remove', 'toggle'):
            import warnings
            warnings.warn(
                "The selop argument of selection() is deprecated "
                "and will be removed in Python 3.7, "
                "use selection_%s() instead" % (selop,),
                DeprecationWarning, 3)
        else:
            raise TypeError('Unsupported operation')
        return self.tk.splitlist(self.tk.call(self._w, "selection", selop, items))
def _extract_proposal_features(self, preprocessed_inputs, scope):
    
    

    preprocessed_inputs.get_shape().assert_has_rank(4)
    shape_assert = tf.Assert(
        tf.logical_and(tf.greater_equal(tf.shape(preprocessed_inputs)[1], 33),
                       tf.greater_equal(tf.shape(preprocessed_inputs)[2], 33)),
        ['image size must at least be 33 in both height and width.'])

    with tf.control_dependencies([shape_assert]):
      with tf.variable_scope('InceptionV2',
                             reuse=self._reuse_weights) as scope:
        with _batch_norm_arg_scope([slim.conv2d, slim.separable_conv2d],
                                   batch_norm_scale=True,
                                   train_batch_norm=self._train_batch_norm):
          _, activations = inception_v2.inception_v2_base(
              preprocessed_inputs,
              final_endpoint='Mixed_4e',
              min_depth=self._min_depth,
              depth_multiplier=self._depth_multiplier,
              scope=scope)

    return activations['Mixed_4e'], activations
def log(data):
    
    
    
    if G.LOGGER:
        G.LOGGER.log("info", data, 0)
def bounded_iou_loss(pred: Tensor,
                     target: Tensor,
                     beta: float = 0.2,
                     eps: float = 1e-3) -> Tensor:
    
    
    pred_ctrx = (pred[:, 0] + pred[:, 2]) * 0.5
    pred_ctry = (pred[:, 1] + pred[:, 3]) * 0.5
    pred_w = pred[:, 2] - pred[:, 0]
    pred_h = pred[:, 3] - pred[:, 1]
    with torch.no_grad():
        target_ctrx = (target[:, 0] + target[:, 2]) * 0.5
        target_ctry = (target[:, 1] + target[:, 3]) * 0.5
        target_w = target[:, 2] - target[:, 0]
        target_h = target[:, 3] - target[:, 1]

    dx = target_ctrx - pred_ctrx
    dy = target_ctry - pred_ctry

    loss_dx = 1 - torch.max(
        (target_w - 2 * dx.abs()) /
        (target_w + 2 * dx.abs() + eps), torch.zeros_like(dx))
    loss_dy = 1 - torch.max(
        (target_h - 2 * dy.abs()) /
        (target_h + 2 * dy.abs() + eps), torch.zeros_like(dy))
    loss_dw = 1 - torch.min(target_w / (pred_w + eps), pred_w /
                            (target_w + eps))
    loss_dh = 1 - torch.min(target_h / (pred_h + eps), pred_h /
                            (target_h + eps))
    # view(..., -1) does not work for empty tensor
    loss_comb = torch.stack([loss_dx, loss_dy, loss_dw, loss_dh],
                            dim=-1).flatten(1)

    loss = torch.where(loss_comb < beta, 0.5 * loss_comb * loss_comb / beta,
                       loss_comb - 0.5 * beta)
    return loss
def backtest(self, processed: Dict, stake_amount: int,
                 start_date, end_date,
                 max_open_trades: int = 0, position_stacking: bool = False) -> DataFrame:
        
        
        
        logger.debug(f"Run backtest, stake_amount: {stake_amount}, "
                     f"start_date: {start_date}, end_date: {end_date}, "
                     f"max_open_trades: {max_open_trades}, position_stacking: {position_stacking}"
        )
        trades = []
        trade_count_lock: Dict = {}

        # Dict of ticker-lists for performance (looping lists is a lot faster than dataframes)
        ticker: Dict = self._get_ticker_list(processed)

        lock_pair_until: Dict = {}
        # Indexes per pair, so some pairs are allowed to have a missing start.
        indexes: Dict = {}
        tmp = start_date + timedelta(minutes=self.timeframe_min)

        # Loop timerange and get candle for each pair at that point in time
        while tmp < end_date:

            for i, pair in enumerate(ticker):
                if pair not in indexes:
                    indexes[pair] = 0

                try:
                    row = ticker[pair][indexes[pair]]
                except IndexError:
                    # missing Data for one pair at the end.
                    # Warnings for this are shown during data loading
                    continue

                # Waits until the time-counter reaches the start of the data for this pair.
                if row.date > tmp.datetime:
                    continue

                indexes[pair] += 1

                if row.buy == 0 or row.sell == 1:
                    continue  # skip rows where no buy signal or that would immediately sell off

                if (not position_stacking and pair in lock_pair_until
                        and row.date <= lock_pair_until[pair]):
                    # without positionstacking, we can only have one open trade per pair.
                    continue

                if max_open_trades > 0:
                    # Check if max_open_trades has already been reached for the given date
                    if not trade_count_lock.get(row.date, 0) < max_open_trades:
                        continue
                    trade_count_lock[row.date] = trade_count_lock.get(row.date, 0) + 1

                # since indexes has been incremented before, we need to go one step back to
                # also check the buying candle for sell conditions.
                trade_entry = self._get_sell_trade_entry(pair, row, ticker[pair][indexes[pair]-1:],
                                                         trade_count_lock, stake_amount,
                                                         max_open_trades)

                if trade_entry:
                    logger.debug(f"{pair} - Locking pair till "
                                 f"close_time={trade_entry.close_time}")
                    lock_pair_until[pair] = trade_entry.close_time
                    trades.append(trade_entry)
                else:
                    # Set lock_pair_until to end of testing period if trade could not be closed
                    lock_pair_until[pair] = end_date.datetime

            # Move time one configured time_interval ahead.
            tmp += timedelta(minutes=self.timeframe_min)
        return DataFrame.from_records(trades, columns=BacktestResult._fields)
def _get_ohlcv_as_lists(self, processed: Dict[str, DataFrame]) -> Dict[str, Tuple]:
        
        
        
        # Every change to this headers list must evaluate further usages of the resulting tuple
        # and eventually change the constants for indexes at the top
        headers = ['date', 'buy', 'open', 'close', 'sell', 'low', 'high', 'buy_tag', 'exit_tag']
        data: Dict = {}
        self.progress.init_step(BacktestState.CONVERT, len(processed))

        # Create dict with data
        for pair in processed.keys():
            pair_data = processed[pair]
            self.check_abort()
            self.progress.increment()
            if not pair_data.empty:
                pair_data.loc[:, 'buy'] = 0  # cleanup if buy_signal is exist
                pair_data.loc[:, 'sell'] = 0  # cleanup if sell_signal is exist
                pair_data.loc[:, 'buy_tag'] = None  # cleanup if buy_tag is exist
                pair_data.loc[:, 'exit_tag'] = None  # cleanup if exit_tag is exist

            df_analyzed = self.strategy.advise_sell(
                self.strategy.advise_buy(pair_data, {'pair': pair}), {'pair': pair}).copy()
            # Trim startup period from analyzed dataframe
            df_analyzed = trim_dataframe(df_analyzed, self.timerange,
                                         startup_candles=self.required_startup)
            # To avoid using data from future, we use buy/sell signals shifted
            # from the previous candle
            df_analyzed.loc[:, 'buy'] = df_analyzed.loc[:, 'buy'].shift(1)
            df_analyzed.loc[:, 'sell'] = df_analyzed.loc[:, 'sell'].shift(1)
            df_analyzed.loc[:, 'buy_tag'] = df_analyzed.loc[:, 'buy_tag'].shift(1)
            df_analyzed.loc[:, 'exit_tag'] = df_analyzed.loc[:, 'exit_tag'].shift(1)

            # Update dataprovider cache
            self.dataprovider._set_cached_df(pair, self.timeframe, df_analyzed)

            df_analyzed = df_analyzed.drop(df_analyzed.head(1).index)

            # Convert from Pandas to list for performance reasons
            # (Looping Pandas is slow.)
            data[pair] = df_analyzed[headers].values.tolist()

            # Do not hold on to old data to reduce memory usage
            processed[pair] = pair_data = None
        return data
def check_order_cancel(
            self, trade: LocalTrade, order: Order, current_time: datetime) -> Optional[bool]:
        
        
        
        timedout = self.strategy.ft_check_timed_out(
            trade,  # type: ignore[arg-type]
            order, current_time)
        if timedout:
            if order.side == trade.entry_side:
                self.timedout_entry_orders += 1
                if trade.nr_of_successful_entries == 0:
                    # Remove trade due to entry timeout expiration.
                    return True
                else:
                    # Close additional entry order
                    del trade.orders[trade.orders.index(order)]
                    return False
            if order.side == trade.exit_side:
                self.timedout_exit_orders += 1
                # Close exit order and retry exiting on next signal.
                del trade.orders[trade.orders.index(order)]
                return False
        return None
def ameribor(
        self,
        start_date: Annotated[
            Union[datetime.date, None, str],
            OpenBBCustomParameter(
                description="Start date of the data, in YYYY-MM-DD format."
            ),
        ] = None,
        end_date: Annotated[
            Union[datetime.date, None, str],
            OpenBBCustomParameter(
                description="End date of the data, in YYYY-MM-DD format."
            ),
        ] = None,
        provider: Optional[Literal["fred"]] = None,
        **kwargs
    ) -> OBBject:
        
          # noqa: E501

        return self._run(
            "/fixedincome/rate/ameribor",
            **filter_inputs(
                provider_choices={
                    "provider": self._get_provider(
                        provider,
                        "/fixedincome/rate/ameribor",
                        ("fred",),
                    )
                },
                standard_params={
                    "start_date": start_date,
                    "end_date": end_date,
                },
                extra_params=kwargs,
            )
        )
def fit(self, X, y=None):
        
        
        tags = self._get_tags()
        X = self._validate_data(
            X,
            accept_sparse="csc",
            ensure_min_features=2,
            force_all_finite=not tags.get("allow_nan", True),
        )
        n_features = X.shape[1]

        error_msg = (
            "n_features_to_select must be either None, an "
            "integer in [1, n_features - 1] "
            "representing the absolute "
            "number of features, or a float in (0, 1] "
            "representing a percentage of features to "
            f"select. Got {self.n_features_to_select}"
        )
        if self.n_features_to_select is None:
            self.n_features_to_select_ = n_features // 2
        elif isinstance(self.n_features_to_select, numbers.Integral):
            if not 0 < self.n_features_to_select < n_features:
                raise ValueError(error_msg)
            self.n_features_to_select_ = self.n_features_to_select
        elif isinstance(self.n_features_to_select, numbers.Real):
            if not 0 < self.n_features_to_select <= 1:
                raise ValueError(error_msg)
            self.n_features_to_select_ = int(n_features * self.n_features_to_select)
        else:
            raise ValueError(error_msg)

        if self.direction not in ("forward", "backward"):
            raise ValueError(
                "direction must be either 'forward' or 'backward'. "
                f"Got {self.direction}."
            )

        cloned_estimator = clone(self.estimator)

        # the current mask corresponds to the set of features:
        # - that we have already *selected* if we do forward selection
        # - that we have already *excluded* if we do backward selection
        current_mask = np.zeros(shape=n_features, dtype=bool)
        n_iterations = (
            self.n_features_to_select_
            if self.direction == "forward"
            else n_features - self.n_features_to_select_
        )
        for _ in range(n_iterations):
            new_feature_idx = self._get_best_new_feature(
                cloned_estimator, X, y, current_mask
            )
            current_mask[new_feature_idx] = True

        if self.direction == "backward":
            current_mask = ~current_mask
        self.support_ = current_mask

        return self
def server_error(request):
    
    
    
    t = loader.get_template('500')
    return httpwrappers.HttpResponseServerError(t.render(Context()))
def page_not_found(request, template_name='404.html'):
    
    
    
    t = loader.get_template(template_name)
    return http.HttpResponseNotFound(t.render(Context({'request_path': request.path})))
def canny(image, sigma=1., low_threshold=None, high_threshold=None, mask=None,
    use_quantiles=False):
    
    

    #
    # The steps involved:
    #
    # * Smooth using the Gaussian with sigma above.
    #
    # * Apply the horizontal and vertical Sobel operators to get the gradients
    #   within the image. The edge strength is the sum of the magnitudes
    #   of the gradients in each direction.
    #
    # * Find the normal to the edge at each point using the arctangent of the
    #   ratio of the Y sobel over the X sobel - pragmatically, we can
    #   look at the signs of X and Y and the relative magnitude of X vs Y
    #   to sort the points into 4 categories: horizontal, vertical,
    #   diagonal and antidiagonal.
    #
    # * Look in the normal and reverse directions to see if the values
    #   in either of those directions are greater than the point in question.
    #   Use interpolation to get a mix of points instead of picking the one
    #   that's the closest to the normal.
    #
    # * Label all points above the high threshold as edges.
    # * Recursively label any point above the low threshold that is 8-connected
    #   to a labeled point as an edge.
    #
    # Regarding masks, any point touching a masked point will have a gradient
    # that is "infected" by the masked point, so it's enough to erode the
    # mask by one and then mask the output. We also mask out the border points
    # because who knows what lies beyond the edge of the image?
    #
    assert_nD(image, 2)

    if low_threshold is None:
        low_threshold = 0.1 * dtype_limits(image)[1]

    if high_threshold is None:
        high_threshold = 0.2 * dtype_limits(image)[1]

    if mask is None:
        mask = np.ones(image.shape, dtype=bool)
    fsmooth = lambda x: gaussian_filter(x, sigma, mode='constant')
    smoothed = smooth_with_function_and_mask(image, fsmooth, mask)
    jsobel = ndi.sobel(smoothed, axis=1)
    isobel = ndi.sobel(smoothed, axis=0)
    abs_isobel = np.abs(isobel)
    abs_jsobel = np.abs(jsobel)
    magnitude = np.hypot(isobel, jsobel)

    #
    # Make the eroded mask. Setting the border value to zero will wipe
    # out the image edges for us.
    #
    s = generate_binary_structure(2, 2)
    eroded_mask = binary_erosion(mask, s, border_value=0)
    eroded_mask = eroded_mask & (magnitude > 0)
    #
    #--------- Find local maxima --------------
    #
    # Assign each point to have a normal of 0-45 degrees, 45-90 degrees,
    # 90-135 degrees and 135-180 degrees.
    #
    local_maxima = np.zeros(image.shape, bool)
    #----- 0 to 45 degrees ------
    pts_plus = (isobel >= 0) & (jsobel >= 0) & (abs_isobel >= abs_jsobel)
    pts_minus = (isobel <= 0) & (jsobel <= 0) & (abs_isobel >= abs_jsobel)
    pts = pts_plus | pts_minus
    pts = eroded_mask & pts
    # Get the magnitudes shifted left to make a matrix of the points to the
    # right of pts. Similarly, shift left and down to get the points to the
    # top right of pts.
    c1 = magnitude[1:, :][pts[:-1, :]]
    c2 = magnitude[1:, 1:][pts[:-1, :-1]]
    m = magnitude[pts]
    w = abs_jsobel[pts] / abs_isobel[pts]
    c_plus = c2 * w + c1 * (1 - w) <= m
    c1 = magnitude[:-1, :][pts[1:, :]]
    c2 = magnitude[:-1, :-1][pts[1:, 1:]]
    c_minus = c2 * w + c1 * (1 - w) <= m
    local_maxima[pts] = c_plus & c_minus
    #----- 45 to 90 degrees ------
    # Mix diagonal and vertical
    #
    pts_plus = (isobel >= 0) & (jsobel >= 0) & (abs_isobel <= abs_jsobel)
    pts_minus = (isobel <= 0) & (jsobel <= 0) & (abs_isobel <= abs_jsobel)
    pts = pts_plus | pts_minus
    pts = eroded_mask & pts
    c1 = magnitude[:, 1:][pts[:, :-1]]
    c2 = magnitude[1:, 1:][pts[:-1, :-1]]
    m = magnitude[pts]
    w = abs_isobel[pts] / abs_jsobel[pts]
    c_plus = c2 * w + c1 * (1 - w) <= m
    c1 = magnitude[:, :-1][pts[:, 1:]]
    c2 = magnitude[:-1, :-1][pts[1:, 1:]]
    c_minus = c2 * w + c1 * (1 - w) <= m
    local_maxima[pts] = c_plus & c_minus
    #----- 90 to 135 degrees ------
    # Mix anti-diagonal and vertical
    #
    pts_plus = (isobel <= 0) & (jsobel >= 0) & (abs_isobel <= abs_jsobel)
    pts_minus = (isobel >= 0) & (jsobel <= 0) & (abs_isobel <= abs_jsobel)
    pts = pts_plus | pts_minus
    pts = eroded_mask & pts
    c1a = magnitude[:, 1:][pts[:, :-1]]
    c2a = magnitude[:-1, 1:][pts[1:, :-1]]
    m = magnitude[pts]
    w = abs_isobel[pts] / abs_jsobel[pts]
    c_plus = c2a * w + c1a * (1.0 - w) <= m
    c1 = magnitude[:, :-1][pts[:, 1:]]
    c2 = magnitude[1:, :-1][pts[:-1, 1:]]
    c_minus = c2 * w + c1 * (1.0 - w) <= m
    local_maxima[pts] = c_plus & c_minus
    #----- 135 to 180 degrees ------
    # Mix anti-diagonal and anti-horizontal
    #
    pts_plus = (isobel <= 0) & (jsobel >= 0) & (abs_isobel >= abs_jsobel)
    pts_minus = (isobel >= 0) & (jsobel <= 0) & (abs_isobel >= abs_jsobel)
    pts = pts_plus | pts_minus
    pts = eroded_mask & pts
    c1 = magnitude[:-1, :][pts[1:, :]]
    c2 = magnitude[:-1, 1:][pts[1:, :-1]]
    m = magnitude[pts]
    w = abs_jsobel[pts] / abs_isobel[pts]
    c_plus = c2 * w + c1 * (1 - w) <= m
    c1 = magnitude[1:, :][pts[:-1, :]]
    c2 = magnitude[1:, :-1][pts[:-1, 1:]]
    c_minus = c2 * w + c1 * (1 - w) <= m
    local_maxima[pts] = c_plus & c_minus

    #
    #---- If use_quantiles is set then calculate the thresholds to use
    #
    if use_quantiles:
        if high_threshold > 1.0 or low_threshold > 1.0:
            raise ValueError("Quantile thresholds must not be > 1.0")
        if high_threshold < 0.0 or low_threshold < 0.0:
            raise ValueError("Quantile thresholds must not be < 0.0")

        high_threshold = np.percentile(magnitude, 100.0 * high_threshold)
        low_threshold = np.percentile(magnitude, 100.0 * low_threshold)

    #
    #---- Create two masks at the two thresholds.
    #
    high_mask = local_maxima & (magnitude >= high_threshold)
    low_mask = local_maxima & (magnitude >= low_threshold)
    #
    # Segment the low-mask, then only keep low-segments that have
    # some high_mask component in them
    #
    strel = np.ones((3, 3), bool)
    labels, count = label(low_mask, strel)
    if count == 0:
        return low_mask

    sums = (np.array(ndi.sum(high_mask, labels,
                             np.arange(count, dtype=np.int32) + 1),
                     copy=False, ndmin=1))
    good_label = np.zeros((count + 1,), bool)
    good_label[1:] = sums > 0
    output_mask = good_label[labels]
    return output_mask
def _extract_rpn_feature_maps(self, preprocessed_inputs):
    
    
    image_shape = tf.shape(preprocessed_inputs)

    rpn_features_to_crop, self.endpoints = self._extract_proposal_features(
        preprocessed_inputs)
    
    # decide if rpn_features_to_crop is a list. If not make it a list
    if not isinstance(rpn_features_to_crop, list):
      rpn_features_to_crop = [rpn_features_to_crop]

    rpn_box_predictor_features = []
    anchors = []
    for single_rpn_features_to_crop in rpn_features_to_crop:
      feature_map_shape = tf.shape(single_rpn_features_to_crop)
      level_anchors = box_list_ops.concatenate(
          self._first_stage_anchor_generator.generate([(feature_map_shape[1],
                                                        feature_map_shape[2])]))
      anchors.append(level_anchors)
      single_rpn_box_predictor_features = (
        self._first_stage_box_predictor_first_conv(single_rpn_features_to_crop))
      rpn_box_predictor_features.append(single_rpn_box_predictor_features)
    return (rpn_box_predictor_features, rpn_features_to_crop,
            anchors, image_shape)
def _compute_second_stage_input_feature_maps(self, features_to_crop,
                                               proposal_boxes_normalized,
                                               **side_inputs):
    
    
    num_levels = len(features_to_crop)
    box_levels = None
    if num_levels != 1:
      # If there are mutiple levels to select, get the box levels 
      box_levels = ops.fpn_feature_levels(num_levels, num_levels - 1,
                                          1.0/224, proposal_boxes_normalized)
    cropped_regions = self._flatten_first_two_dimensions(
        self._crop_and_resize_fn(
            features_to_crop, proposal_boxes_normalized, box_levels,
            [self._initial_crop_size, self._initial_crop_size]))
    return self._maxpool_layer(cropped_regions)
def fetch_my_trades(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' fetchMyTrades() requires a symbol argument')
        self.load_markets()
        market = self.market(symbol)
        request = {
            'pair': market['id'],
        }
        if since is not None:
            request['since'] = since
        if limit is not None:
            request['limit'] = limit
        response = self.privateGetListtrades(self.extend(request, params))
        #
        #      {
        #          "trades":[
        #              {
        #                  "pair":"LTCXBT",
        #                  "sequence":3256813,
        #                  "order_id":"BXEX6XHHDT5EGW2",
        #                  "type":"ASK",
        #                  "timestamp":1648652135235,
        #                  "price":"0.002786",
        #                  "volume":"0.10",
        #                  "base":"0.10",
        #                  "counter":"0.0002786",
        #                  "fee_base":"0.0001",
        #                  "fee_counter":"0.00",
        #                  "is_buy":false,
        #                  "client_order_id":""
        #              },...
        #          ]
        #      }
        #
        trades = self.safe_value(response, 'trades', [])
        return self.parse_trades(trades, market, since, limit)
def print_report(self, unit='auto', file=sys.stdout):
        
        
        entries = [['LinkName', 'ElapsedTime', 'Occurrence']]
        if unit == 'auto':
            max_time = max(
                record['elapsed_time'] for record in self.summary().values())
            factor, unit = self._align(max_time)
        elif unit != 'auto_foreach':
            factor = self.table[unit]
        for function_name, record in self.summary().items():
            second = record['elapsed_time']
            if unit != 'auto_foreach':
                elapsed_time = '%3.2f%s' % (second * factor, unit)
            else:
                elapsed_time = self._humanized_time(second)
            occurrence = str(record['occurrence'])
            entries.append([link_name, elapsed_time, occurrence])
        entry_widths = []
        entry_widths.append(max(len(f) for f, _, _ in entries))
        entry_widths.append(max(len(e) for _, e, _ in entries))
        entry_widths.append(max(len(o) for _, _, o in entries))
        template = '  '.join('{:>%d}' % w for w in entry_widths)
        for link_name, elapsed_time, occurrence in entries:
            line = template.format(link_name, elapsed_time, occurrence)
            file.write(line)
            file.write('\n')
        file.flush()
def from_builtin(cls, func):
        
        

        warnings.warn("inspect.Signature.from_builtin() is deprecated since "
                      "Python 3.5, use Signature.from_callable()",
                      DeprecationWarning, stacklevel=2)
        return _signature_from_builtin(cls, func)
def isawaitable(object):
    
    return (isinstance(object, types.CoroutineType) or
            isinstance(object, types.GeneratorType) and
                bool(object.gi_code.co_flags & CO_ITERABLE_COROUTINE) or
            isinstance(object, collections.abc.Awaitable))
def iscoroutinefunction(obj):
    
    
    return _has_code_flag(obj, CO_COROUTINE) or _has_coroutine_mark(obj)
def _signature_strip_non_python_syntax(signature):
    
    
    

    if not signature:
        return signature, None

    self_parameter = None

    lines = [l.encode('ascii') for l in signature.split('\n') if l]
    generator = iter(lines).__next__
    token_stream = tokenize.tokenize(generator)

    text = []
    add = text.append

    current_parameter = 0
    OP = token.OP
    ERRORTOKEN = token.ERRORTOKEN

    # token stream always starts with ENCODING token, skip it
    t = next(token_stream)
    assert t.type == tokenize.ENCODING

    for t in token_stream:
        type, string = t.type, t.string

        if type == OP:
            if string == ',':
                current_parameter += 1

        if (type == ERRORTOKEN) and (string == '$'):
            assert self_parameter is None
            self_parameter = current_parameter
            continue

        add(string)
        if (string == ','):
            add(' ')
    clean_signature = ''.join(text)
    return clean_signature, self_parameter
def _has_code_flag(f, flag):
    
    f = functools._unwrap_partialmethod(f)
    while ismethod(f):
        f = f.__func__
    f = functools._unwrap_partial(f)
    if not (isfunction(f) or _signature_is_functionlike(f)):
        return False
    return bool(f.__code__.co_flags & flag)
def __init__(
        self,
        nlp: Language,
        name: str = "span_ruler",
        *,
        spans_key: Optional[str] = DEFAULT_SPANS_KEY,
        spans_filter: Optional[
            Callable[[Iterable[Span], Iterable[Span]], Iterable[Span]]
        ] = None,
        annotate_ents: bool = False,
        ents_filter: Callable[
            [Iterable[Span], Iterable[Span]], Iterable[Span]
        ] = util.filter_chain_spans,
        phrase_matcher_attr: Optional[Union[int, str]] = None,
        matcher_fuzzy_compare: Callable = levenshtein_compare,
        validate: bool = False,
        overwrite: bool = False,
        scorer: Optional[Callable] = partial(
            overlapping_labeled_spans_score, spans_key=DEFAULT_SPANS_KEY
        ),
    ) -> None:
        
        
        self.nlp = nlp
        self.name = name
        self.spans_key = spans_key
        self.annotate_ents = annotate_ents
        self.phrase_matcher_attr = phrase_matcher_attr
        self.validate = validate
        self.overwrite = overwrite
        self.spans_filter = spans_filter
        self.ents_filter = ents_filter
        self.scorer = scorer
        self.matcher_fuzzy_compare = matcher_fuzzy_compare
        self._match_label_id_map: Dict[int, Dict[str, str]] = {}
        self.clear()
def get_image_paths(directory, extension=None):
     
    
    logger = logging.getLogger(__name__)  # pylint:disable=invalid-name
    image_extensions = _image_extensions if extension is None else [extension]
    dir_contents = list()

    if not os.path.exists(directory):
        logger.debug("Creating folder: '%s'", directory)
        directory = get_folder(directory)

    dir_scanned = sorted(os.scandir(directory), key=lambda x: x.name)
    logger.debug("Scanned Folder contains %s files", len(dir_scanned))
    logger.trace("Scanned Folder Contents: %s", dir_scanned)

    for chkfile in dir_scanned:
        if any([chkfile.name.lower().endswith(ext)
                for ext in image_extensions]):
            logger.trace("Adding '%s' to image list", chkfile.path)
            dir_contents.append(chkfile.path)

    logger.debug("Returning %s images", len(dir_contents))
    return dir_contents
def get_tf_version() -> tuple[int, int]:
     
    
    global _TF_VERS  # pylint:disable=global-statement
    if _TF_VERS is None:
        import tensorflow as tf  # pylint:disable=import-outside-toplevel
        split = tf.__version__.split(".")[:2]
        _TF_VERS = (int(split[0]), int(split[1]))
    return _TF_VERS
def _model_exists(self):
          
        if isinstance(self.model_path, list):
            retval = all(os.path.exists(pth) for pth in self.model_path)
        else:
            retval = os.path.exists(self.model_path)
        self.logger.trace(retval)
        return retval
def rotate_landmarks(face, rotation_matrix):
    # pylint:disable=c-extension-no-member
      
    logger = logging.getLogger(__name__)  # pylint:disable=invalid-name
    logger.trace("Rotating landmarks: (rotation_matrix: %s, type(face): %s",
                 rotation_matrix, type(face))
    rotated_landmarks = None
    # Detected Face Object
    if isinstance(face, DetectedFace):
        bounding_box = [[face.x, face.y],
                        [face.x + face.w, face.y],
                        [face.x + face.w, face.y + face.h],
                        [face.x, face.y + face.h]]
        landmarks = face.landmarks_xy

    # Alignments Dict
    elif isinstance(face, dict) and "x" in face:
        bounding_box = [[face.get("x", 0), face.get("y", 0)],
                        [face.get("x", 0) + face.get("w", 0),
                         face.get("y", 0)],
                        [face.get("x", 0) + face.get("w", 0),
                         face.get("y", 0) + face.get("h", 0)],
                        [face.get("x", 0),
                         face.get("y", 0) + face.get("h", 0)]]
        landmarks = face.get("landmarks_xy", list())

    else:
        raise ValueError("Unsupported face type")

    logger.trace("Original landmarks: %s", landmarks)

    rotation_matrix = cv2.invertAffineTransform(  # pylint:disable=no-member
        rotation_matrix)
    rotated = list()
    for item in (bounding_box, landmarks):
        if not item:
            continue
        points = np.array(item, np.int32)
        points = np.expand_dims(points, axis=0)
        transformed = cv2.transform(points,  # pylint:disable=no-member
                                    rotation_matrix).astype(np.int32)
        rotated.append(transformed.squeeze())

    # Bounding box should follow x, y planes, so get min/max
    # for non-90 degree rotations
    pt_x = min([pnt[0] for pnt in rotated[0]])
    pt_y = min([pnt[1] for pnt in rotated[0]])
    pt_x1 = max([pnt[0] for pnt in rotated[0]])
    pt_y1 = max([pnt[1] for pnt in rotated[0]])
    width = pt_x1 - pt_x
    height = pt_y1 - pt_y

    if isinstance(face, DetectedFace):
        face.x = int(pt_x)
        face.y = int(pt_y)
        face.w = int(width)
        face.h = int(height)
        face.r = 0
        if len(rotated) > 1:
            rotated_landmarks = [tuple(point) for point in rotated[1].tolist()]
            face.landmarks_xy = rotated_landmarks
    else:
        face["left"] = int(pt_x)
        face["top"] = int(pt_y)
        face["right"] = int(pt_x1)
        face["bottom"] = int(pt_y1)
        rotated_landmarks = face

    logger.trace("Rotated landmarks: %s", rotated_landmarks)
    return face
def keras_backend_quiet():
      
    stderr = sys.stderr
    sys.stderr = open(os.devnull, 'w')
    try:
        from keras import backend as K  # pylint:disable=import-outside-toplevel
    except:
        sys.stderr = stderr
        raise
    sys.stderr = stderr
    return K
def download_model(self):
          
        self.logger.info("Downloading model: '%s' from: %s", self._model_name, self._url_download)
        for attempt in range(self.retries):
            try:
                downloaded_size = self._url_partial_size
                req = urllib.request.Request(self._url_download)
                if downloaded_size != 0:
                    req.add_header("Range", "bytes={}-".format(downloaded_size))
                response = urllib.request.urlopen(req, timeout=10)
                self.logger.debug("header info: {%s}", response.info())
                self.logger.debug("Return Code: %s", response.getcode())
                self.write_zipfile(response, downloaded_size)
                break
            except (socket_error, socket_timeout,
                    urllib.error.HTTPError, urllib.error.URLError) as err:
                if attempt + 1 < self.retries:
                    self.logger.warning("Error downloading model (%s). Retrying %s of %s...",
                                        str(err), attempt + 2, self.retries)
                else:
                    self.logger.error("Failed to download model. Exiting. (Error: '%s', URL: "
                                      "'%s')", str(err), self._url_download)
                    self.logger.info("You can try running again to resume the download.")
                    self.logger.info("Alternatively, you can manually download the model from: %s "
                                     "and unzip the contents to: %s",
                                     self._url_download, self.cache_dir)
                    sys.exit(1)
def get_folder(path, make_folder=True):
     
    
    logger = logging.getLogger(__name__)  # pylint:disable=invalid-name
    logger.debug("Requested path: '%s'", path)
    if not make_folder and not os.path.isdir(path):
        logger.debug("%s does not exist", path)
        return None
    os.makedirs(path, exist_ok=True)
    logger.debug("Returning: '%s'", path)
    return path
def _url_partial_size(self) -> int:
          
        zip_file = self._model_zip_path
        retval = os.path.getsize(zip_file) if os.path.exists(zip_file) else 0
        self.logger.trace(retval)  # type: ignore
        return retval
def write_model_runs(hps, datasets, output_fname=None, push_mean=False):
  
  
  model = build_model(hps, kind=hps.kind, datasets=datasets)
  model.write_model_runs(datasets, output_fname, push_mean)
def getAllReceivers( sender = Any, signal = Any ):
    
    
    receivers = {}
    # Get receivers that receive *this* signal from *this* sender.
    # Add receivers that receive *any* signal from *this* sender.
    # Add receivers that receive *this* signal from *any* sender.
    # Add receivers that receive *any* signal from *any* sender.
    l = []
    i = id(sender)
    if i in connections:
        sender_receivers = connections[i]
        if signal in sender_receivers:
            l.extend(sender_receivers[signal])
        if signal is not Any and Any in sender_receivers:
            l.extend(sender_receivers[Any])

    if sender is not Any:
        i = id(Any)
        if i in connections:
            sender_receivers = connections[i]
            if sender_receivers is not None:
                if signal in sender_receivers:
                    l.extend(sender_receivers[signal])
                if signal is not Any and Any in sender_receivers:
                    l.extend(sender_receivers[Any])

    for receiver in l:
        try:
            if not receiver in receivers:
                if isinstance(receiver, WEAKREF_TYPES):
                    receiver = receiver()
                    # this should only (rough guess) be possible if somehow, deref'ing
                    # triggered a wipe.
                    if receiver is None:
                        continue
                receivers[receiver] = 1
                yield receiver
        except TypeError:
            # dead weakrefs raise TypeError on hash...
            pass
def _remove_receiver(self, receiver=None, receiver_id=None, _make_id=_make_id):
        
        
        
        with self.lock:
            if receiver is not None:
                receiver_id = _make_id(receiver)
            self.receivers[:] = [val for val in self.receivers if val[2] != receiver_id]
            self.sender_receivers_cache.clear()
def connect(self, receiver, sender=None, weak=True, dispatch_uid=None):
        
        
        
        from django.conf import settings

        # If DEBUG is on, check that we got a good receiver
        if settings.configured and settings.DEBUG:
            if not callable(receiver):
                raise TypeError("Signal receivers must be callable.")
            # Check for **kwargs
            if not func_accepts_kwargs(receiver):
                raise ValueError(
                    "Signal receivers must accept keyword arguments (**kwargs)."
                )

        if dispatch_uid:
            lookup_key = (dispatch_uid, _make_id(sender))
        else:
            lookup_key = (_make_id(receiver), _make_id(sender))

        is_async = iscoroutinefunction(receiver)

        if weak:
            ref = weakref.ref
            receiver_object = receiver
            # Check for bound methods
            if hasattr(receiver, "__self__") and hasattr(receiver, "__func__"):
                ref = weakref.WeakMethod
                receiver_object = receiver.__self__
            receiver = ref(receiver)
            weakref.finalize(receiver_object, self._remove_receiver)

        with self.lock:
            self._clear_dead_receivers()
            if not any(r_key == lookup_key for r_key, _, _ in self.receivers):
                self.receivers.append((lookup_key, receiver, is_async))
            self.sender_receivers_cache.clear()
def disconnect(self, receiver=None, sender=None, weak=None, dispatch_uid=None):
        
        
        
        if weak is not None:
            warnings.warn("Passing `weak` to disconnect has no effect.",
                RemovedInDjango21Warning, stacklevel=2)
        if dispatch_uid:
            lookup_key = (dispatch_uid, _make_id(sender))
        else:
            lookup_key = (_make_id(receiver), _make_id(sender))

        disconnected = False
        with self.lock:
            self._clear_dead_receivers()
            for index in range(len(self.receivers)):
                (r_key, _) = self.receivers[index]
                if r_key == lookup_key:
                    disconnected = True
                    del self.receivers[index]
                    break
            self.sender_receivers_cache.clear()
        return disconnected
def send_robust(self, sender, **named):
        
        
        
        if not self.receivers or self.sender_receivers_cache.get(sender) is NO_RECEIVERS:
            return []

        # Call each receiver with whatever arguments it can accept.
        # Return a list of tuple pairs [(receiver, response), ... ].
        responses = []
        for receiver in self._live_receivers(sender):
            try:
                response = receiver(signal=self, sender=sender, **named)
            except Exception as err:
                responses.append((receiver, err))
            else:
                responses.append((receiver, response))
        return responses
def __init__(self, providing_args=None):
        
        
        
        self.receivers = []
        if providing_args is None:
            providing_args = []
        self.providing_args = set(providing_args)
def preprocess_for_eval(image,
                        output_height,
                        output_width,
                        resize_side,
                        use_grayscale=False):
  
  
  image = _aspect_preserving_resize(image, resize_side)
  image = _central_crop([image], output_height, output_width)[0]
  image.set_shape([output_height, output_width, 3])
  image = tf.to_float(image)
  if use_grayscale:
    image = tf.image.rgb_to_grayscale(image)
  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])
def iri_to_uri(iri):
    
    
    
    # The list of safe characters here is constructed from the "reserved" and
    # "unreserved" characters specified in sections 2.2 and 2.3 of RFC 3986:
    #     reserved    = gen-delims / sub-delims
    #     gen-delims  = ":" / "/" / "?" / "#" / "[" / "]" / "@"
    #     sub-delims  = "!" / "$" / "&" / "'" / "(" / ")"
    #                   / "*" / "+" / "," / ";" / "="
    #     unreserved  = ALPHA / DIGIT / "-" / "." / "_" / "~"
    # Of the unreserved characters, urllib.quote already considers all but
    # the ~ safe.
    # The % character is also added to the list of safe characters here, as the
    # end of section 3.1 of RFC 3987 specifically mentions that % must not be
    # converted.
    if iri is None:
        return iri
    elif isinstance(iri, Promise):
        iri = str(iri)
    return quote(iri, safe="/#%[]=:;$&()+,!?*@'~")
def filepath_to_uri(path):
    
    
    if path is None:
        return path
    # I know about `os.sep` and `os.altsep` but I want to leave
    # some flexibility for hardcoding separators.
    return quote(path.replace("\\", "/"), safe="/~!*()'")
def uri_to_iri(uri):
    
    
    
    if uri is None:
        return uri
    uri = force_bytes(uri)
    # Fast selective unqote: First, split on '%' and then starting with the
    # second block, decode the first 2 bytes if they represent a hex code to
    # decode. The rest of the block is the part after '%AB', not containing
    # any '%'. Add that to the output without further processing.
    bits = uri.split(b'%')
    if len(bits) == 1:
        iri = uri
    else:
        parts = [bits[0]]
        append = parts.append
        hextobyte = _hextobyte
        for item in bits[1:]:
            hex = item[:2]
            if hex in hextobyte:
                append(hextobyte[item[:2]])
                append(item[2:])
            else:
                append(b'%')
                append(item)
        iri = b''.join(parts)
    return repercent_broken_unicode(iri).decode()
def create_cluster(
    domain_key: DomainKey,
    engine_version: str,
    domain_endpoint_options: Optional[DomainEndpointOptions],
    preferred_port: Optional[int] = None,
):
    
    
    
    store = opensearch_stores[domain_key.account][domain_key.region]

    manager = cluster_manager()
    engine_version = engine_version or OPENSEARCH_DEFAULT_VERSION
    cluster = manager.create(
        domain_key.arn, engine_version, domain_endpoint_options, preferred_port
    )

    # FIXME: in AWS, the Endpoint is set once the cluster is running, not before (like here), but our tests and
    #  in particular cloudformation currently relies on the assumption that it is set when the domain is created.
    status = store.opensearch_domains[domain_key.domain_name]
    # Replacing only 0.0.0.0 here as usage of this bind address mostly means running in docker which is used locally
    # If another bind address is used we want to keep it in the endpoint as this is a conscious user decision to
    # access from another device on the network.
    status["Endpoint"] = cluster.url.split("://")[-1].replace("0.0.0.0", LOCALSTACK_HOSTNAME)
    status["EngineVersion"] = engine_version

    if cluster.is_up():
        status["Processing"] = False
    else:
        # run a background thread that will update all domains that use this cluster to set
        # the cluster state once it is started, or the CLUSTER_STARTUP_TIMEOUT is reached
        threading.Thread(
            target=_run_cluster_startup_monitor,
            args=(cluster, domain_key.domain_name, domain_key.region),
            daemon=True,
        ).start()
def transcribe_file(
        self,
        path: str,
        task: Optional[str] = None,
        initial_prompt: Optional[str] = None,
        logprob_threshold: Optional[float] = -1.0,
        no_speech_threshold=0.6,
        condition_on_previous_text: bool = False,
        verbose: bool = False,
        use_torchaudio_streaming: bool = False,
        chunk_size: Optional[int] = 30,
        **kwargs,
    ) -> List[ASRWhisperSegment]:
        
        
        results = []
        for whisper_segment in self.transcribe_file_streaming(
            path,
            task=task,
            initial_prompt=initial_prompt,
            logprob_threshold=logprob_threshold,
            no_speech_threshold=no_speech_threshold,
            condition_on_previous_text=condition_on_previous_text,
            verbose=verbose,
            use_torchaudio_streaming=use_torchaudio_streaming,
            chunk_size=chunk_size,
            **kwargs,
        ):
            results.append(whisper_segment)
            if verbose:
                pred = (
                    whisper_segment.words
                    if task != "lang_id"
                    else whisper_segment.lang_id
                )
                print(
                    f"[{whisper_segment.start}s --> {whisper_segment.end}s] {pred}"
                )
        return results
def getlines(filename, module_globals=None):
    

    if filename in cache:
        return cache[filename][2]
    else:
        return updatecache(filename, module_globals)
def __calc_A(self, h):
        
        
        
        A = np.zeros((self.nx, self.nx))
        A[0, 0] = 1.0
        for i in range(self.nx - 1):
            if i != (self.nx - 2):
                A[i + 1, i + 1] = 2.0 * (h[i] + h[i + 1])
            A[i + 1, i] = h[i]
            A[i, i + 1] = h[i]

        A[0, 1] = 0.0
        A[self.nx - 1, self.nx - 2] = 0.0
        A[self.nx - 1, self.nx - 1] = 1.0
        #  print(A)
        return A
def __init__(self, sequence_key, add_context_features, image_ids_to_keep,
               keep_context_features_image_id_list=False,
               subsample_context_features_rate=0,
               keep_only_positives=False,
               context_features_score_threshold=0.7,
               keep_only_positives_gt=False,
               max_num_elements_in_context_features=5000,
               pad_context_features=False,
               output_type='tf_example', max_clip_length=None,
               context_feature_length=2057):
    
    
    self._session = None
    self._num_examples_processed = beam.metrics.Metrics.counter(
        'sequence_data_generation', 'num_seq_examples_processed')
    self._num_keys_processed = beam.metrics.Metrics.counter(
        'sequence_data_generation', 'num_keys_processed')
    self._sequence_key = sequence_key
    self._add_context_features = add_context_features
    self._pad_context_features = pad_context_features
    self._output_type = output_type
    self._max_clip_length = max_clip_length
    if six.ensure_str(image_ids_to_keep) == 'All':
      self._image_ids_to_keep = None
    else:
      with tf.io.gfile.GFile(image_ids_to_keep) as f:
        self._image_ids_to_keep = json.load(f)
    self._keep_context_features_image_id_list = (
        keep_context_features_image_id_list)
    self._subsample_context_features_rate = subsample_context_features_rate
    self._keep_only_positives = keep_only_positives
    self._keep_only_positives_gt = keep_only_positives_gt
    self._context_features_score_threshold = context_features_score_threshold
    self._max_num_elements_in_context_features = (
        max_num_elements_in_context_features)
    self._context_feature_length = context_feature_length

    self._images_kept = beam.metrics.Metrics.counter(
        'sequence_data_generation', 'images_kept')
    self._images_loaded = beam.metrics.Metrics.counter(
        'sequence_data_generation', 'images_loaded')
def iglob(pathname, *, recursive=False):
    
    
    dirname, basename = os.path.split(pathname)
    if not has_magic(pathname):
        if basename:
            if os.path.lexists(pathname):
                yield pathname
        else:
            # Patterns ending with a slash should match only directories
            if os.path.isdir(dirname):
                yield pathname
        return
    if not dirname:
        if recursive and _isrecursive(basename):
            yield from glob2(dirname, basename)
        else:
            yield from glob1(dirname, basename)
        return
    # `os.path.split()` returns the argument itself as a dirname if it is a
    # drive or UNC path.  Prevent an infinite recursion if a drive or UNC path
    # contains magic characters (i.e. r'\\?\C:').
    if dirname != pathname and has_magic(dirname):
        dirs = iglob(dirname, recursive=recursive)
    else:
        dirs = [dirname]
    if has_magic(basename):
        if recursive and _isrecursive(basename):
            glob_in_dir = glob2
        else:
            glob_in_dir = glob1
    else:
        glob_in_dir = glob0
    for dirname in dirs:
        for name in glob_in_dir(dirname, basename):
            yield os.path.join(dirname, name)
def translate(pat, *, recursive=False, include_hidden=False, seps=None):
    
    
    if not seps:
        if os.path.altsep:
            seps = (os.path.sep, os.path.altsep)
        else:
            seps = os.path.sep
    escaped_seps = ''.join(map(re.escape, seps))
    any_sep = f'[{escaped_seps}]' if len(seps) > 1 else escaped_seps
    not_sep = f'[^{escaped_seps}]'
    if include_hidden:
        one_last_segment = f'{not_sep}+'
        one_segment = f'{one_last_segment}{any_sep}'
        any_segments = f'(?:.+{any_sep})?'
        any_last_segments = '.*'
    else:
        one_last_segment = f'[^{escaped_seps}.]{not_sep}*'
        one_segment = f'{one_last_segment}{any_sep}'
        any_segments = f'(?:{one_segment})*'
        any_last_segments = f'{any_segments}(?:{one_last_segment})?'

    results = []
    parts = re.split(any_sep, pat)
    last_part_idx = len(parts) - 1
    for idx, part in enumerate(parts):
        if part == '*':
            results.append(one_segment if idx < last_part_idx else one_last_segment)
        elif recursive and part == '**':
            if idx < last_part_idx:
                if parts[idx + 1] != '**':
                    results.append(any_segments)
            else:
                results.append(any_last_segments)
        else:
            if part:
                if not include_hidden and part[0] in '*?':
                    results.append(r'(?!\.)')
                results.extend(fnmatch._translate(part, f'{not_sep}*', not_sep))
            if idx < last_part_idx:
                results.append(any_sep)
    res = ''.join(results)
    return fr'(?s:{res})\Z'
def _unquote_wrapped_quotes(value):
    
    

    if value and ((value[0] == value[-1] == '"') or (value[0] == value[-1] == "'")):
        value = value[1:-1]

    return value.replace("\\ ", " ").replace('\\"', '"').replace("\\'", "'")
def euclidean_length(self) -> float:
        
        
        
        if len(self.__components) == 0:
            raise Exception("Vector is empty")
        squares = [c ** 2 for c in self.__components]
        return math.sqrt(sum(squares))
def get_nodes_by_type(self, nodetype):
        
        
        
        nodes = []
        if isinstance(self, nodetype):
            nodes.append(self)
        for attr in self.child_nodelists:
            nodelist = getattr(self, attr, None)
            if nodelist:
                nodes.extend(nodelist.get_nodes_by_type(nodetype))
        return nodes
def _fallback_range(self, utterances, lang, message, fb_range):
        
        
        msg = message.reply(
            'mycroft.skills.fallback',
            data={'utterance': utterances[0][0],
                  'lang': lang,
                  'fallback_range': (fb_range.start, fb_range.stop)}
        )
        response = self.bus.wait_for_response(msg, timeout=10)
        if response and response.data['handled']:
            ret = IntentMatch('Fallback', None, {}, None)
        else:
            ret = None
        return ret
def decimal(size: int, precision: int = 1, separator: str = " ") -> str:
    

    
    return _to_str(size, ("kB", "MB", "GB", "TB", "PB", "EB", "ZB", "YB"), 1000, precision=precision, separator=separator)
def tamper(payload, headers=None):
    
    
    

    retVal = payload

    if payload:
        retVal = re.sub(r"(?i)(SELECT|UPDATE|INSERT|DELETE)\s+", r"\g<1>\t", payload)
        retVal = re.sub(r"\s*=\s*", " LIKE ", retVal)

    return retVal
def create_order(self, symbol: str, type: OrderType, side: OrderSide, amount, price=None, params={}):
        
        
        
        self.load_markets()
        market = self.market(symbol)
        marginResult = self.handle_margin_mode_and_params('createOrder', params)
        marginMode = marginResult[0]
        request = self.create_order_request(symbol, type, side, amount, price, params)
        response = None
        if market['swap']:
            response = self.privateSwapPostTradeOrderPlace(request)
        else:
            if marginMode is not None:
                response = self.privateSpotPostMarginOrderNew(request)
            else:
                response = self.privateSpotPostSpotOrderNew(request)
        #
        # spot and margin
        #
        #     {
        #         "code": 0,
        #         "order_id": "198361cecdc65f9c8c9bb2fa68faec40"
        #     }
        #
        # swap
        #
        #     {
        #         "code": 0,
        #         "data": "1590873693003714560"
        #     }
        #
        order = self.parse_order(response, market)
        order['symbol'] = market['symbol']
        order['type'] = type
        order['side'] = side
        order['amount'] = amount
        order['price'] = price
        return order
def cancel_order(self, id: str, symbol: Str = None, params={}):
        
        
        
        self.load_markets()
        market = None
        if symbol is not None:
            market = self.market(symbol)
        marketType = None
        marketType, params = self.handle_market_type_and_params('cancelOrder', market, params)
        method = self.get_supported_mapping(marketType, {
            'spot': 'privateSpotPostSpotOrderCancel',
            'margin': 'privateSpotPostMarginOrderCancel',
            'swap': 'privateSwapPostTradeCancelOrder',
        })
        marginMode, query = self.handle_margin_mode_and_params('cancelOrder', params)
        if marginMode is not None:
            method = 'privateSpotPostMarginOrderCancel'
            marketType = 'margin'
        id = str(id)
        request = {
            'order_id': id,
        }
        if marketType == 'swap':
            if symbol is None:
                raise ArgumentsRequired(self.id + ' cancelOrder() requires a symbol argument')
            request['instrument_id'] = market['id']
        else:
            request['market'] = marketType
        response = getattr(self, method)(self.extend(request, query))
        #
        # spot and margin
        #
        #     {
        #         "code": 0,
        #         "success": [
        #             "198361cecdc65f9c8c9bb2fa68faec40",
        #             "3fb0d98e51c18954f10d439a9cf57de0"
        #         ],
        #         "error": [
        #             "78a7104e3c65cc0c5a212a53e76d0205"
        #         ]
        #     }
        #
        # swap
        #
        #     {
        #         "code": 0,
        #         "data": "1590923061186531328"
        #     }
        #
        if (marketType == 'spot') or (marketType == 'margin'):
            canceledOrders = self.safe_value(response, 'success', [])
            numCanceledOrders = len(canceledOrders)
            if numCanceledOrders != 1:
                raise OrderNotFound(self.id + ' cancelOrder() ' + id + ' not found')
        return response
def fetch_ticker(self, symbol, params={}):
        
        
        
        self.load_markets()
        market = self.market(symbol)
        method = 'publicSpotGetTicker'
        request = {}
        if market['swap']:
            method = 'publicSwapGetPublicTicker'
            request['instrument_id'] = market['id']
        else:
            request['symbol'] = market['id']
        response = getattr(self, method)(self.extend(request, params))
        #
        # spot
        #
        #    {
        #        "ticker": [{
        #            "vol": 40717.4461,
        #            "change": -1.91,
        #            "base_vol": 392447999.65374,
        #            "sell": 9592.23,
        #            "last": 9592.22,
        #            "symbol": "btc_usdt",
        #            "low": 9476.24,
        #            "buy": 9592.03,
        #            "high": 9793.87
        #        }],
        #        "date": 1589874294,
        #        "code": 0
        #    }
        #
        # swap
        #
        #     {
        #         "code": 0,
        #         "data": {
        #             "instrument_id": "BTCUSDTPERP",
        #             "index_price": "20141.9967",
        #             "mark_price": "20139.3404",
        #             "max_buy_price": "21146.4838",
        #             "min_sell_price": "19132.2725",
        #             "best_bid": "20140.0998",
        #             "best_bid_size": "3116",
        #             "best_ask": "20140.0999",
        #             "best_ask_size": "9004",
        #             "high_24h": "20410.6496",
        #             "open_24h": "20308.6998",
        #             "low_24h": "19600",
        #             "last": "20140.0999",
        #             "last_qty": "2",
        #             "volume_24h": "49382816",
        #             "price_change_percent": "-0.008301855936636448",
        #             "open_interest": "-",
        #             "timestamp": 1663221614998
        #         }
        #     }
        #
        date = self.safe_integer(response, 'date')
        tickers = self.safe_value(response, 'ticker', [])
        data = self.safe_value(response, 'data', {})
        firstTicker = self.safe_value(tickers, 0, {})
        result = None
        if market['swap']:
            result = data
        else:
            result = self.extend({'date': date}, firstTicker)
        return self.parse_ticker(result, market)
def fetch_ohlcv(self, symbol, timeframe='1m', since=None, limit=None, params={}):
        
        
        
        self.load_markets()
        market = self.market(symbol)
        method = 'publicSpotGetKline'
        request = {}
        if market['swap']:
            method = 'publicSwapGetPublicCandles'
            request['instrument_id'] = market['id']
            request['granularity'] = timeframe
            if limit is not None:
                request['limit'] = limit
        else:
            request['symbol'] = market['id']
            request['period'] = self.safe_string(self.timeframes, timeframe, timeframe)
            if since is not None:
                startTime = self.parse_to_int(since / 1000)
                request['start_time'] = startTime
                if limit is not None:
                    duration = self.parse_timeframe(timeframe)
                    request['end_time'] = self.sum(startTime, limit * duration)
            elif limit is not None:
                endTime = self.seconds()
                duration = self.parse_timeframe(timeframe)
                request['start_time'] = self.sum(endTime, -limit * duration)
        response = getattr(self, method)(self.extend(request, params))
        #
        # spot
        #
        #     {
        #         "code":0,
        #         "data":[
        #             [1556712900,2205.899,0.029967,0.02997,0.029871,0.029927],
        #             [1556713800,1912.9174,0.029992,0.030014,0.029955,0.02996],
        #             [1556714700,1556.4795,0.029974,0.030019,0.029969,0.02999],
        #         ]
        #     }
        #
        # swap
        #
        #     {
        #         "code": 0,
        #         "data": {
        #             "instrument_id": "BTCUSDTPERP",
        #             "granularity": "1m",
        #             "candles": [
        #                 [1588089660000,"6900","6900","6900","6900","0","0"],
        #                 [1588089720000,"6900","6900","6900","6900","0","0"],
        #                 [1588089780000,"6900","6900","6900","6900","0","0"],
        #             ]
        #         }
        #     }
        #
        candles = None
        if market['swap']:
            data = self.safe_value(response, 'data', {})
            candles = self.safe_value(data, 'candles', [])
        else:
            candles = self.safe_value(response, 'data', [])
        return self.parse_ohlcvs(candles, market, timeframe, since, limit)
def data2json(self, in_file):
        
        
        

        result = self.txt2data(in_file)
        self._save_json(result)
        return result
def pick_vote_log(in_logfile, out_txtfile):
        
        
        
        pattern_vote = r'(Player\d+)\(([A-Za-z]+)\): (\d+) \| (I vote to eliminate Player\d+)'
        ignore_text = reflection
        HINT_TEXT = r"ready to AnnounceGameResult"
        pattern_moderator = r'\[([^\]]+)\]\. Say ONLY: I vote to eliminate ...'
        in_valid_block = False

        with open(in_logfile, "r") as f:
            lines = f.read()
            split_lines = lines.split(HINT_TEXT)

            if len(split_lines) < 2:
                print(f"Key text :{HINT_TEXT} not found in {in_logfile}")
                return

            relevant_lines = split_lines[1].split("\n")
            with open(out_txtfile, "w") as out:
                for line in relevant_lines:
                    if re.search(pattern_moderator, line):
                        in_valid_block = True
                        out.write(line.lstrip() + "\n")

                    elif in_valid_block and re.search(pattern_vote, line):
                        out.write(line + "\n")
                    elif ignore_text in line:
                        in_valid_block = False
def who(self):
        
        return f"{self.name} ({self.bulb.mac_addr})"
def unregister(self, bulb: Light) -> None:
        
        if bulb.mac_addr in self.entities:
            entity = self.entities[bulb.mac_addr]
            entity.registered = False
            entity.async_write_ha_state()
            _LOGGER.debug("Disconnected from %s", entity.who)
def limitQuery(self, num, query, field):
        
        
        

        limitedQuery  = query
        limitStr      = queries[kb.dbms].limit
        fromIndex     = limitedQuery.index(" FROM ")
        untilFrom     = limitedQuery[:fromIndex]
        fromFrom      = limitedQuery[fromIndex+1:]

        if kb.dbms in ( "MySQL", "PostgreSQL" ):
            limitStr = queries[kb.dbms].limit % (num, 1)
            limitedQuery += " %s" % limitStr

        # TODO: fix Partial UNION query SQL injection technique for Oracle
        elif kb.dbms == "Oracle":
            if query.startswith("SELECT "):
                limitedQuery = "%s FROM (%s, %s" % (untilFrom, untilFrom, limitStr)
            else:
                limitedQuery = "%s FROM (SELECT %s, %s" % (untilFrom, field, limitStr)
            limitedQuery  = limitedQuery % fromFrom
            limitedQuery += "=%d" % (num + 1)

        elif kb.dbms == "Microsoft SQL Server":
            if re.search(" ORDER BY ", limitedQuery, re.I):
                untilOrderChar = limitedQuery.index(" ORDER BY ")
                limitedQuery   = limitedQuery[:untilOrderChar]

            limitedQuery  = limitedQuery.replace("SELECT ", (limitStr % 1), 1)
            limitedQuery  = "%s WHERE %s " % (limitedQuery, field)
            limitedQuery += "NOT IN (%s" % (limitStr % num)
            limitedQuery += "%s %s)" % (field, fromFrom)

        return limitedQuery
def forgeInbandQuery(self, query, position, count, comment, prefix, suffix, char, multipleUnions=None):
        
        
        

        if query.startswith("SELECT "):
            query        = query[len("SELECT "):]

        inbandQuery = self.prefixQuery("UNION ALL SELECT ", prefix=prefix)

        if query.startswith("TOP"):
            topNum       = re.search("\ATOP\s+([\d]+)\s+", query, re.I).group(1)
            query        = query[len("TOP %s " % topNum):]
            inbandQuery += "TOP %s " % topNum

        intoRegExp = re.search("(\s+INTO (DUMP|OUT)FILE\s+\'(.+?)\')", query, re.I)

        if intoRegExp:
            intoRegExp = intoRegExp.group(1)
            query = query[:query.index(intoRegExp)]

        if kb.dbms == DBMS.ORACLE and inbandQuery.endswith(" FROM DUAL"):
            inbandQuery = inbandQuery[:-len(" FROM DUAL")]

        for element in range(count):
            if element > 0:
                inbandQuery += ", "

            if element == position:
                if " FROM " in query and not query.startswith("SELECT ") and "(CASE WHEN (" not in query:
                    conditionIndex = query.index(" FROM ")
                    inbandQuery += query[:conditionIndex]
                else:
                    inbandQuery += query
            else:
                inbandQuery += char

        if " FROM " in query and not query.startswith("SELECT ") and "(CASE WHEN (" not in query:
            conditionIndex = query.index(" FROM ")
            inbandQuery += query[conditionIndex:]

        if kb.dbms == DBMS.ORACLE:
            if " FROM " not in inbandQuery:
                inbandQuery += " FROM DUAL"

        if intoRegExp:
            inbandQuery += intoRegExp

        if multipleUnions:
            inbandQuery += " UNION ALL SELECT "

            for element in range(count):
                if element > 0:
                    inbandQuery += ", "

                if element == position:
                    inbandQuery += multipleUnions
                else:
                    inbandQuery += char

            if kb.dbms == DBMS.ORACLE:
                inbandQuery += " FROM DUAL"

        inbandQuery = self.suffixQuery(inbandQuery, comment, suffix)

        return inbandQuery
def prefixQuery(self, expression, prefix=None, where=None, clause=None):
        
        
        

        if conf.direct:
            return self.payloadDirect(expression)

        expression = unescaper.unescape(expression)
        query = None

        if where is None and kb.technique and kb.technique in kb.injection.data:
            where = kb.injection.data[kb.technique].where

        # If we are replacing (<where>) the parameter original value with
        # our payload do not prepend with the prefix
        if where == PAYLOAD.WHERE.REPLACE:
            query = ""

        # If the technique is stacked queries (<stype>) do not put a space
        # after the prefix or it is in GROUP BY / ORDER BY (<clause>)
        elif kb.technique == PAYLOAD.TECHNIQUE.STACKED:
            query = kb.injection.prefix
        elif kb.injection.clause == [2, 3] or kb.injection.clause == [ 2 ] or kb.injection.clause == [ 3 ]:
            query = kb.injection.prefix
        elif clause == [2, 3] or clause == [ 2 ] or clause == [ 3 ]:
            query = prefix

        # In any other case prepend with the full prefix
        else:
            query = kb.injection.prefix or prefix or ""

            if not (expression and expression[0] == ";"):
                query += " "

        query = "%s%s" % (query, expression)
        query = self.cleanupPayload(query)

        return query
def from_model_id_low_bit(
        cls,
        model_id: str,
        model_kwargs: Optional[dict] = None,
        *,
        tokenizer_id: Optional[str] = None,
        **kwargs: Any,
    ) -> LLM:
        
        
        

        logger.warning("BigdlLLM was deprecated. Please use IpexLLM instead.")

        try:
            from bigdl.llm.transformers import (
                AutoModel,
                AutoModelForCausalLM,
            )
            from transformers import AutoTokenizer, LlamaTokenizer

        except ImportError:
            raise ValueError(
                "Could not import bigdl-llm or transformers. "
                "Please install it with `pip install --pre --upgrade bigdl-llm[all]`."
            )

        _model_kwargs = model_kwargs or {}
        _tokenizer_id = tokenizer_id or model_id

        try:
            tokenizer = AutoTokenizer.from_pretrained(_tokenizer_id, **_model_kwargs)
        except Exception:
            tokenizer = LlamaTokenizer.from_pretrained(_tokenizer_id, **_model_kwargs)

        try:
            model = AutoModelForCausalLM.load_low_bit(model_id, **_model_kwargs)
        except Exception:
            model = AutoModel.load_low_bit(model_id, **_model_kwargs)

        if "trust_remote_code" in _model_kwargs:
            _model_kwargs = {
                k: v for k, v in _model_kwargs.items() if k != "trust_remote_code"
            }

        return cls(
            model_id=model_id,
            model=model,
            tokenizer=tokenizer,
            model_kwargs=_model_kwargs,
            **kwargs,
        )
def _make_sp_model_file(self, vocab, prefix="spm", add_mask_token=False):
    
    
    model_prefix = os.path.join(
        tempfile.mkdtemp(dir=self.get_temp_dir()),  # New subdir each time.
        prefix)
    input_file = model_prefix + "_train_input.txt"
    # Create input text for training the sp model from the tokens provided.
    # Repeat tokens, the earlier the more, because they are sorted by frequency.
    input_text = []
    for i, token in enumerate(vocab):
      input_text.append(" ".join([token] * (len(vocab) - i)))
    with tf.io.gfile.GFile(input_file, "w") as f:
      f.write("\n".join(input_text + [""]))
    control_symbols = "[CLS],[SEP]"
    full_vocab_size = len(vocab) + 6  # <pad>, <unk>, [CLS], [SEP], <s>, </s>.
    if add_mask_token:
      control_symbols += ",[MASK]"
      full_vocab_size += 1
    flags = dict(
        model_prefix=model_prefix,
        model_type="word",
        input=input_file,
        pad_id=0, unk_id=1, control_symbols=control_symbols,
        vocab_size=full_vocab_size,
        bos_id=full_vocab_size-2, eos_id=full_vocab_size-1)
    SentencePieceTrainer.Train(
        " ".join(["--{}={}".format(k, v) for k, v in flags.items()]))
    return model_prefix + ".model"
def write_main_index(links: List[Link], out_dir: Path=OUTPUT_DIR, finished: bool=False) -> None:
    

    log_indexing_process_started(len(links))

    try:
        with timed_index_update(out_dir / SQL_INDEX_FILENAME):
            write_sql_main_index(links, out_dir=out_dir)
            os.chmod(out_dir / SQL_INDEX_FILENAME, int(OUTPUT_PERMISSIONS, base=8)) # set here because we don't write it with atomic writes

    except (KeyboardInterrupt, SystemExit):
        stderr('[!] Warning: Still writing index to disk...', color='lightyellow')
        stderr('    Run archivebox init to fix any inconsisntencies from an ungraceful exit.')
        with timed_index_update(out_dir / SQL_INDEX_FILENAME):
            write_sql_main_index(links, out_dir=out_dir)
            os.chmod(out_dir / SQL_INDEX_FILENAME, int(OUTPUT_PERMISSIONS, base=8)) # set here because we don't write it with atomic writes
        raise SystemExit(0)

    log_indexing_process_finished()
def send_message(self, message: str = "", **kwargs: Any) -> None:
        

        _LOGGER.debug("Sending signal message")

        data = kwargs.get(ATTR_DATA)

        try:
            data = DATA_SCHEMA(data)
        except vol.Invalid as ex:
            _LOGGER.error("Invalid message data: %s", ex)
            raise ex

        filenames = self.get_filenames(data)
        attachments_as_bytes = self.get_attachments_as_bytes(
            data, CONF_MAX_ALLOWED_DOWNLOAD_SIZE_BYTES, self._hass
        )

        try:
            self._signal_cli_rest_api.send_message(
                message, self._recp_nrs, filenames, attachments_as_bytes
            )
        except SignalCliRestApiError as ex:
            _LOGGER.error("%s", ex)
            raise ex
def __delete(self):
        ""

        # Notes about running with dummy_thread:
        #
        # Must take care to not raise an exception if dummy_thread is being
        # used (and thus this module is being used as an instance of
        # dummy_threading).  dummy_thread.get_ident() always returns -1 since
        # there is only one thread if dummy_thread is being used.  Thus
        # len(_active) is always <= 1 here, and any Thread instance created
        # overwrites the (if any) thread currently registered in _active.
        #
        # An instance of _MainThread is always created by 'threading'.  This
        # gets overwritten the instant an instance of Thread is created; both
        # threads return -1 from dummy_thread.get_ident() and thus have the
        # same key in the dict.  So when the _MainThread instance created by
        # 'threading' tries to clean itself up when atexit calls this method
        # it gets a KeyError if another Thread instance was created.
        #
        # This all means that KeyError from trying to delete something from
        # _active if dummy_threading is being used is a red herring.  But
        # since it isn't if dummy_threading is *not* being used then don't
        # hide the exception.

        _active_limbo_lock.acquire()
        try:
            try:
                del _active[_get_ident()]
            except KeyError:
                if 'dummy_threading' not in _sys.modules:
                    raise
        finally:
            _active_limbo_lock.release()
def __init__(self, parties, action=None, timeout=None):
        

        
        self._cond = Condition(Lock())
        self._action = action
        self._timeout = timeout
        self._parties = parties
        self._state = 0 #0 filling, 1, draining, -1 resetting, -2 broken
        self._count = 0
def cache_key(self, template_name, skip=None):
        
        
        
        skip_prefix = ''

        if skip:
            matching = [origin.name for origin in skip if origin.template_name == template_name]
            if matching:
                skip_prefix = self.generate_hash(matching)

        return '-'.join(s for s in (str(template_name), skip_prefix) if s)
def newton_cg(grad_hess, func, grad, x0, args=(), tol=1e-4,
              maxiter=100, maxinner=200, line_search=True, warn=True):
    
    
    
    x0 = np.asarray(x0).flatten()
    xk = x0
    k = 1

    if line_search:
        old_fval = func(x0, *args)
        old_old_fval = None

    # Outer loop: our Newton iteration
    while k <= maxiter:
        # Compute a search direction pk by applying the CG method to
        #  del2 f(xk) p = - fgrad f(xk) starting from 0.
        fgrad, fhess_p = grad_hess(xk, *args)

        absgrad = np.abs(fgrad)
        if np.max(absgrad) < tol:
            break

        maggrad = np.sum(absgrad)
        eta = min([0.5, np.sqrt(maggrad)])
        termcond = eta * maggrad

        # Inner loop: solve the Newton update by conjugate gradient, to
        # avoid inverting the Hessian
        xsupi = _cg(fhess_p, fgrad, maxiter=maxinner, tol=termcond)

        alphak = 1.0

        if line_search:
            try:
                alphak, fc, gc, old_fval, old_old_fval, gfkp1 = \
                    _line_search_wolfe12(func, grad, xk, xsupi, fgrad,
                                         old_fval, old_old_fval, args=args)
            except _LineSearchError:
                warnings.warn('Line Search failed')
                break

        xk = xk + alphak * xsupi        # upcast if necessary
        k += 1

    if warn and k > maxiter:
        warnings.warn("newton-cg failed to converge. Increase the "
                      "number of iterations.")
    return xk
def setup(self) -> None:
        
        self.pollable_characteristics: list[tuple[int, int]] = []
        self.watchable_characteristics: list[tuple[int, int]] = []

        char_types = self.get_characteristic_types()

        # Setup events and/or polling for characteristics directly attached to this entity
        for char in self.service.characteristics.filter(char_types=char_types):
            self._setup_characteristic(char)

        # Setup events and/or polling for characteristics attached to sub-services of this
        # entity (like an INPUT_SOURCE).
        for service in self.accessory.services.filter(parent_service=self.service):
            for char in service.characteristics.filter(char_types=char_types):
                self._setup_characteristic(char)
def _get_err_type(self, context: RequestContext, response: Response) -> Optional[str]:
        
        
        
        try:
            parsed_response = parse_response(context.operation, response)
            return parsed_response["Error"]["Code"]
        except Exception:
            if config.DEBUG_ANALYTICS:
                LOG.exception("error parsing error response")
            return None
def _get_err_type(self, context: RequestContext, response: Response) -> Optional[str]:
        
        
        
        try:
            if context.service_exception:
                return context.service_exception.code

            response = parse_response(context.operation, response)
            return response["Error"]["Code"]
        except Exception:
            if config.DEBUG_ANALYTICS:
                LOG.exception("error parsing error response")
            return None
def load(self) -> List[Document]:
        
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": self.cube_api_token,
        }

        logger.info(f"Loading metadata from {self.cube_api_url}...")
        response = requests.get(f"{self.cube_api_url}/meta", headers=headers)
        response.raise_for_status()
        raw_meta_json = response.json()
        cube_data_objects = raw_meta_json.get("cubes", [])

        logger.info(f"Found {len(cube_data_objects)} cube data objects in metadata.")

        if not cube_data_objects:
            raise ValueError("No cubes found in metadata.")

        docs = []

        for cube_data_obj in cube_data_objects:
            cube_data_obj_name = cube_data_obj.get("name")
            cube_data_obj_type = cube_data_obj.get("type")
            cube_data_obj_is_public = cube_data_obj.get("public")
            measures = cube_data_obj.get("measures", [])
            dimensions = cube_data_obj.get("dimensions", [])

            logger.info(f"Processing {cube_data_obj_name}...")

            if not cube_data_obj_is_public:
                logger.info(f"Skipping {cube_data_obj_name} because it is not public.")
                continue

            for item in measures + dimensions:
                column_member_type = "measure" if item in measures else "dimension"
                dimension_values = []
                item_name = str(item.get("name"))
                item_type = str(item.get("type"))

                if (
                    self.load_dimension_values
                    and column_member_type == "dimension"
                    and item_type == "string"
                ):
                    dimension_values = self._get_dimension_values(item_name)

                metadata = dict(
                    table_name=str(cube_data_obj_name),
                    column_name=item_name,
                    column_data_type=item_type,
                    column_title=str(item.get("title")),
                    column_description=str(item.get("description")),
                    column_member_type=column_member_type,
                    column_values=dimension_values,
                    cube_data_obj_type=cube_data_obj_type,
                )

                page_content = f"{str(item.get('title'))}, "
                page_content += f"{str(item.get('description'))}"

                docs.append(Document(page_content=page_content, metadata=metadata))

        return docs
def visit_module(self, node: Module) -> None:
        
        if node.package:
            self.current_package = node.name
        else:
            # Strip name of the current module
            self.current_package = node.name[: node.name.rfind(".")]
def _convert_boxes(self, boxes):
        
        
        
        if isinstance(boxes, Boxes) or isinstance(boxes, RotatedBoxes):
            return boxes.tensor.numpy()
        else:
            return np.asarray(boxes)
def draw_binary_mask(
        self, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10
    ):
        
        
        
        if color is None:
            color = random_color(rgb=True, maximum=1)
        color = mplc.to_rgb(color)

        has_valid_segment = False
        binary_mask = binary_mask.astype("uint8")  # opencv needs uint8
        mask = GenericMask(binary_mask, self.output.height, self.output.width)
        shape2d = (binary_mask.shape[0], binary_mask.shape[1])

        if not mask.has_holes:
            # draw polygons for regular masks
            for segment in mask.polygons:
                area = mask_util.area(mask_util.frPyObjects([segment], shape2d[0], shape2d[1]))
                if area < (area_threshold or 0):
                    continue
                has_valid_segment = True
                segment = segment.reshape(-1, 2)
                self.draw_polygon(segment, color=color, edge_color=edge_color, alpha=alpha)
        else:
            # TODO: Use Path/PathPatch to draw vector graphics:
            # https://stackoverflow.com/questions/8919719/how-to-plot-a-complex-polygon
            rgba = np.zeros(shape2d + (4,), dtype="float32")
            rgba[:, :, :3] = color
            rgba[:, :, 3] = (mask.mask == 1).astype("float32") * alpha
            has_valid_segment = True
            self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))

        if text is not None and has_valid_segment:
            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)
            self._draw_text_in_mask(binary_mask, text, lighter_color)
        return self.output
def assertAlmostEqual(self, first, second, places=None, msg=None, delta=None):
        
        
        if first == second:
            # shortcut
            return
        if delta is not None and places is not None:
            raise TypeError("specify delta or places not both")

        if delta is not None:
            if abs(first - second) <= delta:
                return

            standardMsg = '%s != %s within %s delta' % (safe_repr(first),
                                                        safe_repr(second),
                                                        safe_repr(delta))
        else:
            if places is None:
                places = 7

            if round(abs(second-first), places) == 0:
                return

            standardMsg = '%s != %s within %r places' % (safe_repr(first),
                                                          safe_repr(second),
                                                          places)
        msg = self._formatMessage(msg, standardMsg)
        raise self.failureException(msg)
def assertRaises(self, expected_exception, *args, **kwargs):
        
        
        context = _AssertRaisesContext(expected_exception, self)
        return context.handle('assertRaises', args, kwargs)
def assertDictContainsSubset(self, subset, dictionary, msg=None):
        
        warnings.warn('assertDictContainsSubset is deprecated',
                      DeprecationWarning)
        missing = []
        mismatched = []
        for key, value in subset.items():
            if key not in dictionary:
                missing.append(key)
            elif value != dictionary[key]:
                mismatched.append('%s, expected: %s, actual: %s' %
                                  (safe_repr(key), safe_repr(value),
                                   safe_repr(dictionary[key])))

        if not (missing or mismatched):
            return

        standardMsg = ''
        if missing:
            standardMsg = 'Missing: %s' % ','.join(safe_repr(m) for m in
                                                    missing)
        if mismatched:
            if standardMsg:
                standardMsg += '; '
            standardMsg += 'Mismatched values: %s' % ','.join(mismatched)

        self.fail(self._formatMessage(msg, standardMsg))
def assertSequenceEqual(self, seq1, seq2, msg=None, seq_type=None):
        
        
        if seq_type is not None:
            seq_type_name = seq_type.__name__
            if not isinstance(seq1, seq_type):
                raise self.failureException('First sequence is not a %s: %s'
                                        % (seq_type_name, safe_repr(seq1)))
            if not isinstance(seq2, seq_type):
                raise self.failureException('Second sequence is not a %s: %s'
                                        % (seq_type_name, safe_repr(seq2)))
        else:
            seq_type_name = "sequence"

        differing = None
        try:
            len1 = len(seq1)
        except (TypeError, NotImplementedError):
            differing = 'First %s has no length.    Non-sequence?' % (
                    seq_type_name)

        if differing is None:
            try:
                len2 = len(seq2)
            except (TypeError, NotImplementedError):
                differing = 'Second %s has no length.    Non-sequence?' % (
                        seq_type_name)

        if differing is None:
            if seq1 == seq2:
                return

            seq1_repr = safe_repr(seq1)
            seq2_repr = safe_repr(seq2)
            if len(seq1_repr) > 30:
                seq1_repr = seq1_repr[:30] + '...'
            if len(seq2_repr) > 30:
                seq2_repr = seq2_repr[:30] + '...'
            elements = (seq_type_name.capitalize(), seq1_repr, seq2_repr)
            differing = '%ss differ: %s != %s\n' % elements

            for i in xrange(min(len1, len2)):
                try:
                    item1 = seq1[i]
                except (TypeError, IndexError, NotImplementedError):
                    differing += ('\nUnable to index element %d of first %s\n' %
                                 (i, seq_type_name))
                    break

                try:
                    item2 = seq2[i]
                except (TypeError, IndexError, NotImplementedError):
                    differing += ('\nUnable to index element %d of second %s\n' %
                                 (i, seq_type_name))
                    break

                if item1 != item2:
                    differing += ('\nFirst differing element %d:\n%s\n%s\n' %
                                 (i, item1, item2))
                    break
            else:
                if (len1 == len2 and seq_type is None and
                    type(seq1) != type(seq2)):
                    # The sequences are the same, but have differing types.
                    return

            if len1 > len2:
                differing += ('\nFirst %s contains %d additional '
                             'elements.\n' % (seq_type_name, len1 - len2))
                try:
                    differing += ('First extra element %d:\n%s\n' %
                                  (len2, seq1[len2]))
                except (TypeError, IndexError, NotImplementedError):
                    differing += ('Unable to index element %d '
                                  'of first %s\n' % (len2, seq_type_name))
            elif len1 < len2:
                differing += ('\nSecond %s contains %d additional '
                             'elements.\n' % (seq_type_name, len2 - len1))
                try:
                    differing += ('First extra element %d:\n%s\n' %
                                  (len1, seq2[len1]))
                except (TypeError, IndexError, NotImplementedError):
                    differing += ('Unable to index element %d '
                                  'of second %s\n' % (len1, seq_type_name))
        standardMsg = differing
        diffMsg = '\n' + '\n'.join(
            difflib.ndiff(pprint.pformat(seq1).splitlines(),
                          pprint.pformat(seq2).splitlines()))
        standardMsg = self._truncateMessage(standardMsg, diffMsg)
        msg = self._formatMessage(msg, standardMsg)
        self.fail(msg)
def assertFalse(self, expr, msg=None):
        
        if expr:
            msg = self._formatMessage(msg, "%s is not false" % safe_repr(expr))
            raise self.failureException(msg)
def assertItemsEqual(self, expected_seq, actual_seq, msg=None):
        
        
        with warnings.catch_warnings():
            if sys.py3kwarning:
                # Silence Py3k warning raised during the sorting
                for _msg in ["(code|dict|type) inequality comparisons",
                             "builtin_function_or_method order comparisons",
                             "comparing unequal types"]:
                    warnings.filterwarnings("ignore", _msg, DeprecationWarning)
            try:
                actual = collections.Counter(iter(actual_seq))
                expected = collections.Counter(iter(expected_seq))
            except TypeError:
                # Unsortable items (example: set(), complex(), ...)
                actual = list(actual_seq)
                expected = list(expected_seq)
                missing, unexpected = unorderable_list_difference(expected, actual)
            else:
                if actual == expected:
                    return
                missing = list(expected - actual)
                unexpected = list(actual - expected)

        errors = []
        if missing:
            errors.append('Expected, but missing:\n    %s' %
                           safe_repr(missing))
        if unexpected:
            errors.append('Unexpected, but present:\n    %s' %
                           safe_repr(unexpected))
        if errors:
            standardMsg = '\n'.join(errors)
            self.fail(self._formatMessage(msg, standardMsg))
def assertCountEqual(self, first, second, msg=None):
        

        
        actual_seq, expected_seq = list(first), list(second)
        try:
            actual = collections.Counter(actual_seq)
            expected = collections.Counter(expected_seq)
        except TypeError:
            # Handle case with unhashable elements
            differences = _count_diff_all_purpose(actual_seq, expected_seq)
        else:
            if actual == expected:
                return
            differences = _count_diff_hashable(actual_seq, expected_seq)

        if differences:
            standardMsg = 'Element counts were not equal:\n'
            lines = ['First has %d, Second has %d:  %r' % diff for diff in differences]
            diffMsg = '\n'.join(lines)
            standardMsg = self._truncateMessage(standardMsg, diffMsg)
            msg = self._formatMessage(msg, standardMsg)
            self.fail(msg)
def _get_batch_indices(self, dataloader: object) -> List[List[int]]:  # batches x samples
        
        batch_sampler = getattr(dataloader, "batch_sampler", None)
        if not isinstance(batch_sampler, _IndexBatchSamplerWrapper):
            self._warning_cache.warn(
                f"Couldn't infer the batch indices fetched from your dataloader: `{type(dataloader).__name__}`"
            )
            return []
        return batch_sampler.seen_batch_indices
def _predict_step(
        self, batch: Any, batch_idx: int, dataloader_idx: int, dataloader_iter: Optional[Iterator]
    ) -> None:
        

        
        trainer = self.trainer
        assert (data_fetcher := self._data_fetcher) is not None

        if not (using_dataloader_iter := isinstance(data_fetcher, _DataLoaderIterDataFetcher)):
            batch = trainer.precision_plugin.convert_input(batch)
            batch = trainer.lightning_module._on_before_batch_transfer(batch, dataloader_idx=dataloader_idx)
            batch = call._call_strategy_hook(trainer, "batch_to_device", batch, dataloader_idx=dataloader_idx)

        self.batch_progress.increment_ready()

        if not using_dataloader_iter:
            any_on_epoch = self._store_data_for_prediction_writer(batch_idx, dataloader_idx)

        # the `_step` methods don't take a batch_idx when `dataloader_iter` is used, but all other hooks still do,
        # so we need different kwargs
        hook_kwargs = self._build_kwargs(batch, batch_idx, dataloader_idx if self.num_dataloaders > 1 else None)
        step_args = hook_kwargs.values() if not using_dataloader_iter else (dataloader_iter,)

        call._call_callback_hooks(trainer, "on_predict_batch_start", *hook_kwargs.values())
        call._call_lightning_module_hook(trainer, "on_predict_batch_start", *hook_kwargs.values())

        self.batch_progress.increment_started()

        # configure step_kwargs
        predictions = call._call_strategy_hook(trainer, "predict_step", *step_args)
        if predictions is None:
            self._warning_cache.warn("predict returned None if it was on purpose, ignore this warning...")

        self.batch_progress.increment_processed()

        if using_dataloader_iter:
            # update the hook kwargs now that the step method might have consumed the iterator
            batch = data_fetcher._batch
            batch_idx = data_fetcher._batch_idx
            dataloader_idx = data_fetcher._dataloader_idx
            hook_kwargs = self._build_kwargs(batch, batch_idx, dataloader_idx if self.num_dataloaders > 1 else None)

        call._call_callback_hooks(trainer, "on_predict_batch_end", predictions, *hook_kwargs.values())
        call._call_lightning_module_hook(trainer, "on_predict_batch_end", predictions, *hook_kwargs.values())

        self.batch_progress.increment_completed()

        if self._return_predictions or any_on_epoch:
            self._predictions[dataloader_idx].append(move_data_to_device(predictions, torch.device("cpu")))
def hvac_mode(self) -> HVACMode | str | None:
        
        fibaro_operation_mode = self.fibaro_op_mode
        if isinstance(fibaro_operation_mode, str):
            with suppress(ValueError):
                return HVACMode(fibaro_operation_mode.lower())
        elif fibaro_operation_mode in OPMODES_HVAC:
            return OPMODES_HVAC[fibaro_operation_mode]
        return None
def upload_local_artifacts(
    resource_type: str,
    resource_id: str,
    resource_dict: Dict,
    property_name: str,
    parent_dir: str,
    uploader: S3Uploader,
    extension: Optional[str] = None,
) -> str:
    
    
    

    local_path = jmespath.search(property_name, resource_dict)

    if local_path is None:
        # Build the root directory and upload to S3
        local_path = parent_dir

    if is_s3_protocol_url(local_path):
        # A valid CloudFormation template will specify artifacts as S3 URLs.
        # This check is supporting the case where your resource does not
        # refer to local artifacts
        # Nothing to do if property value is an S3 URL
        LOG.debug("Property %s of %s is already a S3 URL", property_name, resource_id)
        return cast(str, local_path)

    local_path = make_abs_path(parent_dir, local_path)

    # Or, pointing to a folder. Zip the folder and upload (zip_method is changed based on resource type)
    if is_local_folder(local_path):
        return zip_and_upload(
            local_path,
            uploader,
            extension,
            zip_method=make_zip_with_lambda_permissions if resource_type in LAMBDA_LOCAL_RESOURCES else make_zip,
        )

    # Path could be pointing to a file. Upload the file
    if is_local_file(local_path):
        return uploader.upload_with_dedup(local_path)

    raise InvalidLocalPathError(resource_id=resource_id, property_name=property_name, local_path=local_path)
def zip_folder(folder_path):
    
    
    
    md5hash = dir_checksum(folder_path, followlinks=True)
    filename = os.path.join(tempfile.gettempdir(), "data-" + md5hash)

    zipfile_name = make_zip(filename, folder_path)
    try:
        yield zipfile_name, md5hash
    finally:
        if os.path.exists(zipfile_name):
            os.remove(zipfile_name)
def __init__(self,
               is_training,
               depth_multiplier,
               min_depth,
               pad_to_multiple,
               conv_hyperparams,
               freeze_batchnorm,
               inplace_batchnorm_update,
               fpn_min_level=3,
               fpn_max_level=7,
               additional_layer_depth=256,
               reuse_weights=None,
               use_explicit_padding=None,
               use_depthwise=None,
               override_base_feature_extractor_hyperparams=False,
               name='ResNet101V1_FPN'):
    
    
    super(SSDResNet101V1FpnKerasFeatureExtractor, self).__init__(
        is_training=is_training,
        depth_multiplier=depth_multiplier,
        min_depth=min_depth,
        pad_to_multiple=pad_to_multiple,
        conv_hyperparams=conv_hyperparams,
        freeze_batchnorm=freeze_batchnorm,
        inplace_batchnorm_update=inplace_batchnorm_update,
        resnet_v1_base_model=resnet_v1.resnet_v1_101,
        resnet_v1_base_model_name='resnet_v1_101',
        use_explicit_padding=use_explicit_padding,
        use_depthwise=use_depthwise,
        override_base_feature_extractor_hyperparams=
        override_base_feature_extractor_hyperparams,
        name=name)
def parse_qs(qs, keep_blank_values=False, strict_parsing=False,
             encoding='utf-8', errors='replace', max_num_fields=None, separator='&'):
    
    
    parsed_result = {}
    pairs = parse_qsl(qs, keep_blank_values, strict_parsing,
                      encoding=encoding, errors=errors,
                      max_num_fields=max_num_fields, separator=separator)
    for name, value in pairs:
        if name in parsed_result:
            parsed_result[name].append(value)
        else:
            parsed_result[name] = [value]
    return parsed_result
def urlencode(query, doseq=False, safe='', encoding=None, errors=None,
              quote_via=quote_plus):
    
    

    if hasattr(query, "items"):
        query = query.items()
    else:
        # It's a bother at times that strings and string-like objects are
        # sequences.
        try:
            # non-sequence items should not work with len()
            # non-empty strings will fail this
            if len(query) and not isinstance(query[0], tuple):
                raise TypeError
            # Zero-length sequences of all types will get here and succeed,
            # but that's a minor nit.  Since the original implementation
            # allowed empty dicts that type of behavior probably should be
            # preserved for consistency
        except TypeError:
            ty, va, tb = sys.exc_info()
            raise TypeError("not a valid non-string sequence "
                            "or mapping object").with_traceback(tb)

    l = []
    if not doseq:
        for k, v in query:
            if isinstance(k, bytes):
                k = quote_via(k, safe)
            else:
                k = quote_via(str(k), safe, encoding, errors)

            if isinstance(v, bytes):
                v = quote_via(v, safe)
            else:
                v = quote_via(str(v), safe, encoding, errors)
            l.append(k + '=' + v)
    else:
        for k, v in query:
            if isinstance(k, bytes):
                k = quote_via(k, safe)
            else:
                k = quote_via(str(k), safe, encoding, errors)

            if isinstance(v, bytes):
                v = quote_via(v, safe)
                l.append(k + '=' + v)
            elif isinstance(v, str):
                v = quote_via(v, safe, encoding, errors)
                l.append(k + '=' + v)
            else:
                try:
                    # Is this a sufficient test for sequence-ness?
                    x = len(v)
                except TypeError:
                    # not a sequence
                    v = quote_via(str(v), safe, encoding, errors)
                    l.append(k + '=' + v)
                else:
                    # loop over the sequence
                    for elt in v:
                        if isinstance(elt, bytes):
                            elt = quote_via(elt, safe)
                        else:
                            elt = quote_via(str(elt), safe, encoding, errors)
                        l.append(k + '=' + elt)
    return '&'.join(l)
def __init__(self, output_prefix, corpus, num_features, num_best=None, chunksize=1024, shardsize=16384):
        
        

        
        if output_prefix:
            outfile, self.output_prefix = tempfile.mkstemp(prefix='tmpindex')
            outfile.close()
        else:
            self.output_prefix = output_prefix
        self.num_features = num_features
        self.num_best = num_best
        self.normalize = True
        self.chunksize = int(chunksize)
        self.shardsize = shardsize
        self.shards = []
        self.fresh_docs, self.fresh_nnz = [], 0

        if corpus is not None:
            self.add_documents(corpus)
def similarity_by_id(self, docpos):
        
        
        
        query = self.vector_by_id(docpos)
        norm, self.normalize = self.normalize, False
        result = self[query]
        self.normalize = norm
        return result
def __init__(self, corpus, similarity_matrix, num_best=None, chunksize=256, normalized=None,
                 normalize_queries=True, normalize_documents=True):
        

        

        
        self.similarity_matrix = similarity_matrix

        self.corpus = list(corpus)
        self.num_best = num_best
        self.chunksize = chunksize
        if normalized is not None:
            warnings.warn(
                'Parameter normalized will be removed in 5.0.0, use normalize_queries and normalize_documents instead',
                category=DeprecationWarning,
            )
            self.normalized = normalized
        else:
            self.normalized = (normalize_queries, normalize_documents)

        # Normalization of features is undesirable, since soft cosine similarity requires special
        # normalization using the similarity matrix. Therefore, we would just be normalizing twice,
        # increasing the numerical error.
        self.normalize = False

        # index is simply an array from 0 to size of corpus.
        self.index = numpy.arange(len(corpus))
def _get_pi_version():
    
      # noqa
    # Check if file exist
    if not path.isfile('/proc/cpuinfo'):
        return None

    with open('/proc/cpuinfo', 'r') as f:
        cpuinfo = f.read()

    # Match a line like 'Revision   : a01041'
    revision = search(r'^Revision\s+:\s+(\w+)$', cpuinfo,
                      flags=MULTILINE | IGNORECASE)
    if not revision:
        # Couldn't find the hardware revision, assume it is not a Pi
        return None

    # Determine the Pi version using the processor bits using the new-style
    # revision format
    revision = revision.group(1)
    if int(revision, base=16) & 0x800000:
        version = ((int(revision, base=16) & 0xF000) >> 12) + 1
        print('Raspberry Pi revision: {}'.format(version))
        return version

    # Only look a the last five characters, as the first one changes if the
    # warranty bit is set
    revision = revision[-5:]
    if revision in {'0002', '0003', '0004', '0005', '0006', '0007', '0008',
                    '0009', '000d', '000e', '000f', '0010', '0012', '0013',
                    '0015', ' 900032'}:
        print('Raspberry Pi 1')
        return 1
    elif revision in {'01041', '21041', '22042'}:
        print('Raspberry Pi 2')
        return 2
    elif revision in {'02082', '22082', '32082', '220a0', '02082', '020a0'}:
        print('Raspberry Pi 3, CM3')
        return 3
    elif revision in {'02100'}:
        print('Raspberry CM3+')
        return 3
    elif revision in {'03111'}:
        print('Raspberry Pi 4')
        return 4
    else:
        print('Not a Raspberry Pi: {}'.format(revision))
        return None
def _get_pi_version():
    
      # noqa
    # Check if file exist
    if not path.isfile('/proc/cpuinfo'):
        return None

    with open('/proc/cpuinfo', 'r') as f:
        cpuinfo = f.read()

    # Match a line like 'Revision   : a01041'
    revision = search(r'^Revision\s+:\s+(\w+)$', cpuinfo,
                      flags=MULTILINE | IGNORECASE)
    if not revision:
        # Couldn't find the hardware revision, assume it is not a Pi
        return None

    # Determine the Pi version using the processor bits using the new-style
    # revision format
    revision = int(revision.group(1), base=16)
    if revision & 0x800000:
        return ((revision & 0xF000) >> 12) + 1

    # If it is not using the new style revision format,
    # then it must be a Raspberry Pi 1
    return 1
def fit(self, X, y, **fit_params):
        
        
        return self._fit(X, y, **fit_params)
def parse(self, s):
        
        
        
        mutable_dict = json.loads(s)
        immutable_dict = frozendict(mutable_dict)
        frozendict.__repr__ = mutable_dict.__repr__
        return immutable_dict
def parse(self, s):
        
        
        
        return json.loads(s, object_pairs_hook=FrozenOrderedDict)
def __init__(self, *args, **kwargs):
        
        
        super(BoolParameter, self).__init__(*args, is_bool=True, **kwargs)
def is_line_short_enough(  # noqa: C901
    line: Line, *, mode: Mode, line_str: str = ""
) -> bool:
    
    
    if not line_str:
        line_str = line_to_string(line)

    if Preview.multiline_string_handling not in mode:
        return (
            len(line_str) <= mode.line_length
            and "\n" not in line_str  # multiline strings
            and not line.contains_standalone_comments()
        )

    if line.contains_standalone_comments():
        return False
    if "\n" not in line_str:
        # No multiline strings (MLS) present
        return len(line_str) <= mode.line_length

    first, *_, last = line_str.split("\n")
    if len(first) > mode.line_length or len(last) > mode.line_length:
        return False

    # Traverse the AST to examine the context of the multiline string (MLS),
    # tracking aspects such as depth and comma existence,
    # to determine whether to split the MLS or keep it together.
    # Depth (which is based on the existing bracket_depth concept)
    # is needed to determine nesting level of the MLS.
    # Includes special case for trailing commas.
    commas: List[int] = []  # tracks number of commas per depth level
    multiline_string: Optional[Leaf] = None
    # store the leaves that contain parts of the MLS
    multiline_string_contexts: List[LN] = []

    max_level_to_update = math.inf  # track the depth of the MLS
    for i, leaf in enumerate(line.leaves):
        if max_level_to_update == math.inf:
            had_comma: Optional[int] = None
            if leaf.bracket_depth + 1 > len(commas):
                commas.append(0)
            elif leaf.bracket_depth + 1 < len(commas):
                had_comma = commas.pop()
            if (
                had_comma is not None
                and multiline_string is not None
                and multiline_string.bracket_depth == leaf.bracket_depth + 1
            ):
                # Have left the level with the MLS, stop tracking commas
                max_level_to_update = leaf.bracket_depth
                if had_comma > 0:
                    # MLS was in parens with at least one comma - force split
                    return False

        if leaf.bracket_depth <= max_level_to_update and leaf.type == token.COMMA:
            # Ignore non-nested trailing comma
            # directly after MLS/MLS-containing expression
            ignore_ctxs: List[Optional[LN]] = [None]
            ignore_ctxs += multiline_string_contexts
            if not (leaf.prev_sibling in ignore_ctxs and i == len(line.leaves) - 1):
                commas[leaf.bracket_depth] += 1
        if max_level_to_update != math.inf:
            max_level_to_update = min(max_level_to_update, leaf.bracket_depth)

        if is_multiline_string(leaf):
            if len(multiline_string_contexts) > 0:
                # >1 multiline string cannot fit on a single line - force split
                return False
            multiline_string = leaf
            ctx: LN = leaf
            # fetch the leaf components of the MLS in the AST
            while str(ctx) in line_str:
                multiline_string_contexts.append(ctx)
                if ctx.parent is None:
                    break
                ctx = ctx.parent

    # May not have a triple-quoted multiline string at all,
    # in case of a regular string with embedded newlines and line continuations
    if len(multiline_string_contexts) == 0:
        return True

    return all(val == 0 for val in commas)
def can_omit_invisible_parens(
    rhs: RHSResult,
    line_length: int,
) -> bool:
    
    
    line = rhs.body
    bt = line.bracket_tracker
    if not bt.delimiters:
        # Without delimiters the optional parentheses are useless.
        return True

    max_priority = bt.max_delimiter_priority()
    delimiter_count = bt.delimiter_count_with_priority(max_priority)
    if delimiter_count > 1:
        # With more than one delimiter of a kind the optional parentheses read better.
        return False

    if delimiter_count == 1:
        if (
            Preview.wrap_multiple_context_managers_in_parens in line.mode
            and max_priority == COMMA_PRIORITY
            and rhs.head.is_with_stmt
        ):
            # For two context manager with statements, the optional parentheses read
            # better. In this case, `rhs.body` is the context managers part of
            # the with statement. `rhs.head` is the `with (` part on the previous
            # line.
            return False
        # Otherwise it may also read better, but we don't do it today and requires
        # careful considerations for all possible cases. See
        # https://github.com/psf/black/issues/2156.

    if max_priority == DOT_PRIORITY:
        # A single stranded method call doesn't require optional parentheses.
        return True

    assert len(line.leaves) >= 2, "Stranded delimiter"

    # With a single delimiter, omit if the expression starts or ends with
    # a bracket.
    first = line.leaves[0]
    second = line.leaves[1]
    if first.type in OPENING_BRACKETS and second.type not in CLOSING_BRACKETS:
        if _can_omit_opening_paren(line, first=first, line_length=line_length):
            return True

        # Note: we are not returning False here because a line might have *both*
        # a leading opening bracket and a trailing closing bracket.  If the
        # opening bracket doesn't match our rule, maybe the closing will.

    penultimate = line.leaves[-2]
    last = line.leaves[-1]

    if (
        last.type == token.RPAR
        or last.type == token.RBRACE
        or (
            # don't use indexing for omitting optional parentheses;
            # it looks weird
            last.type == token.RSQB
            and last.parent
            and last.parent.type != syms.trailer
        )
    ):
        if penultimate.type in OPENING_BRACKETS:
            # Empty brackets don't help.
            return False

        if is_multiline_string(first):
            # Additional wrapping of a multiline string in this situation is
            # unnecessary.
            return True

        if _can_omit_closing_paren(line, last=last, line_length=line_length):
            return True

    return False
def fib(n: int) -> int:
    
    
    

    # precondition
    assert isinstance(n, int) and (n >= 0), "'n' must been an int and >= 0"

    tmp = 0
    fib1 = 1
    ans = 1  # this will be return

    for _ in range(n - 1):
        tmp = ans
        ans += fib1
        fib1 = tmp

    return ans
def lookup_plex_media(hass, content_type, content_id):
    
    content = json.loads(content_id)

    if isinstance(content, int):
        content = {"plex_key": content}
        content_type = DOMAIN

    plex_server_name = content.pop("plex_server", None)
    shuffle = content.pop("shuffle", 0)

    plex_server = get_plex_server(hass, plex_server_name=plex_server_name)

    media = plex_server.lookup_media(content_type, **content)
    if media is None:
        raise HomeAssistantError(f"Plex media not found using payload: '{content_id}'")

    playqueue = plex_server.create_playqueue(media, shuffle=shuffle)
    return (playqueue, plex_server)
def checkpoint_fs_path(self) -> str:
        
        
        checkpoint_dir_name = StorageContext._make_checkpoint_dir_name(
            self.current_checkpoint_index
        )
        return os.path.join(self.trial_fs_path, checkpoint_dir_name)
def persist_current_checkpoint(self, checkpoint: "Checkpoint") -> "Checkpoint":
        
        
        # TODO(justinvyu): Fix this cyclical import.
        from ray.train._checkpoint import Checkpoint

        logger.debug(
            "Copying checkpoint files to storage path:\n"
            "({source_fs}, {source}) -> ({dest_fs}, {destination})".format(
                source=checkpoint.path,
                destination=self.checkpoint_fs_path,
                source_fs=checkpoint.filesystem,
                dest_fs=self.storage_filesystem,
            )
        )
        self.storage_filesystem.create_dir(self.checkpoint_fs_path)
        _pyarrow_fs_copy_files(
            source=checkpoint.path,
            destination=self.checkpoint_fs_path,
            source_filesystem=checkpoint.filesystem,
            destination_filesystem=self.storage_filesystem,
        )

        persisted_checkpoint = Checkpoint(
            filesystem=self.storage_filesystem,
            path=self.checkpoint_fs_path,
        )
        logger.debug(f"Checkpoint successfully created at: {persisted_checkpoint}")
        return persisted_checkpoint
def get_fs_and_path(
    storage_path: Union[str, os.PathLike],
    storage_filesystem: Optional[pyarrow.fs.FileSystem] = None,
) -> Tuple[pyarrow.fs.FileSystem, str]:
    
    
    storage_path = str(storage_path)

    if storage_filesystem:
        return storage_filesystem, storage_path

    return pyarrow.fs.FileSystem.from_uri(storage_path)
def _should_exclude(self, path: str) -> bool:
        
        path = Path(path)
        relative_path = path.relative_to(self._root_path).as_posix()
        alt = os.path.join(relative_path, "") if path.is_dir() else None

        for excl in self._exclude:
            if fnmatch.fnmatch(relative_path, excl):
                return True
            if alt and fnmatch.fnmatch(alt, excl):
                return True
        return False
def reformat_hex(i: int) -> bytes:
    
    
    
    if i < 0:
        raise ValueError("Input must be non-negative")

    hex_rep = format(i, "08x")[-8:]
    little_endian_hex = b""
    for i in [3, 2, 1, 0]:
        little_endian_hex += hex_rep[2 * i : 2 * i + 2].encode("utf-8")
    return little_endian_hex
def get_product_list_context(
    request: "HttpRequest", filter_set: "ProductCategoryFilter"
) -> dict:
    
    # Avoiding circular dependency
    from ..filters import SORT_BY_FIELDS

    qs = filter_set.qs
    if not filter_set.form.is_valid():
        qs = qs.none()
    products_paginated = get_paginator_items(
        qs, settings.PAGINATE_BY, request.GET.get("page")
    )
    products_and_availability = list(
        products_with_availability(
            products_paginated,
            request.discounts,
            request.country,
            request.currency,
            request.extensions,
        )
    )
    now_sorted_by = get_now_sorted_by(filter_set)
    arg_sort_by = request.GET.get("sort_by")
    is_descending = arg_sort_by.startswith("-") if arg_sort_by else False
    return {
        "filter_set": filter_set,
        "products": products_and_availability,
        "products_paginated": products_paginated,
        "sort_by_choices": SORT_BY_FIELDS,
        "now_sorted_by": now_sorted_by,
        "is_descending": is_descending,
    }
def test_get_total_stats(ray_start_regular_shared, op_two_block):
    
    block_params, block_meta_list = op_two_block
    stats = DatasetStats(
        metadata={"Read": block_meta_list},
        parent=None,
    )

    dataset_stats_summary = stats.to_summary()
    op_stats = dataset_stats_summary.operators_stats[0]

    # simple case with only one block / summary, result should match difference between
    # the start and end time
    assert (
        dataset_stats_summary.get_total_wall_time()
        == op_stats.latest_end_time - op_stats.earliest_start_time
    )

    # total time across all blocks is sum of wall times of blocks
    assert dataset_stats_summary.get_total_time_all_blocks() == sum(
        block_params["wall_time"]
    )

    cpu_time_stats = op_stats.cpu_time
    assert dataset_stats_summary.get_total_cpu_time() == cpu_time_stats.get("sum")

    peak_memory_stats = op_stats.memory
    assert dataset_stats_summary.get_max_heap_memory() == peak_memory_stats.get("max")
def mock_test_setup(
    hass,
    test_api_calendar,
    mock_calendars_list,
    config_entry,
):
    
    mock_calendars_list({"items": [test_api_calendar]})
    config_entry.add_to_hass(hass)
    return
def fetch_ohlcv(self, symbol: str, timeframe='1m', since: Optional[int] = None, limit: Optional[int] = None, params={}):
        
        
        
        self.load_markets()
        market = self.market(symbol)
        request = {
            'symbol': market['id'],
        }
        until = self.safe_integer_2(params, 'until', 'till')
        if limit is None:
            limit = 1000
        request['limit'] = limit
        marketType = 'spot' if market['spot'] else 'swap'
        timeframes = self.options['timeframes'][marketType]
        selectedTimeframe = self.safe_string(timeframes, timeframe, timeframe)
        duration = self.parse_timeframe(timeframe)
        if market['spot']:
            request['period'] = selectedTimeframe
            request['limit'] = limit
            if since is not None:
                request['after'] = since
                if until is None:
                    request['before'] = self.sum(since, limit * duration * 1000)
            if until is not None:
                request['before'] = until
        elif market['contract']:
            request['granularity'] = selectedTimeframe
            now = self.milliseconds()
            if since is None:
                request['startTime'] = now - limit * (duration * 1000)
                request['endTime'] = now
            else:
                request['startTime'] = since
                if until is not None:
                    request['endTime'] = until
                else:
                    request['endTime'] = self.sum(since, limit * duration * 1000)
        ommitted = self.omit(params, ['until', 'till'])
        extended = self.extend(request, ommitted)
        response = None
        if market['spot']:
            response = self.publicSpotGetMarketCandles(extended)
        else:
            response = self.publicMixGetMarketCandles(extended)
        #  [["1645911960000","39406","39407","39374.5","39379","35.526","1399132.341"]]
        data = self.safe_value(response, 'data', response)
        return self.parse_ohlcvs(data, market, timeframe, since, limit)
def fetch_canceled_orders(self, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):
        
        
        
        self.load_markets()
        self.check_required_symbol('fetchCanceledOrders', symbol)
        market = self.market(symbol)
        paginate = False
        paginate, params = self.handle_option_and_params(params, 'fetchCanceledOrders', 'paginate')
        if paginate:
            isStop = self.safe_value_2(params, 'stop', 'trigger', False)
            cursorReceived = 'orderId' if (market['spot'] and not isStop) else 'endId'
            cursorSent = 'after' if (market['spot'] and not isStop) else 'lastEndId'
            return self.fetch_paginated_call_cursor('fetchCanceledOrders', symbol, since, limit, params, cursorReceived, cursorSent, None, 50)
        response = self.fetch_canceled_and_closed_orders(symbol, since, limit, params)
        result = []
        for i in range(0, len(response)):
            entry = response[i]
            status = self.parse_order_status(self.safe_string_2(entry, 'state', 'status'))
            if status == 'canceled':
                result.append(entry)
        return self.parse_orders(result, market, since, limit)
def borrow_cross_margin(self, code: str, amount, params={}):
        
        
        
        self.load_markets()
        currency = self.currency(code)
        request = {
            'coin': currency['code'],
            'borrowAmount': self.currency_to_precision(code, amount),
        }
        response = self.privateMarginPostV2MarginCrossedAccountBorrow(self.extend(request, params))
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1700876470931,
        #         "data": {
        #             "loanId": "1112122013642272769",
        #             "coin": "USDT",
        #             "borrowAmount": "4"
        #         }
        #     }
        #
        data = self.safe_value(response, 'data', {})
        return self.parse_margin_loan(data, currency)
def cancel_all_orders(self, symbol: Optional[str] = None, params={}):
        
        
        
        sandboxMode = self.safe_value(self.options, 'sandboxMode', False)
        self.load_markets()
        market = None
        if symbol is not None:
            market = self.market(symbol)
        subType = None
        subType, params = self.handle_sub_type_and_params('cancelAllOrders', market, params)
        productType = 'UMCBL' if (subType == 'linear') else 'DMCBL'
        if sandboxMode:
            productType = 'S' + productType
        marketType = None
        marketType, params = self.handle_market_type_and_params('cancelAllOrders', market, params)
        marginMode = None
        marginMode, params = self.handle_margin_mode_and_params('cancelAllOrders', params)
        if marketType == 'spot':
            if marginMode is None:
                raise NotSupported(self.id + ' cancelAllOrders() does not support spot markets, only spot-margin')
            self.check_required_symbol('cancelAllOrders', symbol)
            spotMarginRequest = {
                'symbol': market['info']['symbolName'],  # regular id like LTCUSDT_SPBL does not work here
            }
            if marginMode == 'cross':
                return self.privateMarginPostCrossOrderBatchCancelOrder(self.extend(spotMarginRequest, params))
            else:
                return self.privateMarginPostIsolatedOrderBatchCancelOrder(self.extend(spotMarginRequest, params))
        request = {
            'productType': productType,
            'marginCoin': market['settleId'],
        }
        stop = self.safe_value_2(params, 'stop', 'trigger')
        planType = self.safe_string(params, 'planType')
        params = self.omit(params, ['stop', 'trigger'])
        response = None
        if stop is not None or planType is not None:
            if planType is None:
                raise ArgumentsRequired(self.id + ' cancelOrder() requires a planType parameter for stop orders, either normal_plan, profit_plan, loss_plan, pos_profit, pos_loss, moving_plan or track_plan')
            response = self.privateMixPostPlanCancelAllPlan(self.extend(request, params))
        else:
            response = self.privateMixPostOrderCancelAllOrders(self.extend(request, params))
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1663312535998,
        #         "data": {
        #             "result": True,
        #             "order_ids": ["954564352813969409"],
        #             "fail_infos": [
        #                 {
        #                     "order_id": "",
        #                     "err_code": "",
        #                     "err_msg": ""
        #                 }
        #             ]
        #         }
        #     }
        #
        return response
def borrow_margin(self, code: str, amount, symbol: Str = None, params={}):
        
        
        
        self.load_markets()
        currency = self.currency(code)
        request = {
            'coin': currency['info']['coinName'],
            'borrowAmount': self.currency_to_precision(code, amount),
        }
        response = None
        marginMode = None
        marginMode, params = self.handle_margin_mode_and_params('borrowMargin', params)
        if (symbol is not None) or (marginMode == 'isolated'):
            if symbol is None:
                raise ArgumentsRequired(self.id + ' borrowMargin() requires a symbol argument')
            market = self.market(symbol)
            marketId = market['id']
            parts = marketId.split('_')
            marginMarketId = self.safe_string_upper(parts, 0)
            request['symbol'] = marginMarketId
            response = self.privateMarginPostMarginV1IsolatedAccountBorrow(self.extend(request, params))
        else:
            response = self.privateMarginPostMarginV1CrossAccountBorrow(self.extend(request, params))
        #
        # isolated
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1697250952516,
        #         "data": {
        #             "clientOid": null,
        #             "symbol": "BTCUSDT",
        #             "coin": "BTC",
        #             "borrowAmount": "0.001"
        #         }
        #     }
        #
        # cross
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1697251314271,
        #         "data": {
        #             "clientOid": null,
        #             "coin": "BTC",
        #             "borrowAmount": "0.0001"
        #         }
        #     }
        #
        data = self.safe_value(response, 'data', {})
        return self.parse_margin_loan(data, currency)
def fetch_closed_orders(self, symbol=None, since=None, limit=None, params={}):
        
        
        
        self.load_markets()
        self.check_required_symbol('fetchClosedOrders', symbol)
        market = self.market(symbol)
        response = self.fetch_canceled_and_closed_orders(symbol, since, limit, params)
        result = []
        for i in range(0, len(response)):
            entry = response[i]
            status = self.parse_order_status(self.safe_string_2(entry, 'state', 'status'))
            if status == 'closed':
                result.append(entry)
        return self.parse_orders(result, market, since, limit)
def fetch_open_orders(self, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):
        
        
        
        self.load_markets()
        request = {}
        market = None
        marketType = None
        marginMode = None
        response = None
        if symbol is not None:
            market = self.market(symbol)
            symbolRequest = (market['info']['symbolName']) if (marginMode is not None) else (market['id'])
            request['symbol'] = symbolRequest
        marketType, params = self.handle_market_type_and_params('fetchOpenOrders', market, params)
        marginMode, params = self.handle_margin_mode_and_params('fetchOpenOrders', params)
        stop = self.safe_value_2(params, 'stop', 'trigger')
        params = self.omit(params, ['stop', 'trigger'])
        if stop:
            self.check_required_symbol('fetchOpenOrders', symbol)
            if marketType == 'spot':
                if limit is not None:
                    request['pageSize'] = limit
                response = self.privateSpotPostPlanCurrentPlan(self.extend(request, params))
            else:
                response = self.privateMixGetPlanCurrentPlan(self.extend(request, params))
        else:
            if marketType == 'spot':
                if marginMode is not None:
                    clientOrderId = self.safe_string_2(params, 'clientOid', 'clientOrderId')
                    endTime = self.safe_integer_n(params, ['endTime', 'until', 'till'])
                    params = self.omit(params, ['until', 'till', 'clientOrderId'])
                    if clientOrderId is not None:
                        request['clientOid'] = clientOrderId
                    if endTime is not None:
                        request['endTime'] = endTime
                    if since is not None:
                        request['startTime'] = since
                    if limit is not None:
                        request['pageSize'] = limit
                    if marginMode == 'isolated':
                        response = self.privateMarginGetIsolatedOrderOpenOrders(self.extend(request, params))
                    elif marginMode == 'cross':
                        response = self.privateMarginGetCrossOrderOpenOrders(self.extend(request, params))
                else:
                    response = self.privateSpotPostTradeOpenOrders(self.extend(request, params))
            else:
                if market is None:
                    subType = None
                    subType, params = self.handle_sub_type_and_params('fetchOpenOrders', None, params)
                    productType = 'UMCBL' if (subType == 'linear') else 'DMCBL'
                    sandboxMode = self.safe_value(self.options, 'sandboxMode', False)
                    if sandboxMode:
                        productType = 'S' + productType
                    request['productType'] = productType
                    response = self.privateMixGetOrderMarginCoinCurrent(self.extend(request, params))
                else:
                    self.check_required_symbol('fetchOpenOrders', symbol)
                    response = self.privateMixGetOrderCurrent(self.extend(request, params))
        #
        #  spot
        #     {
        #       code: '00000',
        #       msg: 'success',
        #       requestTime: 1645921640193,
        #       data: [
        #         {
        #           accountId: '6394957606',
        #           symbol: 'BTCUSDT_SPBL',
        #           orderId: '881623995442958336',
        #           clientOrderId: '135335e9-b054-4e43-b00a-499f11d3a5cc',
        #           price: '39000.000000000000',
        #           quantity: '0.000700000000',
        #           orderType: 'limit',
        #           side: 'buy',
        #           status: 'new',
        #           fillPrice: '0.000000000000',
        #           fillQuantity: '0.000000000000',
        #           fillTotalAmount: '0.000000000000',
        #           cTime: '1645921460972'
        #         }
        #       ]
        #     }
        #
        # swap
        #     {
        #       code: '00000',
        #       msg: 'success',
        #       requestTime: 1645922324630,
        #       data: [
        #         {
        #           symbol: 'BTCUSDT_UMCBL',
        #           size: 0.001,
        #           orderId: '881627074081226752',
        #           clientOid: '881627074160918528',
        #           filledQty: 0,
        #           fee: 0,
        #           price: 38000,
        #           state: 'new',
        #           side: 'open_long',
        #           timeInForce: 'normal',
        #           totalProfits: 0,
        #           posSide: 'long',
        #           marginCoin: 'USDT',
        #           filledAmount: 0,
        #           orderType: 'limit',
        #           cTime: '1645922194995',
        #           uTime: '1645922194995'
        #         }
        #       ]
        #     }
        #
        # stop
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1652745815697,
        #         "data": [
        #             {
        #                 "orderId": "910246821491617792",
        #                 "symbol": "BTCUSDT_UMCBL",
        #                 "marginCoin": "USDT",
        #                 "size": "16",
        #                 "executePrice": "20000",
        #                 "triggerPrice": "24000",
        #                 "status": "not_trigger",
        #                 "orderType": "limit",
        #                 "planType": "normal_plan",
        #                 "side": "open_long",
        #                 "triggerType": "market_price",
        #                 "presetTakeProfitPrice": "0",
        #                 "presetTakeLossPrice": "0",
        #                 "cTime": "1652745674488"
        #             }
        #         ]
        #     }
        #
        # spot plan order
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1668134581006,
        #         "data": {
        #             "nextFlag": False,
        #             "endId": 974792555020390400,
        #             "orderList": [{
        #                 "orderId": "974792555020390400",
        #                 "symbol": "TRXUSDT_SPBL",
        #                 "size": "151",
        #                 "executePrice": "0.041572",
        #                 "triggerPrice": "0.041572",
        #                 "status": "not_trigger",
        #                 "orderType": "limit",
        #                 "side": "buy",
        #                 "triggerType": "fill_price",
        #                 "cTime": "1668134576563"
        #             }]
        #         }
        #     }
        #
        # isolated and cross margin
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1697773997250,
        #         "data": {
        #             "orderList": [
        #                 {
        #                     "symbol": "BTCUSDT",
        #                     "orderType": "limit",
        #                     "source": "WEB",
        #                     "orderId": "1099108898629627904",
        #                     "clientOid": "f9b55416029e4cc2bbbe2f40ac368c38",
        #                     "loanType": "autoLoan",
        #                     "price": "25000",
        #                     "side": "buy",
        #                     "status": "new",
        #                     "baseQuantity": "0.0002",
        #                     "quoteAmount": "5",
        #                     "fillPrice": "0",
        #                     "fillQuantity": "0",
        #                     "fillTotalAmount": "0",
        #                     "ctime": "1697773902588"
        #                 }
        #             ],
        #             "maxId": "1099108898629627904",
        #             "minId": "1099108898629627904"
        #         }
        #     }
        #
        if isinstance(response, str):
            response = json.loads(response)
        data = self.safe_value(response, 'data', [])
        if marginMode is not None:
            resultList = self.safe_value(data, 'orderList', [])
            return self.parse_orders(resultList, market, since, limit)
        if not isinstance(data, list):
            result = self.safe_value(data, 'orderList', [])
            return self.add_pagination_cursor_to_result(data, result)
        return self.parse_orders(data, market, since, limit)
def cancel_order(self, id: str, symbol: Str = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' cancelOrder() requires a symbol argument')
        self.load_markets()
        sandboxMode = self.safe_value(self.options, 'sandboxMode', False)
        market = None
        if sandboxMode:
            sandboxSymbol = self.convert_symbol_for_sandbox(symbol)
            market = self.market(sandboxSymbol)
        else:
            market = self.market(symbol)
        marginMode = None
        response = None
        marginMode, params = self.handle_margin_mode_and_params('cancelOrder', params)
        request = {}
        trailing = self.safe_value(params, 'trailing')
        stop = self.safe_value_2(params, 'stop', 'trigger')
        params = self.omit(params, ['stop', 'trigger', 'trailing'])
        if not (market['spot'] and stop):
            request['symbol'] = market['id']
        if not ((market['swap'] or market['future']) and stop):
            request['orderId'] = id
        if (market['swap']) or (market['future']):
            productType = None
            productType, params = self.handle_product_type_and_params(market, params)
            request['productType'] = productType
            if stop or trailing:
                orderIdList = []
                orderId = {
                    'orderId': id,
                }
                orderIdList.append(orderId)
                request['orderIdList'] = orderIdList
            if trailing:
                planType = self.safe_string(params, 'planType', 'track_plan')
                request['planType'] = planType
                response = self.privateMixPostV2MixOrderCancelPlanOrder(self.extend(request, params))
            elif stop:
                response = self.privateMixPostV2MixOrderCancelPlanOrder(self.extend(request, params))
            else:
                response = self.privateMixPostV2MixOrderCancelOrder(self.extend(request, params))
        elif market['spot']:
            if marginMode is not None:
                if marginMode == 'isolated':
                    response = self.privateMarginPostV2MarginIsolatedCancelOrder(self.extend(request, params))
                elif marginMode == 'cross':
                    response = self.privateMarginPostV2MarginCrossedCancelOrder(self.extend(request, params))
            else:
                if stop:
                    response = self.privateSpotPostV2SpotTradeCancelPlanOrder(self.extend(request, params))
                else:
                    response = self.privateSpotPostV2SpotTradeCancelOrder(self.extend(request, params))
        else:
            raise NotSupported(self.id + ' cancelOrder() does not support ' + market['type'] + ' orders')
        #
        # spot, swap, future and spot margin
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1697690413177,
        #         "data": {
        #             "orderId": "1098758604547850241",
        #             "clientOid": "1098758604585598977"
        #         }
        #     }
        #
        # swap trigger
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1700711311791,
        #         "data": {
        #             "successList": [
        #                 {
        #                     "clientOid": "1111428059067125760",
        #                     "orderId": "1111428059067125761"
        #                 }
        #             ],
        #             "failureList": []
        #         }
        #     }
        #
        # spot trigger
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1700711728063,
        #         "data": {
        #             "result": "success"
        #         }
        #     }
        #
        data = self.safe_value(response, 'data', {})
        order = None
        if (market['swap'] or market['future']) and stop:
            orderInfo = self.safe_value(data, 'successList', [])
            order = orderInfo[0]
        else:
            order = data
        return self.parse_order(order, market)
def close_all_positions(self, params={}) -> List[Position]:
        
        
        
        self.load_markets()
        productType = None
        productType, params = self.handle_product_type_and_params(None, params)
        request = {
            'productType': productType,
        }
        response = self.privateMixPostV2MixOrderClosePositions(self.extend(request, params))
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1702975017017,
        #         "data": {
        #             "successList": [
        #                 {
        #                     "orderId": "1120923953904893955",
        #                     "clientOid": "1120923953904893956"
        #                 }
        #             ],
        #             "failureList": [],
        #             "result": False
        #         }
        #     }
        #
        data = self.safe_value(response, 'data', {})
        orderInfo = self.safe_value(data, 'successList', [])
        return self.parse_positions(orderInfo, None, params)
def transfer(self, code: str, amount, fromAccount, toAccount, params={}):
        
        
        
        self.load_markets()
        currency = self.currency(code)
        fromSwap = fromAccount == 'swap'
        toSwap = toAccount == 'swap'
        usdt = currency['code'] == 'USDT'
        if fromSwap:
            fromAccount = 'mix_usdt' if usdt else 'mix_usd'
        elif toSwap:
            toAccount = 'mix_usdt' if usdt else 'mix_usd'
        request = {
            'fromType': fromAccount,
            'toType': toAccount,
            'amount': amount,
            'coin': currency['info']['coinName'],
        }
        response = self.privateSpotPostWalletTransferV2(self.extend(request, params))
        #
        #    {
        #        "code": "00000",
        #        "msg": "success",
        #        "requestTime": 1668119107154,
        #        "data": "SUCCESS"
        #    }
        #
        return self.parse_transfer(response, currency)
def fetch_my_trades(self, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):
        
        
        
        self.check_required_symbol('fetchMyTrades', symbol)
        self.load_markets()
        market = self.market(symbol)
        request = {
            'symbol': market['id'],
        }
        if limit is not None:
            request['limit'] = limit
        response = None
        if market['spot']:
            response = self.privateSpotPostTradeFills(self.extend(request, params))
        else:
            response = self.privateMixGetOrderFills(self.extend(request, params))
        #
        #     {
        #       code: '00000',
        #       msg: 'success',
        #       requestTime: '1645918954082',
        #       data: [
        #         {
        #           accountId: '6394957606',
        #           symbol: 'LTCUSDT_SPBL',
        #           orderId: '864752115272552448',
        #           fillId: '864752115685969921',
        #           orderType: 'limit',
        #           side: 'buy',
        #           fillPrice: '127.92000000',
        #           fillQuantity: '0.10000000',
        #           fillTotalAmount: '12.79200000',
        #           feeCcy: 'LTC',
        #           fees: '0.00000000',
        #           cTime: '1641898891373'
        #         }
        #       ]
        #     }
        #
        data = self.safe_value(response, 'data')
        return self.parse_trades(data, market, since, limit)
def fetch_balance(self, params={}):
        
        
        
        sandboxMode = self.safe_value(self.options, 'sandboxMode', False)
        self.load_markets()
        request = {}
        marketType = None
        marginMode = None
        response = None
        marketType, params = self.handle_market_type_and_params('fetchBalance', None, params)
        marginMode, params = self.handle_margin_mode_and_params('fetchBalance', params)
        if (marketType == 'swap') or (marketType == 'future'):
            subType = None
            subType, params = self.handle_sub_type_and_params('fetchBalance', None, params)
            productType = 'UMCBL' if (subType == 'linear') else 'DMCBL'
            if sandboxMode:
                productType = 'S' + productType
            request['productType'] = productType
            response = self.privateMixGetAccountAccounts(self.extend(request, params))
        elif marginMode == 'isolated':
            response = self.privateMarginGetIsolatedAccountAssets(self.extend(request, params))
        elif marginMode == 'cross':
            response = self.privateMarginGetCrossAccountAssets(self.extend(request, params))
        elif marketType == 'spot':
            response = self.privateSpotGetAccountAssets(self.extend(request, params))
        else:
            raise NotSupported(self.id + ' fetchBalance() does not support ' + marketType + ' accounts')
        # spot
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1697507299139,
        #         "data": [
        #             {
        #                 "coinId": 1,
        #                 "coinName": "BTC",
        #                 "available": "0.00000000",
        #                 "frozen": "0.00000000",
        #                 "lock": "0.00000000",
        #                 "uTime": "1697248128000"
        #             },
        #         ]
        #     }
        #
        # swap
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1697507505367,
        #         "data": [
        #             {
        #                 "marginCoin": "STETH",
        #                 "locked": "0",
        #                 "available": "0",
        #                 "crossMaxAvailable": "0",
        #                 "fixedMaxAvailable": "0",
        #                 "maxTransferOut": "0",
        #                 "equity": "0",
        #                 "usdtEquity": "0",
        #                 "btcEquity": "0",
        #                 "crossRiskRate": "0",
        #                 "unrealizedPL": "0",
        #                 "bonus": "0"
        #             },
        #         ]
        #     }
        #
        # isolated margin
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1697501436571,
        #         "data": [
        #             {
        #                 "symbol": "BTCUSDT",
        #                 "coin": "BTC",
        #                 "totalAmount": "0.00021654",
        #                 "available": "0.00021654",
        #                 "transferable": "0.00021654",
        #                 "frozen": "0",
        #                 "borrow": "0",
        #                 "interest": "0",
        #                 "net": "0.00021654",
        #                 "ctime": "1697248128071"
        #             },
        #         ]
        #     }
        #
        # cross margin
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1697515463804,
        #         "data": [
        #             {
        #                 "coin": "BTC",
        #                 "totalAmount": "0.00024996",
        #                 "available": "0.00024996",
        #                 "transferable": "0.00004994",
        #                 "frozen": "0",
        #                 "borrow": "0.0001",
        #                 "interest": "0.00000001",
        #                 "net": "0.00014995",
        #                 "ctime": "1697251265504"
        #             },
        #         ]
        #     }
        #
        data = self.safe_value(response, 'data', [])
        return self.parse_balance(data)
def create_orders(self, orders: List[OrderRequest], params={}):
        
        
        
        self.load_markets()
        ordersRequests = []
        symbol = None
        marginMode = None
        for i in range(0, len(orders)):
            rawOrder = orders[i]
            marketId = self.safe_string(rawOrder, 'symbol')
            if symbol is None:
                symbol = marketId
            else:
                if symbol != marketId:
                    raise BadRequest(self.id + ' createOrders() requires all orders to have the same symbol')
            type = self.safe_string(rawOrder, 'type')
            side = self.safe_string(rawOrder, 'side')
            amount = self.safe_value(rawOrder, 'amount')
            price = self.safe_value(rawOrder, 'price')
            orderParams = self.safe_value(rawOrder, 'params', {})
            marginResult = self.handle_margin_mode_and_params('createOrders', params)
            currentMarginMode = marginResult[0]
            if currentMarginMode is not None:
                if marginMode is None:
                    marginMode = currentMarginMode
                else:
                    if marginMode != currentMarginMode:
                        raise BadRequest(self.id + ' createOrders() requires all orders to have the same margin mode(isolated or cross)')
            orderRequest = self.create_order_request(marketId, type, side, amount, price, orderParams)
            ordersRequests.append(orderRequest)
        market = self.market(symbol)
        symbolRequest = (market['info']['symbolName']) if (marginMode is not None) else (market['id'])
        request = {
            'symbol': symbolRequest,
        }
        response = None
        if market['spot']:
            request['orderList'] = ordersRequests
        if (market['swap']) or (market['future']):
            request['orderDataList'] = ordersRequests
            request['marginCoin'] = market['settleId']
            response = self.privateMixPostOrderBatchOrders(request)
        elif marginMode == 'isolated':
            response = self.privateMarginPostIsolatedOrderBatchPlaceOrder(request)
        elif marginMode == 'cross':
            response = self.privateMarginPostCrossOrderBatchPlaceOrder(request)
        else:
            response = self.privateSpotPostTradeBatchOrders(request)
        #
        # {
        #     "code": "00000",
        #     "data": {
        #       "orderInfo": [
        #         {
        #           "orderId": "1627293504612",
        #           "clientOid": "BITGET#1627293504612"
        #         }
        #       ],
        #       "failure":[
        #         {
        #           "orderId": "1627293504611",
        #           "clientOid": "BITGET#1627293504611",
        #           "errorMsg":"Duplicate clientOid"
        #         }
        #       ]
        #     },
        #     "msg": "success",
        #     "requestTime": 1627293504612
        #   }
        #
        data = self.safe_value(response, 'data', {})
        failure = self.safe_value(data, 'failure', [])
        orderInfo = self.safe_value_2(data, 'orderInfo', 'resultList', [])
        both = self.array_concat(orderInfo, failure)
        return self.parse_orders(both)
def fetch_positions(self, symbols: Strings = None, params={}) -> List[Position]:
        
        
        
        self.load_markets()
        paginate = False
        paginate, params = self.handle_option_and_params(params, 'fetchPositions', 'paginate')
        if paginate:
            return self.fetch_paginated_call_cursor('fetchPositions', None, None, None, params, 'endId', 'idLessThan')
        method = None
        useHistoryEndpoint = self.safe_bool(params, 'useHistoryEndpoint', False)
        if useHistoryEndpoint:
            method = 'privateMixGetV2MixPositionHistoryPosition'
        else:
            method, params = self.handle_option_and_params(params, 'fetchPositions', 'method', 'privateMixGetV2MixPositionAllPosition')
        market = None
        if symbols is not None:
            first = self.safe_string(symbols, 0)
            sandboxMode = self.safe_bool(self.options, 'sandboxMode', False)
            if sandboxMode:
                sandboxSymbol = self.convert_symbol_for_sandbox(first)
                market = self.market(sandboxSymbol)
            else:
                market = self.market(first)
        productType = None
        productType, params = self.handle_product_type_and_params(market, params)
        request = {
            'productType': productType,
        }
        response = None
        isHistory = False
        if method == 'privateMixGetV2MixPositionAllPosition':
            marginCoin = self.safe_string(params, 'marginCoin', 'USDT')
            if symbols is not None:
                marginCoin = market['settleId']
            elif productType == 'USDT-FUTURES':
                marginCoin = 'USDT'
            elif productType == 'USDC-FUTURES':
                marginCoin = 'USDC'
            elif productType == 'SUSDT-FUTURES':
                marginCoin = 'SUSDT'
            elif productType == 'SUSDC-FUTURES':
                marginCoin = 'SUSDC'
            elif (productType == 'SCOIN-FUTURES') or (productType == 'COIN-FUTURES'):
                if marginCoin is None:
                    raise ArgumentsRequired(self.id + ' fetchPositions() requires a marginCoin parameter that matches the productType')
            request['marginCoin'] = marginCoin
            response = self.privateMixGetV2MixPositionAllPosition(self.extend(request, params))
        else:
            isHistory = True
            if market is not None:
                request['symbol'] = market['id']
            response = self.privateMixGetV2MixPositionHistoryPosition(self.extend(request, params))
        #
        # privateMixGetV2MixPositionAllPosition
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1700807810221,
        #         "data": [
        #             {
        #                 "marginCoin": "USDT",
        #                 "symbol": "BTCUSDT",
        #                 "holdSide": "long",
        #                 "openDelegateSize": "0",
        #                 "marginSize": "3.73555",
        #                 "available": "0.002",
        #                 "locked": "0",
        #                 "total": "0.002",
        #                 "leverage": "20",
        #                 "achievedProfits": "0",
        #                 "openPriceAvg": "37355.5",
        #                 "marginMode": "crossed",
        #                 "posMode": "hedge_mode",
        #                 "unrealizedPL": "0.03",
        #                 "liquidationPrice": "31725.023602417",
        #                 "keepMarginRate": "0.004",
        #                 "markPrice": "37370.5",
        #                 "marginRatio": "0.029550120396",
        #                 "cTime": "1700807507275"
        #             }
        #         ]
        #     }
        #
        # privateMixGetV2MixPositionHistoryPosition
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1700808051002,
        #         "data": {
        #             "list": [
        #                 {
        #                     "symbol": "BTCUSDT",
        #                     "marginCoin": "USDT",
        #                     "holdSide": "long",
        #                     "openAvgPrice": "37272.1",
        #                     "closeAvgPrice": "37271.4",
        #                     "marginMode": "crossed",
        #                     "openTotalPos": "0.001",
        #                     "closeTotalPos": "0.001",
        #                     "pnl": "-0.0007",
        #                     "netProfit": "-0.0454261",
        #                     "totalFunding": "0",
        #                     "openFee": "-0.02236326",
        #                     "closeFee": "-0.02236284",
        #                     "utime": "1700720700400",
        #                     "ctime": "1700720651684"
        #                 },
        #             ],
        #             "endId": "1099351653866962944"
        #         }
        #     }
        #
        position = []
        if not isHistory:
            position = self.safe_list(response, 'data', [])
        else:
            data = self.safe_dict(response, 'data', {})
            position = self.safe_list(data, 'list', [])
        result = []
        for i in range(0, len(position)):
            result.append(self.parse_position(position[i], market))
        symbols = self.market_symbols(symbols)
        return self.filter_by_array_positions(result, 'symbol', symbols, False)
def set_leverage(self, leverage: Int, symbol: Str = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' setLeverage() requires a symbol argument')
        self.load_markets()
        sandboxMode = self.safe_value(self.options, 'sandboxMode', False)
        market = None
        if sandboxMode:
            sandboxSymbol = self.convert_symbol_for_sandbox(symbol)
            market = self.market(sandboxSymbol)
        else:
            market = self.market(symbol)
        productType = None
        productType, params = self.handle_product_type_and_params(market, params)
        request = {
            'symbol': market['id'],
            'marginCoin': market['settleId'],
            'leverage': self.number_to_string(leverage),
            'productType': productType,
            # 'holdSide': 'long',
        }
        response = self.privateMixPostV2MixAccountSetLeverage(self.extend(request, params))
        #
        #     {
        #         "code": "00000",
        #         "msg": "success",
        #         "requestTime": 1700864711517,
        #         "data": {
        #             "symbol": "BTCUSDT",
        #             "marginCoin": "USDT",
        #             "longLeverage": "25",
        #             "shortLeverage": "25",
        #             "crossMarginLeverage": "25",
        #             "marginMode": "crossed"
        #         }
        #     }
        #
        return response
def fit(self, X, y, sample_weight=None):
        
        

        if self.strategy not in ("mean", "median", "quantile", "constant"):
            raise ValueError("Unknown strategy type: %s, expected "
                             "'mean', 'median', 'quantile' or 'constant'"
                             % self.strategy)

        y = check_array(y, ensure_2d=False)
        if len(y) == 0:
            raise ValueError("y must not be empty.")

        self.output_2d_ = y.ndim == 2
        if y.ndim == 1:
            y = np.reshape(y, (-1, 1))
        self.n_outputs_ = y.shape[1]

        check_consistent_length(X, y, sample_weight)

        if self.strategy == "mean":
            if sample_weight is None:
                self.constant_ = np.mean(y, axis=0)
            else:
                self.constant_ = np.average(y, axis=0, weights=sample_weight)

        elif self.strategy == "median":
            if sample_weight is None:
                self.constant_ = np.median(y, axis=0)
            else:
                self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
                                                       percentile=50.)
                                  for k in range(self.n_outputs_)]

        elif self.strategy == "quantile":
            if self.quantile is None or not np.isscalar(self.quantile):
                raise ValueError("Quantile must be a scalar in the range "
                                 "[0.0, 1.0], but got %s." % self.quantile)

            percentile = self.quantile * 100.0
            if sample_weight is None:
                self.constant_ = np.percentile(y, axis=0, q=percentile)
            else:
                self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
                                                       percentile=percentile)
                                  for k in range(self.n_outputs_)]

        elif self.strategy == "constant":
            if self.constant is None:
                raise TypeError("Constant target value has to be specified "
                                "when the constant strategy is used.")

            self.constant = check_array(self.constant,
                                        accept_sparse=['csr', 'csc', 'coo'],
                                        ensure_2d=False)

            if self.output_2d_ and self.constant.shape[0] != y.shape[1]:
                raise ValueError(
                    "Constant target value should have "
                    "shape (%d, 1)." % y.shape[1])

            self.constant_ = self.constant

        self.constant_ = np.reshape(self.constant_, (1, -1))
        return self
def predict(self, X, return_std=False):
        
        
        
        check_is_fitted(self, "constant_")
        n_samples = _num_samples(X)

        y = np.ones((n_samples, self.n_outputs_)) * self.constant_
        y_std = np.zeros((n_samples, self.n_outputs_))

        if self.n_outputs_ == 1 and not self.output_2d_:
            y = np.ravel(y)
            y_std = np.ravel(y_std)

        return (y, y_std) if return_std else y
def ohlcv(self, pair: str, ticker_interval: str = None, copy: bool = True) -> DataFrame:
        
        
        
        if self.runmode in (RunMode.DRY_RUN, RunMode.LIVE):
            if ticker_interval:
                pairtick = (pair, ticker_interval)
            else:
                pairtick = (pair, self._config['ticker_interval'])

            return self._exchange.klines(pairtick, copy=copy)
        else:
            return DataFrame()
def historic_ohlcv(self, pair: str, timeframe: str = None) -> DataFrame:
        
        
        
        return load_pair_history(pair=pair,
                                 timeframe=timeframe or self._config['ticker_interval'],
                                 datadir=Path(self._config['datadir'])
                                 )
def orderbook(self, pair: str, maximum: int) -> Dict[str, List]:
        
        
        
        return self._exchange.fetch_l2_order_book(pair, maximum)
def get_analyzed_dataframe(self, pair: str, timeframe: str) -> Tuple[DataFrame, datetime]:
        
        
        
        pair_key = (pair, timeframe, '')
        if pair_key in self.__cached_pairs:
            if self.runmode in (RunMode.DRY_RUN, RunMode.LIVE):
                df, date = self.__cached_pairs[pair_key]
            else:
                df, date = self.__cached_pairs[pair_key]
                if self.__slice_index is not None:
                    max_index = self.__slice_index
                    df = df.iloc[max(0, max_index - MAX_DATAFRAME_CANDLES):max_index]
            return df, date
        else:
            return (DataFrame(), datetime.fromtimestamp(0, tz=timezone.utc))
def _set_cached_df(
        self,
        pair: str,
        timeframe: str,
        dataframe: DataFrame,
        candle_type: CandleType
    ) -> None:
        
        
        
        self.__cached_pairs[(pair, timeframe, candle_type)] = (
            dataframe, datetime.now(timezone.utc))
def send_msg(self, message: str, *, always_send: bool = False) -> None:
        
        
        
        if self.runmode not in (RunMode.DRY_RUN, RunMode.LIVE):
            return

        if always_send or message not in self.__msg_cache:
            self._msg_queue.append(message)
        self.__msg_cache[message] = True
def compress_image(pil_img, path, quality, max_size=None):
    
    
    
    if max_size:
        # The picture will be saved in a size <= max_size*max_size
        pil_img.thumbnail((max_size, max_size), Image.ANTIALIAS)
    quality = int(round(quality))
    if quality <= 0 or quality >= 100:
        raise Exception("SNAPSHOT_QUALITY (" + str(quality) + ") should be an integer in the range [1,99]")
    pil_img.save(path, quality=quality, optimize=True)
def load_root_test_conanfile(self, path, tested_reference, profile_host, profile_build,
                                 update=None, remotes=None, lockfile=None,
                                 tested_python_requires=None):
         
        

        app = ConanApp(self.conan_api.cache_folder)
        # necessary for correct resolution and update of remote python_requires
        app.load_remotes(update=update)

        loader = app.loader
        profile_host.options.scope(tested_reference)

        # do not try apply lock_python_requires for test_package/conanfile.py consumer
        conanfile = loader.load_consumer(path, user=tested_reference.user,
                                         channel=tested_reference.channel,
                                         graph_lock=lockfile, remotes=remotes,
                                         tested_python_requires=tested_python_requires)
        initialize_conanfile_profile(conanfile, profile_build, profile_host, CONTEXT_HOST, False)
        conanfile.display_name = "%s (test package)" % str(tested_reference)
        conanfile.output.scope = conanfile.display_name
        conanfile.tested_reference_str = repr(tested_reference)

        ref = RecipeReference(conanfile.name, conanfile.version, tested_reference.user,
                              tested_reference.channel)
        root_node = Node(ref, conanfile, recipe=RECIPE_CONSUMER, context=CONTEXT_HOST, path=path)
        return root_node
def analyze_binaries(self, graph, build_mode=None, remotes=None, update=None, lockfile=None,
                         build_modes_test=None, tested_graph=None):
         
        
        ConanOutput().title("Computing necessary packages")
        conan_app = ConanApp(self.conan_api.cache_folder)
        binaries_analyzer = GraphBinariesAnalyzer(conan_app)
        binaries_analyzer.evaluate_graph(graph, build_mode, lockfile, remotes, update,
                                         build_modes_test, tested_graph)
def test_exit_null_target_capacity(self, new_target_capacity):
        

        curr_config = ServeDeploySchema(
            target_capacity=None,
            applications=[create_app_config(name="app1")],
        )

        new_config = deepcopy(curr_config)
        new_config.target_capacity = new_target_capacity

        # When Serve is already running the applications at target_capacity
        # None, and then a target_capacity is applied, the direction must
        # become DOWN.
        assert (
            calculate_target_capacity_direction(
                curr_config,
                new_config,
                None,
            )
            == TargetCapacityDirection.DOWN
        )
def test_vm_gc():
    

    
    x = theano.tensor.vector()
    p = RunOnce()(x)
    mode = theano.Mode(linker=theano.gof.vm.VM_Linker(lazy=True))
    f = theano.function([theano.In(x, mutable=True)], [p + 1, p + 2],
                        mode=mode)
    f([1, 2, 3])

    p = RunOnce()(x)
    pp = p + p
    f = theano.function([x], [pp + pp],
                        mode=mode)
    f([1, 2, 3])
def hourglass_32(channel_means, channel_stds, bgr_ordering):
  

  network = hourglass_network.hourglass_32(num_channels=48)
  return CenterNetHourglassFeatureExtractor(
      network, channel_means=channel_means, channel_stds=channel_stds,
      bgr_ordering=bgr_ordering)
def test_use_explicit_o2o_to_parent_as_pk(self):
        
        
        
        self.assertEqual(ParkingLot3._meta.pk.name, "primary_key")
        # the child->parent link
        self.assertEqual(ParkingLot3._meta.get_ancestor_link(Place).name, "parent")
def build_gaussian_pyramid(image, max_layer=-1, downscale=2, sigma=None,
                           order=1, mode='reflect', cval=0):
    

    

    _check_factor(downscale)

    image = img_as_float(image)

    layer = 0
    rows = image.shape[0]
    cols = image.shape[1]

    # cast to float for consistent data type in pyramid
    prev_layer_image = image
    yield image

    # build downsampled images until max_layer is reached or downsampled image
    # has size of 1 in one direction
    while layer != max_layer:
        layer += 1

        layer_image = pyramid_reduce(prev_layer_image, downscale, sigma, order,
                                     mode, cval)

        prev_rows = rows
        prev_cols = cols
        prev_layer_image = layer_image
        rows = layer_image.shape[0]
        cols = layer_image.shape[1]

        # no change to previous pyramid layer
        if prev_rows == rows and prev_cols == cols:
            break

        yield layer_image
def pyramid_expand(image, upscale=2, sigma=None, order=1,
                   mode='reflect', cval=0,
                   preserve_range=False, *, channel_axis=None):
    

    
    _check_factor(upscale)
    image = convert_to_float(image, preserve_range)
    if channel_axis is not None:
        channel_axis = channel_axis % image.ndim
        out_shape = tuple(
            math.ceil(upscale * d) if ax != channel_axis else d
            for ax, d in enumerate(image.shape)
        )
    else:
        out_shape = tuple(math.ceil(upscale * d) for d in image.shape)

    if sigma is None:
        # automatically determine sigma which covers > 99% of distribution
        sigma = 2 * upscale / 6.0

    resized = resize(image, out_shape, order=order,
                     mode=mode, cval=cval, anti_aliasing=False)
    out = _smooth(resized, sigma, mode, cval, channel_axis)

    return out
def multizone_new_cast_status(self, group_uuid, cast_status):
        
        pass
def _media_controller(self):
        
        
        
        media_status = self.media_status
        media_controller = self._chromecast.media_controller

        if (
            media_status is None or media_status.player_state == "UNKNOWN"
        ) and self._dynamic_group_cast is not None:
            media_status = self.dynamic_group_media_status
            media_controller = self._dynamic_group_cast.media_controller

        if media_status is None or media_status.player_state == "UNKNOWN":
            groups = self.mz_media_status
            for k, val in groups.items():
                if val and val.player_state != "UNKNOWN":
                    media_controller = self.mz_mgr.get_multizone_mediacontroller(k)
                    break

        return media_controller
def num_tokens_for_base64_image(
        self, image_base64: str, detail: str = "high"
    ) -> int:
        
        
        

        if detail == "low":
            return 85  # Fixed cost for low detail images

        # Decode image from base64
        image_data = base64.b64decode(image_base64)

        # Convert byte data to image for size extraction
        image = Image.open(io.BytesIO(image_data))

        # Calculate the initial scale to fit within 2048 square while maintaining aspect ratio
        max_dimension = max(image.size)
        scale_factor = min(2048 / max_dimension, 1)  # Ensure we don't scale up
        new_width = int(image.size[0] * scale_factor)
        new_height = int(image.size[1] * scale_factor)

        # Scale such that the shortest side is 768px
        shortest_side = min(new_width, new_height)
        if shortest_side > 768:
            resize_factor = 768 / shortest_side
            new_width = int(new_width * resize_factor)
            new_height = int(new_height * resize_factor)

        # Calculate the number of 512px tiles needed
        width_tiles = math.ceil(new_width / 512)
        height_tiles = math.ceil(new_height / 512)
        total_tiles = width_tiles * height_tiles

        # Each tile costs 170 tokens, plus a base cost of 85 tokens for high detail
        token_cost = total_tiles * 170 + 85

        return token_cost
def test_32_64_decomposition_shape():
    
    
    X, y = make_blobs(
        n_samples=30,
        centers=[[0, 0, 0], [1, 1, 1]],
        random_state=0,
        cluster_std=0.1
    )
    X = StandardScaler().fit_transform(X)
    X -= X.min()

    # Compare the shapes (corresponds to the number of non-zero eigenvalues)
    kpca = KernelPCA()
    assert (kpca.fit_transform(X).shape ==
            kpca.fit_transform(X.astype(np.float32)).shape)
def get_package(self, package_reference, dest_folder, remote):
        
        
        zipped_files = self._call_remote(remote, "get_package", package_reference, dest_folder)
        files = unzip_and_get_files(zipped_files, dest_folder, PACKAGE_TGZ_NAME)
        # Issue #214 https://github.com/conan-io/conan/issues/214
        for dirname, _, files in os.walk(dest_folder):
            for fname in files:
                touch(os.path.join(dirname, fname))

        return files
def get_package_info(self, pref, remote):
         
        
        pref = self._resolve_latest_pref(pref, remote)
        return self._call_remote(remote, "get_package_info", pref), pref
def check_weekday(date) -> str:
    
    
    if pd.to_datetime(date).weekday() > 4:
        return next_workday(pd.to_datetime(date)).strftime("%Y-%m-%d")
    return date
def chmod_r(path: str, mode: int):
    
    
    
    if not os.path.exists(path):
        return
    idempotent_chmod(path, mode)
    for root, dirnames, filenames in os.walk(path):
        for dirname in dirnames:
            idempotent_chmod(os.path.join(root, dirname), mode)
        for filename in filenames:
            idempotent_chmod(os.path.join(root, filename), mode)
def project_state(self, nodes=None, at_end=True):
        
        
        
        if nodes is None:
            nodes = list(self.leaf_nodes())
        if len(nodes) == 0:
            return ProjectState()
        if not isinstance(nodes[0], tuple):
            nodes = [nodes]
        plan = []
        for node in nodes:
            for migration in self.forwards_plan(node):
                if migration not in plan:
                    if not at_end and migration in nodes:
                        continue
                    plan.append(migration)
        project_state = ProjectState()
        for node in plan:
            project_state = self.nodes[node].mutate_state(project_state)
        return project_state
def dfs(self, start, get_children):
        
        
        
        self.ensure_not_cyclic(start, get_children)
        visited = deque()
        visited.append(start)
        stack = deque(sorted(get_children(start)))
        while stack:
            node = stack.popleft()
            visited.appendleft(node)
            children = sorted(get_children(node), reverse=True)
            # reverse sorting is needed because prepending using deque.extendleft
            # also effectively reverses values
            stack.extendleft(children)

        return list(OrderedSet(visited))
def block_till_done(self):
        
        
        while not self.queue.empty():
            time.sleep(0.025)
def _async_recorder_ready(self):
        
        self._async_setup_periodic_tasks()
        self.async_recorder_ready.set()
def _setup_run(self):
        
        with session_scope(session=self.get_session()) as session:
            start = self.recording_start
            end_incomplete_runs(session, start)
            self.run_info = RecorderRuns(start=start, created=dt_util.utcnow())
            session.add(self.run_info)
            session.flush()
            session.expunge(self.run_info)
            self._schedule_compile_missing_statistics(session)

        self._open_event_session()
def add_param(self, name, shape=None, dtype=numpy.float32,
                  initializer=None):
        

        
        warnings.warn('''\
Parameter registeration via Link.__init__ and Link.add_param are deprecated.
Substitute a chainer.Parameter object directly to an attribute of the link \
instead.
''', DeprecationWarning)
        if initializer is None:
            initializer = initializers.NaN(dtype)
        param = variable.Parameter(initializer, shape)
        with self.init_scope():
            setattr(self, name, param)
def zerograds(self):
        

        
        warnings.warn(
            'Link.zerograds is deprecated. Use Link.cleargrads instead.',
            DeprecationWarning)
        for param in self.params():
            param.zerograd()
def add_link(self, name, link):
        

        
        if name in self.__dict__:
            raise AttributeError(
                'cannot register a new link %s: attribute exists' % name)
        if not isinstance(link, Link):
            raise TypeError('cannot register a non-link object as a child')
        with self.init_scope():
            setattr(self, name, link)
def repeat(self, n_repeat, mode='init'):
        

        
        ret = chainer.Sequential()
        if n_repeat <= 0:
            return ret
        if mode not in ['init', 'copy', 'share']:
            raise ValueError(
                'The \'mode\' argument should be either \'init\','
                '\'copy\', or \'share\'. But {} was given.'.format(mode))
        link = self
        for _ in range(n_repeat):
            if mode in ['init', 'copy']:
                link = copy.deepcopy(link)
            if mode == 'init':
                for param in link.params(include_uninit=False):
                    param.initialize(param.shape)
            ret.append(link)
        return ret
def count_params(self):
        

        

        size = 0
        for name, param in self.namedparams():
            if param.array is None:
                warnings.warn(
                    'Parameter \'{}\' has not been initialized, so the '
                    'resulting count will not include the number of parameters'
                    ' in it.'.format(name))
                continue
            size += param.size
        return size
def to_cpu(self):
        

        
        return self.to_device(backend.CpuDevice())
def add_hook(self, hook, name=None):
        

        
        if not isinstance(hook, link_hook.LinkHook):
            raise TypeError('Hook must be of type LinkHook')
        if name is None:
            name = hook.name
        hooks = self.local_link_hooks
        if name in hooks:
            raise KeyError('Hook %s already exists' % name)
        hooks[name] = hook
        hook.added(self)
        return self
def to_gpu(self, device=None, sticky=None):
        # type: (tp.Optional[types.CudaDeviceSpec], tp.Optional[bool]) -> 'Link' # NOQA
        

        
        cuda.check_cuda_available()
        return self._to_device(
            cuda._get_device_or_current(device),
            skip_between_cupy_devices=sticky)
def _update_plot_range(self) -> None:
        
        
        
        view = self._first_plot.getViewBox()
        view_range = view.viewRange()

        min_ix = max(0, int(view_range[0][0]))
        max_ix = min(self._manager.get_count(), int(view_range[0][1]))

        # Update limit for y-axis
        for item, plot in self._item_plot_map.items():
            y_range = item.get_y_range(min_ix, max_ix)
            plot.setRange(yRange=y_range)
def scalar_constructor(value, name=None, strict=False, allow_downcast=None,
                       borrow=False):
    

    
    if not isinstance(value, (numpy.number, float, int, complex)):
        raise TypeError()
    try:
        dtype = value.dtype
    except Exception:
        dtype = numpy.asarray(value).dtype

    dtype = str(dtype)
    value = theano._asarray(value, dtype=dtype)
    tensor_type = TensorType(dtype=str(value.dtype), broadcastable=[])

    try:
        # Do not pass the dtype to asarray because we want this to fail if
        # strict is True and the types do not match.
        rval = ScalarSharedVariable(type=tensor_type,
                value=numpy.array(value, copy=True),
                name=name, strict=strict, allow_downcast=allow_downcast)
        return rval
    except Exception:
        traceback.print_exc()
        raise
def __init__(
        self,
        coordinator: DataUpdateCoordinator,
        unique_id,
        name,
        icon,
        host_name,
        node_name,
        vm_id,
    ):
        
        super().__init__(
            coordinator, unique_id, name, icon, host_name, node_name, vm_id
        )

        self._state = None
def test_get_import_name_without_alias(self):
        
        
        
        import_name_with_alias = "requests as R"
        expected_import_name_without_alias = "requests"
        import_name_without_aliases = pipreqs.get_name_without_alias(
            import_name_with_alias)
        self.assertEqual(
            import_name_without_aliases,
            expected_import_name_without_alias
            )
def create_pipelines(
        project_id: str,
        cluster_id: str,
        cloud_region: str = "gcp-us-west1",
        api_key: str = None,
        collection_name: str = "zcp_llamalection",
        data_type: str = "text",
        metadata_schema: Optional[Dict] = None,
        **kwargs: Any,
    ) -> dict:
        
        
        if data_type == "text":
            ingest_action = "INDEX_TEXT"
            search_action = "SEARCH_TEXT"
        elif data_type == "doc":
            ingest_action = "INDEX_DOC"
            search_action = "SEARCH_DOC_CHUNK"
        else:
            raise Exception("Only text or doc is supported as the data type.")

        params_dict = {}
        additional_params = kwargs or {}

        language = additional_params.pop("language", "ENGLISH")
        embedding = additional_params.pop("embedding", "zilliz/bge-base-en-v1.5")
        reranker = additional_params.pop("reranker", None)
        index_func = {
            "name": "llamaindex_index",
            "action": ingest_action,
            "language": language,
            "embedding": embedding,
        }
        index_func.update(additional_params)
        ingest_functions = [index_func]
        if metadata_schema:
            for k, v in metadata_schema.items():
                preserve_func = {
                    "name": f"keep_{k}",
                    "action": "PRESERVE",
                    "inputField": k,
                    "outputField": k,
                    "fieldType": v,
                }
                ingest_functions.append(preserve_func)
        params_dict["INGESTION"] = {
            "name": f"{collection_name}_ingestion",
            "projectId": project_id,
            "clusterId": cluster_id,
            "collectionName": collection_name,
            "type": "INGESTION",
            "functions": ingest_functions,
        }

        search_function = {
            "name": "llamaindex_search",
            "action": search_action,
            "clusterId": cluster_id,
            "collectionName": collection_name,
            "embedding": embedding,
        }
        if reranker:
            search_function["reranker"] = reranker
        params_dict["SEARCH"] = {
            "name": f"{collection_name}_search",
            "projectId": project_id,
            "type": "SEARCH",
            "functions": [search_function],
        }

        params_dict["DELETION"] = {
            "name": f"{collection_name}_deletion",
            "type": "DELETION",
            "functions": [
                {
                    "name": "purge_by_expression",
                    "action": "PURGE_BY_EXPRESSION",
                }
            ],
            "projectId": project_id,
            "clusterId": cluster_id,
            "collectionName": collection_name,
        }

        domain = f"https://controller.api.{cloud_region}.zillizcloud.com/v1/pipelines"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Accept": "application/json",
            "Content-Type": "application/json",
        }
        pipeline_ids = {}

        for k, v in params_dict.items():
            response = requests.post(domain, headers=headers, json=v)
            if response.status_code != 200:
                raise RuntimeError(response.text)
            response_dict = response.json()
            if response_dict["code"] != 200:
                raise RuntimeError(response_dict)
            pipeline_ids[k] = response_dict["data"]["pipelineId"]

        return pipeline_ids
def validate_kms_key_id(kms_key: str, bucket: FakeBucket) -> None:
    
    
    
    try:
        parsed_arn = parse_arn(kms_key)
        key_region = parsed_arn["region"]
        # the KMS key should be in the same region as the bucket, we can raise an exception without calling KMS
        if key_region != bucket.region_name:
            raise CommonServiceException(
                code="KMS.NotFoundException", message=f"Invalid arn {key_region}"
            )

    except InvalidArnException:
        # if it fails, the passed ID is a UUID with no region data
        key_id = kms_key
        # recreate the ARN manually with the bucket region and bucket owner
        # if the KMS key is cross-account, user should provide an ARN and not a KeyId
        kms_key = arns.kms_key_arn(
            key_id=key_id, account_id=bucket.account_id, region_name=bucket.region_name
        )

    # the KMS key should be in the same region as the bucket, create the client in the bucket region
    kms_client = aws_stack.connect_to_service("kms", region_name=bucket.region_name)
    try:
        key = kms_client.describe_key(KeyId=kms_key)
        if not key["KeyMetadata"]["Enabled"]:
            raise CommonServiceException(
                code="KMS.DisabledException", message=f'{key["KeyMetadata"]["Arn"]} is disabled.'
            )

    except ClientError as e:
        if e.response["Error"]["Code"] == "NotFoundException":
            raise CommonServiceException(
                code="KMS.NotFoundException", message=e.response["Error"]["Message"]
            )
        raise
def parse_range_header(range_header: str, object_size: int) -> ObjectRange | None:
    
    
    
    last = object_size - 1
    try:
        _, rspec = range_header.split("=")
    except ValueError:
        return None
    if "," in rspec:
        return None

    try:
        begin, end = [int(i) if i else None for i in rspec.split("-")]
    except ValueError:
        # if we can't parse the Range header, S3 just treat the request as a non-range request
        return None

    if (begin is None and end == 0) or (begin is not None and begin > last):
        raise InvalidRange(
            "The requested range is not satisfiable",
            ActualObjectSize=str(object_size),
            RangeRequested=range_header,
        )

    if begin is not None:  # byte range
        end = last if end is None else min(end, last)
    elif end is not None:  # suffix byte range
        begin = object_size - min(end, object_size)
        end = last
    else:
        # Treat as non-range request
        return None

    if begin > min(end, last):
        # Treat as non-range request if after the logic is applied
        return None

    return ObjectRange(
        content_range=f"bytes {begin}-{end}/{object_size}",
        content_length=end - begin + 1,
        begin=begin,
        end=end,
    )
def uses_host_addressing(headers: Dict[str, str]) -> str | None:
    
    
    
    host = headers.get("host", "")

    # try to extract the bucket from the hostname (the "in" check is a minor optimization, as the regex is very greedy)
    if ".s3." in host and (
        (match := _s3_virtual_host_regex.match(host)) and (bucket_name := match.group("bucket"))
    ):
        return bucket_name
def mock_coro(return_value=None, exception=None):
    
    return mock_coro_func(return_value, exception)()
def async_fire_time_changed(
    hass: HomeAssistant, datetime_: datetime | None = None, fire_all: bool = False
) -> None:
    
    
    if datetime_ is None:
        utc_datetime = date_util.utcnow()
    else:
        utc_datetime = date_util.as_utc(datetime_)

    if utc_datetime.microsecond < 500000:
        # Allow up to 500000 microseconds to be added to the time
        # to handle update_coordinator's and
        # async_track_time_interval's
        # staggering to avoid thundering herd.
        utc_datetime = utc_datetime.replace(microsecond=500000)

    _async_fire_time_changed(hass, utc_datetime, fire_all)
def assert_lists_same(a, b):
    
    
    assert len(a) == len(b)
    for i in a:
        assert i in b
    for i in b:
        assert i in a
def import_and_test_deprecated_constant(
    caplog: pytest.LogCaptureFixture,
    module: ModuleType,
    constant_name: str,
    replacement_name: str,
    replacement: Any,
    breaks_in_ha_version: str,
) -> None:
    
    
    value = import_deprecated_constant(module, constant_name)
    assert value == replacement
    assert (
        module.__name__,
        logging.WARNING,
        (
            f"{constant_name} was used from test_constant_deprecation,"
            f" this is a deprecated constant which will be removed in HA Core {breaks_in_ha_version}. "
            f"Use {replacement_name} instead, please report "
            "it to the author of the 'test_constant_deprecation' custom integration"
        ),
    ) in caplog.record_tuples

    # verify deprecated constant is included in dir()
    assert constant_name in dir(module)
    assert constant_name in module.__all__
def mock_state_change_event(
    hass: HomeAssistant, new_state: State, old_state: State | None = None
) -> None:
    
    event_data = {
        "entity_id": new_state.entity_id,
        "new_state": new_state,
        "old_state": old_state,
    }
    hass.bus.fire(EVENT_STATE_CHANGED, event_data, context=new_state.context)
def join(self, connection, reuse=None, nullable=False, join_field=None):
        
        
        
        lhs, table, join_cols = connection
        assert lhs is None or join_field is not None
        existing = self.join_map.get(connection, ())
        if reuse is None:
            reuse = existing
        else:
            reuse = [a for a in existing if a in reuse]
        for alias in reuse:
            if join_field and self.alias_map[alias].join_field != join_field:
                # The join_map doesn't contain join_field (mainly because
                # fields in Query structs are problematic in pickling), so
                # check that the existing join is created using the same
                # join_field used for the under work join.
                continue
            self.ref_alias(alias)
            return alias

        # No reuse is possible, so we need a new alias.
        alias, _ = self.table_alias(table, True)
        if not lhs:
            # Not all tables need to be joined to anything. No join type
            # means the later columns are ignored.
            join_type = None
        elif self.alias_map[lhs].join_type == self.LOUTER or nullable:
            join_type = self.LOUTER
        else:
            join_type = self.INNER
        join = JoinInfo(table, alias, join_type, lhs, join_cols or ((None, None),), nullable,
                        join_field)
        self.alias_map[alias] = join
        if connection in self.join_map:
            self.join_map[connection] += (alias,)
        else:
            self.join_map[connection] = (alias,)
        return alias
def promote_alias(self, alias, unconditional=False):
        
        
        
        if ((unconditional or self.alias_map[alias][NULLABLE]) and
                self.alias_map[alias] != self.LOUTER):
            data = list(self.alias_map[alias])
            data[JOIN_TYPE] = self.LOUTER
            self.alias_map[alias] = tuple(data)
            return True
        return False
def get_meta(self):
        
        
        
        return self.model._meta
def as_nested_sql(self):
        
        
        
        obj = self.clone()
        obj.clear_ordering(True)
        obj.bump_prefix()
        return obj.as_sql()
def trim_joins(self, target, join_list, last, trim):
        
        
        
        final = len(join_list)
        penultimate = last.pop()
        if penultimate == final:
            penultimate = last.pop()
        if trim and len(join_list) > 1:
            extra = join_list[penultimate:]
            join_list = join_list[:penultimate]
            final = penultimate
            penultimate = last.pop()
            col = self.alias_map[extra[0]][LHS_JOIN_COL]
            for alias in extra:
                self.unref_alias(alias)
        else:
            col = target.column
        alias = join_list[-1]
        while final > 1:
            join = self.alias_map[alias]
            if col != join[RHS_JOIN_COL] or join[JOIN_TYPE] != self.INNER:
                break
            self.unref_alias(alias)
            alias = join[LHS_ALIAS]
            col = join[LHS_JOIN_COL]
            join_list = join_list[:-1]
            final -= 1
            if final == penultimate:
                penultimate = last.pop()
        return col, alias, join_list
def trim_joins(self, target, join_list, last, trim, nonnull_check=False):
        
        
        
        final = len(join_list)
        penultimate = last.pop()
        if penultimate == final:
            penultimate = last.pop()
        if trim and final > 1:
            extra = join_list[penultimate:]
            join_list = join_list[:penultimate]
            final = penultimate
            penultimate = last.pop()
            col = self.alias_map[extra[0]][LHS_JOIN_COL]
            for alias in extra:
                self.unref_alias(alias)
        else:
            col = target.column
        alias = join_list[-1]
        while final > 1:
            join = self.alias_map[alias]
            if (col != join[RHS_JOIN_COL] or join[JOIN_TYPE] != self.INNER or
                    nonnull_check):
                break
            self.unref_alias(alias)
            alias = join[LHS_ALIAS]
            col = join[LHS_JOIN_COL]
            join_list.pop()
            final -= 1
            if final == penultimate:
                penultimate = last.pop()
        return col, alias, join_list
def split_exclude(self, filter_expr, prefix, can_reuse, names_with_path):
        
        
        
        # Generate the inner query.
        query = Query(self.model)
        query.add_filter(filter_expr)
        query.bump_prefix()
        query.clear_ordering(True)
        # Try to have as simple as possible subquery -> trim leading joins from
        # the subquery.
        trimmed_joins = query.trim_start(names_with_path)
        # Add extra check to make sure the selected field will not be null
        # since we are adding a IN <subquery> clause. This prevents the
        # database from tripping over IN (...,NULL,...) selects and returning
        # nothing
        if self.is_nullable(query.select[0].field):
            alias, col = query.select[0].col
            query.where.add((Constraint(alias, col, None), 'isnull', False), AND)

        # Still make sure that the trimmed parts in the inner query and
        # trimmed prefix are in sync. So, use the trimmed_joins to make sure
        # as many path elements are in the prefix as there were trimmed joins.
        # In addition, convert the path elements back to names so that
        # add_filter() can handle them.
        trimmed_prefix = []
        paths_in_prefix = trimmed_joins
        for name, path in names_with_path:
            if paths_in_prefix - len(path) > 0:
                trimmed_prefix.append(name)
                paths_in_prefix -= len(path)
            else:
                trimmed_prefix.append(
                    path[paths_in_prefix - len(path)].from_field.name)
                break
        trimmed_prefix = LOOKUP_SEP.join(trimmed_prefix)
        self.add_filter(('%s__in' % trimmed_prefix, query), negate=True,
                        can_reuse=can_reuse)
def __str__(self):
        
        
        
        sql, params = self.sql_with_params()
        return sql % params
def bump_prefix(self, outer_query):
        
        
        
        self.alias_prefix = chr(ord(self.alias_prefix) + 1)
        while self.alias_prefix in self.subq_aliases:
            self.alias_prefix = chr(ord(self.alias_prefix) + 1)
            assert self.alias_prefix < 'Z'
        self.subq_aliases = self.subq_aliases.union([self.alias_prefix])
        outer_query.subq_aliases = outer_query.subq_aliases.union(self.subq_aliases)
        change_map = OrderedDict()
        for pos, alias in enumerate(self.tables):
            new_alias = '%s%d' % (self.alias_prefix, pos)
            change_map[alias] = new_alias
            self.tables[pos] = new_alias
        self.change_aliases(change_map)
def promote_joins(self, aliases):
        
        
        
        aliases = list(aliases)
        while aliases:
            alias = aliases.pop(0)
            if self.alias_map[alias].join_cols[0][1] is None:
                # This is the base table (first FROM entry) - this table
                # isn't really joined at all in the query, so we should not
                # alter its join type.
                continue
            # Only the first alias (skipped above) should have None join_type
            assert self.alias_map[alias].join_type is not None
            parent_alias = self.alias_map[alias].lhs_alias
            parent_louter = (
                parent_alias
                and self.alias_map[parent_alias].join_type == self.LOUTER)
            already_louter = self.alias_map[alias].join_type == self.LOUTER
            if ((self.alias_map[alias].nullable or parent_louter) and
                    not already_louter):
                data = self.alias_map[alias]._replace(join_type=self.LOUTER)
                self.alias_map[alias] = data
                # Join type of 'alias' changed, so re-examine all aliases that
                # refer to this one.
                aliases.extend(
                    join for join in self.alias_map.keys()
                    if (self.alias_map[join].lhs_alias == alias
                        and join not in aliases))
def join_parent_model(self, opts, model, alias, seen):
        
        
        
        if model in seen:
            return seen[model]
        int_opts = opts
        chain = opts.get_base_chain(model)
        if chain is None:
            return alias
        for int_model in chain:
            if int_model in seen:
                return seen[int_model]
            # Proxy model have elements in base chain
            # with no parents, assign the new options
            # object and skip to the next base in that
            # case
            if not int_opts.parents[int_model]:
                int_opts = int_model._meta
                continue
            link_field = int_opts.get_ancestor_link(int_model)
            int_opts = int_model._meta
            connection = (alias, int_opts.db_table, link_field.column, int_opts.pk.column)
            alias = seen[int_model] = self.join(connection, nullable=False,
                                                join_field=link_field)
        return alias or seen[None]
def _aggregate_select(self):
        
        
        if self._aggregate_select_cache is not None:
            return self._aggregate_select_cache
        elif self.aggregate_select_mask is not None:
            self._aggregate_select_cache = OrderedDict([
                (k, v) for k, v in self.aggregates.items()
                if k in self.aggregate_select_mask
            ])
            return self._aggregate_select_cache
        else:
            return self.aggregates
def add_votes(self, votes):
        
        
        
        self.votes.update(votes)
def annotation_select(self):
        
        
        
        if self._annotation_select_cache is not None:
            return self._annotation_select_cache
        elif not self.annotations:
            return {}
        elif self.annotation_select_mask is not None:
            self._annotation_select_cache = {
                k: v for k, v in self.annotations.items()
                if k in self.annotation_select_mask
            }
            return self._annotation_select_cache
        else:
            return self.annotations
def cancel_order(self, id: str, symbol: Str = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' cancelOrder() requires a symbol argument')
        self.load_markets()
        market = self.market(symbol)
        isTrigger = self.safe_value_2(params, 'trigger', 'stop')
        params = self.omit(params, ['trigger', 'stop'])
        request = {
            'entry_id': id,
            'symbol': market['uppercaseId'],
        }
        response = None
        tail = 'StopLossOrder' if isTrigger else 'Order'
        quoteSide = 'usdtcancel' if (market['quoteId'] == 'USDT') else 'cancel'
        quoteSide += tail
        request['side'] = quoteSide
        response = self.v2PostCancel(self.extend(request, params))
        return self.parse_order(response, market)
def append(self, log_probabilities, targets, length=None):
        
        
        numerator, denominator = Accuracy(log_probabilities, targets, length)
        self.correct += numerator
        self.total += denominator
def back_propagation(self) -> None:
        
        
        

        updated_second_hidden_layer_and_output_layer_weights = numpy.dot(
            self.layer_between_first_hidden_layer_and_second_hidden_layer.T,
            2
            * (self.output_array - self.predicted_output)
            * sigmoid_derivative(self.predicted_output),
        )
        updated_first_hidden_layer_and_second_hidden_layer_weights = numpy.dot(
            self.layer_between_input_and_first_hidden_layer.T,
            numpy.dot(
                2
                * (self.output_array - self.predicted_output)
                * sigmoid_derivative(self.predicted_output),
                self.second_hidden_layer_and_output_layer_weights.T,
            )
            * sigmoid_derivative(
                self.layer_between_first_hidden_layer_and_second_hidden_layer
            ),
        )
        updated_input_layer_and_first_hidden_layer_weights = numpy.dot(
            self.input_array.T,
            numpy.dot(
                numpy.dot(
                    2
                    * (self.output_array - self.predicted_output)
                    * sigmoid_derivative(self.predicted_output),
                    self.second_hidden_layer_and_output_layer_weights.T,
                )
                * sigmoid_derivative(
                    self.layer_between_first_hidden_layer_and_second_hidden_layer
                ),
                self.first_hidden_layer_and_second_hidden_layer_weights.T,
            )
            * sigmoid_derivative(self.layer_between_input_and_first_hidden_layer),
        )

        self.input_layer_and_first_hidden_layer_weights += (
            updated_input_layer_and_first_hidden_layer_weights
        )
        self.first_hidden_layer_and_second_hidden_layer_weights += (
            updated_first_hidden_layer_and_second_hidden_layer_weights
        )
        self.second_hidden_layer_and_output_layer_weights += (
            updated_second_hidden_layer_and_output_layer_weights
        )
def cancel_all_orders(self, symbol: Str = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' cancelAllOrders() requires a symbol argument')
        self.load_markets()
        request = {
            # 'symbol': market['id'],
            # 'untriggerred': False,  # False to cancel non-conditional orders, True to cancel conditional orders
            # 'text': 'up to 40 characters max',
        }
        market = self.market(symbol)
        method = 'privateDeleteSpotOrdersAll'
        if market['settle'] == 'USDT':
            method = 'privateDeleteGOrdersAll'
        elif market['swap']:
            method = 'privateDeleteOrdersAll'
        request['symbol'] = market['id']
        return getattr(self, method)(self.extend(request, params))
def fetch_balance(self, params={}) -> Balances:
        
        
        
        self.load_markets()
        type = None
        type, params = self.handle_market_type_and_params('fetchBalance', None, params)
        code = self.safe_string(params, 'code')
        params = self.omit(params, ['code'])
        response = None
        request = {}
        if (type != 'spot') and (type != 'swap'):
            raise BadRequest(self.id + ' does not support ' + type + ' markets, only spot and swap')
        if type == 'swap':
            settle = None
            settle, params = self.handle_option_and_params(params, 'fetchBalance', 'settle', 'USDT')
            if code is not None or settle is not None:
                coin = None
                if code is not None:
                    coin = code
                else:
                    coin = settle
                currency = self.currency(coin)
                request['currency'] = currency['id']
                if currency['id'] == 'USDT':
                    response = self.privateGetGAccountsAccountPositions(self.extend(request, params))
                else:
                    response = self.privateGetAccountsAccountPositions(self.extend(request, params))
            else:
                currency = self.safe_string(params, 'currency')
                if currency is None:
                    raise ArgumentsRequired(self.id + ' fetchBalance() requires a code parameter or a currency or settle parameter for ' + type + ' type')
                response = self.privateGetSpotWallets(self.extend(request, params))
        else:
            response = self.privateGetSpotWallets(self.extend(request, params))
        #
        # usdt
        #   {
        #       "info": {
        #         "code": "0",
        #         "msg": '',
        #         "data": {
        #           "account": {
        #             "userID": "940666",
        #             "accountId": "9406660003",
        #             "currency": "USDT",
        #             "accountBalanceRv": "99.93143972",
        #             "totalUsedBalanceRv": "0.40456",
        #             "bonusBalanceRv": "0"
        #           },
        #   }
        #
        # spot
        #
        #     {
        #         "code":0,
        #         "msg":"",
        #         "data":[
        #             {
        #                 "currency":"USDT",
        #                 "balanceEv":0,
        #                 "lockedTradingBalanceEv":0,
        #                 "lockedWithdrawEv":0,
        #                 "lastUpdateTimeNs":1592065834511322514,
        #                 "walletVid":0
        #             },
        #             {
        #                 "currency":"ETH",
        #                 "balanceEv":0,
        #                 "lockedTradingBalanceEv":0,
        #                 "lockedWithdrawEv":0,
        #                 "lastUpdateTimeNs":1592065834511322514,
        #                 "walletVid":0
        #             }
        #         ]
        #     }
        #
        # swap
        #
        #     {
        #         "code":0,
        #         "msg":"",
        #         "data":{
        #             "account":{
        #                 "accountId":6192120001,
        #                 "currency":"BTC",
        #                 "accountBalanceEv":1254744,
        #                 "totalUsedBalanceEv":0,
        #                 "bonusBalanceEv":1254744
        #             },
        #             "positions":[
        #                 {
        #                     "accountID":6192120001,
        #                     "symbol":"BTCUSD",
        #                     "currency":"BTC",
        #                     "side":"None",
        #                     "positionStatus":"Normal",
        #                     "crossMargin":false,
        #                     "leverageEr":0,
        #                     "leverage":0E-8,
        #                     "initMarginReqEr":1000000,
        #                     "initMarginReq":0.01000000,
        #                     "maintMarginReqEr":500000,
        #                     "maintMarginReq":0.00500000,
        #                     "riskLimitEv":10000000000,
        #                     "riskLimit":100.00000000,
        #                     "size":0,
        #                     "value":0E-8,
        #                     "valueEv":0,
        #                     "avgEntryPriceEp":0,
        #                     "avgEntryPrice":0E-8,
        #                     "posCostEv":0,
        #                     "posCost":0E-8,
        #                     "assignedPosBalanceEv":0,
        #                     "assignedPosBalance":0E-8,
        #                     "bankruptCommEv":0,
        #                     "bankruptComm":0E-8,
        #                     "bankruptPriceEp":0,
        #                     "bankruptPrice":0E-8,
        #                     "positionMarginEv":0,
        #                     "positionMargin":0E-8,
        #                     "liquidationPriceEp":0,
        #                     "liquidationPrice":0E-8,
        #                     "deleveragePercentileEr":0,
        #                     "deleveragePercentile":0E-8,
        #                     "buyValueToCostEr":1150750,
        #                     "buyValueToCost":0.01150750,
        #                     "sellValueToCostEr":1149250,
        #                     "sellValueToCost":0.01149250,
        #                     "markPriceEp":96359083,
        #                     "markPrice":9635.90830000,
        #                     "markValueEv":0,
        #                     "markValue":null,
        #                     "unRealisedPosLossEv":0,
        #                     "unRealisedPosLoss":null,
        #                     "estimatedOrdLossEv":0,
        #                     "estimatedOrdLoss":0E-8,
        #                     "usedBalanceEv":0,
        #                     "usedBalance":0E-8,
        #                     "takeProfitEp":0,
        #                     "takeProfit":null,
        #                     "stopLossEp":0,
        #                     "stopLoss":null,
        #                     "realisedPnlEv":0,
        #                     "realisedPnl":null,
        #                     "cumRealisedPnlEv":0,
        #                     "cumRealisedPnl":null
        #                 }
        #             ]
        #         }
        #     }
        #
        result = self.parse_swap_balance(response) if (type == 'swap') else self.parse_spot_balance(response)
        return result
def create_order(self, symbol: str, type: OrderType, side: OrderSide, amount: float, price: Num = None, params={}):
        
        
        
        self.load_markets()
        market = self.market(symbol)
        requestSide = self.capitalize(side)
        type = self.capitalize(type)
        reduceOnly = self.safe_bool(params, 'reduceOnly')
        request = {
            # common
            'symbol': market['id'],
            'side': requestSide,  # Sell, Buy
            'ordType': type,  # Market, Limit, Stop, StopLimit, MarketIfTouched, LimitIfTouched(additionally for contract-markets: MarketAsLimit, StopAsLimit, MarketIfTouchedAsLimit)
            # 'stopPxEp': self.to_ep(stopPx, market),  # for conditional orders
            # 'priceEp': self.to_ep(price, market),  # required for limit orders
            # 'timeInForce': 'GoodTillCancel',  # GoodTillCancel, PostOnly, ImmediateOrCancel, FillOrKill
            # ----------------------------------------------------------------
            # spot
            # 'qtyType': 'ByBase',  # ByBase, ByQuote
            # 'quoteQtyEv': self.to_ep(cost, market),
            # 'baseQtyEv': self.to_ev(amount, market),
            # 'trigger': 'ByLastPrice',  # required for conditional orders
            # ----------------------------------------------------------------
            # swap
            # 'clOrdID': self.uuid(),  # max length 40
            # 'orderQty': self.amount_to_precision(amount, symbol),
            # 'reduceOnly': False,
            # 'closeOnTrigger': False,  # implicit reduceOnly and cancel other orders in the same direction
            # 'takeProfitEp': self.to_ep(takeProfit, market),
            # 'stopLossEp': self.to_ep(stopLossEp, market),
            # 'triggerType': 'ByMarkPrice',  # ByMarkPrice, ByLastPrice
            # 'pegOffsetValueEp': integer,  # Trailing offset from current price. Negative value when position is long, positive when position is short
            # 'pegPriceType': 'TrailingStopPeg',  # TrailingTakeProfitPeg
            # 'text': 'comment',
            # 'posSide': Position direction - "Merged" for oneway mode , "Long" / "Short" for hedge mode
        }
        clientOrderId = self.safe_string_2(params, 'clOrdID', 'clientOrderId')
        stopLoss = self.safe_value(params, 'stopLoss')
        stopLossDefined = (stopLoss is not None)
        takeProfit = self.safe_value(params, 'takeProfit')
        takeProfitDefined = (takeProfit is not None)
        if clientOrderId is None:
            brokerId = self.safe_string(self.options, 'brokerId', 'CCXT123456')
            if brokerId is not None:
                request['clOrdID'] = brokerId + self.uuid16()
        else:
            request['clOrdID'] = clientOrderId
            params = self.omit(params, ['clOrdID', 'clientOrderId'])
        triggerPrice = self.safe_string_n(params, ['stopPx', 'stopPrice', 'triggerPrice'])
        if triggerPrice is not None:
            if market['settle'] == 'USDT':
                request['stopPxRp'] = self.price_to_precision(symbol, triggerPrice)
            else:
                request['stopPxEp'] = self.to_ep(triggerPrice, market)
        params = self.omit(params, ['stopPx', 'stopPrice', 'stopLoss', 'takeProfit', 'triggerPrice'])
        if market['spot']:
            qtyType = self.safe_value(params, 'qtyType', 'ByBase')
            if (type == 'Market') or (type == 'Stop') or (type == 'MarketIfTouched'):
                if price is not None:
                    qtyType = 'ByQuote'
            if triggerPrice is not None:
                if type == 'Limit':
                    request['ordType'] = 'StopLimit'
                elif type == 'Market':
                    request['ordType'] = 'Stop'
                request['trigger'] = 'ByLastPrice'
            request['qtyType'] = qtyType
            if qtyType == 'ByQuote':
                cost = self.safe_number(params, 'cost')
                params = self.omit(params, 'cost')
                if self.options['createOrderByQuoteRequiresPrice']:
                    if price is not None:
                        amountString = self.number_to_string(amount)
                        priceString = self.number_to_string(price)
                        quoteAmount = Precise.string_mul(amountString, priceString)
                        cost = self.parse_number(quoteAmount)
                    elif cost is None:
                        raise ArgumentsRequired(self.id + ' createOrder() ' + qtyType + ' requires a price argument or a cost parameter')
                cost = amount if (cost is None) else cost
                costString = self.number_to_string(cost)
                request['quoteQtyEv'] = self.to_ev(costString, market)
            else:
                amountString = self.number_to_string(amount)
                request['baseQtyEv'] = self.to_ev(amountString, market)
        elif market['swap']:
            posSide = self.safe_string_lower(params, 'posSide')
            if posSide is None:
                posSide = 'Merged'
            posSide = self.capitalize(posSide)
            request['posSide'] = posSide
            if reduceOnly is not None:
                request['reduceOnly'] = reduceOnly
            if market['settle'] == 'USDT':
                request['orderQtyRq'] = amount
            else:
                request['orderQty'] = self.parse_to_int(amount)
            if triggerPrice is not None:
                triggerType = self.safe_string(params, 'triggerType', 'ByMarkPrice')
                request['triggerType'] = triggerType
            if stopLossDefined or takeProfitDefined:
                if stopLossDefined:
                    stopLossTriggerPrice = self.safe_value_2(stopLoss, 'triggerPrice', 'stopPrice')
                    if stopLossTriggerPrice is None:
                        raise InvalidOrder(self.id + ' createOrder() requires a trigger price in params["stopLoss"]["triggerPrice"], or params["stopLoss"]["stopPrice"] for a stop loss order')
                    if market['settle'] == 'USDT':
                        request['stopLossRp'] = self.price_to_precision(symbol, stopLossTriggerPrice)
                    else:
                        request['stopLossEp'] = self.to_ep(stopLossTriggerPrice, market)
                    stopLossTriggerPriceType = self.safe_string_2(stopLoss, 'triggerPriceType', 'slTrigger')
                    if stopLossTriggerPriceType is not None:
                        if market['settle'] == 'USDT':
                            if (stopLossTriggerPriceType != 'ByMarkPrice') and (stopLossTriggerPriceType != 'ByLastPrice') and (stopLossTriggerPriceType != 'ByIndexPrice') and (stopLossTriggerPriceType != 'ByAskPrice') and (stopLossTriggerPriceType != 'ByBidPrice') and (stopLossTriggerPriceType != 'ByMarkPriceLimit') and (stopLossTriggerPriceType != 'ByLastPriceLimit'):
                                raise InvalidOrder(self.id + ' createOrder() take profit trigger price type must be one of "ByMarkPrice", "ByIndexPrice", "ByAskPrice", "ByBidPrice", "ByMarkPriceLimit", "ByLastPriceLimit" or "ByLastPrice"')
                        else:
                            if (stopLossTriggerPriceType != 'ByMarkPrice') and (stopLossTriggerPriceType != 'ByLastPrice'):
                                raise InvalidOrder(self.id + ' createOrder() take profit trigger price type must be one of "ByMarkPrice", or "ByLastPrice"')
                        request['slTrigger'] = stopLossTriggerPriceType
                if takeProfitDefined:
                    takeProfitTriggerPrice = self.safe_value_2(takeProfit, 'triggerPrice', 'stopPrice')
                    if takeProfitTriggerPrice is None:
                        raise InvalidOrder(self.id + ' createOrder() requires a trigger price in params["takeProfit"]["triggerPrice"], or params["takeProfit"]["stopPrice"] for a take profit order')
                    if market['settle'] == 'USDT':
                        request['takeProfitRp'] = self.price_to_precision(symbol, takeProfitTriggerPrice)
                    else:
                        request['takeProfitEp'] = self.to_ep(takeProfitTriggerPrice, market)
                    takeProfitTriggerPriceType = self.safe_string_2(stopLoss, 'triggerPriceType', 'tpTrigger')
                    if takeProfitTriggerPriceType is not None:
                        if market['settle'] == 'USDT':
                            if (takeProfitTriggerPriceType != 'ByMarkPrice') and (takeProfitTriggerPriceType != 'ByLastPrice') and (takeProfitTriggerPriceType != 'ByIndexPrice') and (takeProfitTriggerPriceType != 'ByAskPrice') and (takeProfitTriggerPriceType != 'ByBidPrice') and (takeProfitTriggerPriceType != 'ByMarkPriceLimit') and (takeProfitTriggerPriceType != 'ByLastPriceLimit'):
                                raise InvalidOrder(self.id + ' createOrder() take profit trigger price type must be one of "ByMarkPrice", "ByIndexPrice", "ByAskPrice", "ByBidPrice", "ByMarkPriceLimit", "ByLastPriceLimit" or "ByLastPrice"')
                        else:
                            if (takeProfitTriggerPriceType != 'ByMarkPrice') and (takeProfitTriggerPriceType != 'ByLastPrice'):
                                raise InvalidOrder(self.id + ' createOrder() take profit trigger price type must be one of "ByMarkPrice", or "ByLastPrice"')
                        request['tpTrigger'] = takeProfitTriggerPriceType
        if (type == 'Limit') or (type == 'StopLimit') or (type == 'LimitIfTouched'):
            if market['settle'] == 'USDT':
                request['priceRp'] = self.price_to_precision(symbol, price)
            else:
                priceString = self.number_to_string(price)
                request['priceEp'] = self.to_ep(priceString, market)
        takeProfitPrice = self.safe_string(params, 'takeProfitPrice')
        if takeProfitPrice is not None:
            if market['settle'] == 'USDT':
                request['takeProfitRp'] = self.price_to_precision(symbol, takeProfitPrice)
            else:
                request['takeProfitEp'] = self.to_ep(takeProfitPrice, market)
            params = self.omit(params, 'takeProfitPrice')
        stopLossPrice = self.safe_string(params, 'stopLossPrice')
        if stopLossPrice is not None:
            if market['settle'] == 'USDT':
                request['stopLossRp'] = self.price_to_precision(symbol, stopLossPrice)
            else:
                request['stopLossEp'] = self.to_ep(stopLossPrice, market)
            params = self.omit(params, 'stopLossPrice')
        params = self.omit(params, 'reduceOnly')
        response = None
        if market['settle'] == 'USDT':
            response = self.privatePostGOrders(self.extend(request, params))
        elif market['contract']:
            response = self.privatePostOrders(self.extend(request, params))
        else:
            response = self.privatePostSpotOrders(self.extend(request, params))
        #
        # spot
        #
        #     {
        #         "code": 0,
        #         "msg": "",
        #         "data": {
        #             "orderID": "d1d09454-cabc-4a23-89a7-59d43363f16d",
        #             "clOrdID": "309bcd5c-9f6e-4a68-b775-4494542eb5cb",
        #             "priceEp": 0,
        #             "action": "New",
        #             "trigger": "UNSPECIFIED",
        #             "pegPriceType": "UNSPECIFIED",
        #             "stopDirection": "UNSPECIFIED",
        #             "bizError": 0,
        #             "symbol": "sBTCUSDT",
        #             "side": "Buy",
        #             "baseQtyEv": 0,
        #             "ordType": "Limit",
        #             "timeInForce": "GoodTillCancel",
        #             "ordStatus": "Created",
        #             "cumFeeEv": 0,
        #             "cumBaseQtyEv": 0,
        #             "cumQuoteQtyEv": 0,
        #             "leavesBaseQtyEv": 0,
        #             "leavesQuoteQtyEv": 0,
        #             "avgPriceEp": 0,
        #             "cumBaseAmountEv": 0,
        #             "cumQuoteAmountEv": 0,
        #             "quoteQtyEv": 0,
        #             "qtyType": "ByBase",
        #             "stopPxEp": 0,
        #             "pegOffsetValueEp": 0
        #         }
        #     }
        #
        # swap
        #
        #     {
        #         "code":0,
        #         "msg":"",
        #         "data":{
        #             "bizError":0,
        #             "orderID":"7a1ad384-44a3-4e54-a102-de4195a29e32",
        #             "clOrdID":"",
        #             "symbol":"ETHUSD",
        #             "side":"Buy",
        #             "actionTimeNs":1592668973945065381,
        #             "transactTimeNs":0,
        #             "orderType":"Market",
        #             "priceEp":2267500,
        #             "price":226.75000000,
        #             "orderQty":1,
        #             "displayQty":0,
        #             "timeInForce":"ImmediateOrCancel",
        #             "reduceOnly":false,
        #             "closedPnlEv":0,
        #             "closedPnl":0E-8,
        #             "closedSize":0,
        #             "cumQty":0,
        #             "cumValueEv":0,
        #             "cumValue":0E-8,
        #             "leavesQty":1,
        #             "leavesValueEv":11337,
        #             "leavesValue":1.13370000,
        #             "stopDirection":"UNSPECIFIED",
        #             "stopPxEp":0,
        #             "stopPx":0E-8,
        #             "trigger":"UNSPECIFIED",
        #             "pegOffsetValueEp":0,
        #             "execStatus":"PendingNew",
        #             "pegPriceType":"UNSPECIFIED",
        #             "ordStatus":"Created"
        #         }
        #     }
        #
        data = self.safe_dict(response, 'data', {})
        return self.parse_order(data, market)
def edit_order(self, id, symbol, type=None, side=None, amount=None, price=None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' editOrder() requires a symbol argument')
        self.load_markets()
        market = self.market(symbol)
        request = {
            'symbol': market['id'],
        }
        clientOrderId = self.safe_string_2(params, 'clientOrderId', 'clOrdID')
        params = self.omit(params, ['clientOrderId', 'clOrdID'])
        isUSDTSettled = (market['settle'] == 'USDT')
        if clientOrderId is not None:
            request['clOrdID'] = clientOrderId
        else:
            request['orderID'] = id
        if price is not None:
            if isUSDTSettled:
                request['priceRp'] = self.price_to_precision(market['symbol'], price)
            else:
                request['priceEp'] = self.to_ep(price, market)
        # Note the uppercase 'V' in 'baseQtyEV' request. that is exchange's requirement at self moment. However, to avoid mistakes from user side, let's support lowercased 'baseQtyEv' too
        finalQty = self.safe_string(params, 'baseQtyEv')
        params = self.omit(params, ['baseQtyEv'])
        if finalQty is not None:
            request['baseQtyEV'] = finalQty
        elif amount is not None:
            if isUSDTSettled:
                request['baseQtyEV'] = self.amount_to_precision(market['symbol'], amount)
            else:
                request['baseQtyEV'] = self.to_ev(amount, market)
        stopPrice = self.safe_string_2(params, 'stopPx', 'stopPrice')
        if stopPrice is not None:
            if isUSDTSettled:
                request['stopPxRp'] = self.price_to_precision(symbol, stopPrice)
            else:
                request['stopPxEp'] = self.to_ep(stopPrice, market)
        params = self.omit(params, ['stopPx', 'stopPrice'])
        method = 'privatePutSpotOrders'
        if market['inverse']:
            method = 'privatePutOrdersReplace'
        elif isUSDTSettled:
            method = 'privatePutGOrdersReplace'
            posSide = self.safe_string(params, 'posSide')
            if posSide is None:
                request['posSide'] = 'Merged'
        response = getattr(self, method)(self.extend(request, params))
        data = self.safe_value(response, 'data', {})
        return self.parse_order(data, market)
def fetch_closed_orders(self, symbol: Str = None, since: Int = None, limit: Int = None, params={}) -> List[Order]:
        
        
        
        self.load_markets()
        market = None
        if symbol is not None:
            market = self.market(symbol)
        request = {
        }
        if market is not None:
            request['symbol'] = market['id']
        if since is not None:
            request['start'] = since
        if limit is not None:
            request['limit'] = limit
        response = None
        if (symbol is None) or (self.safe_string(market, 'settle') == 'USDT'):
            request['currency'] = self.safe_string(params, 'settle', 'USDT')
            response = self.privateGetExchangeOrderV2OrderList(self.extend(request, params))
        elif market['swap']:
            response = self.privateGetExchangeOrderList(self.extend(request, params))
        else:
            response = self.privateGetExchangeSpotOrder(self.extend(request, params))
        #
        # spot
        #
        #     {
        #         "code":0,
        #         "msg":"OK",
        #         "data":{
        #             "total":8,
        #             "rows":[
        #                 {
        #                     "orderID":"99232c3e-3d6a-455f-98cc-2061cdfe91bc",
        #                     "stopPxEp":0,
        #                     "avgPriceEp":0,
        #                     "qtyType":"ByBase",
        #                     "leavesBaseQtyEv":0,
        #                     "leavesQuoteQtyEv":0,
        #                     "baseQtyEv":"1000000000",
        #                     "feeCurrency":"4",
        #                     "stopDirection":"UNSPECIFIED",
        #                     "symbol":"sETHUSDT",
        #                     "side":"Buy",
        #                     "quoteQtyEv":250000000000,
        #                     "priceEp":25000000000,
        #                     "ordType":"Limit",
        #                     "timeInForce":"GoodTillCancel",
        #                     "ordStatus":"Rejected",
        #                     "execStatus":"NewRejected",
        #                     "createTimeNs":1592675305266037130,
        #                     "cumFeeEv":0,
        #                     "cumBaseValueEv":0,
        #                     "cumQuoteValueEv":0
        #                 },
        #             ]
        #         }
        #     }
        #
        data = self.safe_value(response, 'data', {})
        if isinstance(data, list):
            return self.parse_orders(data, market, since, limit)
        else:
            rows = self.safe_value(data, 'rows', [])
            return self.parse_orders(rows, market, since, limit)
def fetch_open_orders(self, symbol: Optional[str] = None, since: Optional[int] = None, limit: Optional[int] = None, params={}):
        
        
        
        if symbol is None:
            raise ArgumentsRequired(self.id + ' fetchOpenOrders() requires a symbol argument')
        self.load_markets()
        market = self.market(symbol)
        method = 'privateGetSpotOrders'
        if market['settle'] == 'USDT':
            method = 'privateGetGOrdersActiveList'
        elif market['swap']:
            method = 'privateGetOrdersActiveList'
        request = {
            'symbol': market['id'],
        }
        response = None
        try:
            response = getattr(self, method)(self.extend(request, params))
        except Exception as e:
            if isinstance(e, OrderNotFound):
                return []
            raise e
        data = self.safe_value(response, 'data', {})
        if isinstance(data, list):
            return self.parse_orders(data, market, since, limit)
        else:
            rows = self.safe_value(data, 'rows', [])
            return self.parse_orders(rows, market, since, limit)
def fetch_positions(self, symbols: Strings = None, params={}):
        
        
        
        self.load_markets()
        symbols = self.market_symbols(symbols)
        subType = None
        code = self.safe_string(params, 'currency')
        settle = None
        market = None
        firstSymbol = self.safe_string(symbols, 0)
        if firstSymbol is not None:
            market = self.market(firstSymbol)
            settle = market['settle']
            code = market['settle']
        else:
            settle, params = self.handle_option_and_params(params, 'fetchPositions', 'settle', 'USD')
        subType, params = self.handle_sub_type_and_params('fetchPositions', market, params)
        isUSDTSettled = settle == 'USDT'
        if isUSDTSettled:
            code = 'USDT'
        elif code is None:
            code = 'USD' if (subType == 'linear') else 'BTC'
        else:
            params = self.omit(params, 'code')
        currency = self.currency(code)
        request = {
            'currency': currency['id'],
        }
        response = None
        if isUSDTSettled:
            method = None
            method, params = self.handle_option_and_params(params, 'fetchPositions', 'method', 'privateGetGAccountsAccountPositions')
            if method == 'privateGetGAccountsAccountPositions':
                response = self.privateGetGAccountsAccountPositions(self.extend(request, params))
            else:
                response = self.privateGetAccountsPositions(self.extend(request, params))
        else:
            response = self.privateGetAccountsAccountPositions(self.extend(request, params))
        #
        #     {
        #         "code":0,"msg":"",
        #         "data":{
        #             "account":{
        #                 "accountId":6192120001,
        #                 "currency":"BTC",
        #                 "accountBalanceEv":1254744,
        #                 "totalUsedBalanceEv":0,
        #                 "bonusBalanceEv":1254744
        #             },
        #             "positions":[
        #                 {
        #                     "accountID":6192120001,
        #                     "symbol":"BTCUSD",
        #                     "currency":"BTC",
        #                     "side":"None",
        #                     "positionStatus":"Normal",
        #                     "crossMargin":false,
        #                     "leverageEr":100000000,
        #                     "leverage":1.00000000,
        #                     "initMarginReqEr":100000000,
        #                     "initMarginReq":1.00000000,
        #                     "maintMarginReqEr":500000,
        #                     "maintMarginReq":0.00500000,
        #                     "riskLimitEv":10000000000,
        #                     "riskLimit":100.00000000,
        #                     "size":0,
        #                     "value":0E-8,
        #                     "valueEv":0,
        #                     "avgEntryPriceEp":0,
        #                     "avgEntryPrice":0E-8,
        #                     "posCostEv":0,
        #                     "posCost":0E-8,
        #                     "assignedPosBalanceEv":0,
        #                     "assignedPosBalance":0E-8,
        #                     "bankruptCommEv":0,
        #                     "bankruptComm":0E-8,
        #                     "bankruptPriceEp":0,
        #                     "bankruptPrice":0E-8,
        #                     "positionMarginEv":0,
        #                     "positionMargin":0E-8,
        #                     "liquidationPriceEp":0,
        #                     "liquidationPrice":0E-8,
        #                     "deleveragePercentileEr":0,
        #                     "deleveragePercentile":0E-8,
        #                     "buyValueToCostEr":100225000,
        #                     "buyValueToCost":1.00225000,
        #                     "sellValueToCostEr":100075000,
        #                     "sellValueToCost":1.00075000,
        #                     "markPriceEp":135736070,
        #                     "markPrice":13573.60700000,
        #                     "markValueEv":0,
        #                     "markValue":null,
        #                     "unRealisedPosLossEv":0,
        #                     "unRealisedPosLoss":null,
        #                     "estimatedOrdLossEv":0,
        #                     "estimatedOrdLoss":0E-8,
        #                     "usedBalanceEv":0,
        #                     "usedBalance":0E-8,
        #                     "takeProfitEp":0,
        #                     "takeProfit":null,
        #                     "stopLossEp":0,
        #                     "stopLoss":null,
        #                     "cumClosedPnlEv":0,
        #                     "cumFundingFeeEv":0,
        #                     "cumTransactFeeEv":0,
        #                     "realisedPnlEv":0,
        #                     "realisedPnl":null,
        #                     "cumRealisedPnlEv":0,
        #                     "cumRealisedPnl":null
        #                 }
        #             ]
        #         }
        #     }
        #
        data = self.safe_value(response, 'data', {})
        positions = self.safe_value(data, 'positions', [])
        result = []
        for i in range(0, len(positions)):
            position = positions[i]
            result.append(self.parse_position(position))
        return self.filter_by_array_positions(result, 'symbol', symbols, False)
def set_leverage(self, leverage, symbol: Str = None, params={}):
        
        
        
        # WARNING: THIS WILL INCREASE LIQUIDATION PRICE FOR OPEN ISOLATED LONG POSITIONS
        # AND DECREASE LIQUIDATION PRICE FOR OPEN ISOLATED SHORT POSITIONS
        if symbol is None:
            raise ArgumentsRequired(self.id + ' setLeverage() requires a symbol argument')
        if (leverage < -100) or (leverage > 100):
            raise BadRequest(self.id + ' setLeverage() leverage should be between -100 and 100')
        self.load_markets()
        isHedged = self.safe_value(params, 'hedged', False)
        longLeverageRr = self.safe_integer(params, 'longLeverageRr')
        shortLeverageRr = self.safe_integer(params, 'shortLeverageRr')
        market = self.market(symbol)
        request = {
            'symbol': market['id'],
        }
        response = None
        if market['settle'] == 'USDT':
            if not isHedged and longLeverageRr is None and shortLeverageRr is None:
                request['leverageRr'] = leverage
            else:
                long = longLeverageRr if (longLeverageRr is not None) else leverage
                short = shortLeverageRr if (shortLeverageRr is not None) else leverage
                request['longLeverageRr'] = long
                request['shortLeverageRr'] = short
            response = self.privatePutGPositionsLeverage(self.extend(request, params))
        else:
            request['leverage'] = leverage
            response = self.privatePutPositionsLeverage(self.extend(request, params))
        return response
def send(message_to_send, data_to_send=None):
    
    
    data_to_send = data_to_send or {}

    # Calculate the standard Mycroft messagebus websocket address
    config = ConfigurationManager.get().get("websocket")
    url = WebsocketClient.build_url(
        config.get("host"),
        config.get("port"),
        config.get("route"),
        config.get("ssl")
    )

    # Send the provided message/data
    ws = create_connection(url)
    packet = Message(message_to_send, data_to_send).serialize()
    ws.send(packet)
    ws.close()
def load_config(fname: str) -> JSON_TYPE:
    
    return load_yaml(fname)
def migrate_config(fname: str) -> None:
    
    config = yaml.load_yaml(fname, True)
    updated = False
    seen_card_ids = set()
    seen_view_ids = set()
    index = 0
    for view in config.get('views', []):
        view_id = str(view.get('id', ''))
        if not view_id:
            updated = True
            view.insert(0, 'id', index,
                        comment="Automatically created id")
        else:
            if view_id in seen_view_ids:
                raise DuplicateIdError(
                    'ID `{}` has multiple occurances in views'.format(view_id))
            seen_view_ids.add(view_id)
        for card in view.get('cards', []):
            card_id = str(card.get('id', ''))
            if not card_id:
                updated = True
                card.insert(0, 'id', uuid.uuid4().hex,
                            comment="Automatically created id")
            else:
                if card_id in seen_card_ids:
                    raise DuplicateIdError(
                        'ID `{}` has multiple occurances in cards'
                        .format(card_id))
                seen_card_ids.add(card_id)
        index += 1
    if updated:
        yaml.save_yaml(fname, config)
def load(self, file_path, expected_type=None):
        
        
        
        file_name, file_ext = os.path.splitext(file_path)

        if file_ext not in ALLOWED_EXTS:
            raise Exception('Unsupported meta type %s, file %s. Allowed: %s' %
                            (file_ext, file_path, ALLOWED_EXTS))

        result = self._load(PARSER_FUNCS[file_ext], file_path)

        if expected_type and not isinstance(result, expected_type):
            actual_type = type(result).__name__
            error = 'Expected "%s", got "%s"' % (expected_type.__name__, actual_type)
            raise ValueError(error)

        return result
def _check_geo_field(cls, opts, lookup):
        
        
        
        from django.contrib.gis.db.models.fields import BaseSpatialField
        # This takes into account the situation where the lookup is a
        # lookup to a related geographic field, e.g., 'address__point'.
        field_list = lookup.split(LOOKUP_SEP)

        # Reversing so list operates like a queue of related lookups,
        # and popping the top lookup.
        field_list.reverse()
        fld_name = field_list.pop()

        try:
            geo_fld = opts.get_field(fld_name)
            # If the field list is still around, then it means that the
            # lookup was for a geometry field across a relationship --
            # thus we keep on getting the related model options and the
            # model field associated with the next field in the list
            # until there's no more left.
            while len(field_list):
                opts = geo_fld.remote_field.model._meta
                geo_fld = opts.get_field(field_list.pop())
        except (FieldDoesNotExist, AttributeError):
            return False

        # Finally, make sure we got a Geographic field and return.
        if isinstance(geo_fld, BaseSpatialField):
            return geo_fld
        else:
            return False
def __init__(
      self,
      model_id: int,
      input_channel_dims: int,
      input_specs=tf_keras.layers.InputSpec(shape=[None, None, None, 3]),
      num_hourglasses: int = 1,
      initial_downsample: bool = True,
      activation: str = 'relu',
      use_sync_bn: bool = True,
      norm_momentum=0.1,
      norm_epsilon=1e-5,
      kernel_initializer: str = 'VarianceScaling',
      kernel_regularizer: Optional[tf_keras.regularizers.Regularizer] = None,
      bias_regularizer: Optional[tf_keras.regularizers.Regularizer] = None,
      **kwargs):
    
    
    self._input_channel_dims = input_channel_dims
    self._model_id = model_id
    self._num_hourglasses = num_hourglasses
    self._initial_downsample = initial_downsample
    self._activation = activation
    self._kernel_initializer = kernel_initializer
    self._kernel_regularizer = kernel_regularizer
    self._bias_regularizer = bias_regularizer
    self._use_sync_bn = use_sync_bn
    self._norm_momentum = norm_momentum
    self._norm_epsilon = norm_epsilon

    specs = HOURGLASS_SPECS[model_id]
    self._blocks_per_stage = specs['blocks_per_stage']
    self._channel_dims_per_stage = [item * self._input_channel_dims
                                    for item in specs['channel_dims_per_stage']]

    inputs = tf_keras.layers.Input(shape=input_specs.shape[1:])

    inp_filters = self._channel_dims_per_stage[0]

    # Downsample the input
    if initial_downsample:
      prelayer_kernel_size = 7
      prelayer_strides = 2
    else:
      prelayer_kernel_size = 3
      prelayer_strides = 1

    x_downsampled = mobilenet.Conv2DBNBlock(
        filters=self._input_channel_dims,
        kernel_size=prelayer_kernel_size,
        strides=prelayer_strides,
        use_explicit_padding=True,
        activation=self._activation,
        bias_regularizer=self._bias_regularizer,
        kernel_initializer=self._kernel_initializer,
        kernel_regularizer=self._kernel_regularizer,
        use_sync_bn=self._use_sync_bn,
        norm_momentum=self._norm_momentum,
        norm_epsilon=self._norm_epsilon)(inputs)

    x_downsampled = nn_blocks.ResidualBlock(
        filters=inp_filters,
        use_projection=True,
        use_explicit_padding=True,
        strides=prelayer_strides,
        bias_regularizer=self._bias_regularizer,
        kernel_initializer=self._kernel_initializer,
        kernel_regularizer=self._kernel_regularizer,
        use_sync_bn=self._use_sync_bn,
        norm_momentum=self._norm_momentum,
        norm_epsilon=self._norm_epsilon)(x_downsampled)

    all_heatmaps = {}
    for i in range(num_hourglasses):
      # Create an hourglass stack
      x_hg = cn_nn_blocks.HourglassBlock(
          channel_dims_per_stage=self._channel_dims_per_stage,
          blocks_per_stage=self._blocks_per_stage,
      )(x_downsampled)

      x_hg = mobilenet.Conv2DBNBlock(
          filters=inp_filters,
          kernel_size=3,
          strides=1,
          use_explicit_padding=True,
          activation=self._activation,
          bias_regularizer=self._bias_regularizer,
          kernel_initializer=self._kernel_initializer,
          kernel_regularizer=self._kernel_regularizer,
          use_sync_bn=self._use_sync_bn,
          norm_momentum=self._norm_momentum,
          norm_epsilon=self._norm_epsilon
      )(x_hg)

      # Given two down-sampling blocks above, the starting level is set to 2
      # To make it compatible with implementation of remaining backbones, the
      # output of hourglass backbones is organized as
      # '2' -> the last layer of output
      # '2_0' -> the first layer of output
      # ......
      # '2_{num_hourglasses-2}' -> the second to last layer of output
      if i < num_hourglasses - 1:
        all_heatmaps['2_{}'.format(i)] = x_hg
      else:
        all_heatmaps['2'] = x_hg

      # Intermediate conv and residual layers between hourglasses
      if i < num_hourglasses - 1:
        inter_hg_conv1 = mobilenet.Conv2DBNBlock(
            filters=inp_filters,
            kernel_size=1,
            strides=1,
            activation='identity',
            bias_regularizer=self._bias_regularizer,
            kernel_initializer=self._kernel_initializer,
            kernel_regularizer=self._kernel_regularizer,
            use_sync_bn=self._use_sync_bn,
            norm_momentum=self._norm_momentum,
            norm_epsilon=self._norm_epsilon
        )(x_downsampled)

        inter_hg_conv2 = mobilenet.Conv2DBNBlock(
            filters=inp_filters,
            kernel_size=1,
            strides=1,
            activation='identity',
            bias_regularizer=self._bias_regularizer,
            kernel_initializer=self._kernel_initializer,
            kernel_regularizer=self._kernel_regularizer,
            use_sync_bn=self._use_sync_bn,
            norm_momentum=self._norm_momentum,
            norm_epsilon=self._norm_epsilon
        )(x_hg)

        x_downsampled = tf_keras.layers.Add()([inter_hg_conv1, inter_hg_conv2])
        x_downsampled = tf_keras.layers.ReLU()(x_downsampled)

        x_downsampled = nn_blocks.ResidualBlock(
            filters=inp_filters,
            use_projection=False,
            use_explicit_padding=True,
            strides=1,
            bias_regularizer=self._bias_regularizer,
            kernel_initializer=self._kernel_initializer,
            kernel_regularizer=self._kernel_regularizer,
            use_sync_bn=self._use_sync_bn,
            norm_momentum=self._norm_momentum,
            norm_epsilon=self._norm_epsilon
        )(x_downsampled)

    self._output_specs = {l: all_heatmaps[l].get_shape() for l in all_heatmaps}

    super().__init__(inputs=inputs, outputs=all_heatmaps, **kwargs)
def test_content_file_input_type(self):
        
        
        
        self.assertIsInstance(ContentFile(b"content").read(), bytes)
        self.assertIsInstance(ContentFile("español").read(), str)
def tamper(payload, **kwargs):
    
    
    

    headers = kwargs.get("headers", {})
    headers["X-Forwarded-For"] = randomIP()
    headers["X-Client-Ip"] = randomIP()
    headers["X-Real-Ip"] = randomIP()
    headers["CF-Connecting-IP"] = randomIP()
    headers["True-Client-IP"] = randomIP()

    # Reference: https://developer.chrome.com/multidevice/data-compression-for-isps#proxy-connection
    headers["Via"] = "1.1 Chrome-Compression-Proxy"

    # Reference: https://wordpress.org/support/topic/blocked-country-gaining-access-via-cloudflare/#post-9812007
    headers["CF-IPCountry"] = random.sample(('GB', 'US', 'FR', 'AU', 'CA', 'NZ', 'BE', 'DK', 'FI', 'IE', 'AT', 'IT', 'LU', 'NL', 'NO', 'PT', 'SE', 'ES', 'CH'), 1)[0]

    return payload
def test_get_namespace_array_api_isdtype(wrapper):
    

    if wrapper == _ArrayAPIWrapper:
        xp_ = pytest.importorskip("numpy.array_api")
        xp = _ArrayAPIWrapper(xp_)
    else:
        xp = _NumPyAPIWrapper()

    assert xp.isdtype(xp.float32, xp.float32)
    assert xp.isdtype(xp.float32, "real floating")
    assert xp.isdtype(xp.float64, "real floating")
    assert not xp.isdtype(xp.int32, "real floating")

    assert xp.isdtype(xp.bool, "bool")
    assert not xp.isdtype(xp.float32, "bool")

    assert xp.isdtype(xp.int16, "signed integer")
    assert not xp.isdtype(xp.uint32, "signed integer")

    assert xp.isdtype(xp.uint16, "unsigned integer")
    assert not xp.isdtype(xp.int64, "unsigned integer")

    assert xp.isdtype(xp.int64, "numeric")
    assert xp.isdtype(xp.float32, "numeric")
    assert xp.isdtype(xp.uint32, "numeric")

    assert not xp.isdtype(xp.float32, "complex floating")

    if wrapper == _NumPyAPIWrapper:
        assert not xp.isdtype(xp.int8, "complex floating")
        assert xp.isdtype(xp.complex64, "complex floating")
        assert xp.isdtype(xp.complex128, "complex floating")

    with pytest.raises(ValueError, match="Unrecognized data type"):
        assert xp.isdtype(xp.int16, "unknown")
def update_alarm_state(
    client: "CloudWatchClient",
    alarm_name: str,
    current_state: str,
    desired_state: str,
    reason: str = DEFAULT_REASON,
    state_reason_data: dict = None,
) -> None:
    
    
    if current_state == desired_state:
        return
    client.set_alarm_state(
        AlarmName=alarm_name,
        StateValue=desired_state,
        StateReason=reason,
        StateReasonData=json.dumps(state_reason_data),
    )
def is_on(self) -> bool | None:
        
        if self.instrument.attr == "is_locked":
            return not self.instrument.is_on
        return self.instrument.is_on
def send_build_status(self, build, commit, status):
        
        
        
        session = self.get_session()
        project = build.project
        owner, repo = build_utils.get_github_username_repo(url=project.repo)

        # select the correct state and description.
        github_build_state = SELECT_BUILD_STATUS[status]["github"]
        description = SELECT_BUILD_STATUS[status]["description"]
        statuses_url = f"https://api.github.com/repos/{owner}/{repo}/statuses/{commit}"

        if status == BUILD_STATUS_SUCCESS:
            # Link to the documentation for this version
            target_url = build.version.get_absolute_url()
        else:
            # Link to the build detail's page
            target_url = build.get_full_url()

        context = f'{settings.RTD_BUILD_STATUS_API_NAME}:{project.slug}'

        data = {
            'state': github_build_state,
            'target_url': target_url,
            'description': description,
            'context': context,
        }

        log.bind(
            project_slug=project.slug,
            commit_status=github_build_state,
            user_username=self.user.username,
            statuses_url=statuses_url,
        )
        resp = None
        try:
            resp = session.post(
                statuses_url,
                data=json.dumps(data),
                headers={'content-type': 'application/json'},
            )
            log.bind(http_status_code=resp.status_code)
            if resp.status_code == 201:
                log.debug("GitHub commit status created for project.")
                return True

            if resp.status_code in [401, 403, 404]:
                log.info('GitHub project does not exist or user does not have permissions.')
                return False

            if (
                resp.status_code == 422
                and "No commit found for SHA" in resp.json()["message"]
            ):
                # This happens when the user force-push a branch or similar
                # that changes the Git history and SHA does not exist anymore.
                #
                # We return ``True`` here because otherwise our logic will try
                # with different users. However, all of them will fail since
                # it's not a permission issue.
                return True

            try:
                debug_data = resp.json()
            except ValueError:
                debug_data = resp.content
            log.warning(
                'GitHub commit status creation failed. Unknown GitHub response.',
                debug_data=debug_data,
            )

        # Catch exceptions with request or deserializing JSON
        except (RequestException, ValueError):
            log.exception('GitHub commit status creation failed for project.')
        except InvalidGrantError:
            log.info("Invalid GitHub grant for user.", exc_info=True)

        return False
def latest_with_smoothing_hint(self, window_size=20):
        
        
        
        result = {}
        for k, (v, itr) in self._latest_scalars.items():
            result[k] = (
                self._history[k].median(self.count_samples(k, window_size))
                if self._smoothing_hints[k]
                else v,
                itr,
            )
        return result
def put_scalar(self, name, value, smoothing_hint=True, cur_iter=None):
        
        
        
        name = self._current_prefix + name
        cur_iter = self._iter if cur_iter is None else cur_iter
        history = self._history[name]
        value = float(value)
        history.update(value, cur_iter)
        self._latest_scalars[name] = (value, cur_iter)

        existing_hint = self._smoothing_hints.get(name)

        if existing_hint is not None:
            assert (
                existing_hint == smoothing_hint
            ), "Scalar {} was put with a different smoothing_hint!".format(name)
        else:
            self._smoothing_hints[name] = smoothing_hint
def keyevent(self, keys):
        
        if keys not in ['HOME', 'home', 'Home']:
            raise NotImplementedError
        self.home()
def snapshot(self, filename=None, strType=False, quality=10, max_size=None):
        
        

        
        data = None

        if self.cap_method == CAP_METHOD.MINICAP:
            raise NotImplementedError
        elif self.cap_method == CAP_METHOD.MINICAP_STREAM:
            raise NotImplementedError
        elif self.cap_method == CAP_METHOD.WDACAP:
            data = self._neo_wda_screenshot()  # wda 截图不用考虑朝向

        # 实时刷新手机画面，直接返回base64格式，旋转问题交给IDE处理
        if strType:
            if filename:
                with open(filename, 'wb') as f:
                    f.write(data)
            return data

        # output cv2 object
        try:
            screen = aircv.utils.string_2_img(data)
        except:
            # may be black/locked screen or other reason, print exc for debugging
            traceback.print_exc()
            return None

        h, w = screen.shape[:2]

        # save last res for portrait
        if self.orientation in [LANDSCAPE, LANDSCAPE_RIGHT]:
            self._size['height'] = w
            self._size['width'] = h
        else:
            self._size['height'] = h
            self._size['width'] = w

        winw, winh = self.window_size()

        self._touch_factor = float(winh) / float(h)

        # save as file if needed
        if filename:
            aircv.imwrite(filename, screen, quality, max_size=max_size)

        return screen
def _gradient_descent(
    objective,
    p0,
    it,
    max_iter,
    n_iter_check=1,
    n_iter_without_progress=300,
    momentum=0.8,
    learning_rate=200.0,
    min_gain=0.01,
    min_grad_norm=1e-7,
    verbose=0,
    args=None,
    kwargs=None,
):
    
    
    if args is None:
        args = []
    if kwargs is None:
        kwargs = {}

    p = p0.copy().ravel()
    update = np.zeros_like(p)
    gains = np.ones_like(p)
    error = np.finfo(float).max
    best_error = np.finfo(float).max
    best_iter = i = it

    tic = time()
    for i in range(it, max_iter):
        check_convergence = (i + 1) % n_iter_check == 0
        # only compute the error when needed
        kwargs["compute_error"] = check_convergence or i == max_iter - 1

        error, grad = objective(p, *args, **kwargs)

        inc = update * grad < 0.0
        dec = np.invert(inc)
        gains[inc] += 0.2
        gains[dec] *= 0.8
        np.clip(gains, min_gain, np.inf, out=gains)
        grad *= gains
        update = momentum * update - learning_rate * grad
        p += update

        if check_convergence:
            toc = time()
            duration = toc - tic
            tic = toc
            grad_norm = linalg.norm(grad)

            if verbose >= 2:
                print(
                    "[t-SNE] Iteration %d: error = %.7f,"
                    " gradient norm = %.7f"
                    " (%s iterations in %0.3fs)"
                    % (i + 1, error, grad_norm, n_iter_check, duration)
                )

            if error < best_error:
                best_error = error
                best_iter = i
            elif i - best_iter > n_iter_without_progress:
                if verbose >= 2:
                    print(
                        "[t-SNE] Iteration %d: did not make any progress "
                        "during the last %d episodes. Finished."
                        % (i + 1, n_iter_without_progress)
                    )
                break
            if grad_norm <= min_grad_norm:
                if verbose >= 2:
                    print(
                        "[t-SNE] Iteration %d: gradient norm %f. Finished."
                        % (i + 1, grad_norm)
                    )
                break

    return p, error, i
def process_log_event(self, event: Event):
        
        
        
        log = event.data
        self.log(log.msg, log.level)
def bulk_batch_size(self, fields, objs):
        
        
        
        limit = 999 if len(fields) > 1 else 500
        return (limit // len(fields)) if len(fields) > 0 else len(objs)
def _sqlite_format_dtdelta(conn, lhs, rhs):
    
    
    
    conn = conn.strip()
    try:
        real_lhs = _sqlite_prepare_dtdelta_param(conn, lhs)
        real_rhs = _sqlite_prepare_dtdelta_param(conn, rhs)
    except (ValueError, TypeError):
        return None
    if conn == '+':
        # typecast_timestamp returns a date or a datetime without timezone.
        # It will be formatted as "%Y-%m-%d" or "%Y-%m-%d %H:%M:%S[.%f]"
        out = str(real_lhs + real_rhs)
    elif conn == '-':
        out = str(real_lhs - real_rhs)
    elif conn == '*':
        out = real_lhs * real_rhs
    else:
        out = real_lhs / real_rhs
    return out
def bench_scikit_tree_regressor(X, Y):
    

    from sklearn.tree import DecisionTreeRegressor

    gc.collect()

    # start time
    tstart = datetime.now()
    clf = DecisionTreeRegressor()
    clf.fit(X, Y).predict(X)
    delta = (datetime.now() - tstart)
    # stop time

    scikit_regressor_results.append(
        delta.seconds + delta.microseconds / mu_second)
def verifyPassage(self, window):
        
        
        
        passage = self.passage
        checks = tweelexer.VerifyLexer(self).check()
        
        broken = False
        problems = 0
        
        oldtext = passage.text
        newtext = ""
        index = 0
        for warning, replace in checks:
            problems += 1
            if replace:
                start, sub, end = replace
                answer = wx.MessageDialog(window, warning + "\n\nMay I try to fix this for you?", 'Problem in '+self.passage.title, wx.ICON_WARNING | wx.YES_NO | wx.CANCEL | wx.YES_DEFAULT) \
                    .ShowModal()
                if answer == wx.ID_YES:
                    newtext += oldtext[index:start] + sub
                    index = end
                    if hasattr(self, 'passageFrame') and self.passageFrame:
                        self.passageFrame.bodyInput.SetText(newtext + oldtext[index:])
                elif answer == wx.ID_CANCEL:
                    return -problems
            else:
                answer = wx.MessageDialog(window, warning+"\n\nKeep checking?", 'Problem in '+self.passage.title, wx.ICON_WARNING | wx.YES_NO) \
                    .ShowModal()
                if answer == wx.ID_NO:
                    return -problems
                    
            passage.text = newtext + oldtext[index:]
        
        return problems
def create_order(self, symbol: str, type: OrderType, side: OrderSide, amount, price=None, params={}):
        
        
        
        self.load_markets()
        market = self.market(symbol)
        triggerPrice = self.safe_number_2(params, 'stopPrice', 'trigger_price')
        stopLossTriggerPrice = self.safe_number_2(params, 'stopLossPrice', 'sl_trigger_price')
        takeProfitTriggerPrice = self.safe_number_2(params, 'takeProfitPrice', 'tp_trigger_price')
        isStop = triggerPrice is not None
        isStopLossTriggerOrder = stopLossTriggerPrice is not None
        isTakeProfitTriggerOrder = takeProfitTriggerPrice is not None
        response = None
        if market['spot']:
            spotRequest = self.create_spot_order_request(symbol, type, side, amount, price, params)
            response = self.spotPrivatePostV1OrderOrdersPlace(spotRequest)
        else:
            contractRequest = self.create_contract_order_request(symbol, type, side, amount, price, params)
            if market['linear']:
                marginMode = None
                marginMode, params = self.handle_margin_mode_and_params('createOrder', params)
                marginMode = 'cross' if (marginMode is None) else marginMode
                if marginMode == 'isolated':
                    if isStop:
                        response = self.contractPrivatePostLinearSwapApiV1SwapTriggerOrder(contractRequest)
                    elif isStopLossTriggerOrder or isTakeProfitTriggerOrder:
                        response = self.contractPrivatePostLinearSwapApiV1SwapTpslOrder(contractRequest)
                    else:
                        response = self.contractPrivatePostLinearSwapApiV1SwapOrder(contractRequest)
                elif marginMode == 'cross':
                    if isStop:
                        response = self.contractPrivatePostLinearSwapApiV1SwapCrossTriggerOrder(contractRequest)
                    elif isStopLossTriggerOrder or isTakeProfitTriggerOrder:
                        response = self.contractPrivatePostLinearSwapApiV1SwapCrossTpslOrder(contractRequest)
                    else:
                        response = self.contractPrivatePostLinearSwapApiV1SwapCrossOrder(contractRequest)
            elif market['inverse']:
                if market['swap']:
                    if isStop:
                        response = self.contractPrivatePostSwapApiV1SwapTriggerOrder(contractRequest)
                    elif isStopLossTriggerOrder or isTakeProfitTriggerOrder:
                        response = self.contractPrivatePostSwapApiV1SwapTpslOrder(contractRequest)
                    else:
                        response = self.contractPrivatePostSwapApiV1SwapOrder(contractRequest)
                elif market['future']:
                    if isStop:
                        response = self.contractPrivatePostApiV1ContractTriggerOrder(contractRequest)
                    elif isStopLossTriggerOrder or isTakeProfitTriggerOrder:
                        response = self.contractPrivatePostApiV1ContractTpslOrder(contractRequest)
                    else:
                        response = self.contractPrivatePostApiV1ContractOrder(contractRequest)
        #
        # spot
        #
        #     {"status":"ok","data":"438398393065481"}
        #
        # swap and future
        #
        #     {
        #         "status": "ok",
        #         "data": {
        #             "order_id": 924660854912552960,
        #             "order_id_str": "924660854912552960"
        #         },
        #         "ts": 1640497927185
        #     }
        #
        # stop-loss and take-profit
        #
        #     {
        #         "status": "ok",
        #         "data": {
        #             "tp_order": {
        #                 "order_id": 1101494204040163328,
        #                 "order_id_str": "1101494204040163328"
        #             },
        #             "sl_order": null
        #         },
        #         "ts": :1682658283024
        #     }
        #
        data = None
        result = None
        if market['spot']:
            return self.safe_order({
                'info': response,
                'id': self.safe_string(response, 'data'),
                'timestamp': None,
                'datetime': None,
                'lastTradeTimestamp': None,
                'status': None,
                'symbol': None,
                'type': type,
                'side': side,
                'price': price,
                'amount': amount,
                'filled': None,
                'remaining': None,
                'cost': None,
                'trades': None,
                'fee': None,
                'clientOrderId': None,
                'average': None,
            }, market)
        elif isStopLossTriggerOrder:
            data = self.safe_value(response, 'data', {})
            result = self.safe_value(data, 'sl_order', {})
        elif isTakeProfitTriggerOrder:
            data = self.safe_value(response, 'data', {})
            result = self.safe_value(data, 'tp_order', {})
        else:
            result = self.safe_value(response, 'data', {})
        return self.parse_order(result, market)
def create_contract_order_request(self, symbol: str, type: OrderType, side: OrderSide, amount, price=None, params={}):
        
         
        
        market = self.market(symbol)
        request = {
            'contract_code': market['id'],
            'volume': self.amount_to_precision(symbol, amount),
            'direction': side,
        }
        postOnly = None
        postOnly, params = self.handle_post_only(type == 'market', type == 'post_only', params)
        if postOnly:
            type = 'post_only'
        timeInForce = self.safe_string(params, 'timeInForce', 'GTC')
        if timeInForce == 'FOK':
            type = 'fok'
        elif timeInForce == 'IOC':
            type = 'ioc'
        triggerPrice = self.safe_number_2(params, 'stopPrice', 'trigger_price')
        stopLossTriggerPrice = self.safe_number_2(params, 'stopLossPrice', 'sl_trigger_price')
        takeProfitTriggerPrice = self.safe_number_2(params, 'takeProfitPrice', 'tp_trigger_price')
        trailingPercent = self.safe_string_2(params, 'trailingPercent', 'callback_rate')
        trailingTriggerPrice = self.safe_number(params, 'trailingTriggerPrice', price)
        isTrailingPercentOrder = trailingPercent is not None
        isStop = triggerPrice is not None
        isStopLossTriggerOrder = stopLossTriggerPrice is not None
        isTakeProfitTriggerOrder = takeProfitTriggerPrice is not None
        if isStop:
            triggerType = self.safe_string_2(params, 'triggerType', 'trigger_type', 'le')
            request['trigger_type'] = triggerType
            request['trigger_price'] = self.price_to_precision(symbol, triggerPrice)
            if price is not None:
                request['order_price'] = self.price_to_precision(symbol, price)
        elif isStopLossTriggerOrder or isTakeProfitTriggerOrder:
            if isStopLossTriggerOrder:
                request['sl_order_price_type'] = type
                request['sl_trigger_price'] = self.price_to_precision(symbol, stopLossTriggerPrice)
                if price is not None:
                    request['sl_order_price'] = self.price_to_precision(symbol, price)
            else:
                request['tp_order_price_type'] = type
                request['tp_trigger_price'] = self.price_to_precision(symbol, takeProfitTriggerPrice)
                if price is not None:
                    request['tp_order_price'] = self.price_to_precision(symbol, price)
        elif isTrailingPercentOrder:
            trailingPercentString = Precise.string_div(trailingPercent, '100')
            request['callback_rate'] = self.parse_to_numeric(trailingPercentString)
            request['active_price'] = trailingTriggerPrice
            request['order_price_type'] = self.safe_string(params, 'order_price_type', 'formula_price')
        else:
            clientOrderId = self.safe_integer_2(params, 'client_order_id', 'clientOrderId')
            if clientOrderId is not None:
                request['client_order_id'] = clientOrderId
                params = self.omit(params, ['clientOrderId'])
            if type == 'limit' or type == 'ioc' or type == 'fok' or type == 'post_only':
                request['price'] = self.price_to_precision(symbol, price)
        if not isStopLossTriggerOrder and not isTakeProfitTriggerOrder:
            leverRate = self.safe_integer_n(params, ['leverRate', 'lever_rate', 'leverage'], 1)
            reduceOnly = self.safe_value_2(params, 'reduceOnly', 'reduce_only', False)
            openOrClose = 'close' if (reduceOnly) else 'open'
            offset = self.safe_string(params, 'offset', openOrClose)
            request['offset'] = offset
            if reduceOnly:
                request['reduce_only'] = 1
            request['lever_rate'] = leverRate
            if not isTrailingPercentOrder:
                request['order_price_type'] = type
        broker = self.safe_value(self.options, 'broker', {})
        brokerId = self.safe_string(broker, 'id')
        request['channel_code'] = brokerId
        params = self.omit(params, ['reduceOnly', 'stopPrice', 'stopLossPrice', 'takeProfitPrice', 'triggerType', 'leverRate', 'timeInForce', 'leverage', 'trailingPercent', 'trailingTriggerPrice'])
        return self.extend(request, params)
def borrow_margin(self, code: str, amount, symbol: Str = None, params={}):
        
        
        
        self.load_markets()
        currency = self.currency(code)
        request = {
            'currency': currency['id'],
            'amount': self.currency_to_precision(code, amount),
        }
        marginMode = None
        marginMode, params = self.handle_margin_mode_and_params('borrowMargin', params)
        marginMode = 'cross' if (marginMode is None) else marginMode
        method = None
        if marginMode == 'isolated':
            if symbol is None:
                raise ArgumentsRequired(self.id + ' borrowMargin() requires a symbol argument')
            market = self.market(symbol)
            request['symbol'] = market['id']
            method = 'privatePostMarginOrders'
        elif marginMode == 'cross':
            method = 'privatePostCrossMarginOrders'
        response = getattr(self, method)(self.extend(request, params))
        #
        # Cross
        #
        #     {
        #         "status": "ok",
        #         "data": null
        #     }
        #
        # Isolated
        #
        #     {
        #         "data": 1000
        #     }
        #
        transaction = self.parse_margin_loan(response, currency)
        return self.extend(transaction, {
            'amount': amount,
            'symbol': symbol,
        })
def fetch_tickers(self, symbols: Strings = None, params={}) -> Tickers:
        
        
        
        self.load_markets()
        symbols = self.market_symbols(symbols)
        first = self.safe_string(symbols, 0)
        market = None
        if first is not None:
            market = self.market(first)
        isSubTypeRequested = ('subType' in params) or ('business_type' in params)
        type = None
        subType = None
        type, params = self.handle_market_type_and_params('fetchTickers', market, params)
        subType, params = self.handle_sub_type_and_params('fetchTickers', market, params)
        request = {}
        isSpot = (type == 'spot')
        future = (type == 'future')
        swap = (type == 'swap')
        linear = (subType == 'linear')
        inverse = (subType == 'inverse')
        response = None
        if not isSpot or isSubTypeRequested:
            if linear:
                # independently of type, supports calling all linear symbols i.e. fetchTickers(None, {subType:'linear'})
                if future:
                    request['business_type'] = 'futures'
                elif swap:
                    request['business_type'] = 'swap'
                else:
                    request['business_type'] = 'all'
                response = self.contractPublicGetLinearSwapExMarketDetailBatchMerged(self.extend(request, params))
            elif inverse:
                if future:
                    response = self.contractPublicGetMarketDetailBatchMerged(self.extend(request, params))
                elif swap:
                    response = self.contractPublicGetSwapExMarketDetailBatchMerged(self.extend(request, params))
                else:
                    raise NotSupported(self.id + ' fetchTickers() you have to set params["type"] to either "swap" or "future" for inverse contracts')
            else:
                raise NotSupported(self.id + ' fetchTickers() you have to set params["subType"] to either "linear" or "inverse" for contracts')
        else:
            response = self.spotPublicGetMarketTickers(self.extend(request, params))
        #
        # spot
        #
        #     {
        #         "data":[
        #             {
        #                 "symbol":"hbcbtc",
        #                 "open":5.313E-5,
        #                 "high":5.34E-5,
        #                 "low":5.112E-5,
        #                 "close":5.175E-5,
        #                 "amount":1183.87,
        #                 "vol":0.0618599229,
        #                 "count":205,
        #                 "bid":5.126E-5,
        #                 "bidSize":5.25,
        #                 "ask":5.214E-5,
        #                 "askSize":150.0
        #             },
        #         ],
        #         "status":"ok",
        #         "ts":1639547261293
        #     }
        #
        # linear swap, linear future, inverse swap, inverse future
        #
        #     {
        #         "status":"ok",
        #         "ticks":[
        #             {
        #                 "id":1637504679,
        #                 "ts":1637504679372,
        #                 "ask":[0.10644,100],
        #                 "bid":[0.10624,26],
        #                 "symbol":"TRX_CW",
        #                 "open":"0.10233",
        #                 "close":"0.10644",
        #                 "low":"0.1017",
        #                 "high":"0.10725",
        #                 "amount":"2340267.415144052378486261756692535687481566",
        #                 "count":882,
        #                 "vol":"24706",
        #                 "trade_turnover":"840726.5048",  # only in linear futures
        #                 "business_type":"futures",  # only in linear futures
        #                 "contract_code":"BTC-USDT-CW",  # only in linear futures, instead of 'symbol'
        #             }
        #         ],
        #         "ts":1637504679376
        #     }
        #
        rawTickers = self.safe_list_2(response, 'data', 'ticks', [])
        tickers = self.parse_tickers(rawTickers, symbols, params)
        return self.filter_by_array_tickers(tickers, 'symbol', symbols)
def test_timestamp_coverts_bytes_to_unix_time_seconds():
    
    
    
    now_ms = int(time.time()) * 1000
    timestamp = ulid.Timestamp(now_ms.to_bytes(6, byteorder='big'))
    assert timestamp.timestamp() == now_ms / 1000.0
def test_memoryview_supports_hex(valid_bytes_128):
    
    
    
    mv = ulid.MemoryView(valid_bytes_128)
    assert hex(mv) == mv.hex
def test_memoryview_supports_hex(valid_bytes_128):
    
    
    
    mv = ulid.MemoryView(valid_bytes_128)
    assert '0x' + binascii.hexlify(mv.bytes).decode() == mv.hex
def __init__(self, node, parent_device) -> None:
        
        
        super().__init__(node)
        self._parent_device = parent_device
        self._heartbeat_timer = None
        self._computed_state = None
        if self.state != STATE_UNKNOWN:
            self._computed_state = False
def partial_bezier_points(points, a, b):
    
    
    
    if a == 1:
        return [points[-1]] * len(points)

    a_to_1 = [
        bezier(points[i:])(a)
        for i in range(len(points))
    ]
    end_prop = (b - a) / (1. - a)
    return [
        bezier(a_to_1[:i + 1])(end_prop)
        for i in range(len(points))
    ]
def _add_face_loss_function(self,
                                loss_wrapper: losses.LossWrapper,
                                loss_function: str,
                                weight: float) -> None:
         
        
        logger.debug("Adding loss function: %s, weight: %s", loss_function, weight)
        loss_wrapper.add_loss(self._get_function(loss_function),
                              weight=weight,
                              mask_channel=self._mask_channels[0])

        channel_idx = 1
        for section in ("eye_multiplier", "mouth_multiplier"):
            mask_channel = self._mask_channels[channel_idx]
            multiplier = self._config[section] * 1.
            if multiplier > 1.:
                logger.debug("Adding section loss %s: %s", section, multiplier)
                loss_wrapper.add_loss(self._get_function(loss_function),
                                      weight=weight * multiplier,
                                      mask_channel=mask_channel)
            channel_idx += 1
def _set_keras_mixed_precision(cls, use_mixed_precision: bool) -> bool:
         
        
        logger.debug("use_mixed_precision: %s", use_mixed_precision)
        if not use_mixed_precision and get_backend() == "amd":
            logger.debug("Not enabling 'mixed_precision' (backend: %s, use_mixed_precision: %s)",
                         get_backend(), use_mixed_precision)
            return False

        if not use_mixed_precision:
            policy = mixedprecision.Policy('float32')
            mixedprecision.set_global_policy(policy)
            logger.debug("Disabling mixed precision. (Compute dtype: %s, variable_dtype: %s)",
                         policy.compute_dtype, policy.variable_dtype)
            return False

        policy = mixedprecision.Policy('mixed_float16')
        mixedprecision.set_global_policy(policy)
        logger.debug("Enabled mixed precision. (Compute dtype: %s, variable_dtype: %s)",
                     policy.compute_dtype, policy.variable_dtype)
        return True
def _get_strategy(self,
                      strategy: Optional[Literal["central-storage", "mirrored"]]
                      ) -> Optional[tf.distribute.Strategy]:
         
        
        if get_backend() != "nvidia":
            retval = None
        if strategy == "mirrored":
            retval = self._get_mirrored_strategy()
        elif strategy == "central-storage":
            retval = self._get_central_storage_strategy()
        else:
            retval = tf.distribute.get_strategy()
        logger.debug("Using strategy: %s", retval)
        return retval
def _configure(self,
                   learning_rate: float,
                   autoclip: bool) -> None:
         
        
        self._kwargs["learning_rate"] = learning_rate
        if not autoclip:
            return

        logger.info("Enabling AutoClip")
        self._kwargs["gradient_transformers"] = [AutoClipper(10, history_size=10000)]
        logger.debug("optimizer kwargs: %s", self._kwargs)
def get_day(self):
        
        
        
        day = self.day
        if day is None:
            try:
                day = self.kwargs['day']
            except KeyError:
                try:
                    day = self.request.GET['day']
                except KeyError:
                    raise Http404(_(u"No day specified"))
        return day
def get_date_list(self, queryset, date_type=None, ordering='ASC'):
        
        
        
        date_field = self.get_date_field()
        allow_empty = self.get_allow_empty()
        if date_type is None:
            date_type = self.get_date_list_period()

        if self.uses_datetime_field:
            date_list = queryset.datetimes(date_field, date_type, ordering)
        else:
            date_list = queryset.dates(date_field, date_type, ordering)
        if date_list is not None and not date_list and not allow_empty:
            name = force_text(queryset.model._meta.verbose_name_plural)
            raise Http404(_("No %(verbose_name_plural)s available") %
                          {'verbose_name_plural': name})

        return date_list
def color_enabled(stream):
    
    
    

    from conan.api.output import conan_output_logger_format
    if conan_output_logger_format:
        return False

    if os.getenv("CLICOLOR_FORCE", "0") != "0":
        # CLICOLOR_FORCE != 0, ANSI colors should be enabled no matter what.
        return True

    if os.getenv("NO_COLOR") is not None:
        return False
    return is_terminal(stream)
def request_login(self, remote_name, username=None):
        

        if not username:
            if self._interactive:
                self._out.write("Remote '%s' username: " % remote_name)
            username = self._get_env_username(remote_name)
            if not username:
                self._raise_if_non_interactive()
                username = self.get_username()

        if self._interactive:
            self._out.write('Please enter a password for "%s" account: ' % username)
        try:
            pwd = self._get_env_password(remote_name)
            if not pwd:
                self._raise_if_non_interactive()
                pwd = self.get_password()
        except ConanException:
            raise
        except Exception as e:
            raise ConanException('Cancelled pass %s' % e)
        return username, pwd
def _schedule_event(self, handler, when, data=None, name=None,
                        repeat=None):
        
        
        if not name:
            name = self.skill.name + handler.__name__
        unique_name = self._unique_name(name)
        if repeat:
            self.scheduled_repeats.append(name)  # store "friendly name"

        data = data or {}
        self.skill.add_event(unique_name, handler, once=not repeat)
        event_data = {}
        event_data['time'] = time.mktime(when.timetuple())
        event_data['event'] = unique_name
        event_data['repeat'] = repeat
        event_data['data'] = data
        self.skill.bus.emit(Message('mycroft.scheduler.schedule_event',
                                    data=event_data))
def _schedule_event(self, handler, when, data, name, repeat_interval=None):
        

        
        if isinstance(when, (int, float)) and when >= 0:
            when = datetime.now() + timedelta(seconds=when)
        if not name:
            name = self.skill.name + handler.__name__
        unique_name = self._create_unique_name(name)
        if repeat_interval:
            self.scheduled_repeats.append(name)  # store "friendly name"

        data = data or {}
        self.skill.add_event(unique_name, handler, once=not repeat_interval)
        event_data = {'time': time.mktime(when.timetuple()),
                      'event': unique_name,
                      'repeat': repeat_interval,
                      'data': data}
        self.skill.bus.emit(Message('mycroft.scheduler.schedule_event',
                                    data=event_data))
def schedule_event(self, event, sched_time, repeat=None, data=None, context=None):
        
        
        data = data or {}
        with self.event_lock:
            # get current list of scheduled times for event, [] if missing
            event_list = self.events.get(event, [])

            # Don't schedule if the event is repeating and already scheduled
            if repeat and event in self.events:
                LOG.debug('Repeating event {} is already scheduled, discarding'
                          .format(event))
            else:
                # add received event and time
                event_list.append((sched_time, repeat, data, context))
                self.events[event] = event_list
def schedule_event(self, event, sched_time, repeat=None,
                       data=None, context=None):
        
        
        data = data or {}
        with self.event_lock:
            # get current list of scheduled times for event, [] if missing
            event_list = self.events.get(event, [])

            # Don't schedule if the event is repeating and already scheduled
            if repeat and event in self.events:
                LOG.debug('Repeating event {} is already scheduled, discarding'
                          .format(event))
            else:
                # add received event and time
                event_list.append((sched_time, repeat, data, context))
                self.events[event] = event_list
def __init__(self, bus, schedule_file='schedule.json'):
        
            
        
        super(EventScheduler, self).__init__()
        data_dir = expanduser(Configuration.get()['data_dir'])

        self.events = {}
        self.event_lock = Lock()

        self.bus = bus
        self.isRunning = True
        self.schedule_file = join(data_dir, schedule_file)
        if self.schedule_file:
            self.load()

        self.bus.on('mycroft.scheduler.schedule_event',
                    self.schedule_event_handler)
        self.bus.on('mycroft.scheduler.remove_event',
                    self.remove_event_handler)
        self.bus.on('mycroft.scheduler.update_event',
                    self.update_event_handler)
        self.bus.on('mycroft.scheduler.get_event',
                    self.get_event_handler)
        self.start()
def mobilenet_v1_arg_scope(
    is_training=True,
    weight_decay=0.00004,
    stddev=0.09,
    regularize_depthwise=False,
    batch_norm_decay=0.9997,
    batch_norm_epsilon=0.001,
    batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS,
    normalizer_fn=slim.batch_norm):
  
  
  batch_norm_params = {
      'center': True,
      'scale': True,
      'decay': batch_norm_decay,
      'epsilon': batch_norm_epsilon,
      'updates_collections': batch_norm_updates_collections,
  }
  if is_training is not None:
    batch_norm_params['is_training'] = is_training

  # Set weight_decay for weights in Conv and DepthSepConv layers.
  weights_init = tf.truncated_normal_initializer(stddev=stddev)
  regularizer = tf.contrib.layers.l2_regularizer(weight_decay)
  if regularize_depthwise:
    depthwise_regularizer = regularizer
  else:
    depthwise_regularizer = None
  with slim.arg_scope([slim.conv2d, slim.separable_conv2d],
                      weights_initializer=weights_init,
                      activation_fn=tf.nn.relu6, normalizer_fn=normalizer_fn):
    with slim.arg_scope([slim.batch_norm], **batch_norm_params):
      with slim.arg_scope([slim.conv2d], weights_regularizer=regularizer):
        with slim.arg_scope([slim.separable_conv2d],
                            weights_regularizer=depthwise_regularizer) as sc:
          return sc
def create_tf_record(output_filename,
                     num_shards,
                     label_map_dict,
                     annotations_dir,
                     image_dir,
                     examples,
                     faces_only=True,
                     mask_type='png'):
  
  
  with contextlib2.ExitStack() as tf_record_close_stack:
    output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(
        tf_record_close_stack, output_filename, num_shards)
    for idx, example in enumerate(examples):
      if idx % 100 == 0:
        logging.info('On image %d of %d', idx, len(examples))
      xml_path = os.path.join(annotations_dir, 'xmls', example + '.xml')
      mask_path = os.path.join(annotations_dir, 'trimaps', example + '.png')

      if not os.path.exists(xml_path):
        logging.warning('Could not find %s, ignoring example.', xml_path)
        continue
      with tf.gfile.GFile(xml_path, 'r') as fid:
        xml_str = fid.read()
      xml = etree.fromstring(xml_str)
      data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']

      try:
        tf_example = dict_to_tf_example(
            data,
            mask_path,
            label_map_dict,
            image_dir,
            faces_only=faces_only,
            mask_type=mask_type)
        if tf_example:
          shard_idx = idx % num_shards
          output_tfrecords[shard_idx].write(tf_example.SerializeToString())
      except ValueError:
        logging.warning('Invalid example: %s, ignoring.', xml_path)
def predict(self, df: pd.DataFrame = None, vectorized: bool = True) -> pd.DataFrame:
        
        
        if self.history is None:
            raise Exception('Model has not been fit.')

        if df is None:
            df = self.history.copy()
        else:
            if df.shape[0] == 0:
                raise ValueError('Dataframe has no rows.')
            df = self.setup_dataframe(df.copy())

        df['trend'] = self.predict_trend(df)
        seasonal_components = self.predict_seasonal_components(df)
        if self.uncertainty_samples:
            intervals = self.predict_uncertainty(df, vectorized)
        else:
            intervals = None

        # Drop columns except ds, cap, floor, and trend
        cols = ['ds', 'trend']
        if 'cap' in df:
            cols.append('cap')
        if self.logistic_floor:
            cols.append('floor')
        # Add in forecast components
        df2 = pd.concat((df[cols], intervals, seasonal_components), axis=1)
        df2['yhat'] = (
                df2['trend'] * (1 + df2['multiplicative_terms'])
                + df2['additive_terms']
        )
        return df2
def sample_model(self, df, seasonal_features, iteration, s_a, s_m) -> Dict[str, np.ndarray]:
        
        
        trend = self.sample_predictive_trend(df, iteration)

        beta = self.params['beta'][iteration]
        Xb_a = np.matmul(seasonal_features.values,
                         beta * s_a.values) * self.y_scale
        Xb_m = np.matmul(seasonal_features.values, beta * s_m.values)

        sigma = self.params['sigma_obs'][iteration]
        noise = np.random.normal(0, sigma, df.shape[0]) * self.y_scale

        return {
            'yhat': trend * (1 + Xb_m) + Xb_a + noise,
            'trend': trend
        }
def checksum(self, name: str) -> str:
        
        
        
        url: str = "/".join(
            (
                self._host,
                self._repository,
                self.RELEASE_PATH,
                self._release,
                self.CHECKSUM_INDEX,
            )
        )
        response: httpx.Response = httpx.get(url)
        response.raise_for_status()
        index: Dict = response.json()
        if name not in index:
            raise ValueError(f"No checksum for model {name}")
        return index[name]
def __init__(
            self,
            host: str,
            repository: str,
            release: str) -> None:
         
        
        self._host: str = host
        self._repository: str = repository
        self._release: str = release
def run(
        self,
        flow: Union[str, PathLike] = None,
        *,
        data: Union[str, PathLike] = None,
        run: Union[str, Run] = None,
        column_mapping: dict = None,
        variant: str = None,
        connections: dict = None,
        environment_variables: dict = None,
        name: str = None,
        display_name: str = None,
        tags: Dict[str, str] = None,
        resume_from: Union[str, Run] = None,
        code: Union[str, PathLike] = None,
        init: Optional[dict] = None,
        **kwargs,
    ) -> Run:
        
        
        if resume_from:
            unsupported = {
                k: v
                for k, v in {
                    "flow": flow,
                    "data": data,
                    "run": run,
                    "column_mapping": column_mapping,
                    "variant": variant,
                    "connections": connections,
                    "environment_variables": environment_variables,
                }.items()
                if v
            }
            if any(unsupported):
                raise ValueError(
                    f"'resume_from' is not supported to be used with the with following parameters: {unsupported}. "
                )
            resume_from = resume_from.name if isinstance(resume_from, Run) else resume_from
            return self.runs._create_by_resume_from(
                resume_from=resume_from, name=name, display_name=display_name, tags=tags, **kwargs
            )
        if not flow:
            raise ValueError("'flow' is required to create a run.")
        if not os.path.exists(flow) and not is_python_flex_flow_entry(entry=flow):
            # check if it's eager flow's entry
            raise UserErrorException(f"Flow path {flow} does not exist and it's not a valid entry point.")
        if data and not os.path.exists(data):
            raise FileNotFoundError(f"data path {data} does not exist")
        if not run and not data:
            raise ValueError("at least one of data or run must be provided")
        if code and not os.path.exists(code):
            raise FileNotFoundError(f"code path {code} does not exist")
        code = Path(code) if code else Path(os.getcwd())
        with generate_yaml_entry(entry=flow, code=code) as flow:
            # load flow object for validation and early failure
            flow_obj = load_flow(source=flow)
            # validate param conflicts
            if isinstance(flow_obj, FlexFlow):
                if variant or connections:
                    logger.warning("variant and connections are not supported for eager flow, will be ignored")
                    variant, connections = None, None
            run = Run(
                name=name,
                display_name=display_name,
                tags=tags,
                data=data,
                column_mapping=column_mapping,
                run=run,
                variant=variant,
                flow=Path(flow),
                connections=connections,
                environment_variables=environment_variables,
                config=Configuration(overrides=self._config),
                init=init,
            )
            return self.runs.create_or_update(run=run, **kwargs)
def test(
        self,
        flow: Union[str, PathLike],
        *,
        inputs: Union[dict, PathLike] = None,
        variant: str = None,
        node: str = None,
        environment_variables: dict = None,
        init: Optional[dict] = None,
    ) -> dict:
        
        
        # Load the inputs for the flow test from sample file.
        if isinstance(inputs, (str, Path)):
            if Path(inputs).suffix not in [".json", ".jsonl"]:
                raise UserErrorException("Only support jsonl or json file as input.")
            if not Path(inputs).exists():
                raise UserErrorException(f"Cannot find inputs file {inputs}.")
            if Path(inputs).suffix == ".json":
                with open(inputs, "r") as f:
                    inputs = json.load(f)
            else:
                from promptflow._utils.load_data import load_data

                inputs = load_data(local_path=inputs)[0]
        return self.flows.test(
            flow=flow, inputs=inputs, variant=variant, environment_variables=environment_variables, node=node, init=init
        )
def initialize(ep_size=1, mpu=None, num_ep_list=None):
    
    

    

    if num_ep_list is None:
        num_ep_list = [ep_size]

    assert max(num_ep_list) >= ep_size, f"ep_size={ep_size} is larger than the largest num_ep_list={max(num_ep_list)}, you should reduce expert parallel size"

    num_ep_list = list(set(num_ep_list))  # remove duplicates
    num_ep_list.sort()  # sort in ascending order
    for num_ep in num_ep_list:
        assert num_ep > 0, 'num_ep must be positive'
        assert num_ep % ep_size == 0 or ep_size % num_ep == 0, 'num_ep must be divisible/divided by ep_size'

    if mpu is not None:
        log_dist(message="initializing deepspeed groups using mpu", ranks=[0])
        initialize_model_and_expert_parallel(ep_size, mpu, num_ep_list)
    else:
        log_dist(message="initializing deepspeed groups", ranks=[0])
        initialize_model_parallel(1)
        initialize_expert_parallel(ep_size, num_ep_list)
def _create_expert_data_and_model_parallel(expert_parallel_size_, mpu):
    
        
    
    assert torch.distributed.is_initialized(), "torch distributed is not initialized"
    model_parallel_size_ = mpu.get_model_parallel_world_size()

    world_size = torch.distributed.get_world_size()
    rank = torch.distributed.get_rank()
    dp_world_size = mpu.get_data_parallel_world_size()
    dp_rank = mpu.get_data_parallel_rank()

    log_dist(
        f"Creating deepspeed groups with model parallel size {model_parallel_size_}, expert parallel size {expert_parallel_size_}, world size {world_size}, dp world size {dp_world_size}",
        [0])

    global _EXPERT_PARALLEL_GROUP, _EXPERT_DATA_PARALLEL_GROUP

    # Get world size and rank. Ensure some consistencies.
    _DATA_PARALLEL_GROUP = mpu.get_data_parallel_group()
    _MODEL_PARALLEL_GROUP = mpu.get_model_parallel_group()

    expert_parallel_size_ = min(expert_parallel_size_, dp_world_size)
    _ensure_divisibility(world_size, expert_parallel_size_)

    group_name = f"ep_size_{expert_parallel_size_}"

    # Only create groups if they don't already exist
    # Need to check conditions outside the group creation loop because of the way torch.dist group creation works
    if group_name not in _EXPERT_DATA_PARALLEL_GROUP and group_name not in _EXPERT_PARALLEL_GROUP:
        expert_parallel_groups, expert_data_parallel_groups = _get_expert_parallel_ranks(
            world_size, model_parallel_size_, expert_parallel_size_)
        for ranks in expert_parallel_groups:
            group = torch.distributed.new_group(ranks)
            if rank in list(ranks):
                _EXPERT_PARALLEL_GROUP[group_name] = group

        for ranks in expert_data_parallel_groups:
            group = torch.distributed.new_group(ranks)
            if rank in list(ranks):
                _EXPERT_DATA_PARALLEL_GROUP[group_name] = group
def _clone_world_group():
    
    
    assert dist.is_initialized(), "dist is not initialized"
    global _WORLD_GROUP
    if _WORLD_GROUP is None:
        # If not cloned already, clone the world group
        _WORLD_GROUP = dist.new_group(ranks=range(dist.get_world_size()))
    return _WORLD_GROUP
def _create_expert_and_data_parallel(expert_parallel_size_, use_data_before_expert_parallel_=False):
    
        
    
    assert dist.is_initialized()

    log_dist(f'Creating expert and data parallel groups with size {expert_parallel_size_}', ranks=[0])
    world_size = dist.get_world_size()
    rank = dist.get_rank()

    _ensure_divisibility(world_size, expert_parallel_size_)

    group_name = f"ep_size_{expert_parallel_size_}"

    # Build the expert data parallel groups.
    global _EXPERT_DATA_PARALLEL_GROUP

    ep_stride = world_size // expert_parallel_size_

    # Only create group if it does not already exist
    if group_name not in _EXPERT_DATA_PARALLEL_GROUP:
        for i in range(expert_parallel_size_):
            if use_data_before_expert_parallel_:
                ranks = range(i * ep_stride, (i + 1) * ep_stride)
            else:
                ranks = range(i, world_size, expert_parallel_size_)
            group = dist.new_group(ranks)
            log_dist(f'Creating expert data parallel process group named {group_name} with ranks: {list(ranks)}', [0])
            if use_data_before_expert_parallel_:
                if i == (rank // ep_stride):
                    _EXPERT_DATA_PARALLEL_GROUP[group_name] = group
            else:
                if i == (rank % expert_parallel_size_):
                    _EXPERT_DATA_PARALLEL_GROUP[group_name] = group

    # Build the expert parallel groups.
    global _EXPERT_PARALLEL_GROUP

    # Only create group if it does not already exist
    if group_name not in _EXPERT_PARALLEL_GROUP:
        if use_data_before_expert_parallel_:
            for i in range(ep_stride):
                ranks = range(i, world_size, ep_stride)
                group = dist.new_group(ranks)
                log_dist(f'creating expert parallel process group named {group_name} with ranks: {list(ranks)}', [0])
                if i == (rank % ep_stride):
                    _EXPERT_PARALLEL_GROUP[group_name] = group
        else:
            for i in range(world_size // expert_parallel_size_):
                ranks = range(i * expert_parallel_size_, (i + 1) * expert_parallel_size_)
                group = dist.new_group(ranks)
                log_dist(f'creating expert parallel process group named {group_name} with ranks: {list(ranks)}', [0])
                if i == (rank // expert_parallel_size_):
                    _EXPERT_PARALLEL_GROUP[group_name] = group
def _get_expert_parallel_ranks(world_size,
                               tensor_parallel_size_,
                               expert_parallel_size_,
                               pipeline_parallel_size_=1,
                               use_data_before_expert_parallel_=False):
    
    
    _ensure_divisibility(world_size, tensor_parallel_size_ * pipeline_parallel_size_)
    dp_world_size = world_size // (tensor_parallel_size_ * pipeline_parallel_size_)
    _ensure_divisibility(dp_world_size, expert_parallel_size_)

    # Generate data parallel groups
    data_parallel_groups = []
    dp_group_size = tensor_parallel_size_
    pp_stride = world_size // pipeline_parallel_size_

    if use_data_before_expert_parallel_:
        dp_stride = world_size // expert_parallel_size_ // tensor_parallel_size_ // pipeline_parallel_size_
        for pp_stage_start in range(0, world_size, pp_stride):
            pp_stage_next = pp_stage_start + pp_stride
            for i in range(dp_group_size):
                data_parallel_groups.append(list())
                for ds in range(dp_stride):
                    # [0, 4, 8, 12, 16, 20, 24, 28, 2, 6, 10, 14, 18, 22, 26, 30]
                    # [1, 5, 9, 13, 17, 21, 25, 29, 3, 7, 11, 15, 19, 23, 27, 31]
                    data_parallel_groups[-1].extend(
                        list(
                            range(pp_stage_start + i + ds * tensor_parallel_size_, pp_stage_next,
                                  dp_stride * tensor_parallel_size_)))
    else:
        for pp_stage_start in range(0, world_size, pp_stride):
            pp_stage_next = pp_stage_start + pp_stride
            for i in range(dp_group_size):
                data_parallel_groups.append(list(range(pp_stage_start + i, pp_stage_next, dp_group_size)))

    expert_parallel_groups = []
    expert_data_parallel_groups = []
    for dp_ranks in data_parallel_groups:
        # partition of expert parallel groups, e.g. [0,2,4,6], [8,10,12,14]
        part_ep_groups = []
        for i in range(0, dp_world_size, expert_parallel_size_):
            part_ep_groups.append(dp_ranks[i:i + expert_parallel_size_])
        expert_parallel_groups.extend(part_ep_groups)

        # zip part_ep_groups get expert data parallel ranks, e.g [0,8],[2,10],[4,12],[6,14]
        for expert_dp_ranks in zip(*part_ep_groups):
            expert_data_parallel_groups.append(list(expert_dp_ranks))

    return expert_parallel_groups, expert_data_parallel_groups
def icon(self):
        
        return SENSOR_TYPES[self._sensor_type][ATTR_ICON]
def run(self, stat_name, criticality, commands, repeat, mustache_dict=None):
        
        
        if (self.get(stat_name) == criticality and not repeat) or \
           not self.start_timer.finished():
            # Action already executed => Exit
            return False

        logger.debug("{} action {} for {} ({}) with stats {}".format(
            "Repeat" if repeat else "Run",
            commands, stat_name, criticality, mustache_dict))

        # Run all actions in background
        for cmd in commands:
            # Replace {{arg}} by the dict one (Thk to {Mustache})
            if chevron_tag:
                cmd_full = chevron.render(cmd, mustache_dict)
            else:
                cmd_full = cmd
            # Execute the action
            logger.info("Action triggered for {} ({}): {}".format(stat_name,
                                                                  criticality,
                                                                  cmd_full))
            try:
                ret = secure_popen(cmd_full)
            except OSError as e:
                logger.error("Action error for {} ({}): {}".format(stat_name,
                                                                   criticality,
                                                                   e))
            else:
                logger.debug("Action result for {} ({}): {}".format(stat_name,
                                                                    criticality,
                                                                    ret))

        self.set(stat_name, criticality)

        return True
def get_number_of_zones(self) -> int:
        
        
        return len(self.device.color_zones) if self.device.color_zones else 1
def test_unsupported_nowait_raises_error(self):
        
        
        
        with self.assertRaisesMessage(DatabaseError, 'NOWAIT is not supported on this database backend.'):
            with transaction.atomic():
                Person.objects.select_for_update(nowait=True).get()
def get_style(self, name: Union[str, Style]) -> Optional[Style]:
        

        
        if isinstance(name, Style):
            return name
        try:
            return self._styles.get(name, None) or Style.parse(name)
        except errors.StyleSyntaxError:
            return None
def render_str(self, text: str, style: Union[str, Style] = "") -> "Text":
        

        
        if self._markup:
            return markup.render(text, style=style)

        from .text import Text

        return Text(text, style=style)
def _current_style(self) -> Style:
        
        return getattr(self._thread_locals, "current_style", Style())
def render_lines(
        self,
        renderable: RenderableType,
        options: Optional[ConsoleOptions] = None,
        *,
        style: Optional[Style] = None,
        pad: bool = True,
        new_lines: bool = False,
    ) -> List[List[Segment]]:
        
        
        render_options = options or self.options
        _rendered = self.render(renderable, render_options)
        if style is not None:
            _rendered = Segment.apply_style(_rendered, style)
        lines = list(
            islice(
                Segment.split_and_crop_lines(
                    _rendered,
                    render_options.max_width,
                    include_new_lines=new_lines,
                    pad=pad,
                ),
                None,
                render_options.height,
            )
        )
        if render_options.height is not None:
            extra_lines = render_options.height - len(lines)
            if extra_lines > 0:
                pad_line = [
                    [Segment(" " * render_options.max_width, style), Segment("\n")]
                    if new_lines
                    else [Segment(" " * render_options.max_width, style)]
                ]
                lines.extend(pad_line * extra_lines)

        return lines
def input(
        self,
        prompt: TextType = "",
        *,
        markup: bool = True,
        emoji: bool = True,
        password: bool = False,
        stream: TextIO = None,
    ) -> str:
        
        
        if prompt:
            self.print(prompt, markup=markup, emoji=emoji, end="")
        result = (
            getpass("", stream=stream)
            if password
            else (stream.readline() if stream else input())
        )
        return result
def export_text(self, *, clear: bool = True, styles: bool = False) -> str:
        

        
        assert (
            self.record
        ), "To export console contents set record=True in the constructor or instance"

        with self._record_buffer_lock:
            if styles:
                text = "".join(
                    (style.render(text) if style else text)
                    for text, style, _ in self._record_buffer
                )
            else:
                text = "".join(
                    segment.text
                    for segment in self._record_buffer
                    if not segment.is_control
                )
            if clear:
                del self._record_buffer[:]
        return text
def print(
        self,
        *objects: Any,
        sep=" ",
        end="\n",
        style: Union[str, Style] = None,
        justify: JustifyMethod = None,
        overflow: OverflowMethod = None,
        no_wrap: bool = None,
        emoji: bool = None,
        markup: bool = None,
        highlight: bool = None,
        width: int = None,
        crop: bool = True,
        soft_wrap: bool = None,
    ) -> None:
        
        
        if not objects:
            self.line()
            return

        if soft_wrap is None:
            soft_wrap = self.soft_wrap
        if soft_wrap:
            if no_wrap is None:
                no_wrap = True
            if overflow is None:
                overflow = "ignore"
            crop = False

        with self:
            renderables = self._collect_renderables(
                objects,
                sep,
                end,
                justify=justify,
                emoji=emoji,
                markup=markup,
                highlight=highlight,
            )
            for hook in self._render_hooks:
                renderables = hook.process_renderables(renderables)
            render_options = self.options.update(
                justify=justify,
                overflow=overflow,
                width=min(width, self.width) if width else None,
                no_wrap=no_wrap,
            )
            new_segments: List[Segment] = []
            extend = new_segments.extend
            render = self.render
            if style is None:
                for renderable in renderables:
                    extend(render(renderable, render_options))
            else:
                for renderable in renderables:
                    extend(
                        Segment.apply_style(
                            render(renderable, render_options), self.get_style(style)
                        )
                    )
            if crop:
                buffer_extend = self._buffer.extend
                for line in Segment.split_and_crop_lines(
                    new_segments, self.width, pad=False
                ):
                    buffer_extend(line)
            else:
                self._buffer.extend(new_segments)
def rule(
        self,
        title: TextType = "",
        *,
        characters: str = "─",
        style: Union[str, Style] = "rule.line",
        align: str = "center",
    ) -> None:
        
        
        from .rule import Rule

        rule = Rule(title=title, characters=characters, style=style, align=align)
        self.print(rule)
def log(
        self,
        *objects: Any,
        sep=" ",
        end="\n",
        style: Union[str, Style] = None,
        justify: JustifyMethod = None,
        emoji: bool = None,
        markup: bool = None,
        highlight: bool = None,
        log_locals: bool = False,
        _stack_offset=1,
    ) -> None:
        
        
        if not objects:
            self.line()
            return
        with self:
            renderables = self._collect_renderables(
                objects,
                sep,
                end,
                justify=justify,
                emoji=emoji,
                markup=markup,
                highlight=highlight,
            )
            if style is not None:
                renderables = [Styled(renderable, style) for renderable in renderables]

            caller = inspect.stack()[_stack_offset]
            link_path = (
                None
                if caller.filename.startswith("<")
                else os.path.abspath(caller.filename)
            )
            path = caller.filename.rpartition(os.sep)[-1]
            line_no = caller.lineno
            if log_locals:
                locals_map = {
                    key: value
                    for key, value in caller.frame.f_locals.items()
                    if not key.startswith("__")
                }
                renderables.append(render_scope(locals_map, title="[i]locals"))

            renderables = [
                self._log_render(
                    self,
                    renderables,
                    log_time=self.get_datetime(),
                    path=path,
                    line_no=line_no,
                    link_path=link_path,
                )
            ]
            for hook in self._render_hooks:
                renderables = hook.process_renderables(renderables)
            new_segments: List[Segment] = []
            extend = new_segments.extend
            render = self.render
            render_options = self.options
            for renderable in renderables:
                extend(render(renderable, render_options))
            buffer_extend = self._buffer.extend
            for line in Segment.split_and_crop_lines(
                new_segments, self.width, pad=False
            ):
                buffer_extend(line)
def print_json(
        self,
        json: Optional[str] = None,
        *,
        data: Any = None,
        indent: int = 2,
        highlight: bool = True,
    ) -> None:
        
        
        from rich.json import JSON

        if json is None:
            json_renderable = JSON.from_data(data, indent=indent, highlight=highlight)
        else:
            if not isinstance(json, str):
                raise TypeError(
                    f"json must be str. Did you mean print_json(data={json!r}) ?"
                )
            json_renderable = JSON(json, indent=indent, highlight=highlight)
        self.print(json_renderable)
def print_exception(
        self,
        *,
        width: Optional[int] = 100,
        extra_lines: int = 3,
        theme: Optional[str] = None,
        word_wrap: bool = False,
        show_locals: bool = False,
    ) -> None:
        
        
        from .traceback import Traceback

        traceback = Traceback(
            width=width,
            extra_lines=extra_lines,
            theme=theme,
            word_wrap=word_wrap,
            show_locals=show_locals,
        )
        self.print(traceback)
def pager(
        self, pager: Pager = None, styles: bool = False, links: bool = False
    ) -> PagerContext:
        
        
        return PagerContext(self, pager=pager, styles=styles, links=links)
def status(
        self,
        status: RenderableType,
        *,
        spinner: str = "dots",
        spinner_style: str = "status.spinner",
        speed: float = 1.0,
        refresh_per_second: float = 12.5,
    ) -> "Status":
        
        
        from .status import Status

        status_renderable = Status(
            status,
            console=self,
            spinner=spinner,
            spinner_style=spinner_style,
            speed=speed,
            refresh_per_second=refresh_per_second,
        )
        return status_renderable
def __init__(
      self,
      input_specs,
      model_id='yolov7',
      use_sync_bn=False,
      norm_momentum=0.99,
      norm_epsilon=0.001,
      activation='swish',
      use_separable_conv=False,
      kernel_initializer='VarianceScaling',
      kernel_regularizer=None,
      bias_initializer='zeros',
      bias_regularizer=None,
      **kwargs,
  ):
    
    

    self._input_specs = input_specs
    self._model_id = model_id
    self._use_sync_bn = use_sync_bn
    self._norm_momentum = norm_momentum
    self._norm_epsilon = norm_epsilon
    self._activation = activation
    self._use_separable_conv = use_separable_conv

    self._kernel_initializer = initializer_ops.pytorch_kernel_initializer(
        kernel_initializer
    )
    self._kernel_regularizer = kernel_regularizer
    self._bias_initializer = bias_initializer
    self._bias_regularizer = bias_regularizer

    inputs = self._generate_inputs(input_specs)
    outputs = []
    endpoints = {}
    level = int(min(inputs.keys()))
    block_specs = DECODERS[model_id.lower()]

    for spec in block_specs:
      block_kwargs = dict(zip(_BLOCK_SPEC_SCHEMAS[spec[0]], spec))
      block_fn_str = block_kwargs.pop('block_fn')
      from_index = block_kwargs.pop('from')
      is_output = block_kwargs.pop('is_output')

      x = self._group_layer_inputs(from_index, inputs, outputs)

      if block_fn_str in ['convbn', 'sppcspc', 'repconv']:
        block_kwargs.update({
            'use_sync_bn': self._use_sync_bn,
            'norm_momentum': self._norm_momentum,
            'norm_epsilon': self._norm_epsilon,
            'activation': self._activation,
            'use_separable_conv': self._use_separable_conv,
            'kernel_initializer': self._kernel_initializer,
            'kernel_regularizer': self._kernel_regularizer,
            'bias_initializer': self._bias_initializer,
            'bias_regularizer': self._bias_regularizer,
        })
      block_fn = _BLOCK_FNS[block_fn_str](**block_kwargs)

      x = block_fn(x)
      outputs.append(x)
      if is_output:
        endpoints[str(level)] = x
        level += 1
    self._output_specs = {k: v.get_shape() for k, v in endpoints.items()}
    super().__init__(inputs=inputs, outputs=endpoints, **kwargs)
def on_not_implemented_error(
        self,
        serializer: ResponseSerializer,
        context: RequestContext,
        exception: NotImplementedError,
    ) -> HttpResponse:
        
        
        
        operation = context.operation

        action_name = operation.name
        service_name = operation.service_model.service_name
        exception_message: str | None = exception.args[0] if exception.args else None
        message = exception_message or get_coverage_link_for_service(service_name, action_name)
        LOG.info(message)
        error = CommonServiceException("InternalFailure", message, status_code=501)
        # record event
        analytics.log.event(
            "services_notimplemented", payload={"s": service_name, "a": action_name}
        )
        context.service_exception = error

        return serializer.serialize_error_to_response(
            error, operation, context.request.headers, context.request_id
        )
def calc_grid_position(self, index, min_position):
        
        
        
        pos = index * self.resolution + min_position
        return pos
def test_scikit_vs_scipy():
    
    
    n, p, k = 10, 5, 3
    rnd = np.random.RandomState(0)

    # Not using a lil_matrix here, just to check that non sparse
    # matrices are well handled
    connectivity = np.ones((n, n))
    for linkage in _TREE_BUILDERS.keys():
        for i in range(5):
            X = .1 * rnd.normal(size=(n, p))
            X -= 4 * np.arange(n)[:, np.newaxis]
            X -= X.mean(axis=1)[:, np.newaxis]

            out = hierarchy.linkage(X, method=linkage)

            children_ = out[:, :2].astype(np.int)
            children, _, n_leaves, _ = _TREE_BUILDERS[linkage](X, connectivity)

            cut = _hc_cut(k, children, n_leaves)
            cut_ = _hc_cut(k, children_, n_leaves)
            assess_same_labelling(cut, cut_)

    # Test error management in _hc_cut
    assert_raises(ValueError, _hc_cut, n_leaves + 1, children, n_leaves)
def forecast(self) -> list[Forecast]:
        
        return self._forecast(self._mode)
def deploy(config,
           model_fn,
           args=None,
           kwargs=None,
           optimizer=None,
           summarize_gradients=False):
  

  
  # Gather initial summaries.
  summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))

  # Create Clones.
  clones = create_clones(config, model_fn, args, kwargs)
  first_clone = clones[0]

  # Gather update_ops from the first clone. These contain, for example,
  # the updates for the batch_norm variables created by model_fn.
  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone.scope)

  train_op = None
  total_loss = None
  with tf.device(config.optimizer_device()):
    if optimizer:
      # Place the global step on the device storing the variables.
      with tf.device(config.variables_device()):
        global_step = slim.get_or_create_global_step()

      # Compute the gradients for the clones.
      total_loss, clones_gradients = optimize_clones(clones, optimizer)

      if clones_gradients:
        if summarize_gradients:
          # Add summaries to the gradients.
          summaries |= set(_add_gradients_summaries(clones_gradients))

        # Create gradient updates.
        grad_updates = optimizer.apply_gradients(clones_gradients,
                                                 global_step=global_step)
        update_ops.append(grad_updates)

        update_op = tf.group(*update_ops)
        with tf.control_dependencies([update_op]):
          train_op = tf.identity(total_loss, name='train_op')
    else:
      clones_losses = []
      regularization_losses = tf.get_collection(
          tf.GraphKeys.REGULARIZATION_LOSSES)
      for clone in clones:
        with tf.name_scope(clone.scope):
          clone_loss = _gather_clone_loss(clone, len(clones),
                                          regularization_losses)
          if clone_loss is not None:
            clones_losses.append(clone_loss)
          # Only use regularization_losses for the first clone
          regularization_losses = None
      if clones_losses:
        total_loss = tf.add_n(clones_losses, name='total_loss')

    # Add the summaries from the first clone. These contain the summaries
    # created by model_fn and either optimize_clones() or _gather_clone_loss().
    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,
                                       first_clone.scope))

    if total_loss is not None:
      # Add total_loss to summary.
      summaries.add(tf.summary.scalar('total_loss', total_loss))

    if summaries:
      # Merge all summaries together.
      summary_op = tf.summary.merge(list(summaries), name='summary_op')
    else:
      summary_op = None

  return DeployedModel(train_op, summary_op, total_loss, clones)
def _optimizer_step(
        self,
        batch_idx: int,
        train_step_and_backward_closure: Callable[[], Optional[Tensor]],
    ) -> None:
        
        
        trainer = self.trainer

        # wraps into LightningOptimizer only for running step
        optimizer = trainer.strategy._lightning_optimizers[0]

        # if `strategy.handles_gradient_accumulation`, this method will be called to route into the strategy, but we
        # need to check again if `should_accumulate` before increasing the counters
        should_accumulate = trainer.fit_loop._should_accumulate()
        if not should_accumulate:
            self.optim_progress.optimizer.step.increment_ready()

        # model hook
        call._call_lightning_module_hook(
            trainer,
            "optimizer_step",
            trainer.current_epoch,
            batch_idx,
            optimizer,
            train_step_and_backward_closure,
        )

        if not should_accumulate:
            self.optim_progress.optimizer.step.increment_completed()
def run(self, optimizer: Optimizer, batch_idx: int, kwargs: OrderedDict) -> _OUTPUTS_TYPE:
        

        
        closure = self._make_closure(kwargs, optimizer, batch_idx)

        if (
            # when the strategy handles accumulation, we want to always call the optimizer step
            not self.trainer.strategy.handles_gradient_accumulation
            and self.trainer.fit_loop._should_accumulate()
        ):
            # For gradient accumulation

            # -------------------
            # calculate loss (train step + train step end)
            # -------------------
            # automatic_optimization=True: perform ddp sync only when performing optimizer_step
            with _block_parallel_sync_behavior(self.trainer.strategy, block=True):
                closure()

        # ------------------------------
        # BACKWARD PASS
        # ------------------------------
        # gradient update with accumulated gradients
        else:
            self._optimizer_step(batch_idx, closure)

        result = closure.consume_result()
        if result.loss is None:
            return {}
        return result.asdict()
def _invoke_action(self, action_db, runnertype_db, params, context=None,
                       additional_contexts=None):
        
        
        
        action_ref = action_db.ref
        runnertype_db = action_utils.get_runnertype_by_name(action_db.runner_type['name'])

        liveaction_db = LiveActionDB(action=action_ref, context=context, parameters=params)

        try:
            liveaction_db.parameters = self.get_resolved_parameters(
                runnertype_db=runnertype_db,
                action_db=action_db,
                params=liveaction_db.parameters,
                context=liveaction_db.context,
                additional_contexts=additional_contexts)
        except param_exc.ParamException as e:
            # We still need to create a request, so liveaction_db is assigned an ID
            liveaction_db, execution_db = action_service.create_request(liveaction_db)

            # By this point the execution is already in the DB therefore need to mark it failed.
            _, e, tb = sys.exc_info()
            action_service.update_status(
                liveaction=liveaction_db,
                new_status=action_constants.LIVEACTION_STATUS_FAILED,
                result={'error': str(e), 'traceback': ''.join(traceback.format_tb(tb, 20))})

            # Might be a good idea to return the actual ActionExecution rather than bubble up
            # the exception.
            raise validation_exc.ValueValidationException(str(e))

        liveaction_db, execution_db = action_service.request(liveaction_db)

        return execution_db
def detach_skill(self, skill_id):
        
        
        with self.lock:
            skill_parsers = [
                p.name for p in self.engine.intent_parsers if
                p.name.startswith(skill_id)
            ]
            self.engine.drop_intent_parser(skill_parsers)
            self._detach_skill_keywords(skill_id)
            self._detach_skill_regexes(skill_id)
def register_vocab(self, start_concept, end_concept, alias_of, regex_str):
        
        
        self.register_vocabulary(start_concept, end_concept,
                                 alias_of, regex_str)
def get_metadata(stream, extract_cover=True):
      
    stream.seek(0)
    reader = OCFZipReader(stream)
    mi = reader.opf.to_book_metadata()
    if extract_cover:
        try:
            cdata = get_cover(reader.opf, reader.opf_path, stream, reader=reader)
            if cdata is not None:
                mi.cover_data = ('jpg', cdata)
        except:
            import traceback
            traceback.print_exc()
    mi.timestamp = None
    return mi
def __init__(self,
                 resources_to_build,
                 build_dir,
                 base_dir,
                 cache_dir,
                 cached=False,
                 is_building_specific_resource=False,
                 manifest_path_override=None,
                 container_manager=None,
                 parallel=False,
                 mode=None):
        
        
        
        self._resources_to_build = resources_to_build
        self._build_dir = build_dir
        self._base_dir = base_dir
        self._cache_dir = cache_dir
        self._cached = cached
        self._manifest_path_override = manifest_path_override
        self._is_building_specific_resource = is_building_specific_resource

        self._container_manager = container_manager
        self._parallel = parallel
        self._mode = mode

        self._deprecated_runtimes = {"nodejs4.3", "nodejs6.10", "nodejs8.10", "dotnetcore2.0"}
        self._colored = Colored()
def _get_build_options(
        function_name: str,
        language: str,
        handler: Optional[str],
        dependency_manager: Optional[str] = None,
        metadata: Optional[dict] = None,
    ) -> Optional[Dict]:
        
        
        

        if metadata and dependency_manager and dependency_manager == "npm-esbuild":
            build_props = metadata.get(BUILD_PROPERTIES, {})
            # Esbuild takes an array of entry points from which to start bundling
            # as a required argument. This corresponds to the lambda function handler.
            if handler and not build_props.get("EntryPoints"):
                entry_points = [handler.split(".")[0]]
                build_props["entry_points"] = entry_points
            return ResourceMetadataNormalizer.normalize_build_properties(build_props)

        _build_options: Dict = {
            "go": {"artifact_executable_name": handler},
            "provided": {"build_logical_id": function_name},
        }
        return _build_options.get(language, None)
def _make_env_vars(
        resource: Union[Function, LayerVersion], file_env_vars: Dict, inline_env_vars: Optional[Dict]
    ) -> Dict:
        

        

        name = resource.name
        result = {}

        # validate and raise OverridesNotWellDefinedError
        for env_var in list((file_env_vars or {}).values()) + list((inline_env_vars or {}).values()):
            if not isinstance(env_var, dict):
                reason = "Environment variables {} in incorrect format".format(env_var)
                LOG.debug(reason)
                raise OverridesNotWellDefinedError(reason)

        if file_env_vars:
            parameter_result = file_env_vars.get("Parameters", {})
            result.update(parameter_result)

        if inline_env_vars:
            inline_parameter_result = inline_env_vars.get("Parameters", {})
            result.update(inline_parameter_result)

        if file_env_vars:
            specific_result = file_env_vars.get(name, {})
            result.update(specific_result)

        if inline_env_vars:
            inline_specific_result = inline_env_vars.get(name, {})
            result.update(inline_specific_result)

        return result
def _formatparam(param, value=None, quote=True):
    
    
    if value is not None and len(value) > 0:
        # A tuple is used for RFC 2231 encoded parameter values where items
        # are (charset, language, value).  charset is a string, not a Charset
        # instance.  RFC 2231 encoded values are never quoted, per RFC.
        if isinstance(value, tuple):
            # Encode as per RFC 2231
            param += '*'
            value = utils.encode_rfc2231(value[2], value[0], value[1])
            return '%s=%s' % (param, value)
        else:
            try:
                value.encode('ascii')
            except UnicodeEncodeError:
                param += '*'
                value = utils.encode_rfc2231(value, 'utf-8', '')
                return '%s=%s' % (param, value)
        # BAW: Please check this.  I think that if quote is set it should
        # force quoting even if not necessary.
        if quote or tspecials.search(value):
            return '%s="%s"' % (param, utils.quote(value))
        else:
            return '%s=%s' % (param, value)
    else:
        return param
def as_string(self, unixfrom=False, maxheaderlen=0, policy=None):
        
        
        from email.generator import Generator
        policy = self.policy if policy is None else policy
        fp = StringIO()
        g = Generator(fp,
                      mangle_from_=False,
                      maxheaderlen=maxheaderlen,
                      policy=policy)
        g.flatten(self, unixfrom=unixfrom)
        return fp.getvalue()
def __init__(self, start, goal, obstacle_list, rand_area,
                 max_iter=200,
                 connect_circle_dist=50.0,
                 robot_radius=0.0
                 ):
        
        

        
        self.start = self.Node(start[0], start[1], start[2])
        self.end = self.Node(goal[0], goal[1], goal[2])
        self.min_rand = rand_area[0]
        self.max_rand = rand_area[1]
        self.max_iter = max_iter
        self.obstacle_list = obstacle_list
        self.connect_circle_dist = connect_circle_dist
        self.robot_radius = robot_radius

        self.curvature = 1.0
        self.goal_yaw_th = np.deg2rad(1.0)
        self.goal_xy_th = 0.5
def planning(self, animation=True, search_until_max_iter=True):
        
        
        

        self.node_list = [self.start]
        for i in range(self.max_iter):
            print("Iter:", i, ", number of nodes:", len(self.node_list))
            rnd = self.get_random_node()
            nearest_ind = self.get_nearest_node_index(self.node_list, rnd)
            new_node = self.steer(self.node_list[nearest_ind], rnd)

            if self.check_collision(new_node, self.obstacle_list):
                near_indexes = self.find_near_nodes(new_node)
                new_node = self.choose_parent(new_node, near_indexes)
                if new_node:
                    self.node_list.append(new_node)
                    self.rewire(new_node, near_indexes)
                    self.try_goal_path(new_node)

            if animation and i % 5 == 0:
                self.plot_start_goal_arrow()
                self.draw_graph(rnd)

            if (not search_until_max_iter) and new_node:  # check reaching the goal
                last_index = self.search_best_goal_node()
                if last_index:
                    return self.generate_final_course(last_index)

        print("reached max iteration")

        last_index = self.search_best_goal_node()
        if last_index:
            return self.generate_final_course(last_index)
        else:
            print("Cannot find path")

        return None
def test_model_forward_fx(model_name, batch_size):
    
    
    
    if not has_fx_feature_extraction:
        pytest.skip("Can't test FX because Torch >= 1.10 and Torchvision >= 0.11 are required")

    model = create_model(model_name, pretrained=False)
    model.eval()

    input_size = _get_input_size(model=model, target=TARGET_FWD_SIZE)
    if max(input_size) > MAX_FWD_SIZE:
        pytest.skip("Fixed input size model > limit.")

    # This block of code does a bit of juggling to handle any case where there are multiple outputs in train mode
    # So we trace once and look at the graph, and get the indices of the nodes that lead into the original fx output
    # node. Then we use those indices to select from train_nodes returned by torchvision get_graph_node_names
    tracer = NodePathTracer(leaf_modules=list(_leaf_modules), autowrap_functions=list(_autowrap_functions))
    graph = tracer.trace(model)
    graph_nodes = list(reversed(graph.nodes))
    output_node_names = [n.name for n in graph_nodes[0]._input_nodes.keys()]
    graph_node_names = [n.name for n in graph_nodes]
    output_node_indices = [-graph_node_names.index(node_name) for node_name in output_node_names]
    train_nodes, eval_nodes = get_graph_node_names(
        model, tracer_kwargs={'leaf_modules': list(_leaf_modules), 'autowrap_functions': list(_autowrap_functions)})
    eval_return_nodes = [eval_nodes[ix] for ix in output_node_indices]

    fx_model = create_feature_extractor(
        model, train_return_nodes=[train_nodes[-1]], eval_return_nodes=eval_return_nodes,
        tracer_kwargs={'leaf_modules': list(_leaf_modules), 'autowrap_functions': list(_autowrap_functions)})

    inputs = torch.randn((batch_size, *input_size))
    outputs = model(inputs)
    if isinstance(outputs, tuple):
        outputs = torch.cat(outputs)
    fx_outputs = tuple(fx_model(inputs).values())
    if isinstance(fx_outputs, tuple):
        fx_outputs = torch.cat(fx_outputs)

    assert torch.all(fx_outputs == outputs)
    assert outputs.shape[0] == batch_size
    assert not torch.isnan(outputs).any(), 'Output included NaNs'
def _fix_timestamp(the_time):
        
        
        if isinstance(the_time, (float, int)):
            return the_time
        else:
            return from_iso8601_to_timestamp(the_time)
def create_collective_group(self, backend, world_size, rank, group_name):
        
        
        backend = types.Backend(backend)
        if backend == types.Backend.MPI:
            raise NotImplementedError()
        elif backend == types.Backend.NCCL:
            # create the ncclUniqueID
            if rank == 0:
                # availability has been checked before entering here.
                group_uid = nccl_util.get_nccl_unique_id()
                store_name = get_nccl_store_name(group_name)
                # Avoid a potential circular dependency in ray/actor.py
                from ray.util.collective.util import NCCLUniqueIDStore
                store = NCCLUniqueIDStore.options(
                    name=store_name, lifetime="detached").remote(store_name)
                ray.wait([store.set_id.remote(group_uid)])

            logger.debug("creating NCCL group: '{}'".format(group_name))
            g = NCCLGroup(world_size, rank, group_name)
            self._name_group_map[group_name] = g
            self._group_name_map[g] = group_name
        return self._name_group_map[group_name]
def __call__(self, X, Y=None, eval_gradient=False):
        
        
        X = np.atleast_2d(X)
        if Y is None:
            Y = X
        elif eval_gradient:
            raise ValueError("Gradient can only be evaluated when Y is None.")

        K = self.c * np.ones((X.shape[0], Y.shape[0]))
        if eval_gradient:
            if self.c_bounds is not "fixed":
                return K, np.ones((X.shape[0], X.shape[0], 1))
            else:
                return K, np.empty((X.shape[0], X.shape[0], 0))
        else:
            return K
def __call__(self, X, Y=None, eval_gradient=False):
        
        
        X = np.atleast_2d(X)
        if Y is None:
            Y = X
        elif eval_gradient:
            raise ValueError("Gradient can only be evaluated when Y is None.")

        K = self.c * np.ones((X.shape[0], Y.shape[0]))
        if eval_gradient:
            if self.c_bounds is not "fixed":
                return K, self.c * np.ones((X.shape[0], X.shape[0], 1))
            else:
                return K, np.empty((X.shape[0], X.shape[0], 0))
        else:
            return K
def base_order_total(order: "Order", lines: Iterable["OrderLine"]) -> Money:
    
    
    currency = order.currency
    subtotal = base_order_subtotal(order, lines)
    shipping_price = order.base_shipping_price
    order_discounts = order.discounts.all()
    order_discounts_to_update = []
    for order_discount in order_discounts:
        subtotal_before_discount = subtotal
        shipping_price_before_discount = shipping_price
        if order_discount.type == DiscountType.VOUCHER:
            subtotal = apply_discount_to_value(
                value=order_discount.value,
                value_type=order_discount.value_type,
                currency=currency,
                price_to_discount=subtotal,
            )
        elif order_discount.value_type == DiscountValueType.PERCENTAGE:
            subtotal = apply_discount_to_value(
                value=order_discount.value,
                value_type=order_discount.value_type,
                currency=currency,
                price_to_discount=subtotal,
            )
            shipping_price = apply_discount_to_value(
                value=order_discount.value,
                value_type=order_discount.value_type,
                currency=currency,
                price_to_discount=shipping_price,
            )
        else:
            temporary_undiscounted_total = subtotal + shipping_price
            if temporary_undiscounted_total.amount > 0:
                temporary_total = apply_discount_to_value(
                    value=order_discount.value,
                    value_type=order_discount.value_type,
                    currency=currency,
                    price_to_discount=temporary_undiscounted_total,
                )
                total_discount = temporary_undiscounted_total - temporary_total
                subtotal_discount = (
                    subtotal / temporary_undiscounted_total
                ) * total_discount
                shipping_discount = total_discount - subtotal_discount

                subtotal -= subtotal_discount
                shipping_price -= shipping_discount
        shipping_discount_amount = shipping_price_before_discount - shipping_price
        subtotal_discount_amount = subtotal_before_discount - subtotal
        total_discount_amount = shipping_discount_amount + subtotal_discount_amount
        if order_discount.amount != total_discount_amount:
            order_discount.amount = total_discount_amount
            order_discounts_to_update.append(order_discount)
    if order_discounts_to_update:
        OrderDiscount.objects.bulk_update(order_discounts_to_update, ["amount_value"])
    return max(subtotal + shipping_price, zero_money(currency))
def gabor_filter(image, frequency, theta=0, bandwidth=1, sigma_x=None,
                 sigma_y=None, n_stds=3, offset=0, mode='reflect', cval=0):
    
    
    assert_nD(image, 2)
    g = gabor_kernel(frequency, theta, bandwidth, sigma_x, sigma_y, n_stds,
                     offset)

    filtered_real = ndi.convolve(image, np.real(g), mode=mode, cval=cval)
    filtered_imag = ndi.convolve(image, np.imag(g), mode=mode, cval=cval)

    return filtered_real, filtered_imag
def reset(
        self,
        *,
        seed: Optional[Union[int, List[int]]] = None,
        options: Optional[dict] = None,
    ):
        
        
        self.reset_async(seed=seed, options=options)
        return self.reset_wait(seed=seed, options=options)
def verify_whitelist(self, pairlist: List[str], logmethod,
                         keep_invalid: bool = False) -> List[str]:
        
        
        
        if keep_invalid:
            try:
                whitelist = self.expanded_whitelist_keep_invalid
            except ValueError as err:
                logger.error(f"Pair blacklist contains an invalid Wildcard: {err}")
                return []
        else:
            try:
                whitelist = self.expanded_whitelist
            except ValueError as err:
                logger.error(f"Pair blacklist contains an invalid Wildcard: {err}")
                return []
        return whitelist
def hyperopt_filter_epochs(epochs: List, filteroptions: dict) -> List:
    
    
    
    if filteroptions['only_best']:
        epochs = [x for x in epochs if x['is_best']]
    if filteroptions['only_profitable']:
        epochs = [x for x in epochs if x['results_metrics'].get(
            'profit', x['results_metrics'].get('profit_total', 0)) > 0]

    epochs = _hyperopt_filter_epochs_trade_count(epochs, filteroptions)

    epochs = _hyperopt_filter_epochs_duration(epochs, filteroptions)

    epochs = _hyperopt_filter_epochs_profit(epochs, filteroptions)

    epochs = _hyperopt_filter_epochs_objective(epochs, filteroptions)

    logger.info(f"{len(epochs)} " +
                ("best " if filteroptions['only_best'] else "") +
                ("profitable " if filteroptions['only_profitable'] else "") +
                "epochs found.")
    return epochs
def create_estimator_and_inputs(run_config,
                                hparams,
                                pipeline_config_path,
                                config_override=None,
                                train_steps=None,
                                sample_1_of_n_eval_examples=1,
                                sample_1_of_n_eval_on_train_examples=1,
                                model_fn_creator=create_model_fn,
                                use_tpu_estimator=False,
                                use_tpu=False,
                                num_shards=1,
                                params=None,
                                override_eval_num_epochs=True,
                                save_final_config=False,
                                postprocess_on_cpu=False,
                                export_to_tpu=None,
                                **kwargs):
  
  
  get_configs_from_pipeline_file = MODEL_BUILD_UTIL_MAP[
      'get_configs_from_pipeline_file']
  merge_external_params_with_configs = MODEL_BUILD_UTIL_MAP[
      'merge_external_params_with_configs']
  create_pipeline_proto_from_configs = MODEL_BUILD_UTIL_MAP[
      'create_pipeline_proto_from_configs']
  create_train_input_fn = MODEL_BUILD_UTIL_MAP['create_train_input_fn']
  create_eval_input_fn = MODEL_BUILD_UTIL_MAP['create_eval_input_fn']
  create_predict_input_fn = MODEL_BUILD_UTIL_MAP['create_predict_input_fn']
  detection_model_fn_base = MODEL_BUILD_UTIL_MAP['detection_model_fn_base']

  configs = get_configs_from_pipeline_file(
      pipeline_config_path, config_override=config_override)
  kwargs.update({
      'train_steps': train_steps,
      'sample_1_of_n_eval_examples': sample_1_of_n_eval_examples,
      'use_bfloat16': configs['train_config'].use_bfloat16 and use_tpu
  })
  if override_eval_num_epochs:
    kwargs.update({'eval_num_epochs': 1})
    tf.logging.warning(
        'Forced number of epochs for all eval validations to be 1.')
  configs = merge_external_params_with_configs(
      configs, hparams, kwargs_dict=kwargs)
  model_config = configs['model']
  train_config = configs['train_config']
  train_input_config = configs['train_input_config']
  eval_config = configs['eval_config']
  eval_input_configs = configs['eval_input_configs']
  eval_on_train_input_config = copy.deepcopy(train_input_config)
  eval_on_train_input_config.sample_1_of_n_examples = (
      sample_1_of_n_eval_on_train_examples)
  if override_eval_num_epochs and eval_on_train_input_config.num_epochs != 1:
    tf.logging.warning('Expected number of evaluation epochs is 1, but '
                       'instead encountered `eval_on_train_input_config'
                       '.num_epochs` = '
                       '{}. Overwriting `num_epochs` to 1.'.format(
                           eval_on_train_input_config.num_epochs))
    eval_on_train_input_config.num_epochs = 1

  # update train_steps from config but only when non-zero value is provided
  if train_steps is None and train_config.num_steps != 0:
    train_steps = train_config.num_steps

  detection_model_fn = functools.partial(
      detection_model_fn_base, model_config=model_config)

  # Create the input functions for TRAIN/EVAL/PREDICT.
  train_input_fn = create_train_input_fn(
      train_config=train_config,
      train_input_config=train_input_config,
      model_config=model_config)
  eval_input_fns = [
      create_eval_input_fn(
          eval_config=eval_config,
          eval_input_config=eval_input_config,
          model_config=model_config) for eval_input_config in eval_input_configs
  ]
  eval_input_names = [
      eval_input_config.name for eval_input_config in eval_input_configs
  ]
  eval_on_train_input_fn = create_eval_input_fn(
      eval_config=eval_config,
      eval_input_config=eval_on_train_input_config,
      model_config=model_config)
  predict_input_fn = create_predict_input_fn(
      model_config=model_config, predict_input_config=eval_input_configs[0])

  # Read export_to_tpu from hparams if not passed.
  if export_to_tpu is None:
    export_to_tpu = hparams.get('export_to_tpu', False)
  tf.logging.info('create_estimator_and_inputs: use_tpu %s, export_to_tpu %s',
                  use_tpu, export_to_tpu)
  model_fn = model_fn_creator(detection_model_fn, configs, hparams, use_tpu,
                              postprocess_on_cpu)
  if use_tpu_estimator:
    estimator = tf.contrib.tpu.TPUEstimator(
        model_fn=model_fn,
        train_batch_size=train_config.batch_size,
        # For each core, only batch size 1 is supported for eval.
        eval_batch_size=num_shards * 1 if use_tpu else 1,
        use_tpu=use_tpu,
        config=run_config,
        export_to_tpu=export_to_tpu,
        eval_on_tpu=False,  # Eval runs on CPU, so disable eval on TPU
        params=params if params else {})
  else:
    estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)

  # Write the as-run pipeline config to disk.
  if run_config.is_chief and save_final_config:
    pipeline_config_final = create_pipeline_proto_from_configs(configs)
    config_util.save_pipeline_config(pipeline_config_final, estimator.model_dir)

  return dict(
      estimator=estimator,
      train_input_fn=train_input_fn,
      eval_input_fns=eval_input_fns,
      eval_input_names=eval_input_names,
      eval_on_train_input_fn=eval_on_train_input_fn,
      predict_input_fn=predict_input_fn,
      train_steps=train_steps)
def calc_grid_position(self, index, min_position):
        
        
        
        pos = index * self.resolution + min_position
        return pos
def load_query_constructor_chain(
    llm: BaseLanguageModel,
    document_contents: str,
    attribute_info: Sequence[Union[AttributeInfo, dict]],
    examples: Optional[List] = None,
    allowed_comparators: Sequence[Comparator] = tuple(Comparator),
    allowed_operators: Sequence[Operator] = tuple(Operator),
    enable_limit: bool = False,
    schema_prompt: Optional[BasePromptTemplate] = None,
    **kwargs: Any,
) -> LLMChain:
    
    
    prompt = get_query_constructor_prompt(
        document_contents,
        attribute_info,
        examples=examples,
        allowed_comparators=allowed_comparators,
        allowed_operators=allowed_operators,
        enable_limit=enable_limit,
        schema_prompt=schema_prompt,
    )
    allowed_attributes = []
    for ainfo in attribute_info:
        allowed_attributes.append(
            ainfo.name if isinstance(ainfo, AttributeInfo) else ainfo["name"]
        )
    output_parser = StructuredQueryOutputParser.from_components(
        allowed_comparators=allowed_comparators,
        allowed_operators=allowed_operators,
        allowed_attributes=allowed_attributes,
    )
    # For backwards compatibility.
    prompt.output_parser = output_parser
    return LLMChain(llm=llm, prompt=prompt, output_parser=output_parser, **kwargs)
def inspect(
    obj: Any,
    *,
    console: "Console" = None,
    title: str = None,
    help: bool = False,
    methods: bool = False,
    docs: bool = True,
    private: bool = False,
    dunder: bool = False,
    sort: bool = True,
    all: bool = False,
    value: bool = True
):
    
    
    _console = console or get_console()
    from rich._inspect import Inspect

    # Special case for inspect(inspect)
    is_inspect = obj is inspect

    _inspect = Inspect(
        obj,
        title=title,
        help=is_inspect or help,
        methods=is_inspect or methods,
        docs=is_inspect or docs,
        private=private,
        dunder=dunder,
        sort=sort,
        all=all,
        value=value,
    )
    _console.print(_inspect)
def print_json(
    json: Optional[str] = None,
    *,
    data: Any = None,
    indent: int = 2,
    highlight: bool = True,
) -> None:
    
    

    get_console().print_json(json, data=data, indent=indent, highlight=highlight)
def is_on(self):
        
        parent_is_on = super().is_on
        from homematicip.base.enums import SmokeDetectorAlarmType
        if parent_is_on or \
                self._device.powerMainsFailure or \
                self._device.moistureDetected or \
                self._device.waterlevelDetected or \
                self._device.lowBat:
            return True
        if self._device.smokeDetectorAlarmType is not None and \
                self._device.smokeDetectorAlarmType != \
                SmokeDetectorAlarmType.IDLE_OFF:
            return True
        return False
def _validate_input(points):
    
    
    

    if not points:
        raise ValueError(f"Expecting a list of points but got {points}")

    if isinstance(points, set):
        points = list(points)

    try:
        if hasattr(points, "__iter__") and not isinstance(points[0], Point):
            if isinstance(points[0], (list, tuple)):
                points = _construct_points(points)
            else:
                raise ValueError(
                    "Expecting an iterable of type Point, list or tuple. "
                    f"Found objects of type {type(points[0])} instead"
                )
        elif not hasattr(points, "__iter__"):
            raise ValueError(
                f"Expecting an iterable object but got an non-iterable type {points}"
            )
    except TypeError:
        print("Expecting an iterable of type Point, list or tuple.")
        raise

    return points
def test_calling_healthz_method(self):
        
        
        context = FakeGrpcContext()
        request_proto = serve_pb2.HealthzRequest()
        service_method = "/ray.serve.RayServeAPIService/Healthz"
        proxy_request = gRPCProxyRequest(
            request_proto=request_proto,
            context=context,
            service_method=service_method,
            stream=False,
        )
        assert isinstance(proxy_request, ProxyRequest)
        assert proxy_request.is_route_request is False
        assert proxy_request.is_health_request is True
